<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Categories on </title>
		<link>https://ixiaopan.github.io/blog/categories/</link>
		<description>Recent content in Categories </description>
		<generator>Hugo -- gohugo.io</generator>
		
  		<language>en</language>
		
		<managingEditor>Page(/categories) (ixiaopan)</managingEditor>
    	
  		<lastBuildDate>Mon, 14 Jun 2021 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="/blog/categories/" rel="self" type="application/rss+xml" />
		
		<item>
			<title>K-means</title>
			<link>https://ixiaopan.github.io/blog/post/kmeans/</link>
			<pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/kmeans/</guid>
			<description>&lt;p&gt;So far, we&amp;rsquo;ve talked much about supervised learning. Aside from it, there exist many other learning types such as unspervised learning, semi-supervised learning and so on. This post will introduce one of the most widely used unsupervised clustering algorithms — K-means. We will cover the implementation of K-means algorithm, the limitation of this algorithm as well as its applications.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Perceptron</title>
			<link>https://ixiaopan.github.io/blog/post/perceptron/</link>
			<pubDate>Sun, 13 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/perceptron/</guid>
			<description>&lt;p&gt;Perceptron is a traditional classification algorithm and it is the basis of the neural network. Though it&amp;rsquo;s out of date, it plays a historical role in the development of neural network. Knowing how it works will help us lay the foundations for the future study of the neural network.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>K-nearest neighbours</title>
			<link>https://ixiaopan.github.io/blog/post/knn/</link>
			<pubDate>Sat, 12 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/knn/</guid>
			<description>&lt;p&gt;K-Nearest Neighbors(KNN) is a distance-based algorithm in machine learning used for both classification and regression. It&amp;rsquo;s simple and intutive to understand because it doesn&amp;rsquo;t require extra training step and the idea behind it is simple enough — similar points are tends to be close to each other. In this article, we will learn how KNN works.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Information Theory</title>
			<link>https://ixiaopan.github.io/blog/post/information-theory/</link>
			<pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/information-theory/</guid>
			<description>&lt;p&gt;I remembered that I learned a little stuff about information when I was in my undergraduate course. Well, maybe I was wrong. It’s been almost ten years. But when it comes to information theory, I know that we will talk about a person — Shannon. So what’s the information theory? What problems does it help to solve?&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Constrained Optimisation</title>
			<link>https://ixiaopan.github.io/blog/post/constrained-optimisation/</link>
			<pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/constrained-optimisation/</guid>
			<description>&lt;p&gt;When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&amp;rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Probabilistic Model</title>
			<link>https://ixiaopan.github.io/blog/post/probabilistic-model/</link>
			<pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/probabilistic-model/</guid>
			<description>In the previous post Descriptive Statistics, we focused on summary statistics on a particular data set. However, in machine learning, we usually attempt to make inference, i.e. infer the unknown parameters from the given data. For unknown things, we use probability to describe its uncertainty. For inference, we use Bayes&#39; rule to invert it into a forward process.
Probability At the beginning, let&amp;rsquo;s have a quick refresh on probability. Conventionly, we use a capital letter, such as $X$ or $Y$, to represent a random variable.</description>
		</item>
      	
		<item>
			<title>Semantic Web</title>
			<link>https://ixiaopan.github.io/blog/post/semanticweb/</link>
			<pubDate>Sat, 08 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/semanticweb/</guid>
			<description>&lt;p&gt;In semester 2, I took a module called &lt;strong&gt;Semantic Web Technologies&lt;/strong&gt;. There are some reasons why I choose this module. At first glance, the name itself sounds not appealing. Actually, it does. But it&amp;rsquo;s still on my shortlist because I&amp;rsquo;ve been working on the web for many years and I wondered what the semantic web was.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Singular Value Decomposition</title>
			<link>https://ixiaopan.github.io/blog/post/svd/</link>
			<pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/svd/</guid>
			<description>&lt;p&gt;Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into the multiplication of three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>An E2E Project - EDA</title>
			<link>https://ixiaopan.github.io/blog/post/end2end-project-01/</link>
			<pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/end2end-project-01/</guid>
			<description>&lt;p&gt;So far, we have discussed many algorithms, such as linear regression and ensemble methods. It&amp;rsquo;s time to kick off a project from scratch to learn how a real machine learning project works.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Ensemble Methods</title>
			<link>https://ixiaopan.github.io/blog/post/ensemble-methods/</link>
			<pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/ensemble-methods/</guid>
			<description>&lt;p&gt;Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.&lt;/p&gt;</description>
		</item>
      	
	</channel>
</rss>
