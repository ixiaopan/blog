<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Utilities on </title>
		<link>https://ixiaopan.github.io/blog/categories/utilities/</link>
		<description>Recent content in Utilities </description>
		<generator>Hugo -- gohugo.io</generator>
		
  		<language>en</language>
		
		<managingEditor>Page(/categories/utilities) (ixiaopan)</managingEditor>
    	
  		<lastBuildDate>Thu, 28 Oct 2021 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="/blog/categories/utilities/" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Deep Learning - Weight Initialization</title>
			<link>https://ixiaopan.github.io/blog/post/dl-04-initilisation/</link>
			<pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-04-initilisation/</guid>
			<description>&lt;p&gt;The goal of machine learning is to find the best parameters. To train the model, we must start from some values. So, what do the initial parameters look like? Can we set all of them to the same value? Or we assign random values to the weights? In this post, we will talk about another important aspect of learning — weight initialization.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Deep Learning - Regularization</title>
			<link>https://ixiaopan.github.io/blog/post/dl-03-regularization/</link>
			<pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-03-regularization/</guid>
			<description>L1 L2 Dropout  By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections.
 Data Augmentation Early Stopping Refer  Dropout: a simple way to prevent neural networks from overfitting Regularization of neural networks using dropconnect Simplified Math Behind Dropout  </description>
		</item>
      	
		<item>
			<title>NLP - Literature Review on NER</title>
			<link>https://ixiaopan.github.io/blog/post/nlp-ner-lr/</link>
			<pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/nlp-ner-lr/</guid>
			<description>&lt;p&gt;Named entity recognition (NER) is a classical task in NLP, which aims to identify meaningful chunks of text and classify them into appropriate entity types, such as PERSON or LOCATION. For instance, &amp;ldquo;Tom&amp;rdquo; is classified as &amp;ldquo;PERSON&amp;rdquo; while &amp;ldquo;England&amp;rdquo; is labelled as &amp;ldquo;LOCATION&amp;rdquo; in the sentence &amp;ldquo;Tom comes from England&amp;rdquo;. NER serves as the foundation for a range of downstream tasks such as information extraction. In practical applications, you may notice that some websites will autofill the application form from your resume. In this series of posts, we will explain NER in detail, from models, datasets to domain adaptation. First, let&amp;rsquo;s do a brief literative review on NER.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Optimization</title>
			<link>https://ixiaopan.github.io/blog/post/optimization/</link>
			<pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/optimization/</guid>
			<description>&lt;p&gt;We&amp;rsquo;ve talked about many ML algorithms and DL architectures so far, but how to find the optimal parameters? Mathematically, the process of minimizing the objective functions is called optimization, but it&amp;rsquo;s a bit different in DL — the global minimum point does not always achieve the best generalization performance. After all, we are minimizing the training error. Besides, the loss function typically is very complex, and there is no analytical solution to it. In this case, we have only one choice — optimization algorithms.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Recommender Systems</title>
			<link>https://ixiaopan.github.io/blog/post/recommendation/</link>
			<pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/recommendation/</guid>
			<description>&lt;p&gt;Recommender system is one of the most popular studying fields in machine learning due to its wide application in our daily life. When you shop online, such as Amazon, you can see similar items just below the item you are looking at. The goal of a recommender system is to make recommendations that fit the user&amp;rsquo;s taste. In this post, we will go through the very basic concepts and several classcial techniques in the field of recommender systems.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>REMOVING ANNOYING SPLASH ADs!!!</title>
			<link>https://ixiaopan.github.io/blog/post/remove-splash/</link>
			<pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/remove-splash/</guid>
			<description>What and Why I don&amp;rsquo;t know when it happened, but without a doubt, you will see a statistic or dynamic image on your screen when you launch some APPs nowadays (especially if you are somewhere on the Earth). The picture looks like the figure below with a small button to enable people to skip ads. Generally, the showing time of ads lasts at least 3 seconds. The most disgusting thing is that there is no upper bound for this, so you have to suffer from this unless you don&amp;rsquo;t use any app at all.</description>
		</item>
      	
		<item>
			<title>Metrics for classification</title>
			<link>https://ixiaopan.github.io/blog/post/metrics-classification/</link>
			<pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/metrics-classification/</guid>
			<description>&lt;p&gt;In linear regression, we choose $R^2$ as our metrics to measure how good our algorithms are. But how about classification? In this post, we are going to talk about several commonly used metrics for classification.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Deep Learning - RNN</title>
			<link>https://ixiaopan.github.io/blog/post/dl-06-rnn/</link>
			<pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-06-rnn/</guid>
			<description>&lt;p&gt;When I first learned RNN, I found lots of articles about it. That&amp;rsquo;s good news, but most of them focus on the design of the architecture.The network itself is not difficult to understand, the real problem is why it works. What&amp;rsquo;s the intuition behind it? Why do we need &amp;ldquo;recurrence&amp;rdquo;? Unfortunately, few articles explain it clearly. Luckily, the book &lt;a href=&#34;https://www.amazon.co.uk/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618&#34;&gt;Deep Learning&lt;/a&gt; gives the answers.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Deep Learning - CNN</title>
			<link>https://ixiaopan.github.io/blog/post/dl-05-cnn/</link>
			<pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-05-cnn/</guid>
			<description>&lt;p&gt;Convolutional Neural Networks (CNN) are widely used for image classification, object detection and other tasks related to images or videos in the field of computer vision. Plenty of architectures based on convolution operation have been proposed in recent years, such as AlexNet and ResNet. So, how do CNNs work?  Why not use MLP? In this post, we will go through the fundamentals of CNN and several ConvNets to develop a big picture of CNN.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Model Selection</title>
			<link>https://ixiaopan.github.io/blog/post/model-selection/</link>
			<pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/model-selection/</guid>
			<description>Model Complexity What is model complexity? Perhaps it mainly refers to the number of parameters that the model has. However, it does not mean that a model is complex if it has more parameters, although it is often the case. Generally, a simple model could result in underfitting, while a complex model could lead to overfitting. So, what factors affect the complexity of a model? Well, it is hard to say.</description>
		</item>
      	
	</channel>
</rss>
