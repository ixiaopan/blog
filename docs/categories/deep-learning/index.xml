<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Deep Learning on </title>
		<link>https://ixiaopan.github.io/blog/categories/deep-learning/</link>
		<description>Recent content in Deep Learning </description>
		<generator>Hugo -- gohugo.io</generator>
		
  		<language>en</language>
		
		<managingEditor>Page(/categories/deep-learning) (ixiaopan)</managingEditor>
    	
  		<lastBuildDate>Wed, 24 Nov 2021 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="/blog/categories/deep-learning/" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Tools for daily work</title>
			<link>https://ixiaopan.github.io/blog/post/tools/</link>
			<pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/tools/</guid>
			<description>&lt;p&gt;Sharp tools make good work.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Video Format Intro</title>
			<link>https://ixiaopan.github.io/blog/post/media/</link>
			<pubDate>Mon, 13 Dec 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/media/</guid>
			<description>Codec   Figure 1: Process of video creation.  Codec, short for compressor-decompressor, is an algorithm designed for compressing and decompressing video and audio data. Simply put, it&amp;rsquo;s an encoding and decoding algorithm.
Why encoding?
 raw data can be very large and noisy encoding makes it easy to transmit and storage data  Why decoding?
 preview or playback  Video Codec  H.264/AVC (MPEG-4 part 10/Advanced Video Coding) is the most widely used video codec on the market VP8, free and open source codec owned by Google Ogg Theora, typically used with OGG container H.</description>
		</item>
      	
		<item>
			<title>Deep Learning - Attention</title>
			<link>https://ixiaopan.github.io/blog/post/dl-09-attention/</link>
			<pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-09-attention/</guid>
			<description></description>
		</item>
      	
		<item>
			<title>Deep Learning - Seq2seq</title>
			<link>https://ixiaopan.github.io/blog/post/dl-07-seq2seq/</link>
			<pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-07-seq2seq/</guid>
			<description></description>
		</item>
      	
		<item>
			<title>Deep Learning - Weight Initialization</title>
			<link>https://ixiaopan.github.io/blog/post/dl-04-initilisation/</link>
			<pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-04-initilisation/</guid>
			<description>&lt;p&gt;The goal of machine learning is to find the best parameters. To train the model, we must start from some values. So, what do the initial parameters look like? Can we set all of them to the same value? Or we assign random values to the weights? In this post, we will talk about another important aspect of learning — weight initialization.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Deep Learning - Regularization</title>
			<link>https://ixiaopan.github.io/blog/post/dl-03-regularization/</link>
			<pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/dl-03-regularization/</guid>
			<description>&lt;p&gt;In the previous post, we introduced how to initialize weights. However, weights can be very large or small (great variance in weights). For larger weights, a tiny change in data will lead to large variance. For smaller weights, it has little influence on the model, which can be totally discarded.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>NLP - CRF</title>
			<link>https://ixiaopan.github.io/blog/post/nlp-crf/</link>
			<pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/nlp-crf/</guid>
			<description>Direced Graphical Model In a direction graph, if there is a link starting from a to b, we say a is the parent of b and b is the child of a. The joint distribution over all variables $\bold x={x_1, x_2, &amp;hellip;, x_N}$ in this graph is described as $$ p(\bold x) = \prod_i^N p(x_i|pa_i) $$ where $N$ is the number of nodes, and $pa_i$ indicates the set of parents of $x_i$.</description>
		</item>
      	
		<item>
			<title>NLP - Literature Review on NER</title>
			<link>https://ixiaopan.github.io/blog/post/nlp-ner-lr/</link>
			<pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/nlp-ner-lr/</guid>
			<description>&lt;p&gt;Named entity recognition (NER) is a classical task in NLP, which aims to identify meaningful chunks of text and classify them into appropriate entity types, such as PERSON or LOCATION. For instance, &amp;ldquo;Tom&amp;rdquo; is classified as &amp;ldquo;PERSON&amp;rdquo; while &amp;ldquo;England&amp;rdquo; is labelled as &amp;ldquo;LOCATION&amp;rdquo; in the sentence &amp;ldquo;Tom comes from England&amp;rdquo;. NER serves as the foundation for a range of downstream tasks such as information extraction. In practical applications, you may notice that some websites will autofill the application form from your resume. In this series of posts, we will explain NER in detail, from models, datasets to domain adaptation. First, let&amp;rsquo;s do a brief literative review on NER.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Optimization</title>
			<link>https://ixiaopan.github.io/blog/post/optimization/</link>
			<pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/optimization/</guid>
			<description>&lt;p&gt;We&amp;rsquo;ve talked about many ML algorithms and DL architectures so far, but how to find the optimal parameters? Mathematically, the process of minimizing the objective functions is called optimization, but it&amp;rsquo;s a bit different in DL — the global minimum point does not always achieve the best generalization performance. After all, we are minimizing the training error. Besides, the loss function typically is very complex, and there is no analytical solution to it. In this case, we have only one choice — optimization algorithms.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Recommender Systems</title>
			<link>https://ixiaopan.github.io/blog/post/recommendation/</link>
			<pubDate>Sun, 07 Nov 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/recommendation/</guid>
			<description>&lt;p&gt;Recommender system is one of the most popular studying fields in machine learning due to its wide application in our daily life. When you shop online, such as Amazon, you can see similar items just below the item you are looking at. The goal of a recommender system is to make recommendations that fit the user&amp;rsquo;s taste. In this post, we will go through the very basic concepts and several classcial techniques in the field of recommender systems.&lt;/p&gt;</description>
		</item>
      	
	</channel>
</rss>
