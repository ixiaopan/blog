<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Machine Learning on </title>
		<link>https://ixiaopan.github.io/blog/categories/machine-learning/</link>
		<description>Recent content in Machine Learning </description>
		<generator>Hugo -- gohugo.io</generator>
		
  		<language>en</language>
		
		<managingEditor>Page(/categories/machine-learning) (ixiaopan)</managingEditor>
    	
  		<lastBuildDate>Wed, 16 Jun 2021 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="/blog/categories/machine-learning/" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Naive Bayes Classification</title>
			<link>https://ixiaopan.github.io/blog/post/naive-bayes/</link>
			<pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/naive-bayes/</guid>
			<description>&lt;p&gt;A Naive Bayes classifier is a probabilistic model that is used for classification, which is based on the Bayes&#39; Theorem.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Support Vector Machine</title>
			<link>https://ixiaopan.github.io/blog/post/svm/</link>
			<pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/svm/</guid>
			<description>Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,
$$ y_i d_i \ge \Delta $$
where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin.</description>
		</item>
      	
		<item>
			<title>K-means</title>
			<link>https://ixiaopan.github.io/blog/post/kmeans/</link>
			<pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/kmeans/</guid>
			<description>&lt;p&gt;So far, we&amp;rsquo;ve talked much about supervised learning. Aside from it, there exist many other learning types such as unspervised learning, semi-supervised learning and so on. This post will introduce one of the most widely used unsupervised clustering algorithms — K-means. We will cover the implementation of K-means algorithm, the limitation of this algorithm as well as its applications.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Logistic Regression</title>
			<link>https://ixiaopan.github.io/blog/post/logistic-regression/</link>
			<pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/logistic-regression/</guid>
			<description>&lt;p&gt;We&amp;rsquo;ve known that linear regression can be used to predict a continuous value, but sometimes the target variable might be categorical, i.e. whether tomorrow is sunny or someone has a cancer. Can we still use linear regression to solve this classification problem? The answer is Yes and the algorithm that we will introduce in this article is known as logistic regression. Well, we can also try Perceptron since it&amp;rsquo;s also a linear classifier. PS: Don&amp;rsquo;t mix it up with linear regression. Logistic regression is a classification algorithm.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Perceptron</title>
			<link>https://ixiaopan.github.io/blog/post/perceptron/</link>
			<pubDate>Sun, 13 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/perceptron/</guid>
			<description>&lt;p&gt;Perceptron is a traditional classification algorithm and it is the basis of the neural network. Though it&amp;rsquo;s out of date, it plays a historical role in the development of neural network. Knowing how it works will help us lay the foundations for the future study of the neural network.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>K-nearest neighbours</title>
			<link>https://ixiaopan.github.io/blog/post/knn/</link>
			<pubDate>Sat, 12 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/knn/</guid>
			<description>&lt;p&gt;K-Nearest Neighbors(KNN) is a distance-based algorithm in machine learning used for both classification and regression. It&amp;rsquo;s simple and intutive to understand because it doesn&amp;rsquo;t require extra training step and the idea behind it is simple enough — similar points are tends to be close to each other. In this article, we will learn how KNN works.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Information Theory</title>
			<link>https://ixiaopan.github.io/blog/post/information-theory/</link>
			<pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/information-theory/</guid>
			<description>&lt;p&gt;I remembered that I learned a little stuff about information when I was in my undergraduate course. Well, maybe I was wrong. It’s been almost ten years. But when it comes to information theory, I know that we will talk about a person — Shannon. So what’s the information theory? What problems does it help to solve?&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Constrained Optimisation</title>
			<link>https://ixiaopan.github.io/blog/post/constrained-optimisation/</link>
			<pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/constrained-optimisation/</guid>
			<description>&lt;p&gt;When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&amp;rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Probabilistic Model</title>
			<link>https://ixiaopan.github.io/blog/post/probabilistic-model/</link>
			<pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/probabilistic-model/</guid>
			<description>In the previous post Descriptive Statistics, we focused on summary statistics on a particular data set. However, in machine learning, we usually attempt to make inference, i.e. infer the unknown parameters from the given data. For unknown things, we use probability to describe its uncertainty. For inference, we use Bayes&#39; rule to invert it into a forward process.
Probability At the beginning, let&amp;rsquo;s have a quick refresh on probability. Conventionly, we use a capital letter, such as $X$ or $Y$, to represent a random variable.</description>
		</item>
      	
		<item>
			<title>Semantic Web</title>
			<link>https://ixiaopan.github.io/blog/post/semanticweb/</link>
			<pubDate>Sat, 08 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/semanticweb/</guid>
			<description>&lt;p&gt;In semester 2, I took a module called &lt;strong&gt;Semantic Web Technologies&lt;/strong&gt;. There are some reasons why I choose this module. At first glance, the name itself sounds not appealing. Actually, it does. But it&amp;rsquo;s still on my shortlist because I&amp;rsquo;ve been working on the web for many years and I wondered what the semantic web was.&lt;/p&gt;</description>
		</item>
      	
	</channel>
</rss>
