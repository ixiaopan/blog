[
    
    
    
        
            {
                "id": 0,
                "href": "https://ixiaopan.github.io/blog/about/",
                "title": "",
                "section": "",
                "date" : "0001.01.01",
                "body": "PROFILE MSc Data Science graduate with 6 years of work experience in frontend development. Solid programming experience and problem-solving skills gained through many large-scale projects in Baidu and Ant Group. Also, a Ukulele Fan.\n .timeline p { margin: 0; } .timeline .time { margin-right: 28px; color: rgba(0,0,0,0.6); min-width: 133px; } .timeline { display: flex; justify-content: flex-start; align-items: flex-start; } .timeline+.timeline { margin-top: 8px; } li { margin-left: -20px }  PROFESSIONAL SKILLS  Strong understanding of Machine Learning and Deep Learning Excellent hands-on experience with Python, Scikit-Learn, and PyTorch Proficient in JavaScript, Vue/React, CSS3; able to build a web application from frontend to backend independently and effectively Extensive practical frontend experience across PC, mobile web, hybrid app and mini-program Solid hands-on experience in no-linear editing for video Rich experience in the agile software development workflow with Git and CI/CD Good at data visualization and UI design  EMPLOYMENT HISTORY  12/2021 - present\nFront-end Technical Specialist, startup, Hangzhou\n 07/2017 - 04/2020\nFront-end Engineer II, Ant Group, Hangzhou\n 07/2014 - 07/2017\nFront-end Engineer, Baidu, Beijing \u0026 Shanghai\n  EDUCATION 09/2020 - 11/2021\nMSc Data Science, University of Southampton\n 09/2010 - 07/2014\nBSc Computer Science and Technology, University of Electronic Science and Technology of China (UESTC)   "
            }
    
        ,
            {
                "id": 1,
                "href": "https://ixiaopan.github.io/blog/post/misc-october/",
                "title": "Some thoughts after going back to work",
                "section": "post",
                "date" : "2022.10.03",
                "body": "What am I doing my old job\nWhy 也曾试着转行，但是门槛要求高（论文、实习、比赛等），钱也不如继续做老本行来的多。曾经我也以为我不会为五斗米折腰，然而真正到做这个选择的时刻，才发现我只不过是个普通不过的普通人，在现实面前，梦想什么的是如此不着边际。\n我听过不少人对我说过可惜，费这么大劲读书，回来没有做这一行，挺可惜的。不可惜是假的，但是至少到现在，我没后悔，就足矣。即使时光倒流，我还是会坚持出去读书的想法，因为做决定的时候，我就知道会有什么样的结果。相反，不去做，对于我来说，才是最大的遗憾。\n继续做老本行，是我的退路，我也并不抗拒，因为我还保留对这份工作的热情，只不过不会像大学刚毕业那样特别充满干劲了，毕竟我也不是刚入行的菜鸟了。\n那还会不会考虑继续转行呢？会的，因为我内心深处从来没有放弃过这个想法，毕竟路还很长。\nHow 即使是选择继续做老本行，也要有所特别才是，我当时所想的无外乎这两个\n 大数据平台 图像、音视频  选定了方向，还有公司偏好，大公司不在名单之内，没有啥就是厌倦了，再说gap2年也很难进去，即使进去了钱也没多少而且也很累。 那不看大公司，就只看小公司了，实际上我7月份在英国面试的时候，就是一直面小公司，应该说国外有很多这样的小作坊，Linkedin 上甚至有只有1、2个人的团队找我过去。我一直相信冥冥之中有这种设定，我必须去个小作坊，当然这也有主观的自我心理建设的影响，现在回过头来看，这确实不失为一个好时机去见识一下传说中的小公司。\n国内对小公司、创业公司都有刻板印象和偏见，当我说要去创业公司以及我在面试别人的时候，听到的最多的话就是 —— 小公司不稳定。这是事实，不可否认，进来之后我多少也会有这种忧患认识。这个缺点对于当时的我(也是现在的我)来说，根本不是问题。因为我的想法是，我也是一个不稳定的人，我那会什么都没有(物质上)，不在乎短时间内再次什么都没有，有种光脚的不怕穿鞋的感觉。 再说，此时不去更待何时？大多数人都是图稳定的，就国内的就业环境而言，年龄成为劣势之后，更不敢轻易换工作，还是换到一个不稳定的环境。\n究其根本，是我的人生观左右了这个选择。正如大家耳熟能详的那句话所言：我来到这世上，是为了看太阳怎么升起，水怎么流，经历有趣的事，遇见难忘的人，倘能如我所愿，我的一生就算成功。所以，人生苦短，至少现在你还有自由去决定自己短暂人生的某个岔路，没有被世俗所裹挟，真正的做到了做自己，听从你的内心，为什么还要想这么有的没的呢，在意别人的看法呢？\n在哪里，做什么都不是很重要，重要的是，这是你想要的吗？\nWhat have I done Tech 这大半年，都做了什么？一句话，做各种编辑器。。。\n 整了一个组件库 整了个大文件批量上传 开发 NLE 编辑器，扒了不少源码 pixijs/vidar/wasm/web audio/web codec 开发 PSD 编辑器 开发 图片视频尺寸变换 编辑器  最大的感受，资源要优化管理，合理复用；字体预览最难整；这些技术点我都会慢慢整理出来。\nFuture 谁知道以后会是什么样呢？？我只知道，所有的过去和现在造就了以后，现在的你就是以后的你。\n今天上午看了一本书，里面一句话让我受益匪浅，也是我一直努力的方向，在此分享一下\n 做教学和研究的人，要有由浅入深的学习和研究能力，还要有由深返浅的表述和应用能力\n 前者 『由浅入深的学习和研究能力』，要求我们\n 对自己的领域，有深入的专业能力，理解大多人看不懂的东西，提高深度 对不熟悉的领域，也能快速领悟其要领，拓展广度  后者，『由深返浅的表述和应用能力』\n 身为工程师，要有变现技术的能力 『表述』要求我们可以用通俗的言语把技术难点表达出来，因为『凡是你说不清的，就说明你并未真正的理解』，这一点很多人欠缺  这也是我想写博客的原因，现在这个信息过载的社会，人人们很难静下来读完一页书，写下一篇文章，长久如此，就会懒得思考。日常工作更多是重复的劳动，很多时候，急匆匆的应付完工作，实际上并未真正的理解。写作会让人变得安静，集中精力去消化那些似懂非懂的知识点，进而形成体系。\n"
            }
    
        ,
            {
                "id": 2,
                "href": "https://ixiaopan.github.io/blog/post/fe-nle-layer/",
                "title": "[draft] NLE - Layer",
                "section": "post",
                "date" : "2022.08.07",
                "body": "A brief overview on Non-Linear Video Editor.\nBackground"
            }
    
        ,
            {
                "id": 3,
                "href": "https://ixiaopan.github.io/blog/post/fe-nle-web-audio/",
                "title": "[draft] NLE - WebAudio",
                "section": "post",
                "date" : "2022.08.07",
                "body": "WebAudio\n"
            }
    
        ,
            {
                "id": 4,
                "href": "https://ixiaopan.github.io/blog/post/fe-nle/",
                "title": "[draft] Non-Linear Video Editor(NLE) 101",
                "section": "post",
                "date" : "2022.08.07",
                "body": "A brief overview on Non-Linear Video Editor.\nBackground"
            }
    
        ,
            {
                "id": 5,
                "href": "https://ixiaopan.github.io/blog/post/fe-upload/",
                "title": "[draft] Upload Manager",
                "section": "post",
                "date" : "2022.08.07",
                "body": "Background"
            }
    
        ,
            {
                "id": 6,
                "href": "https://ixiaopan.github.io/blog/post/fe-pooling/",
                "title": "Pooling",
                "section": "post",
                "date" : "2022.08.06",
                "body": "Attention: it\u0026rsquo;s NOT the \u0026lsquo;pooling\u0026rsquo; in Deep Learning; It\u0026rsquo;s a common frontend technique to build a real-time(though it\u0026rsquo;s not) connection with server.\nPooling Long Pooling WebSocket Well, I have no practical experience on it, so let\u0026rsquo;s skip it.\nReference  https://javascript.info/long-polling "
            }
    
        ,
            {
                "id": 7,
                "href": "https://ixiaopan.github.io/blog/post/fe-video-power/",
                "title": "Video autoplay not working when low power mode is on (iphone)",
                "section": "post",
                "date" : "2022.08.06",
                "body": "An interesting bug about the HTML5 video.\nLast week, I inserted a video element in a page. As you can expect, the video is muted and autoplay when page is loaded.\n\u0026lt;video src=\u0026#34;/trailer.mp4\u0026#34; loop muted autoplay playsinline /\u0026gt; Unfortunately, it didn\u0026rsquo;t play when I opened it using safari. Technically speaking, there is nothing wrong with the above code snippet. So, what\u0026rsquo;s the problem?\nThanks to StackOverflow, I found a clue.\n I had same issue with apple devices like iPhone and iPad, I turned off the low power mode\u0026hellip;.\n The reason is that I turned on the low power mode, emmm\u0026hellip;\nRefer  https://stackoverflow.com/questions/20347352/html5-video-tag-not-working-in-safari-iphone-and-ipad "
            }
    
        ,
            {
                "id": 8,
                "href": "https://ixiaopan.github.io/blog/post/fe-axios/",
                "title": "[Draft] - Axios",
                "section": "post",
                "date" : "2022.06.04",
                "body": "Learn the modern http library - Axios.\nChain Transformation Cancellation Data Type"
            }
    
        ,
            {
                "id": 9,
                "href": "https://ixiaopan.github.io/blog/post/fe-text-label/",
                "title": "[draft] A Text Labelling Component",
                "section": "post",
                "date" : "2022.06.03",
                "body": "Develop a vue-based Text Labelling Component.\nBackground"
            }
    
        ,
            {
                "id": 10,
                "href": "https://ixiaopan.github.io/blog/post/fe-message/",
                "title": "A message component",
                "section": "post",
                "date" : "2022.06.03",
                "body": "Develop a message component using vue.\nBackground message 不同于一般的组件，其生命周期非常短暂，只有在需要的时候，才会出现，然后快速被销毁，这就要求我们要动态的创建一条 message。不管是 jQuery 还是 vue/react，调用方法都是类似的\n// jQuery $.toast({ text: \u0026#39;hello world\u0026#39; duration: 5000 }) $.loading({ text: \u0026#39;loading...\u0026#39; }) // vue const msg = inject(\u0026#39;message\u0026#39;) msg.info({ message: \u0026#39;hello world\u0026#39; duration: 5000 }) msg.loading({ message: \u0026#39;loading...\u0026#39; duration: 5000 }) 通过 jQuery 我们知道，调用 $.toast() 实际上是动态的创建了一个含有 text 的 html tag 然后插入到 body 最后，\nconst div = document.createElement(\u0026#39;div\u0026#39;) div.textContext = text document.body.appendChild(div) const timer = setTimeout(() =\u0026gt; { document.body.removeChild(div) clearTimeout(timer) }, duration) 类似的，vue 的实现也是如此，只不过我们是通过 VNode 生成真实的 DOM。\nImplementation render \u0026lt;template\u0026gt; \u0026lt;div\u0026gt; {{ message }} \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; 以上定义 HTML 模板，接下来 \u0026lsquo;编译\u0026rsquo; 为 VNode，渲染为 HTML 插入到 body\nimport { createVNode, render } from \u0026#39;vue\u0026#39; import MessageConstructor from \u0026#39;./message.vue\u0026#39; const message = (options) =\u0026gt; { const props = { zIndex: options.zIndex, offset: verticalOffset, ...options, } // create a VNode  const vm = createVNode( MessageConstructor, props, null, ) const container = document.createElement(\u0026#39;div\u0026#39;) // remove it from the document  vm.props.onDestroy = () =\u0026gt; { render(null, container) container = null } // transition starts  vm.props.onBeforeLeave = () =\u0026gt; {} // generate a html fragment  render(vm, container) // insert into the document  document.body.appendChild(container.firstElementChild) } timer vue 中我们在 onMounted 开启计时器，由于使用了 transition 实现动画效果，所以在动画结束即 after-leave 触发 onDestroy 移除 DOM\n\u0026lt;template\u0026gt; \u0026lt;transition @before-leave=\u0026#34;onBeforeLeave\u0026#34; @after-leave=\u0026#34;onDestroy\u0026#34;\u0026gt; \u0026lt;div v-show=\u0026#34;visible\u0026#34;\u0026gt; {{ message }} \u0026lt;/div\u0026gt; \u0026lt;/transition\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script lang=\u0026#34;ts\u0026#34;\u0026gt; function startTimer() { if (props.duration \u0026gt; 0) { timerId = setTimeout(() =\u0026gt; { if (visible.value) { // trigger transition  visible.value = false clearTimeout(timerId) } }, props.duration) } } onMounted(() =\u0026gt; { startTimer() }) \u0026lt;/script\u0026gt; Multiple Instances 现在这段代码只支持一个 message 的创建，如果需要同时显示多个 message，这就要全局维护一个队列\n  创建的时候，添加到数组\n  移除的时候，从数组中移除\n  有一个 UI 要注意的细节，当移除第 index 个 message，它后面的 message 的 offset 要更新。一般 message 都是出现在上方，也就是说，后面的 message 的 top 都要减少，向上偏移。\n这个操作可以放在 before-leave 完成，这样向上移动的效果和 transition 的效果就是同时的，体验上会更连贯。\n"
            }
    
        ,
            {
                "id": 11,
                "href": "https://ixiaopan.github.io/blog/post/fe-cascader/",
                "title": "A Cascader Component",
                "section": "post",
                "date" : "2022.05.29",
                "body": "实现一个业务定制的级联选择器\nBackground 常见的级联选择器，比如省市区选择器，实现起来比较简单，而且父子之间没有过多的依赖关系。然而由于我们业务的特殊性，这些选择器根本无法满足需求，不得已只能从0到1实现。先梳理一下要实现的功能\n  支持单选、多选\n 单选即只能选择一个子项，又分为两种情况  case 1, 只能选择叶子节点 case 2, 可以选到任一个层级   多选 case 3  可以选到任一个层级，可以选择多个项 选择了通用节点，所有其他节点都不能被选择，即互斥关系 选择了父节点，其所有孩子节点都不能被选择，但是依然可以访问其路径      支持筛选\n 只支持叶子节点筛选，适用于 case 1 (单选且只能选择到叶子节点) 所有层级都支持筛选，使用 case2 \u0026amp; case 3    效果如下，\n单选 多选 实现 数据格式 显然，这是个树结构\n[ { id: \u0026#34;L1\u0026#34;, label: \u0026#34;level 1\u0026#34;, children: [ { id: \u0026#34;L2\u0026#34;, label: \u0026#34;level 1-2\u0026#34; } ] } ] 从 UI 来看，切换节点，需要在下一级菜单显示对应的 children。如果每次切换，都要遍历一下树，太慢了效率不高，比较好的方式是在一开始就计算好对应的数据，切换的时候从 map 里取出来就好。\n这里借鉴了文件目录的思想\n  根目录是 ROOT\n  查看包含的子文件列表 $ ls ROOT\n  文件所在的目录 $ cwd\n  采用这个思想，那么每个 node 的数据结构如下\ntype INode = { id: // the key  childrenCount: // the number of children  cwd: // the current working directory  parent: //  depth: // the depth of this node } 接下来，全局维护一个 map 对象，其 key 即是 cwd,\n{ \u0026#39;ROOT\u0026#39;: INode[], \u0026#39;ROOT/L1\u0026#39;: INode[], \u0026#39;ROOT/L1/L1-2\u0026#39;: INode[], } 最后是深度遍历\ntreeData = flattenTree(tree, \u0026#39;ROOT\u0026#39;, {}, null) function flattenTree(list, footpath, treeMap, parent) { if (!treeMap[footpath]) { treeMap[footpath] = [] } list.forEach(t =\u0026gt; { t.id = t.id t.childrenCount = t.children?.length t.depth = splitFootPath(footpath).length t.parent = parent t.cwd = concatFootPath(footpath, t.id) treeMap[footpath].push(omit(t, [\u0026#39;children\u0026#39;])) flattenTree(t.children, t.cwd, treeMap, t) }) } 层级联动 接下来就好办了，监听节点的 click 事件，这里要注意\n 可以选择中间层的，点击了 radio/checkbox 才会被选中；点击文本，只是展示下一层几  是否只能选择叶子节点，我们借鉴 checkbox 的设计，新增属性 intermediate\n  如果是叶子节点，直接选择完成\n  这里通过 classList 判断是否点击了 radio/checkbox，if so, 直接选择完成\n  switchLevel(record, true) 通过添加标识(第2个参数 true) 标记 选择完成\n  function rowClick(record, e) { if (props.intermediate) { // 如果是叶子节点，点自己也是可以的  // 选择了 radio/checkbox，新增 `true` 表示选择结束  if ( !record.childrenCount || (e.target \u0026amp;\u0026amp; e.target.classList.contains(\u0026#39;wxp-cascader-list-option-radio\u0026#39;)) ) { e.stopPropagation() radioCheckedNode = record switchLevel(record, true) } // 点击的是文本，展开下一层  else if (record.childrenCount) { radioCheckedNode = null switchLevel(record) } } else { switchLevel(record) } emit(\u0026#39;click\u0026#39;, record) } 层级的切换，有几个情况\n  向更深的层级展开\n  在本层级不同兄弟节点之间\n  回到父层级\n  这也很好办，结合当前路径 currentPath ，所选节点的信息比如 cwd, depth，就可以知道是是哪个情况\nfunction switchLevel() { // 向更深的层级展开  if (currentPath.value.length == record.depth) { currentPath.value = currentPath.value.concat(record.cwd) } else { // 切换到上一级，或者本层级  const temp = currentPath.value.slice(0, record.depth) currentPath.value = temp.concat(record.cwd) } } 最后就是渲染数据了，监听 currentPath 重新计算 currentList 即可\n// currentPath: [\u0026#39;ROOT\u0026#39;, \u0026#39;L1\u0026#39;, \u0026#39;L1-2\u0026#39;] nextList = currentPath?.map((fp, idx) =\u0026gt; { return treeMap[fp]?.map(ot =\u0026gt; { const t = { ...ot } // copy  // UI样式相关的逻辑  const selecting = t.cwd == currentPath.value[idx+1] const selectingEnded = soleEnded.value \u0026amp;\u0026amp; selecting const selectingEndedLeaf = props.intermediate ? t.cwd == radioCheckedNode?.cwd : !t.childrenCount \u0026amp;\u0026amp; selectingEnded return { ...t, selecting, selectingEnded, selectingEndedLeaf, } }) }) if (props.intermediate \u0026amp;\u0026amp; radioCheckedNode) { currentList.value = nextList.slice(0, radioCheckedNode.depth) } else { currentList.value = nextList } 节点互斥 前面提到，节点之间存在互斥行为\n  通用节点，权限最高，与所有其他节点互斥\n  选择了父节点，其孩子节点都不可选\n  不可选，那就是 disabled， 在上面的 nextList = ... 中新增 disabled 的判断即可\n针对通用节点（该节点只有一个），我们新增属性 majestyId (your majesty 女王陛下)，由业务指定节点 id\ndisabled = (majestySelected.value \u0026amp;\u0026amp; (item.id !== props.majestyId)) 针对第二个情况，遍历每一层的节点 item，如果\n  当前层级深度比选择的节点所在的层级深，说明当前渲染的是孩子节点\n  然后判断 item 是否依赖选择的节点 t，即是否有 \u0026ldquo;血缘关系\u0026rdquo;\n  disabled = !!(nextTagList?.find((t) =\u0026gt; t.depth \u0026lt; item.depth \u0026amp;\u0026amp; hasParentDependency(item, t))) 搜索筛选 最后是搜索的实现(前端搜索，因为业务数据量不大)，也分两种情况\n  只在叶子节点搜\n  对整棵树进行搜索，且搜索结果需要按深度层级显示(换句话说，也是深度遍历)\n  Node_L1_1 匹配，继续匹配孩子节点\n  Node_L1_1_L2_2 匹配，继续匹配孩子节点\n Node_L1_1_L2_2_L3_3匹配 ，继续匹配孩子节点    如果 Node_L1_1_L2_6 匹配，继续匹配孩子节点\n    Node_L1_2 匹配，继续匹配孩子节点\n  \u0026hellip;\n    叶子搜索 叶子节点搜索比较简单，首先要搜集所有的叶子节点，在 flattenTree() 中即可以实现\n// 缓存所有叶子节点 if (!t.childrenCount) { leafNodeList.push({ ...(omit(t, [\u0026#39;children\u0026#39;])) }) } 然后根据关键字过滤一下即可\nqueryList = leafNodeList.filter(t =\u0026gt; t.label.indexOf(q) \u0026gt; -1) 路径搜索 既然知道了叶子节点( flattenTree() 的深度遍历和这里层级搜索是一一对应的，所以按顺序遍历叶子节点就是我们所需要的路径顺序)，那么我们可以通过 parent 回溯对应的链路\n思考下面的路径\n[A, B, C, D1] [A, B, C, D2] 最终所有的路径顺序\n[A] [A, B] [A, B, C] [A, B, C, D1] [A, B, C, D2] 这一步也简单，不断找 parent 就好了\nfunction reachMe(node) { let tempList = [ node ] let o = node.parent while (o) { tempList.unshift(o) o = o.parent } return tempList } 当给定一个节点，比如说 D1 ，这个函数就会返回 [A, B, C, D1]，所以要得所有路径，还需要在遍历 D1.parent 直到 null\n这里要注意一个问题，当我们继续遍历 D2 的时候，[A], [A, B], [A, B, C] 已经存在了，所以要去重。去重也很简单，只需要看一下最后一个节点是不是当前节点就行，\n  比如 D2，这是肯定没有的，所以需要计算；\n  但是到了D2.parent 即 C 的时候，memo 里已经存在 [A, B, C] 了，所以不用再算了\n  oneLevelList = leafNodeList.reduce((memo, node) =\u0026gt; { let resultList = [] let o = node while (o) { if (!memo.length || !memo.find(nl =\u0026gt; nl[nl.length - 1][\u0026#39;id\u0026#39;] == o.id)) { resultList.unshift(reachMe(o)) } o = o.parent } memo = memo.concat(resultList) return memo }, []) 最后就是搜索匹配，类似的，只需要匹配每条路径的最后一个节点即可\nif (props.intermediate) { // [[p1], [p1, p2], [p1, p2, leaf]]  nextList = oneLevelList.filter((res) =\u0026gt; { const lastNode = res[res.length - 1] return lastNode.label.indexOf(q) \u0026gt; -1 }) } "
            }
    
        ,
            {
                "id": 12,
                "href": "https://ixiaopan.github.io/blog/post/fe-directives/",
                "title": "Common directives/plugins in Vue",
                "section": "post",
                "date" : "2022.05.29",
                "body": "Directives and plugins are different from third-party libraries. Usually, they are used to do simple tasks, such as lazy load image, format text, and so on.\n注册指令/插件 注册指令\napp.directive(\u0026#39;name\u0026#39;, directive) 注册插件\nexport default { install: (app: App, options) =\u0026gt; { app.component(\u0026#39;foo\u0026#39;, fooComp) app.directive(\u0026#39;lazy\u0026#39;, lazyDirective) } } 可以看到，插件其实更为灵活，你可以在插件里注册组件、注册指令等。举个例子，\n如果需要做全局配置，插件再好不过了。比如之前做懒加载图片的时候，一开始是写成一个指令，后来希望支持 webp ，也就只需要在 url 后添加参数即可。当然可以直接写死在指令里，不过这也太死了，所以打算加开关，这就改成了插件模式，在插件里注册一个指令。这样，既支持全局开关统一开启 webp，也支持单个图片是否开启 webp。\nlet enableWebp const lazyDirective: Directive = { mounted(el: HTMLElement, binding: DirectiveBinding\u0026lt;any\u0026gt;) { // 图片自己的开关  el.webp = binding.value?.indexOf(\u0026#39;webp\u0026#39;) \u0026gt; -1 }, } export default { install: (app: App, options) =\u0026gt; { // 全局开关  enableWebp = options?.webp || false app.directive(\u0026#39;lazy\u0026#39;, lazyDirective) } } 防重复 /** * Prevent repeated clicks * @Example v-throttle=\u0026#34;500\u0026#34; */ import type { App, Directive, DirectiveBinding } from \u0026#39;vue\u0026#39;; import { on } from \u0026#39;@mogic-ui/utils\u0026#39;; const throttleDirective: Directive = { beforeMount(el: Element, binding: DirectiveBinding\u0026lt;any\u0026gt;) { let timer const threshold = binding.value || 1000 on(el, \u0026#39;click\u0026#39;, (e) =\u0026gt; { if ((e as any).button !== 0) return; if (!timer) { timer = setTimeout(() =\u0026gt; { clearTimeout(timer) timer = null }, threshold); } else { e.stopImmediatePropagation() } }, true) }, } export function setupRepeatDirective(app: App) { app.directive(\u0026#39;throttle\u0026#39;, throttleDirective) } Then We can use v-throttle like this\n\u0026lt;button v-throttle\u0026gt;Submit\u0026lt;/button\u0026gt; 一键复制 /** * https://clipboardjs.com/ * @Example v-copy */ import type { App, Directive, DirectiveBinding } from \u0026#39;vue\u0026#39; import Clipboard from \u0026#39;clipboard\u0026#39; import { Message } from \u0026#39;./components/message\u0026#39; import { on } from \u0026#39;./utils\u0026#39; const copyDirective: Directive = { beforeMount(el: Element, binding: DirectiveBinding\u0026lt;any\u0026gt;) { on(el, \u0026#39;click\u0026#39;, () =\u0026gt; { const clipboard = new Clipboard(el) clipboard.on(\u0026#39;success\u0026#39;, () =\u0026gt; { Message.success(\u0026#39;复制成功\u0026#39;) clipboard.destroy() }) clipboard.on(\u0026#39;error\u0026#39;, () =\u0026gt; { Message.success(\u0026#39;复制失败\u0026#39;) clipboard.destroy() }) }, true) }, } export function setupCopyDirective(app: App) { app.directive(\u0026#39;copy\u0026#39;, copyDirective) } Then We can use v-copy like this\n\u0026lt;div v-copy data-clipboard-text=\u0026#34;you\u0026#39;ve copied me\u0026#34;\u0026gt; copy me \u0026lt;/div\u0026gt; 懒加载 按需加载不仅仅是指图片，任何资源如js、css都可以懒加载。比如只有1、2个页面才有上传的功能，那么可以按需加载对应的依赖库(比如ali-oss)。\n图片 使用最新的 intersectionObserver API，如果不支持，全部加载不考虑降级(业务不需要，面向 chrome 开发)\nlet enableWebp let previewHost class Lazyload { io: any constructor() { this.io = this.initObserve() } initObserve() { if (!intersectionObserverEnabled) { return } return new IntersectionObserver(entries =\u0026gt; { entries.forEach(item =\u0026gt; { const elem = item.target as HTMLElement if (item.isIntersecting) { this.load(elem) .then(() =\u0026gt; { this.io.unobserve(elem) }) .catch(() =\u0026gt; { console.log(\u0026#39;loading image error\u0026#39;); }) } }) }) } observe(elem: HTMLElement) { if (!elem.getAttribute(\u0026#39;data-src\u0026#39;)) return if (this.io) { this.io.observe(elem) } else { this.load(elem) } } load(elem: HTMLElement) { return new Promise\u0026lt;void\u0026gt;((resolve, reject) =\u0026gt; { let src = elem.getAttribute(\u0026#39;data-src\u0026#39;) if (!src) return resolve() const isImageNode = elem.nodeName.toLowerCase() === \u0026#39;img\u0026#39; const img = isImageNode ? elem : new Image() // 使用 webp 的话  // @ts-ignore  const webpUrl = enableWebp || elem.webp ? (webpSupported() ? src + webpExt : src) : src on(img, \u0026#39;load\u0026#39;, () =\u0026gt; { if (!isImageNode) { // @ts-ignore  elem.style[\u0026#39;background-image\u0026#39;] = \u0026#39;url(\u0026#39; + webpUrl + \u0026#39;)\u0026#39; } elem.removeAttribute(\u0026#39;data-src\u0026#39;) resolve() }) on(img, \u0026#39;error\u0026#39;, () =\u0026gt; { elem.removeAttribute(\u0026#39;src\u0026#39;) reject() }) img.setAttribute(\u0026#39;src\u0026#39;, webpUrl) }) } } let lazy const lazyDirective: Directive = { beforeMount() { if (!lazy) { lazy = new Lazyload() } }, mounted(el: HTMLElement, binding: DirectiveBinding\u0026lt;any\u0026gt;) { lazy.observe(el) }, updated(el, binding: DirectiveBinding\u0026lt;any\u0026gt;) { lazy.observe(el) }, beforeUnmount() { lazy.io?.disconnect() } } js 正如之前提到上传功能，\n 只有用户使用了上传才加载额外的第三方库 上传本身就会 loading，所以延迟加载不会有什么体验问题  const customCache = new Set() function loadScriptFromRemote() { if (customCache.has(scriptUrl)) { return resolve() } const script = document.createElement(\u0026#39;script\u0026#39;) script.setAttribute(\u0026#39;src\u0026#39;, scriptUrl) script.onload = () =\u0026gt; { resolve() } script.onerror = () =\u0026gt; { reject() } customCache.add(scriptUrl) document.body.appendChild(script) } let url = \u0026#39;\u0026#39; const remoteJSDirective: Directive = { beforeMount(el: Element, binding: DirectiveBinding\u0026lt;any\u0026gt;) { loadScriptFromRemote(url) }, } export default { install: (app: App, options) =\u0026gt; { // TODO: 可以扩展到 url collections  if (!options?.url) { return console.error(\u0026#39;url is required\u0026#39;) } url = options?.url app.directive(\u0026#39;v-remote\u0026#39;, remoteJSDirective) } } "
            }
    
        ,
            {
                "id": 13,
                "href": "https://ixiaopan.github.io/blog/post/fe-antd-theme/",
                "title": "Replace antd theme",
                "section": "post",
                "date" : "2022.05.29",
                "body": "没有整啥插件，使用 less variable 覆盖即可\nBackground 现在大多数中后台开发都是基于第三方UI(如 antd, element-ui)，通常业务也会要求自己的主题定制，这就涉及到所谓的 换肤。\n庆幸的是，这些 UI 提供了这样的功能，拿 antd 来说，themes/default.less 包含了其使用的全部颜色变量，业务只需要覆盖即可。\n// -------- Colors ----------- @primary-color: @blue-6; // \u0026gt;\u0026gt;\u0026gt; Warning @warning-color: @gold-6; Why 颜色定制的原理，其实是 less 提供了对 less variables 的修改，参考 文档\nless.modifyVars({ \u0026#39;@buttonFace\u0026#39;: \u0026#39;#5B83AD\u0026#39;, \u0026#39;@buttonText\u0026#39;: \u0026#39;#D9EEF2\u0026#39; }); 所以只需要在构建里加一个自动 modifyVars，注入对应的变量即可\ncss: { preprocessorOptions: { less: { modifyVars: generateModifyVars(), javascriptEnabled: true, }, }, }, function generateModifyVars() { return { hack: \u0026#39;; @import (reference) ./src/styles/theme.less\u0026#39; } } 这是我在网上找到的大多数版本，看到这几行代码，我有几个疑惑\n  javascriptEnabled 干啥的\n  除了 modifyVars 还有其他参数吗\n  hack 干啥的\n  @import (reference) 和 @import 有什么区别\n  Q1 javascriptEnabled 开启这个即允许 code injection，但是\n False by default starting in v3.0.0. Enables evaluation of JavaScript inline in .less files. This created a security problem for some developers who didn\u0026rsquo;t expect user input for style sheets to have executable code.\n 理论上来说，修改 less variable 不需要这个参数，但是因为我们依赖的是 antd ，这个 UI 定义了一个基于 less 变量的调色板，这里有一堆 js function，比如\n.colorPaletteMixin() { @functions: ~`(function() { var getHue = function(hsv, i, isLight) { }; } 这就导致我们不得不开启这个选项\nQ2 modifyVars 还有 globalVars ，区别就是一个是在文件头部，一个在文件最后注入。当然，如果你在文件头部注入，业务恰好有同名变量，也是会被覆盖，这个要注意，所以我们采用 modifyVars\nlessc --global-var=\u0026#34;color1=red\u0026#34;\t{ globalVars: { color1: \u0026#39;red\u0026#39; } } lessc --modify-var=\u0026#34;color1=red\u0026#34;\t{ modifyVars: { color1: \u0026#39;red\u0026#39; } } 除了这两个，我们还可以注入 banner，看一下源码\nparse: function (str, callback, additionalData) { let globalVars; let modifyVars; let preText = \u0026#39;\u0026#39;; globalVars = (additionalData \u0026amp;\u0026amp; additionalData.globalVars) ? `${Parser.serializeVars(additionalData.globalVars)}\\n` : \u0026#39;\u0026#39;; modifyVars = (additionalData \u0026amp;\u0026amp; additionalData.modifyVars) ? `\\n${Parser.serializeVars(additionalData.modifyVars)}` : \u0026#39;\u0026#39;; if (globalVars || (additionalData \u0026amp;\u0026amp; additionalData.banner)) { preText = ((additionalData \u0026amp;\u0026amp; additionalData.banner) ? additionalData.banner : \u0026#39;\u0026#39;) + globalVars; } } Q3 hack 这个其实你可以用任意的名字，不信看 源码实现\nParser.serializeVars = vars =\u0026gt; { let s = \u0026#39;\u0026#39;; for (const name in vars) { if (Object.hasOwnProperty.call(vars, name)) { const value = vars[name]; s += `${((name[0] === \u0026#39;@\u0026#39;) ? \u0026#39;\u0026#39; : \u0026#39;@\u0026#39;) + name}: ${value}${(String(value).slice(-1) === \u0026#39;;\u0026#39;) ? \u0026#39;\u0026#39; : \u0026#39;;\u0026#39;}`; } } return s; }; 所以，\n{ hack: \u0026#39;; @import (reference) ./src/styles/theme.less;\u0026#39; } 最终会变成 @hack: ; @import (reference) ./src/styles/theme.less';\nQ4 @import (reference) 为啥不是 @import 呢？？看下 官方文档 怎么说的？\n Imagine that reference marks every at-rule and selector with a reference flag in the imported file, imports as normal, but when the CSS is generated, \u0026ldquo;reference\u0026rdquo; selectors (as well as any media queries containing only reference selectors) are not output. reference styles will not show up in your generated CSS unless the reference styles are used as mixins or extended.\n 这种方式可以按需引入，打包的时候，只会编译业务用到的一些变量、类等。\n"
            }
    
        ,
            {
                "id": 14,
                "href": "https://ixiaopan.github.io/blog/post/fe-composition/",
                "title": "Composition bug in Safari",
                "section": "post",
                "date" : "2022.05.21",
                "body": "中文输入法 composition event 在 chrome 和 safari 表现相反\nBackground 在开发编辑输入组件的时候，如果输入的是中文，监听 delete 事件会删除输入框里已经写好的文案，期望效果是删除正在输入的拼音。\nbug 复现如下\n 期望效果\n Why composition 有几个事件\n compositionstart compositionupdate compositionend  具体示例 MDN 上给了一个 demo 可以体验\n让我们看下中文输入 asd，然后 delete 后，发生了什么\nchrome 事件顺序如下\n  delete\n  onCompositionUpdate\n  delete\n  onCompositionUpdate\n  delete\n  onCompositionUpdate\n  onCompositionEnd\n  safari 事件顺序如下\n  onCompositionUpdate\n  delete\n  onCompositionUpdate\n  delete\n  onCompositionEnd\n  delete\n  如果输入法正在 composition，我们是知道的，只需要监听 compositionstart, compositionupdate 打个标记即可，然后在 delete 拦截掉\nfunction onCompositionStart() { isOnComposition = true } function onCompositionUpdate() { isOnComposition = true } function onCompositionEnd() { isOnComposition = false } function mockDelete() { // 拦截，false 的时候，删除业务 tag  if (!isOnComposition \u0026amp;\u0026amp; isMultipleMode \u0026amp;\u0026amp; tags.value?.length \u0026amp;\u0026amp; !phone.value) { const lastTag = tags.value[tags.value?.length - 1] removeTag(lastTag?._id) } } 目前看没啥问题，但是到最后一个字符 safari 就和 chrome 表现不一样了，\n因为 compositonend 在前 delete 在后，这就导致 safari 在监听到 delete 的时候，isOnComposition 已经被置为 false 了，这就导致会同时删除中文拼音以及业务 tag\n解决方法也简单，在 compositionend 继续打个标记\nfunction onCompositionEnd() { isOnComposition = false mockDelete.safariBug = true } function mockDelete() { // safari bug: compositionEnd =\u0026gt; delete  if (isSafari \u0026amp;\u0026amp; mockDelete.safariBug) { return mockDelete.safariBug = false } } "
            }
    
        ,
            {
                "id": 15,
                "href": "https://ixiaopan.github.io/blog/post/fe-gif-frame/",
                "title": "Extract any frame of a gif image",
                "section": "post",
                "date" : "2022.05.21",
                "body": "说到提取视频帧，我们会想到 canvas.drawImage 来实现，但是，如果是要提取 GIF 的任意一帧呢?\nBackground 工作中遇到一个需求，GIF 默认显示第一帧画面，hover 的时候在显示 gif 本身，效果如下\n How canvas.drawImage() 可以提取序列帧，代码实现如下\nfunction getFirstFrameFromGif(img) { const canvas = document.createElement(\u0026#39;canvas\u0026#39;) const ctx = canvas.getContext(\u0026#39;2d\u0026#39;) canvas.width = img.naturalWidth canvas.height = img.naturalHeight try { ctx.drawImage(img, 0, 0, img.naturalWidth, img.naturalHeight) return canvas.toDataURL(\u0026#39;image/png\u0026#39;, 0.75) } catch (e) { return \u0026#39;\u0026#39; } } Q1 拿到帧画面了，就需要交替显示图片。一开始是使用 opacity/display ，有几个缺点\n  显隐切换会抖动\n  底下的 gif 始终在播放，那么 hover 的时候，画面不是从头开始播放的\n  另个方法就是直接换 src\n\u0026lt;div class=\u0026#34;gif\u0026#34; @mouseenter=\u0026#34;toggleGif\u0026#34; @mouseleave=\u0026#34;toggleGif\u0026#34;\u0026gt; \u0026lt;img :class=\u0026#34;[\u0026#39;gif-placeholder\u0026#39;, innerSrc == src ? \u0026#39;gif-real\u0026#39; : \u0026#39;gif-frame\u0026#39; ]\u0026#34; :src=\u0026#34;innerSrc\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; function toggleGif() { if (!frameSuccess.value) return if (gifVisible.value) { innerSrc.value = frameSrc.value } else { innerSrc.value = props.src } gifVisible.value = !gifVisible.value } Q2 这个组件接下来要显示在一个 div 里而且是 cover 整个容器的效果，但是组件用的 img 标签，无法直接用 background-size: cover，一个方法是用 js 手动算，但是还有更简单的方法，指定 object-fit\n The object-fit CSS property sets how the content of a replaced element, such as an \u0026lt;img\u0026gt; or \u0026lt;video\u0026gt;, should be resized to fit its container. \u0026ndash; MDN\n 实现代码如下\n.gif { width: 100%; height: 100%; } .mogic-gif-frame { width: 100%; height: 100%; object-fit: cover; } 任意一帧 第一帧是很简单了，还是和提取视频帧一样的方法，现在如果想要提取任意时刻的一帧呢？？\n说实话，几个月之前，我是真不知道的，也就是今天我才在 MDN 上看到方法，就是用 ImageCoder 实现，官方给的例子就可以满足我们的需求。\nStep 1 加载图片，实例化解码器\nlet decoder fetch(url).then((res) =\u0026gt; { decoder = new ImageDecoder({ data: res.body, type: \u0026#39;image/gif\u0026#39;, }) }) Step 2 解析指定位置的帧，渲染到 canvas\nfunction main() { decoder.decode({ frameIndex: n, }).then(renderFrame) } function renderFrame(res) { const canvas = document.createElement(\u0026#39;canvas\u0026#39;) const context = canvas.getContext(\u0026#39;2d\u0026#39;) context.drawImag(res.image, 0, 0, canvas.width, canvas.height) } Draw GIF on the canvas 既然可以得到任意帧，那么把 GIF 绘制出来，也不是难事，来排个队依次渲染就好了。需要注意的是，这个依次渲染，不是 0 delay 的，比如 我们执行一个 for loop。 因为渲染1帧也是有时长的，比如我们说帧率 25fps，那么绘制一帧需要 40ms，也就是说，需要等 40ms 后，才能绘制第2桢。\nlet n = 0 function main() { decoder.decode({ frameIndex: n, }).then(renderFrame) } function renderFrame(res) { // render the nth frame  const canvas = document.createElement(\u0026#39;canvas\u0026#39;) const context = canvas.getContext(\u0026#39;2d\u0026#39;) context.drawImag(res.image, 0, 0, canvas.width, canvas.height) Promise.all([ // decode the next image ahead  decoder.decode({ frameIndex: ++n, }), // duration for rendering the current frame  new Promise((resolve) =\u0026gt; { const delay = res.image.duration / 1000 setTimeout(resolve, dealy) }) ]).then(([res, _]) =\u0026gt; { renderFrame(res) }) } 上面的代码还是有问题，n 会越界从而触发 RangeError，要想解决，需要知道这个 GIF 总共有多少帧\n// get the current image track const track = decoder.tracks.selectedTrack if (decoder.complete) { // static image  if (track.frameCount == 1) { return } // rewind  if (n + 1 \u0026gt;= track.frameCount) { n = 0 } } "
            }
    
        ,
            {
                "id": 16,
                "href": "https://ixiaopan.github.io/blog/post/misci-python-epub/",
                "title": "html2epub",
                "section": "post",
                "date" : "2022.02.19",
                "body": "A simple book scraper written in Python, converting html to epub.\nInstall pip install bs4 inquirer html2epub tqdm Usage wget https://raw.githubusercontent.com/ixiaopan/DataScience/master/Utilities/Scraper/download.py python download.py After running the above commands, you need to select one source, as shown below.\n Note that this script can only convert html to epub. In other words, only books in HTML format can be downloaded.\nBooks with chapter list Books may or may not have a chapter list. For books with a chapter list, just type the URL. For example, the below one.\nhttps://booksvooks.com/selfish-shallow-and-self-absorbed-sixteen-writers-on-the-decision-not-to-have-kids-pdf.html Books without chapter list For books without a chapter list, you need to add the total page number to the page URL. For example, the book this is going to hurt does not have a chapter list, so the url is\nhttps://booksvooks.com/fullbook/this-is-going-to-hurt-pdf-adam-kay.html?page=32 The book will be downloaded in the folder booksVooks/ or jinjiang based on your source.\n- booksVooks - thisisgoingtohurtpdfadamkaypage32.epub "
            }
    
        ,
            {
                "id": 17,
                "href": "https://ixiaopan.github.io/blog/post/fe-antd-test/",
                "title": "Antd modal has no transition in test env",
                "section": "post",
                "date" : "2022.01.23",
                "body": "记录一下 antd modal 在测试环境打开关闭 modal 组件没有 transition 过渡效果的 bug，说来也是我手欠，改构建改的。\nbug 复现 业务中使用了 ant-design-vue，在使用 modal 这个组件的时候，发现测试环境 modal 的打开关闭没有 transition 动画，但是本地是有的；\n排查原因 思路一： 有可能是构建没有把相关的 css 打包进去？但是打开 dist 发现是有的\n思路二： 发现是相关的类比如 fade-enter 压根就没挂在对应的元素上，那么就是代码本身有问题了。开始单点调试，发现打包后的 getTransitionProps 和源码不一致。\n这是源码\n 这是打包后的\n 这就清楚明了了，跟环境配置有关系，在测试环境部署执行的是 vite build --mode test 那么这句话就会把 NODE_ENV 设置为 test ，这样就导致构建后没有 transition\n解决方法 修改 mode 值 mode !== test 即可\n"
            }
    
        ,
            {
                "id": 18,
                "href": "https://ixiaopan.github.io/blog/post/fe-ui-lib/",
                "title": "Build your own UI component library from scratch",
                "section": "post",
                "date" : "2022.01.23",
                "body": "In this post, we will learn how to build your own UI component library based on monorepo.\nFirst of all, let\u0026rsquo;s have a peek at the finished project structure.\n- vue-ui - docs - play - packages - components - themes - utils Initialize a project First, we install the package manager pnpm globally.\nnpm i pnpm -g Then we create an empty project and initialize it. In this tutorial, the project is called ui-demo.\nmkdir ui-demo cd ui-demo pnpm init -y Next, install vue \u0026amp; typescript using pnpm, and then initialize the ts config file.\npnpm install vue@next typescript -D npx tsc --init Generate the .npmrc and pnpm-workspace.yaml files under the root of ui-demo.\ntouch .npmrc # content shamefully-hoist = true Since we are using vue + ts, a declare ts file vue-shim.d.ts should be included.\nmkdir typings cd typings touch vue-shim.d.ts Then copy the following content into vue-shim.d.ts.\ndeclare module \u0026#39;*.vue\u0026#39; { import type { DefineComponent } from \u0026#39;vue\u0026#39; // eslint-disable-next-line @typescript-eslint/ban-types  const component: DefineComponent\u0026lt;{}, {}, any\u0026gt; export default component } Finally, declare the packages in pnpm config file pnpm-workspace.yaml.\ntouch pnpm-workspace.yaml # the content packages: - \u0026#39;packages/**\u0026#39; - docs - play - \u0026#39;!**/__tests__/**\u0026#39; Now, your project will look like this.\n Playground This is where you write components demos to debug components.\nmkdir play cd play pnpm init -y pnpm install vite @vitejs/plugin-vue -D Packages By default, packages are stored in the folder packages. Of course, you can change the name. In this project, we create four packages, as shown below.\nmkdir packages mkdir packages/components mkdir packages/themes mkdir packages/utils mkdir packages/wxp-ui Then, we initialize the first three package and add a scope for them by modifying the name field in the file package.json. Say, the scope name is @wxp-ui\ncd components \u0026amp;\u0026amp; pnpm init -y cd themes \u0026amp;\u0026amp; pnpm init -y cd utils \u0026amp;\u0026amp; pnpm init -y components -\u0026gt; @wxp-ui/components themes -\u0026gt; @wxp-ui/themes utils -\u0026gt; @wxp-ui/utils The last package is a bit special. In fact, it\u0026rsquo;s the entry file that export all components.\nimport * as components from \u0026#39;@wxp-ui/components\u0026#39; import type { App } from \u0026#39;vue\u0026#39; export default { install: (app: App) =\u0026gt; { Object.entries(components).forEach(([ name, comp ]) =\u0026gt; { app.component(name, comp) }) } } export * from \u0026#39;@wxp-ui/components\u0026#39; The package.json for this package is served as the final package.json for the published package.\n{ \u0026#34;name\u0026#34;: \u0026#34;@wxp/wxp-ui\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.0.1-alpha.1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Vue UI Components\u0026#34;, \u0026#34;exports\u0026#34;: { \u0026#34;.\u0026#34;: { \u0026#34;require\u0026#34;: \u0026#34;./lib/index.js\u0026#34;, \u0026#34;import\u0026#34;: \u0026#34;./es/index.mjs\u0026#34; }, \u0026#34;./es\u0026#34;: \u0026#34;./es/index.mjs\u0026#34;, \u0026#34;./lib\u0026#34;: \u0026#34;./lib/index.js\u0026#34;, \u0026#34;./*\u0026#34;: \u0026#34;./*\u0026#34; }, \u0026#34;main\u0026#34;: \u0026#34;lib/index.js\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;es/index.mjs\u0026#34;, \u0026#34;style\u0026#34;: \u0026#34;dist/index.css\u0026#34;, \u0026#34;unpkg\u0026#34;: \u0026#34;dist/index.full.js\u0026#34;, \u0026#34;peerDependencies\u0026#34;: { \u0026#34;vue\u0026#34;: \u0026#34;^3.2.20\u0026#34; }, \u0026#34;repository\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;git\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;\u0026#34; } } Now, install these packages at the root directory.\npnpm install @wxp-ui/components -w pnpm install @wxp-ui/themes -w pnpm install @wxp-ui/utils -w pnpm install @wxp/wxp-ui -w You can see that the above dependencies has been added in package.json.\n\u0026#34;dependencies\u0026#34;: { \u0026#34;@wxp-ui/components\u0026#34;: \u0026#34;workspace:^1.0.0\u0026#34;, \u0026#34;@wxp-ui/themes\u0026#34;: \u0026#34;workspace:^1.0.0\u0026#34;, \u0026#34;@wxp-ui/utils\u0026#34;: \u0026#34;workspace:^1.0.0\u0026#34;, \u0026#34;@wxp/wxp-ui\u0026#34;: \u0026#34;workspace:^0.0.1-alpha.1\u0026#34;, }, Components - packages/components - button - __docs__ - button.md - __tests__ - src - button.ts // props, types  - button.vue // template + js  - style - css.ts // css module  - index.ts // less module  - index.ts // entry To avoid conflicting with the vanilla HTML element, our components should have a prefix name, say M. For example, the button component name is MButton.\nindex.ts index.ts is the entry file. It does two things\n  export the component when you use it in your project\n  register the component globally when using app.use(vue-ui)\n  import { withInstall } from \u0026#34;@wxp/utils/with-install\u0026#34; import Button from \u0026#39;./src/button.vue\u0026#39; const MButton = withInstall(Button) export { MButton } export default MButton src \u0026lt;template\u0026gt; \u0026lt;a-button :type=\u0026#34;type\u0026#34; :size=\u0026#34;size\u0026#34;\u0026gt; \u0026lt;slot\u0026gt;\u0026lt;/slot\u0026gt; \u0026lt;/a-button\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script lang=\u0026#34;ts\u0026#34;\u0026gt; import { Button } from \u0026#39;ant-design-vue\u0026#39; export default defineComponent({ name: \u0026#39;MButton\u0026#39;, components: { [Button.name]: Button, }, }); \u0026lt;/script\u0026gt; style There are two ways of importing css.\ncss.ts\nimport \u0026#39;@wxp/theme/base.css\u0026#39; import \u0026#39;@wxp/theme/m-button.css\u0026#39; index.ts\nimport \u0026#39;@wxpi/theme/src/base.less\u0026#39; import \u0026#39;@wxp/theme/src/button.less\u0026#39; Debug Generally, we can use npm link to install our package globally, and then we install it in other projects. However, if you are using other versions of this package, it could cause confusion. Thus, I don\u0026rsquo;t recommend this method.\nThe method I used is to write some bas scripts, like this,\npnpm run build cp -R dist/wxp-ui ../your-project/node_modules/@wxp/ cd ../your-project rm -rf node_modules/.vite Well, there are two things to note\n your project should have the level as the ui library you must restart your server if you use vite  Of course, you can improve this script further by watching file changes and restarting server automatically.\nBuild Themes pnpm install gulp-less @types/gulp-less @types/sass gulp-autoprefixer @types/gulp-autoprefixer @types/gulp-clean-css gulp-clean-css -w -D ESM Docs Usage # create a new component pnpm run create # Build the ui components pnpm run build # Debug pnpm run debug # Playground pnpm run dev # publish pnpm run pub ## after publishing, commit your content to the remote repo git push # Doc ## preview pnpm run docs ## build pnpm run docs:build "
            }
    
        ,
            {
                "id": 19,
                "href": "https://ixiaopan.github.io/blog/post/fe-verdaccio/",
                "title": "Build a private npm registry using verdaccio",
                "section": "post",
                "date" : "2022.01.07",
                "body": "In this post, we will learn how to create your own npm registry using verdaccio. Why do you bother using a private registry? This is because we do not want to publish our code to the public due to data security and privacy.\nInstall First, we need to install verdaccio and pm2 globally.\nnpm install -g verdaccio npm install -g pm2 pm2  is used to guard our service, the common commands are shown below.\npm2 start verdaccio pm2 stop verdaccio pm2 delete verdaccio To check the service status, we use the following command. status: online shows that everything is ok.\npm2 list  Configuration The config file is located at\n/Users/wuxiaopan/.config/verdaccio/config.yaml Storage This is where we place all packages.\n# path to a directory with all packages storage: ./store Uplinks # a list of other known repositories we can talk to uplinks: npmjs: url: https://registry.npmjs.org/ Packages Here, we define our own scope, which means that they can only be accessed internally. Other packages that are not started with your scope name will be downloaded from the official npmjs.\npackages: \u0026#39;@your-scope/*\u0026#39;: # scoped packages access: $all publish: $authenticated unpublish: $authenticated # proxy: npmjs Listen The last step is to specify your ip and port.\nlisten: 0.0.0.0:4873 Restart your server and verdaccio. We are done.\nNotify notify: method: POST headers: [{\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}] endpoint: yourhooks content: \u0026#39;{\u0026#34;content\u0026#34;: {\u0026#34;text\u0026#34;: \u0026#34;{{ publishedPackage }} has published\u0026#34;}, \u0026#34;msg_type\u0026#34;:\u0026#34;text\u0026#34;}\u0026#39; Web Page If everything works well, open http://localhost:4873/ and you will see the page like the below one.\n Publish your package Once your package is done, you can publish it to your private registry.\nadd user The first step is to register. The below command will ask you to input your name, password, and email.\nnpm adduser --registry http://localhost:4873/ delete user The registered users are stored in the following file, so we simply remove the corresponding record if we want to delete a user.\n~/.config/verdaccio/htpasswd publish Since we\u0026rsquo;re publishing our package to the private registry, we should explicitly specify where the private registry is. To avoid specifing mannually each time, we use nrm to manage registries.\nnpm i -g nrm nrm add localnpm http://localhost:4873 nrm use localnpm nrm ls  The above command set the default registry to localnpm, which allows us to download the private packages from the localhost and the public packages from other public sources, such as npmjs.\nThen we can publish\nnpm publish "
            }
    
        ,
            {
                "id": 20,
                "href": "https://ixiaopan.github.io/blog/post/fe-npm/",
                "title": "Package Manager",
                "section": "post",
                "date" : "2022.01.07",
                "body": "Learn how to use the modern package managers.\nnpm version  major minor path prerelease  alpha: WIP, unstable beta: features completed but with unknown bugs rc: ready to release    Suppose the current version is 1.2.3, there are many ways to update the version.\n# 2.0.0 npm version major # 1.3.0 npm version minor # 1.2.4 npm version patch npm version \u0026lt;version\u0026gt; # 1.2.4-0 npm version prerelease -m \u0026#34;update %s\u0026#34; --no-git-tag-version # if the current version is 1.2.3-alpha.0, then it becomes 1.2.3-alpha.1 npm version prerelease --preid=\u0026lt;prerelease-id\u0026gt; tag By default, npm publish will tag your package with the latest tag.\n# add tags to the existed pkg npm \u0026lt;yourtag\u0026gt; add \u0026lt;pkg\u0026gt;@\u0026lt;version\u0026gt; npm publish --tag beta npm install somepkg@beta link npm link can be used to debug your local package that is still developing. Suppose your project relies on the developing package pkgA as shown below.\n- project - node_modules - pkgA To debug pkgA, we follow th following steps\n~ cd pkgA ~ npm link ~ cd project ~ npm link pkgA When you are done, you can remove local dependency using npm unlink\ncd project npm unlink pkgA You can also opt to remove the global link\ncd pkgA npm uninstall pkgA -g npmrc This is the npm config file, which can be used to update the global or user-level config files. There are four relevant files\n  project-related file (path-to-project/.npmrc)\n  user-level config file (~/.npmrc)\n  the global config file ($PREFIX/etc/.npmrc)\n  the built-in config file\n  To access the the user-level config file, you can run\nnpm config get userconfig Likewise, to access the the global config file, run this command,\nnpm config get prefix  How to use it? One of the useful settings is registry. For example, we can set the default registry for our project to speed up the download progress.\ncd project touch .npmrc Then specify your registry in .npmrc\nregistry=https://registry.npm.taobao.org/ If you have multiple project, you can also set the user-level or global config file\nnpm config set registry https://registry.npm.taobao.org/ [-g] To remove the configuration, you just need to specify the key and delete it\nnpm config delete \u0026lt;key\u0026gt; yarn lock yarn is another widely used package manager. When running yarn install, the file yarn.lock will be created.\nSo, what\u0026rsquo;s the use of this file? As its name suggests, yarn.lock will lock down the versions of dependencies specified in the package.json, which ensures that everyone in a team has the same dependencies.\nupgrade Wait, how to upgrade versions of dependencies? Simply run yarn upgrade.\nBut this command can only upgrade versions between specified version range. If you need the latest version ignoring the specified version range, run yarn upgrade --latest.\npnpm npx nrm install\nnpm install nrm -g nrm ls add your own registry\nnrm add localnpm http://localhost:4873/ nrm use localnpm  remove\nnrm del localnpm References  publish-pkgs-docs package.json "
            }
    
        ,
            {
                "id": 21,
                "href": "https://ixiaopan.github.io/blog/post/fe-layout/",
                "title": "CSS Layout 101",
                "section": "post",
                "date" : "2022.01.03",
                "body": "This post will introduce the modern CSS layout modes, including flexbox, multi-columns, and grid.\nFlexbox Nowadays, flex layout is widely used to implement both the basic and complex layout in the morden web applications. Instead of block layout, flexbox allows us to\n specify which direction to layout the content ( flex-direction ) rearrange the display order of flex items ( order ) utilize the extra space to adjust the size of flex items ( flex ) specify the way in which flex items are aligned ( justify-content )  If you want to implement a two-column layout, you simply declare the following statement,\n\u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;left side\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;right content\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .item { display: flex; } However, without flex, the alternate way to achieve this is to use float or position\n.left { float: left; } .right { overflow: hidden; } Flexibility It\u0026rsquo;s not common to change the property flex, but it\u0026rsquo;s very useful if we understand how it works. flex is composed of three parts:\n flex-grow flex-shrink flex-basis  When omitted, the default value of flex-grow and flex-shrink is 1. However, the values are quite confusing in some cases. To clarify it, I make a table shown below. Unfortunately, we have to remember them.\n   Case Value     Initial flex: 0 1 auto;   None flex: 0 0 auto;   Auto flex: 1 1 auto;   Number flex: Number 1 0;    I really love the last one flex: \u0026lt;number\u0026gt; because it can implement the even-spaced layout easily. For example, we can implement a dynamic ruler ( I mean when the ruler width is fixed, the width of major and minor scales are flexible when we increase or decrease the number of major scales or minor scales).\n\u0026lt;div class=\u0026#34;ruler\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;major\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;minor\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;minor\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;minor\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;major\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;minor\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;minor\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;minor\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .ruler { display: flex; width: 240px; height: 10px; background: #ccc; } .major { flex: 1; display: flex; border-left: 1px solid #f00; } .major:last-child { border-right: 1px solid #f00; } .minor { flex: 1; height: 6px; border-right: 1px solid blue; box-sizing: border-box; } .minor:last-child { border-right: none; }  Order By default, flex items are displayed in the same order as they appear in HTML. Sometimes we may want to change the display order without modifying the HTML structure. For example, to swap the above HTML to put the image on the right, we simply declare\n.right { order: -1; } // or .left { order: 1; } Why it works?\n  The default value of order is 0, which means all flex items have the order of 0. Thus, the display order is determined by the order of appearing in HTML.\n  order takes an integer value, and items with the smaller order are displayed first.\n  Auto Margin This is my favorite trick. Simply put, setting an auto margin on one flex item will push it away from other elements. One common scenario is to put the login module at the right side of a header, as shown below,\n \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt; \u0026lt;nav class=\u0026#34;nav\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Home\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;Blog\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;About\u0026lt;/span\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;div class=\u0026#34;login\u0026#34;\u0026gt; Log in \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .item { display: flex; } .login { margin-left: auto; } We can see that justify-content or align-self are ignored when extra space are available for auto margins. Besides, overflow containers will ignore the auto magins and overflow in the end direction, as shown below.\n \u0026lt;div class=\u0026#34;flex-column\u0026#34;\u0026gt; \u0026lt;div\u0026gt;Book\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;CD\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;ad\u0026#34;\u0026gt;Advertisement\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .flex-column { border: 1px solid #ccc; width: 50px; margin: 0 50px 0 50px; display: flex; flex-direction: column; } .ad { align-self: center; } .ad { margin-left: auto; } Absolute Pos An absolute-positioned flex item has the following characteristics\n it\u0026rsquo;s out of flow, it seems that it were the sole item in that container; it means that even though we have more than one absolute-positioned flex item, they are independent with each other Both the absolute-positioned flex item and the flex container are fixed-size boxes, so we cannot change their width or height. In other words, stretch will have no effect on them.  \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;left\u0026#34;\u0026gt;left\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt;content....\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;right\u0026#34;\u0026gt;right\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; .item { display: flex; justify-content: flex-start; border: 1px solid #000; width: 600px; padding: 20px; height: 100px; } .left { background: #f00; width: 200px; } .content { background: #0ff; position: absolute; /* align-self: center; */ } .right { background: #00f; }  From the above figure, we see that the absolute-positioned item is out of flow and is considered as the sole item in that flex container. Since justify-content is specified as flex-start, the content area is placed at the start of the main axis. If we modify it to center, it will be placed in the center of the main axis.\nOn the other hand, the default value of align-items is stretch, but it has no effect on the abs-positioned flex item. Instead, if we specify align-self: center, we can see it\u0026rsquo;s centered.\nSince there are some unexpected and confusing behaviors, it\u0026rsquo;s recommended that we should avoid writing code like this.\nMulticol Grid Reference  Flexbox W3C Multicol MDN A Complete Guide to Grid Grid Gardern "
            }
    
        ,
            {
                "id": 22,
                "href": "https://ixiaopan.github.io/blog/post/fe-tools/",
                "title": "Tools for daily work",
                "section": "post",
                "date" : "2022.01.03",
                "body": "Sharp tools make good work.\nDaily Tools Terminal   Alfred\n  iTerm2 Download\n  oh-my-zsh\n  z - jump around\n  ## download z.sh, and then put it at your favorite place wget https://raw.githubusercontent.com/rupa/z/master/z.sh ## add z.sh to the .zashrc # method 1 # vi .zashc # method 2 echo \u0026#39;path/to/z.sh\u0026#39; \u0026gt;\u0026gt; ~/.zshrc ## reload  source ~/.zshrc Coding   Xcode - App Store\n  VSCode Download\n  VSCode Plugin List\n Visual Studio IntelliCode (ES6) code snippets ES7 React/Redux/GraphQL/React-Native snippets Vetur Volar  Prettier — Code formatter ESLint EditorConfig for VS Code dotenv  Path Intellisense Auto Rename Tag Code Runner filesize GitLens Better Comments Code Spell Checker    Package Manager Homebrew /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; You might encounter the error message brew not found when you type brew on your terminal due to the wrong path. First check the place where your brew is installed,\n~ which brew # /opt/homebrew/bin/brew then add it to the global PATH env as follows.\nexport PATH=$HOME/bin:/usr/local/bin:/opt/homebrew/bin:$PATH pip https://pip.pypa.io/en/stable/installation/\npython -m ensurepip --upgrade wget https://bootstrap.pypa.io/get-pip.py python get-pip.py nvm We use nvm to manage different versions of node.\nbrew install nvm nvm install v16.13.1 nvm ls nvm ls-remote Git brew install git # identity git config --global user.name \u0026#39;yourname\u0026#39; git config --global user.email \u0026#39;xx@163.com\u0026#39; # view your config vi ~/.gitconfig # or git config --list git config --global init.defaultBranch main git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status git config --global alias.glog \u0026#39;log --all --decorate --oneline --graph\u0026#39; # remove git config --global --unset alias.co # clean branches ## look up git br | grep \u0026#39;pattern\u0026#39; ## delete it git br | grep \u0026#39;pattern\u0026#39; | xargs git branch -D Download   Library Genesis\n  jiumodiary\n  gutenberg.org\n  allitbooks.net\n  UX   SimpleIcons\n  IconFinder\n  SVG Sprite Generator\n  English Vocabulary   vocabulary.com\n  Grammar Byte\n  Paraphrase   powerthesaurus\n  quillbot (Strongly Recommend)\n  Ginger Grammar Checker\n  lingohelpme\n  BibGuru\n  FrontEnd Fundamentals  CSS - MDN CSS Secrets JavaScript.info ES6 - You Don\u0026rsquo;t Know JS  TypeScript Deep Dive Functional-Light JS  Advanced  WebAssembly Webgl 2 Web Audio  CI/CD  Pro Git - Book Semantic Version  Database  SQL Zoo SQL Tutorial  Statistics  Math24 XMathMain (Math Collection) Introduction to Statistics Intro to Probability Applied Multivariate Statistical Analysis  Machine Learning  from-python-to-numpy shanelynn blog  Visualisation  Data Visualization Book Reviews NAPA Cards datavizproject data-to-viz datavizcatalogue makeovermonday  Complexity  introduction-to-complexity "
            }
    
        ,
            {
                "id": 23,
                "href": "https://ixiaopan.github.io/blog/post/fe-media/",
                "title": "Intro to Video Format",
                "section": "post",
                "date" : "2021.12.13",
                "body": "Codec   Figure 1: Process of video creation.  Codec, short for compressor-decompressor, is an algorithm designed for compressing and decompressing video and audio data. Simply put, it\u0026rsquo;s an encoding and decoding algorithm.\nWhy encoding?\n raw data can be very large and noisy encoding makes it easy to transmit and storage data  Why decoding?\n preview or playback  Video Codec  H.264/AVC (MPEG-4 part 10/Advanced Video Coding) is the most widely used video codec on the market VP8, free and open source codec owned by Google Ogg Theora, typically used with OGG container H.265, successor of H.264  Lossy Audio  AAC is an ISO standard audio codec widely used in iTunes, iPhone, etc. Much better than MP3 Ogg Vorbis, free and open source, better than MP3, typically used with OGG container MP3, the earliest and most popular audio format  Lossless Audio  FLAC, typically compress a file by 50% WAV/WMA, owned by Microsoft. Started in early 1990s  Container Now we have the encoded video and audio, are we ready for playing? No. We still need a container to include the encoded data and other metadata, such as video thumbnail and subtitles. Table below shows some common cotainer formats for storage and online video streaming and distribution.\n   Container Desc Video Codec Audio Codec     MP4 - MPEG 4 Part-14, ISO/IEC standard container format\n- supports multiple codecs\n- .m4v is for video while .m4a is for audio only H.264, MPEG-2, MPEG-4 AAC, MPEG-1   OGG  Theora Vorbis   WebM developed by Google based on the open-source container Matroska (.mkv) VP8 Vorbis   QuickTime .mov or .qt, widely used in Apple\u0026rsquo;s products     AVI - developed by Microsoft in early 1990s\n- does not support streaming media      We can see that\n some containers (MP4) support variety of codecs while some containers (WebM) support only one type of codec AVI does not support straming media, that is, playing and downloading at the same time  Caniuse Browers that support HTML5 audio and video\n IE\u0026gt;=9 FF \u0026gt;= 3.5 safari \u0026gt;= 4 opera \u0026gt;= 10.5， chrome safari for iOS webkit for android  Besides, different browsers support different container formats and codecs. In other words, there is no standard specification about video format and codecs for web development. Table below shows the container format between browsers.\n   Browsers Supported codecs and containers     Chrome theora/Vorbis/Ogg; VP8/Vorbis/WebM   FF theora/Vorbis/Ogg; VP8/Vorbis/WebM   IE H.264/AAC/MPEG 4; VP8/Vorbis/WebM   Opera theora/Vorbis/Ogg; VP8/Vorbis/WebM   Safari H.264/AAC/MPEG 4    The audio codecs supported between modern browsers.\n    IE FF Chrome Opera Safari     MP3 9 NO 5 NO 3.1   Ogg Vorbis NO 3.6 5 10.5 NO   WAV NO 3.6 8 10.5 3.1    From the above tables, we see that we need to prepare at least two formats to accommodate the most browsers.\nVideo\n H.264/AAC/MPEG 4 theora/Vorbis/Ogg VP8/Vorbis/WebM  Audio\n MP3 OGG  Feature Detection canPlayType(mediaType) tells us how likely it is that the media can be played. We can dynamically detect whether the browser supports the audio element.\nvar hasMedia = !!(document.createElement(\u0026#34;audio\u0026#34;).canPlayType; If not, a simple way to degrade is to insert fallback content into the audio/video tags, as the code snippet below shows.\n\u0026lt;audio id=\u0026#34;myAudio\u0026#34; controls\u0026gt; \u0026lt;source src=\u0026#34;song.ogg\u0026#34; type=\u0026#34;audio/ogg\u0026#34;\u0026gt; \u0026lt;source src=\u0026#34;song.mp3\u0026#34; type=\u0026#34;audio/mpeg\u0026#34;\u0026gt; Audio player not available. \u0026lt;/audio\u0026gt; However, even though browsers support some kind of format, it does not mean that it can play it. So, to ensure the successfull palyback, we specify the codecs using the type attribute.\n   Container type value     theora/vorbis/Ogg type=\u0026ldquo;video/ogg;codecs=\u0026lsquo;theora,vorbis\u0026rsquo;\u0026rdquo;   vorbis/Ogg type=\u0026ldquo;audio/ogg;codecs=\u0026lsquo;vorbis\u0026rsquo;\u0026rdquo;   vp8/vorbis/webm type=\u0026ldquo;video/webm;codecs=\u0026lsquo;vp8,vorbis\u0026rsquo;\u0026rdquo;   H.264/vorbis/mp4 type=\u0026ldquo;video/mp4;codecs=\u0026lsquo;avc1.42E01E,mp4a.40.2\u0026rsquo;\u0026rdquo;    References  Intro to Video Codecs 视音频编解码技术零基础学习方法 FFmpeg  "
            }
    
        ,
            {
                "id": 24,
                "href": "https://ixiaopan.github.io/blog/post/dl-04-initilisation/",
                "title": "Deep Learning - Weight Initialization",
                "section": "post",
                "date" : "2021.11.24",
                "body": "The goal of machine learning is to find the best parameters. To train the model, we must start from some values. So, what do the initial parameters look like? Can we set all of them to the same value? Or we assign random values to the weights? In this post, we will talk about another important aspect of learning — weight initialization.\nConstant Values What if we set all parameters to the same values, say $c$? Assuming that all the biases are $0$, the $m_{th}$ hidden unit in the first layer is\n$$ z_{1m} = \\sigma (cx_1+cx_2+\u0026hellip;+cx_n) $$ and the $k_{th}$ hidden unit in the second layer is $$ z_{2k} = \\sigma ( cz_{11} + cz_{12} + .. cz_{1m}) = \\sigma ( m \\cdot c \\cdot z_{1m}) $$ We find that the hidden units in the same layer are the same (the same structure, you can imagine that they are copies of one node), ultimately leading to the same gradient for weights belonging to the same hidden layer. Since both the weights and the gradients are the same, we learn nothing.\nWhat kind of weights  too small $\\rarr$ gradient vanishing, slow learning too large $\\rarr$ gradient exploding  So what kind of values should we use to initialize weights? According to [1], there are two rules of thumb:\n   The mean of the activations should be zero\n  The variance of the activations should stay the same across every layer\n   Mathematically, they can be expressed as\n$$ \\text {E}(z_{n-1}) = \\text {E}(z_n) = 0 \\\\ \\text {Var} (z_{n-1}) = \\text {Var}(z_n) $$\nwhere $n$ indicates the $n_{th}$ hidden layer and $z_n$ is the random variable representing the hidden node. Under the above two assumptions, it would avoid gradient exploding and gradient vanishing.\nBut why should the variance stay the same? Well, we find an explanation from [2]. Suppose\n all the inputs are $x_n = 1$ and all the weights follows the Gaussian distribution with a mean of $0$ and a variance of 1, that is $w_i \\sim N(0, 1)$  the hidden nodes in the first layer is given as follows (we omit the activation function) $$ h_{1m} = w_1 x_1 + w_2x_2 + .. w_nx_n = w_1 + w_2 + .. w_n $$\nNow, let\u0026rsquo;s compute the mean and variance of this new variable $h_{1m}$,\n$$ E(h_{1m}) = E(w_1 + w_2 + .. +w_n) = E(w_1) + E(w_2) + \u0026hellip; +E(w_n) = 0 \\\\ \\text{Var}(h_{1m}) = \\text{Var}(w_1 + w_2 + .. +w_n) = \\text{Var}(w_1) + \\text{Var}(w_2) + \u0026hellip; + \\text{Var}(w_n) = n $$\nGenerally, $n$ is quite large, say 100, then the standard deviation is $10$. This means, if we randomly obtain a hidden node, it will lie 10 units far away from the mean. After the activation function, say Sigmoid, the result is close to 1 or 0. As we discussed earlier, this will lead to gradient vanishing. Thus, to avoid this, we should shrink the variance.\nXavier Initializaiton Xavier Initialization works on the assumption that the variance of the activations stays the same across layers, which lead to the following initialization.\n$$ W_n \\sim N(0, \\frac{1}{m_{in}}) $$ where $m_{in}$ is the number of the input nodes. Why? Where does this value $\\frac{1}{m_{in}}$ come from? Assuming again that all the biases are $0$, we\u0026rsquo;re tring to find the variance of $W_n$ using the following equation.\n$$ \\text {Var} (z_{n+1} )= \\text {Var} (z_{n} ) = \\text {Var} ( \\text{tanh} (\\sum_{i=1}^m w_i z_i )) $$\nSince we\u0026rsquo;re just starting training, the values are so small that they are in the liner regime of $tanh$. Thus, we have $$ z = \\text {tanh(y)} = y $$\nWith this, we obtain\n$$ \\text {Var} (z_{n} ) = \\text {Var} ( \\text{tanh} (\\sum_{i=1}^m w_i z_i )) = \\text {Var} (\\sum_{i=1}^m w_i z_i ) = \\sum_{i=1}^m \\text {Var} ( w_i z_i ) $$\nNow we assume that\n $w_i$ is i.i.d $z_i$ is i.i.d $w_i$ is independent of $x_i$ $E(z_i) =0$ $E(w_i) = 0$  then we can decompose the variance of a product into a product of variances, as shown below\n$$ \\text {Var} (z_{n+1} )= \\text {Var} (z_{n} ) = \\sum_{i=1}^m \\text {Var} ( w_i z_i ) \\\\ = E^2[w_i] Var(z_i) + Var(w_i)E^2[z_i] + Var(w_i)Var(z_i) \\\\ = \\sum_{i=1}^m Var(w_i)Var(z_i) = m \\text{Var} (W_n) \\text{Var} (z_n) $$\nWe obtain\n$$ \\text{Var} (W_n) = \\frac{1}{m} $$\nOne thing worth noting is that the initial suggested variance was $\\frac{2}{m_{in} + m_{out}}$, which is the harmonic mean of the input and the output,\n$$ 2 * (\\frac{1}{m_{in}}* \\frac{1}{m_{in}} ) / (\\frac{1}{m_{in}} + \\frac{1}{m_{out}}) $$\nIf we\u0026rsquo;re using ReLu, the suggested value is $\\frac{2}{m}$, yep, multiplying by 2 the variance of Xavier, also known as He initialization.\nReferences [1] Katanforoosh \u0026amp; Kunin, \u0026ldquo;Initializing neural networks\u0026rdquo;, deeplearning.ai, 2019.\n[2] Weight Initialization Explained | A Way To Reduce The Vanishing Gradient Problem\n"
            }
    
        ,
            {
                "id": 25,
                "href": "https://ixiaopan.github.io/blog/post/dl-03-regularization/",
                "title": "Deep Learning - Regularization",
                "section": "post",
                "date" : "2021.11.22",
                "body": "In the previous post, we introduced how to initialize weights. However, weights can be very large or small (great variance in weights). For larger weights, a tiny change in data will lead to large variance. For smaller weights, it has little influence on the model, which can be totally discarded.\nRegularization is such a technique that sets an upper threshold for weights, thereby producing a set of weights with smaller variance. Ridge (L2) and Lasso (L1) are two widely used regularization methods, which can help alleviate overfitting and produce stable computation. Apart from this, dropout is another common technique used in Deep Learning. In this post, we will explain how regularization works in detail.\nRegularization L2 Ridge Take linear regression as an example, our goal is to minimize the following loss function with L2 regularization\n$$ L_{min} = \\sum_{i=0}^m (y_i - \\bold x_i^t \\bold w)^2 \\\\ \\text { subject to } \\sum_{j=1}^n \\bold w_j^2 \\le t $$ PS: $w_0$ is exclude in L2 regularization. It depends (I am not sure, but pls be careful.)\nThis is an inequality constraints optimization, and we can apply Lagrange to solve it,\n$$ L = f(x) - \\lambda g(x) \\\\ = \\frac{1}{2m} \\sum_{i=0}^m (y_i - \\bold x_i^T \\bold w)^2 + \\lambda \\sum_{j=1}^n \\bold w_j^2 - \\lambda t \\\\ = \\frac{1}{2m} (\\bold X \\bold w - \\bold y)^T (\\bold X \\bold w - \\bold y) + \\lambda \\bold w^T \\bold w $$\nwhere $\\lambda \\ge 0$ according to KKT conditions. Take the derivative of $L$ w.r.t $\\bold w$, we have\n$$ \\frac{\\partial}{\\partial \\bold w} L = \\frac{1}{m} X^T(\\bold X^T \\bold w -\\bold y) + \\lambda \\bold w $$\nThe let $\\frac{\\partial}{\\partial \\bold w} L = 0$, we obtain an analytical solution to $\\bold w$\n$$ \\bold w' = (\\bold X^T \\bold X + \\lambda I)^{-1} \\bold X^T \\bold y $$\n $\\lambda = 0$, it\u0026rsquo;s the OLS $\\lambda \\rarr \\infin$, $w' \\rarr 0$  L1 Lasso L1 is a bit different from L2. The regularization term is the sum of the absolute value of the weights, also known as the L1 norm. Unfortunately, there is no closed-form solution, so we use gradient descent to find the estimate.\n$$ L = (\\bold X \\bold w - \\bold y)^T (\\bold X \\bold w - \\bold y) + \\lambda \\sum_{d=1}^D |\\theta_d| $$\nTake the derivative of $L$ w.r.t $w_k$\n$$ \\frac{\\partial}{ w_k} L = - \\sum_{i=0}^m (y_i - \\sum_{j \\ne k}^d x_{ij} w_j - w_kx_{ik}) x_{ik} \\\\ = - \\sum_{i=0}^m (y_i - \\sum_{j \\ne k}^d x_{ij} w_j) x_{ik} + w_k \\sum_{i=0}^m x_{ik}^2 \\\\ \\ \\\\ \\sim - \\rho_k + w_k z_k $$\nwhere $z_k$ is the sum of the squared features of all the examples. If the data is normalized, $z_k = 1$\nIntuition Though L1 and L2 both shrink the weights, L1 can further reduce weights to $0$ to make feature selection automatically. Note that L2 is also known as weight decay because it encourages weight values to decay towards 0 in sequential learning.\n  Figure 1: Lasso Vs Ridge. The lasso gives a sparse solution.  From the view of Lagrange, the solution is the intersection point between the loss function and the regularization term, as Figure 1 shows. We can see that L1 is spikier than L2 in high dimension. Thus, the ellipse of the loss function is more likely to touch the sharp points in L1.\nDropout Dropout means that we randomly drop out hidden/visible units during training a neural network. The dropout rate is denoted by $p$. When a node is dropped out, all connections, including incoming and outgoing connections, are removed from the network temporarily (in each epoch, so it could be kept during the next epoch.)\nFor each layer $l$, there is a random variable $\\bold r$ following Bernoulli distribution, i.e. $r \\sim \\text {Bern} (p)$. The value of $r_l$ is either 0 or 1, so with dropout, the forward process is shown below\n$$ y_{l}' = r_{l} \\odot y_{l} \\\\ \\ \\\\ z_{l+1} = w_{l+1} y_l' + b_{l+1} \\\\ \\ \\\\ y_{l+1} = \\sigma(z_{l+1}) $$\n  Figure 2: Dropout network. Source from [3]  At test time, we do not drop out nodes, but we scale the network by $1-p$. From the below equation, we see that the expected hidden value at training time is $(1-p)h_i$ for each input node. Similarly, at test time, we hope the expected value stay the same, so we multiply each node by $1-p$. In practice, many deep learning libraries will rescale the network by multiplying $\\frac{1}{1-p}$ during training phrase, so there is no need to scale the network again when testing.\n$$ E(h_i) = (1-p) h_i + p * 0 = (1-p) h_i $$\nIntuitively, dropout is an ensemble method, we combine many different models to reduce variance and improve generalisation. Typically, $p$ is set to $0.5$ to achieve the best regularization.\nEarly Stopping Early stopping is simple and efficient regularization technique, which stops training when the validation error stops consecutive decreasing, typically we observe 5 times.\nearly_stop_num_epoch = 5 def train(): ... for epoch in range(20): if cur_loss \u0026lt; prev_loss: epoch_no_improve = 0 else: epoch_no_improve += 1 if epoch_no_improve \u0026gt;= early_stop_num_epoch: break ... Refer [1] Ridge regression and L2 regularization - Introduction\n[2] does-l2-normalization-of-ridge-regression-punish-intercept-if-not-how-to-solve\n[3] Dropout: a simple way to prevent neural networks from overfitting\n[4] Simplified Math Behind Dropout\n"
            }
    
        ,
            {
                "id": 26,
                "href": "https://ixiaopan.github.io/blog/post/nlp-ner-lr/",
                "title": "NLP - Literature Review on NER",
                "section": "post",
                "date" : "2021.11.17",
                "body": "Named entity recognition (NER) is a classical task in NLP, which aims to identify meaningful chunks of text and classify them into appropriate entity types, such as PERSON or LOCATION. For instance, \u0026ldquo;Tom\u0026rdquo; is classified as \u0026ldquo;PERSON\u0026rdquo; while \u0026ldquo;England\u0026rdquo; is labelled as \u0026ldquo;LOCATION\u0026rdquo; in the sentence \u0026ldquo;Tom comes from England\u0026rdquo;. NER serves as the foundation for a range of downstream tasks such as information extraction. In practical applications, you may notice that some websites will autofill the application form from your resume. In this series of posts, we will explain NER in detail, from models, datasets to domain adaptation. First, let\u0026rsquo;s do a brief literative review on NER.\nNER Models Traditional Method The earliest NER methods are rule-based and dictionary-based methods[1].\nRule-based How\n lexical patterns + domain knowledge + regular expressions  Pros \u0026amp; cons\n  simple\n  mannually creating rule is time-consuming\n  rules are language or domain dependent\n  Dictionary-based How\n look up a terminological dictionary  Pros \u0026amp; Cons\n  focus on the most important terms rather than all words in the field, e.g. biological field\n  only named entities occurred in dictionaries can be identified\n  dictionaries need to be updated frequently\n  Statistical-based How\n learn a statistical model composed of a set of rules derived from the annotated corpus  popular methods\n HMM CRF SVM  Neural Network The contemporary NER models are based on neural networks and various combinations of pre-trained word embedding and character embedding [2].\nPros \u0026amp; Cons\n  do not require extensive feature engineering or domain-specific resources\n  a large amount of trainning data\n  Based on word representation, neural architectures for NER can be classified into three types[3]\n  the word-based architecture\n word embedding (window approach) -\u0026gt; CNN_CRF variants of LSTM_CRF -\u0026gt; BiLSTM_CRF    the character-based architecture\n character embedding (CNN) -\u0026gt; LSTM    the hybrid architecture combining both word-level and character-level representation\n character (CNN) and word embedding -\u0026gt; BiLSTM_Softmax character (CNN) and word embedding -\u0026gt; BiLSTM_CRF character (BiLSTM) and word embedding (skip-N-gram) -\u0026gt; BiLSTM_CRF character (GRU) and word embedding (GRU) -\u0026gt; BiLSTM_CRF    Domain Adaptation Transfer learning in NLP[5] is broadly classified into four subcategories\n domain adaptation (our study area)  the source and target domain share the same language but have different word distributions   cross-lingual learning multi-task learning sequential transfer learning.  Feature-based mapping two domains into a low-dimensional space\nParameter-based  Fine-Tuning Multi-Task Learning  Multi-source Domain Adaptation The standard domain adaptation setting covers a single source domain and a single target domain with few or no labelled data. Extensions include multiple source domains and a single target domain or a single source domain and multiple target domains.\n pooling  pool all training data from all source domains and train a single model for the pooled data, ignoring the domain discrepancy.   Ensemble each model trained on each source domain MULT with shared CRF layer MUTL with private CRF layer  NER Datasets  MUC CoNLL-2002 and CoNLL-2003  a collection of newswire articles covering 4 languages: Spanish, Dutch, English, and German PER, LOC, ORG, and MISC   Twitter  collected from multiple English-speaking countries and social user from 2009 to 2019 the same entity types as CoNLL-2003   OntoNotes  a large-scale multilingual annotated corpus with multi-layer linguis- tic information like predicate structure and word sense. 3 languages (English, Arabic, and Chinese) and many genres:broadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire(NW), telephone conversation (TC), and web data (WB)    Most NER datasets only contain flat entities, but there are particular datasets that contain nested entities.\n ACE  a multilingual corpus composed of data selected from various sources, including broadcast and newswire   GENIA[4]  created for biology dut to the complicated naming conventions in biology, there are plenty of nested disease names or cell names    Summary Table    Related work NER Model Results     Rau (1991)[@rauExtractingCompanyNames1991] Rule-based method The author designed the first NER system based on hand-designed rules.   Zhou et al. (2006) [@zhouMaxMatcherBiologicalConcept2006] Dictionary-based method They proposed a new dictionary-based method that focused on important biological terms and achieved the best performance on GENIA corpus.   Bikel et al. (1999) [@bikelAlgorithmThatLearns1999] HMM-based method They designed the first NER tagger based on HMM, outperforming the rule-based method on the MUC-6 dataset.   Liu et al. (2015)[@liuEffectsSemanticFeatures2015] CRF-based method The authors implemented a drug name recognition system based on CRF, showing significant improvements compared to dictionary-based methods.   Neural Networks     Collobert et al. (2011)[@collobertNaturalLanguageProcessing2011] CNN-CRF(word-level embedding) They were the first to employ word-level representation in the neural-based NER models and achieved 89.59 F1-scores on the CoNLL-2003 English dataset.   Huang et al. (2015)[@huangBidirectionalLSTMCRFModels2015] LSTM-CRF(word-level embedding) They compared four variants of LSTM-based NER models combining pre-trained word embeddings and other hand-crafted features, and showed that BiLSTM performed the best.   Kim et al. (2015)[@150806615CharacterAware] LSTM-CNN(char-level embedding) They used character-level embedding for word representation based on CNN and demonstrated that using character-level information only is sufficient for neural-based NER models.   Chiu and Nichols (2016)[@chiuNamedEntityRecognition2016] BiLSTM-CNN(char- and word-level embedding) They concatenated char-level representation generated from CNN and pre-trained word-level embeddings as the inputs of BiLSTM, achieving 91.62 F1-scores on CoNLL-2003.   Lample et al. (2016) [@lampleNeuralArchitecturesNamed2016] BiLSTM-CRF(char and word-level embedding) They used BiLSTM rather than CNN for char-level representation and CRF to make predictions, serving as the baseline model in recent research.   Yang et al. (2016)[@yangMultiTaskCrossLingualSequence2016] GRU-CRF(char and word-level embedding) The authors replaced BiLSTM with GRU to study multi-task and cross-lingual learning, showing a slight improvement on the CoNLL-2003 English dataset.   Domain Adaptation     Lee et al. (2018)[@leeTransferLearningNamedEntity2018] INIT The INIT method based on fine-tuning was first applied in domain adaptation for NER.   Yang et al. (2017)[@yangTransferLearningSequence2017] MULT They were the first to design parameter-sharing architectures following the multi-task learning strategy to study cross-domain NER.   Lin and Lu (2018)[@lin2018neural] neural adaptation layer They directly modified the BiLSTM-CRF architecture by introducing three layers to perform domain adaptation for NER.   Jia et al. (2019)[@jiaCrossDomainNERUsing2019] language modelling The authors introduced language modelling to deal with the target domain with no labelled data. Their results on CoNLL-2003 exhibited great improvements over supervised domain adaptation methods.   Jia and Zhang (2020)[@jiaMultiCellCompositionalLSTM2020] the multi-cell structure They proposed a novel transfer method based on MULT from the aspect of the entity-type level instead of the entity-instance level. Their new model outperformed many BiLSTM models using MULT on Twitter dataset and others.   Wang et al. (2020)[@MultiDomainNamedEntity] MultDomain–SP–Aux They employed various domain adaptation methods, including INIT and MULT, to investigate the robustness of NER models when data from multiple domains are available.    References [1] Yan Wen, Cong Fan, Geng Chen, Xin Chen, and Ming Chen. 2020. A survey on named entity recognition. In Communications, signal processing, and systems, Springer Singapore, Singapore, 1803–1810.\n[2] Arya Roy. 2021. Recent Trends in Named Entity Recognition (NER). arXiv:2101.11420 [cs] (January 2021). Retrieved from http://arxiv.org/abs/2101. 11420\n[3] Vikas Yadav and Steven Bethard. 2019. A survey on recent advances in named entity recognition from deep learning models. Retrieved from http://arxiv.org/abs/ 1910.11470\n[4] J D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GENIA corpus—a semantically annotated corpus for bio-textmining. Bioinformatics 19, (July 2003), i180–i182. DOI:https://doi.org/10.1093/bioinformatics/btg1023\n[5] Sebastian Ruder. 2019. Neural transfer learning for natural language processing. Thesis. NUI Galway.\n"
            }
    
        ,
            {
                "id": 27,
                "href": "https://ixiaopan.github.io/blog/post/optimization/",
                "title": "Optimization",
                "section": "post",
                "date" : "2021.11.12",
                "body": "We\u0026rsquo;ve talked about many ML algorithms and DL architectures so far, but how to find the optimal parameters? Mathematically, the process of minimizing the objective functions is called optimization, but it\u0026rsquo;s a bit different in DL — the global minimum point does not always achieve the best generalization performance. After all, we are minimizing the training error. Besides, the loss function typically is very complex, and there is no analytical solution to it. In this case, we have only one choice — optimization algorithms.\nNewton\u0026rsquo;s Method Bascially, Newton\u0026rsquo;s method is a root-finding method, for example, the root of $f(x) = x^2 - 8$. Assuming that we start from the point $x_0$, and the intersection of the tangent line and the x-axis is the point $(x_1, 0)$, then we have\n$$ \\hat f(x_1) = f(x_0) + (x_1 - x_0) f'(x_0) $$\nlet $\\hat f(x_1)=0$,\n$$ x_1 = x_0 - \\frac{ f(x_0)}{f'(x_0)} $$\nWe can repeat this operation starting from $x_1$ until we think the precision is sufficient. Typically, we generalize these steps as follows,\n$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$ Let\u0026rsquo;s start from $x_0 = 4$, the results are shown below. We see that $x_3$ is very close to the actual value $2\\sqrt2$, so we can stop.\nx_0 =4 x_1 = 3 x_2 = 2.833333 x_3 = 2.828431 The above linear approximation holds when $x_1 - x_0$ is really small. For a larger $x_1-x_0$, we can use the second-order taylor expression. Recall that Taylor expanding around $x^*$ is\n$$ f(x) = f(x^*) + (x - x^*)f'(x^*) + \\frac{1}{2}(x-x^*)^2f''(x^*) + \u0026hellip; $$ If $x - x^*$ is sufficiently small, the higher order terms are ignored. The derivative of $f(x)$ w.r.t $x - x^*$ is $$ f'(x^*) + (x - x^*)f''(x^*) = 0 $$\nThus,\n$$ x = x^* - \\frac{f'(x^*)}{f''(x^*)} $$\nHigh Dimension For high dimension, the Taylor expansion at $\\bold x_0$ is\n$$ f(\\bold x) = f(\\bold x_0) + (\\bold x - \\bold x_0)^T \\nabla f(\\bold x_0) + \\frac{1}{2} (\\bold x - \\bold x_0)^T H (\\bold x - \\bold x_0) + \u0026hellip; $$ where $H$ is the Hessian matrix with elements $$ H_{ij} = \\frac{\\partial^2 f(\\bold x_0)}{\\partial x_i \\partial x_j} $$ So, the minimum point can be found at\n$$ \\bold x = \\bold x^* - H^{-1} \\nabla f(x^* ) $$\nPros and Cons  Efficient when we are close to the minimum. Computing Hessian matrix is time-consuming, $O(N^2)$ Liable to arrive at saddle points Loss would increase for nonconvex problems because Hessian would be negative  take absolute value change learning rate    Gradient Descent Intuition Suppose $x-x^* = \\eta f'(x^*)$ and $\\eta\u0026gt;0$ is small enough, based on the first order Taylor expansion, we obtain\n$$ f(x^* - \\eta f'(x^*)) = f(x^*) - \\eta f'^2(x^*) \\le f(x^*) $$ This means the loss would decrease when we move againt the direction of gradient. $$ x \\larr x - \\eta f'(x) $$ Pros and cons\n large learning rating would osillate; small and we make slow progress GD would get stuck at a local minimum  Batch It\u0026rsquo;s vanilla gradient descent, which computes the gradient for all the training dataset. $$ \\theta = \\theta - \\eta \\cdot \\frac{1}{N} \\sum_{i=1}^{|D|} \\nabla_\\theta J(\\theta, x_i, y_i) $$\nStochastic SGD updates parameters for each example, so it has a high variance in the change in parameter values. $$ \\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x_i; y_i) $$\nMini-batch Mini-batch updates parameters for a small batch of training data. $$ \\theta = \\theta - \\eta \\cdot \\frac{1}{|B|} \\sum_{i=1}^{|B|} \\nabla_\\theta J(\\theta, x_i, y_i) $$\n reduce variance of the parameter updates and lead to stable convergence add noise (stochasic) as we are sampling data (might help escape local-minima) taking many smaller steps of gradients reduces likelihood of divergence make use of the matrix optimization Typically, mini-batch size is the power of $2$, like 64, 128, 256  Dynamic Learning Rate Replacing $\\eta$ with a time-dependent $\\eta$\n piecewise constant  $$ \\eta(t) = \\eta_i \\text{ if } t_i \\le t \\le t_{i+1} $$\n exponential decay  $$ \\eta(t) = \\eta_0 e^{\\lambda t} $$\n polynomial decay $$ \\eta(t) = \\eta_0 (\\beta t + 1)^{-\\alpha} $$  Momentum One problem with SGD is that it could take much time if the shape of loss function is anisotropic, as shown in Figure 1. This is because we are moving along the direction of gradient, but it doesn\u0026rsquo;t point to the mnimum. In other words, in high dimension, we zig zag\n stepping consistently in directions with low curvature jumping backwards and forwards past the minimum in directions with high curvature    Figure 1: In high dimenstion, sometimes you might move a long way in some direction.  Momentum is a method to increase steps in low curvature directions and decrease it in high curvature directions by adding the gradients of the past steps to the current gradient. $\\beta$ is usually set to $0.9$.\n$$ v_{t+1} = \\beta v_t + g_t \\\\ \\theta = \\theta - \\eta \\cdot v_{t+1} $$\nExponentially weighted average $$ v_{t} = \\beta v_{t-1} + (1 - \\beta) g_t $$ correct bias\n$$ v_{t} = \\frac{v_t}{1-\\beta^t} $$\nThe exponentially weighted average allows us to obtain the average over a range of days ($\\frac{1}{1-\\beta}$) without explicitly storing the data of these data and calculting the numerical average.\nIn the case of gradient, $v_t$ tells us the weighted average of the past $\\frac{1}{1-\\beta}$ gradients. Thus, a large $\\beta$ will lead to a smooth change due to a long-range average.\nAdaGrad The idea of learning rate decay is to decrease learning rate as the learning progresses, but different parameters have different gradient. We cannot reduce learning rate in the same level for all parameters. Instead, it would be better to adapt the learning rate to the parameters.\nAdaGrad is such a method that perform smaller updates for frequent updated parameters and large updates for less frequent updates parameters by accumulating the square of the gradients of the past steps.\n$$ h \\larr h + g\\odot g \\\\ \\bold w \\larr \\bold w - \\frac{\\eta}{\\sqrt{h + \\epsilon}} \\odot g $$ Pros and Cons\n it will shink the learning rate and no longer update at some point  RMSProp $$ h \\larr \\beta h + (1 - \\beta) g\\odot g \\\\ \\bold w \\larr \\bold w - \\frac{\\eta}{\\sqrt{h + \\epsilon}} \\odot g $$\n$\\beta$ is usually set to $0.9$ and $\\eta$ is $0.001$.\nAdam Adam combines momentum and RMSprop to compute the adaptive learning rate.\n$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ h_t = \\beta_2 h_{t-1} + (1 - \\beta_2) g_t \\odot g_t \\\\ \\theta \\larr \\theta - \\frac{\\eta}{\\sqrt{h_t} + \\epsilon} m_t $$\nReferences  Newton\u0026rsquo;s Method Adam: A Method for stochastic Opimization GD/Momentum/RMSprop/Adam "
            }
    
        ,
            {
                "id": 28,
                "href": "https://ixiaopan.github.io/blog/post/recommendation/",
                "title": "Recommender Systems",
                "section": "post",
                "date" : "2021.11.07",
                "body": "Recommender system is one of the most popular studying fields in machine learning due to its wide application in our daily life. When you shop online, such as Amazon, you can see similar items just below the item you are looking at. The goal of a recommender system is to make recommendations that fit the user\u0026rsquo;s taste. In this post, we will go through the very basic concepts and several classcial techniques in the field of recommender systems.\nOverview   Figure 1: Different algorithms for recommender systems.  Recommendations can be classified as two categories, as Figure 1 shows,\n non-personalised  recommendations are the same for all users e.g. the most popular movies, the top ten hotels, \u0026hellip;   personalised  recommendations vary by users    Obviously, we focus on the latter one. The mainstream algorithms includes\n content-based filtering  recommend items based on the attribtues of items, users, or both (feature engineering) e.g. when you buy the novel \u0026ldquo;The ABC Murders \u0026ldquo;, the system will also recommend other novels written by Agatha Christie to you.   collaborative filtering  employ the wisdom of the crowd instead of extracting good attributes manually    There are three main types of collaborative algorithms, namely,\n user-based filtering item-based filtering matrix factorisation  Data Data is all we need. Figure 2 shows that a recommender system needs three types of data,\n items data user data the interaction between items and users    Figure 2: Data involved in recommender systems.  Each type of data has its own properties. Items like clothes have attributes of size and colour. Users have age and gender. The interactions between items and users include contextual attributes, such as weather, geographical location, day of the week, and so on.\nItem Content Matrix (ICM) As mentioned earlier, items have attributes. Mathematically, we can describe that relation using Item Content Matrix or ICM, where each row indicates an item and each column represents an attribute. The value of ICM usually is 1 or 0. If an item contains a specific attribute listed in ICM, the corresponding cell will be set to one, zero otherwise. But you can also set a value between 0 and 1 to describe the importance of that attribute or other numerical values for quantitative variables like year.\nUser Rating Matrix (URM) Similarly, the opinions of users on items are described mathematically through the User Rating Matrix or URM. Each row represents a user, and each column represents an item. If we have no idea the opinion of someone on an item, the corresponding value is missing (when computing, you can treat it as zero).\nImplict Ratings Ratings can be implicit or explicit. Implicit ratings are deduced by looking at how you interact with the system, for instance, the time-on-page, the viewing time of a movie, clicks, purchases, and so on. If one stops watching a movie after 10 minutes, we assume that that user does not like that movie. However, one may have to stop watching a film because of an important call.\nExplict Ratings Explicit ratings are done by asking users to rate items on a rating scale, for example, five stars for Educated. However, designing a rating scale is not an easy thing. It depends on your application, customers, and so on. As shown in the following table, a smaller and odd rating scale is generally preferable due to its simplicity and variety. In practice, we always see a one-to-five rating or \u0026lsquo;like/dislike\u0026rsquo; buttons at almost every website that needs to collect users' opinions.\n   Large rating scale Smaller rating scale Even rating scale Odd rating scale     complex and requires more effort easy to understand either negative or positive ratings, forcing people to express their opinions introduces the neutral rating, which makes people feel more comfortable   fewer ratings more ratings fewer ratings people tend to choose the neutral rating unless they have a strong positive or negative opinion    Non-personalised recs Top 10 As we said before, the top 10 perhaps is the most common type of non-personalised recs. You can see Top 10 everywhere on the Internet.\nMethod 1: Top Popular An intuitive method to get the top 10 items is to order items by the number of users that have bought, seen, or given their opinions.\n# top 10 movies movies_has_seen = Log.objects.values(\u0026#39;content_id\u0026#39;).filter(event=\u0026#39;seen\u0026#39;).annotate(Count(\u0026#39;user_id\u0026#39;)) sorted_items = sorted(movies_has_seen, key=lambda item: -float(item[\u0026#39;user_id__count\u0026#39;])) sorted_items[:10] Method 2: The Best Rated Items Another technique is to calculate the average rating for each item and retrieve the top 10 items ranked by the average rating in descending order.\nmovies_avg_rated = Rating.objects.values(\u0026#39;movie_id\u0026#39;).annotate(Avg(\u0026#39;rating\u0026#39;)) sorted(movies_avg_rated, key=lambda item: -float(item[\u0026#39;rating__avg\u0026#39;]))[:10] To calculate the average rating, we need the help of URM. Let $r_{ui}$ be the non-zero rating given by user $u$ to item $i$, and $N_i$ be the number of users who rated item $i$, then the average rating of item $i$ is given as\n$$ b_i = \\frac{\\sum_u r_{ui} }{N_i} $$\nMarket Basket Analysis Apart from the \u0026ldquo;Top 10\u0026rdquo;, another type of recommendations also appears often when you look at an item at Amazon. That is \u0026ldquo;Frequently Bought Together (People who bought this item also bought)\u0026rdquo;. Basically, this kind of recommendations are based on association rule analysis (market basket analysis), which is a technique widely used by retailers to discover associations between items. They want to know what items are frequently bought together so that they can place them in a similar manner.\nLet $I = { I_1, I_2, \u0026hellip;, I_m }$ be the items and $T = { t_1, t_2, .., t_N }$ a list of transactions.\nRule Association rules can be written in the form of \u0026ldquo;IF-THEN\u0026rdquo;, $$ \\{{ A \\}} \\rarr \\{{ B \\}} $$ where $A \\sub I$, $ B \\sub I$, and $A \\cap B = \\empty$. The above rules means that if a customer buys $A$, then he is likely to buy $B$. A and B can include many items, $$ \\{{\\text{bread},\\text{milk}\\}}\\rarr\\{{\\text{carrots},\\text{yogurt}\\}} $$\nSupport Support tells us how frequently A and B appear together by calculating the fraction of transactions that contains A and B.\n$$ \\text {supp} (A \\rarr B) = \\frac{|A \\cup B|}{|T|} $$ Thus, we can filter out the item sets that occur less frequently by setting a minimum support.\nConfidence Confidence shows the percentage in which B is bought with A.\n$$ \\text {conf} (A \\rarr B) = \\frac{\\text{supp}(A \\rarr B)}{\\text{supp}(A)} $$\nLift Lift indicates the strength of a rule.\n$$ \\text {list} (A \\rarr B) = \\frac{\\text{supp}(A \\rarr B)}{\\text{supp}(A)\\text{supp}(B)} $$\nWith these terms, now we can make an association analysis using the Apriori algorithm.\ndata = { \u0026#39;1\u0026#39;: [1,0,1,0], \u0026#39;2\u0026#39;: [0,1,1,1], \u0026#39;3\u0026#39;: [1,1,1,0], \u0026#39;4\u0026#39;: [1,0,0,0], \u0026#39;5\u0026#39;: [0,1,1,1] } index = [ \u0026#39;InvoiceNo_\u0026#39; + str(i) for i in range(4) ] basket_set = pd.DataFrame(data=data, index=index) freq_itemsets = apriori(basket_set, min_support=0.5, use_colnames=True) freq_itemsets[\u0026#39;length\u0026#39;] = freq_itemsets[\u0026#39;itemsets\u0026#39;].apply(lambda x: len(x)) freq_itemsets # 0.25 (4) 1 # since 0.25 \u0026lt; 0.5, so it\u0026#39;s not included. support\titemsets\tlength 0\t0.50\t(1)\t1 1\t0.75\t(2) 1 2\t0.75\t(3) 1 3\t0.75\t(5)\t1 4\t0.50\t(3, 1) 2 5\t0.50\t(2, 3) 2 6\t0.75\t(2, 5) 2 7\t0.50\t(3, 5) 2 8\t0.50\t(2, 3, 5)\t3 association_rules(freq_itemsets, metric=\u0026#39;confidence\u0026#39;, min_threshold=0.75) antecedents\tconsequents\tantecedent_support\tconsequent_support\tsupport confidence\tlift (1)\t(3)\t0.50\t0.75\t0.50\t1.0\t1.333333 (2)\t(5)\t0.75\t0.75\t0.75\t1.0\t1.333333 (5)\t(2)\t0.75\t0.75\t0.75\t1.0\t1.333333 (2, 3)\t(5)\t0.50\t0.75\t0.50\t1.0\t1.333333 (3, 5)\t(2)\t0.50\t0.75\t0.50\t1.0\t1.333333 PS: Here is the complete code.\nSimilarity Before introducing the personalised recs, we should be familiar with similarity. There are many ways to measure similarity or distance between two vectors, for example,\n Jaccard Distance Mahantan Distance Euclidean Distance Cosine Similarity Pearson Correlation Coefficient K-means clustering  Cosine Similarity Cosine similarity between users is defined as,\n$$ S_{uv} = \\frac{r_u \\cdot r_v}{||r_u|| * ||r_v||} $$\nwhere $r_u$ is the $u_{th}$ row of URM.\nAdjusted Cosine Similarity On the other hand, we should notice is that people use different rating scales. For example, Alice is generous to rating while Bob is a bit harsh on it, so a rating of 3 in Bob is equivalent to 5 in Alice. To deal with this, we normalise the ratings by subtracting the user\u0026rsquo;s average rating, denoted by $\\overline r_u$, as follows,\n$$ S_{uv} = \\frac{(r_u - \\overline r_u) \\cdot (r_v - \\overline r_v)}{||r_u - \\overline r_u|| * ||r_v - \\overline r_v||} $$\nPearson Correlation Coefficient The above formula is the same as Pearson Correlation Coefficient. The only difference between them is that the Pearson only works for vectors with the same size (the rated items in common between users), while the adjusted cosine similarity considers all items by treating missing values as zero.\nFor instance, below are the ratings of Alice and Bob for six films.\nAlice: [4, 5, 4, NaN, 3, 3] Bob: [3, 3, 3, 2, 4, 5] Step 1: calculating the average rating\n Alice: $(4+5+4+3+3)/5 = 3.8$ Bob: $(3+3+3+2+4+5)/6= 3.33$  Step 2: Normalising ratings\n Adjusted Cosine  Alice: [0.2, 1.2, 0.2, -3.8, -0.8, -0.8] Bob: [-0.33, -0.33, -0.33, -1.33, 0.67, 1.67] Notice that we treat $NaN$ as $0$.\n Pearson  Alice: [0.2, 1.2, 0.2, NaN, -0.8, -0.8] Bob: [-0.33, -0.33, -0.33, -1.33, 0.67, 1.67] Step 3: Calculating similarity\n Adjusted Cosine  $$ \\frac{0.2*-0.33+1.2*-0.33+0.2*-0.33+ -3.8*-1.33 +-0.80.67+-0.81.67}{\\sqrt{0.2^2+1.2^2+0.2^2+ -3.8^2+-0.8^22} \\sqrt{-0.33^23+-1.33^2 + 0.67^2+1.67^2}} \\\\ = -0.62 $$\n Pearson  $$ \\frac{0.2*-0.33+1.2*-0.33+0.2*-0.33+-0.80.67+-0.81.67}{\\sqrt{0.2^2+1.2^2+0.2^2+(-0.8)^2+(-0.8)^2} \\sqrt{-0.33^2 + -0.33^2+-0.33^2+0.67^2+1.67^2}} \\\\ = -0.75 $$\nOverlapping However, the cosine similarity is not perfect enough. Figure 3 shows that the similarity between the users who rated one item only is greater than those who have more items in common. However, the result is not convincing because there are less common in the first pair of users. In other words, the similarity is not reliable when the support is small. The support is the number of non-zero ratings of a user.\n3  Figure 6: Small support would lead to seemingly greater similarity.  So how to solve it? We add a shrink term $C$ to the denominator of the cosine to reduce the magnitude of the cosine similarity\n$$ S_{uv} = \\frac{r_u \\cdot r_v}{||r_u|| * ||r_v|| + C} $$\nAlternatively, you can also set a threshold for the minimum support to filter out the users who have rated only one or two items. The codes below return the number of overlapping elements between users. On the contrary, if there are too many overlapping users, users are so alike that it\u0026rsquo;s difficult to recommend special items (the recommendations are too general).\noverlap_matrix = URM.astype(bool).astype(int) overlap_matrix = overlap_matrix @ overlap_matrix.T Neighborhood In practice, we only consider a small set of similar users or items, which is called as the neighborhood of the target user or item.\nCluster Clustering is one technique to divide data into several similar groups, so the neighborhood of a user is the group that he belongs to. However, clustering has some drawsbacks, e.g. sensitive to the shape of data.\nKNN We have introduced KNN before. In the case of recs, for example, $K=2$ means that we only keep the most two similar items for each column.\n  Figure 4: Reduced similarity matrix after applying KNN (K=2).  Threshold Another simple way is to set a threshold for similarity. The figure below illustrates the difference between KNN and threshold.\n  [Source: Practical Recommender Systems]  Content-based Filtering Now we focus on personalised recs. We first look at content-based filtering, which is the first strategy applied in recommendations. As its name suggests, it relies on the metadata of the items without using any opinion of others, as Figure 5 shows.\n  Figure 5: Example of content-based recommendation pipeline. [Source: Practical Recommender Systems]  The core of content-based filtering is to make recommendations based on the attributes of the content. However, not all attributes are useful and equally important. Besides, some features may not be explicitly visible to us. Thus, we need to extract knowledge from the content and select features carefully.\nSimilarity We first find similarity between items by applying cosine similarity to ICM, as shown in Figure 6.\n  Figure 4: The similarity matrix calculated from ICM.  The cosine similarity is defined as, $$ S_{ij} = \\frac{\\overrightarrow I_i \\cdot \\overrightarrow I_j}{||\\overrightarrow I_i|| * || \\overrightarrow I_j||} $$ where $I_j$ indicates the $j_{th}$ item (row) in ICM.\nTF-IDF As mentioned previously, attributes are not of equal importance, so it\u0026rsquo;s essential to know what features are more important and do feature selections to improve performance. TF-IDF is a technique used to analyse the importance of something like a word in NLP. In the case of ICM, we consider each item as a document and each column as a word. Suppose we want to know the importance of an attribute, say $a$, then TF and IDF are calculated as follows,\n$$ TF_{a, i} = \\frac{|I_{ai}|}{|I_i|} \\\\ IDF_{a} = - \\text{log} \\frac{|a \\in I|}{|I|} $$\nwhere\n $I_{ai}$ is the number of $a$ appearing in the item $i$; often equal to 1 $I_i$ represents the number of attributes of item $i$ $|a \\in I|$ is the number of items containing attribue $a$ $|I|$ is the total number of items  LDA Pros and Cons Pros\n You can always get recommendations even if it\u0026rsquo;s your first visit It recommends across popularity; that is, it does not care about the popular items now  Cons\n The system is less likely to recommend new or surprising items Limited understanding of content; it\u0026rsquo;s likely to misunderstand what the customers like  Collaborative Filtering User-based User-based filtering works on the idea that we look for users with a similar taste and recommend the items they like most. So there are two questions here\n How to measure similarity between users? (See above) What to recommend to the target user? Two common ways,  Average Vote    With similarity matrix, we make a prediction for item $i$, given the user $u$\n$$ \\hat r_{ui} = \\overline r_u + \\frac{\\sum_{v \\in KNN(u)} S_{uv} (r_{vi} - \\overline r_v)}{\\sum_{v \\in KNN(u)} S_{uv}} $$\nSo, user-based CF calculates a weighted average rating of several nearest neighbours of user $u$ for item $i$.\nItem-based Likewise, the prediction using item-based CF is given as,\n$$ \\hat r_{ui} = \\overline r_u + \\frac{\\sum_{j \\in KNN(i)} S_{ij} (r_{uj} - \\overline r_u)}{\\sum_{j \\in KNN(i)} S_{ij}} $$\nwhere the adjusted $S_{ij}$ is defined as\n$$ S_{ij} = \\frac{\\sum_u (r_{ui} - \\overline r_u) (r_{uj} - \\overline r_u) }{ \\sqrt{\\sum_u(r_{ui} - \\overline r_u) ^2} \\sqrt{\\sum_u(r_{uj} - \\overline r_u) ^2}} $$\nPros and Cons Pros\n It does not need metadata about the content  Cons\n  cold start\n What items to recommend to a new user? How to recommend a new item to users?    sparse URM $$ \\text {sparsity} = 1 - \\frac{|R|}{|U||I|} $$\n  scaling can be challenging for growing datasets.\n  Matrix Factorisation Unlike user-based CF and item-based CF, matrix factorisation aims to factorise URM into two matrices. Basically, this technique tries to find the latent factors underlying the interactions between users and items. Technically speaking, SVD and Funk SVD are two common methods to achieve this goal. They look alike at first sight but are different things.\nSVD We\u0026rsquo;ve known that any matrix can be decomposed into three matrices as shown below,\n$$ A = U\\Sigma V^T $$ where\n $U^{m \\times k}$ - User factor matrix $\\Sigma^{k \\times k}$ - Diagnoal matrix $V^{n\\times k}$ - Item factor matrix  As we know, $\\Sigma$ contains singular values, which are sorted from the largest to the smallest. These values indicate the weight of the factor $k_i$, so we can remove some unimportant features by setting the corresponding singular value to zero. That\u0026rsquo;s called truncated SVD. Besides, reducing the matrix also saves memory space. From Figure 7 we can see that zero sigular values will remove the right-most columns of $U$ and the bottom-most rows of $V^T$.\n  Figure 7: The illustration of truncated SVD.  For coders, below are PyTorch implementation of SVD.\nU, S, V = torch.svd(A) S[-1] = 0 A_hat = U @ torch.diag(S) @ V.T However, there are some obvious problems with SVD. First, it\u0026rsquo;s time-consuming to calculate large matrixes. Second, we have to impute the missing values first. Usually, we impute zero cells with user\u0026rsquo;s average rating. Nevertheless, SVD is not very computationally efficient.\nFunk SVD The idea of Funk SVD proposed by Simon Funk is to compute the lower-rank approximation of a matrix by minimising the squared error loss. The difference wih SVD above is that Funk SVD only considers the known values, which means we discard the missing values in URM. Mathematically, the goal of Funk SVD is to minimise the following loss function,\n$$ \\text{min}_{p, q} \\sum_{(u, i) \\in K} \\epsilon_{ui}^2 = \\text{min}_{p, q} \\sum_{(u, i) \\in K} (r_{ui} - p_u q_i)^2 $$\nwhere\n $p_u$ is the $u_{th}$ row of the user factor matrix $U$ $q_i$ is the $i_{th}$ column of the iterm factor matrix $Q$ $r_{ui}$ is the rating of the item $i$ given by the user $u$ $K$ is the set of all known ratings  To find the solution, we first compute the derivative of $L$ w.r.t $p_u$ and $q_i$, respectively, and then use stochastic gradient descent to arrive at the minimum point. $$ p_u \\larr p_u + lr * \\epsilon_{ui} q_i\\\\ q_i \\larr q_i + lr * \\epsilon_{ui} p_u $$\ndef sgd_factorise(A: torch.Tensor, rank: int, num_epochs=1000, lr=0.01) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: m, n = A.shape U = torch.rand((m, rank)) V = torch.rand((n, rank)) for i in range(num_epochs): for r in range(m): for c in range(n): e = A[r, c] - U[r] @ V[c].T U[r] = U[r] + lr * e * V[c] V[c] = V[c] + lr * e * U[r] return U, V Regularization Alternatively, we can add regularization to Funk SVD to avoid large parameters (overfitting).\n$$ \\text{min}_{p, q} \\sum_{(u, i) \\in K} \\epsilon_{ui}^2 + \\beta (||P||^2 + ||Q||^2) $$ Now the updated rules are\n$$ p_u \\larr p_u + lr * (\\epsilon_{ui} q_i - \\beta p_u) \\\\ q_i \\larr q_i + lr * (\\epsilon_{ui} p_u - \\beta q_i) $$\nBias Finally, we need to consider the bias or intercept. As mentioned earlier, people have their own criterions from rating (user bias) and some items are naturally highly expected (item bias). So, the new predicted rating is defined as\n$$ \\hat r_{ui} = p_u q_i + \\mu + b_u + b_i $$\nwhere $\\mu$ is the global average raing, $b_u$ is the user bias, and $b_i$ is the item bias. Thus, the final loss function is defined as follows,\n$$ \\text{min}_{p, q} \\sum_{(u, i) \\in K} \\epsilon_{ui}^2 + \\beta (||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2) $$\nThe derivatives of $L$ w.r.t $b_u, b_i, p_u, q_i$ are\n$$ b_u += \\lambda (\\epsilon_{ui} - \\beta b_u) \\\\ b_i += \\lambda (\\epsilon_{ui} - \\beta b_i) \\\\ p_u \\larr p_u + lr * (\\epsilon_{ui} q_i - \\beta p_u) \\\\ q_i \\larr q_i + lr * (\\epsilon_{ui} p_u - \\beta q_i) $$\nCold Start Evaluation Baseline Predictors Quality Indicators Relevance、 Coverage、Novelty、Diversity、Consistency、Confidence、Serendipity\nMetrics Online Evaluation Direct user feedback  ques should be meaningful and non\u0026ndash;biased opinion not reliable  A/B test  monitoring user behavior difficult to set up result might be difficult to interpret  Controlled experiments Crowdsourcing Offline evaluation References  Apriori Algorithm: Know How to Find Frequent Itemsets Market Basket Analysis Recommender Systems Specialization Singular Value Decomposition vs. Matrix Factorization in Recommender Systems Matrix Factorization: A Simple Tutorial and Implementation in Python Recommender system: Using singular value decomposition as a matrix factorization approach "
            }
    
        ,
            {
                "id": 29,
                "href": "https://ixiaopan.github.io/blog/post/misc-remove-splash/",
                "title": "REMOVING ANNOYING SPLASH ADs!!!",
                "section": "post",
                "date" : "2021.10.28",
                "body": "What and Why Nowadays, you will often see a statistic or dynamic image on your screen when you launch some APPs. The image looks like the figure below with a small button to enable people to skip ads. Generally, the showing time of ads lasts for at least 3 seconds. The most disgusting thing is that there is no upper bound for this, so you have to suffer from this unless you don\u0026rsquo;t use any app at all.\nTechnically speaking, the first page you see when you open an APP is known as the splash page, which originated from the comics industry. In the beginning, they were used as a poster to explain the whole story to readers. Similarly, the purpose of splash pages in mobile applications is to enhance the impression on people. But now, splash pages are abused as a means of earning advertising profits by commercial companies, getting far away from the original intention.\nUnder the hood Before getting rid of them, we should know how they are implemented. After several hours hard working, I figured out some key points:\n images for ads are usually hosted on some special statistic servers there are specific APIs for advertising data ad/adver/preload/splash... are common words to identify ads we can control some behaviors of loading ads, such as whether to preload, play in wifi only, duration, etc. by modifying the metadata of ads that returned through APIs  Since they are both URIs, we simply reject these requests or response with an empty JSON. With the aid of some proxy tools (Quantumult X), we can rewrite the request as follows,\n^https://api\\.gotokeep\\.com/(op-engine-webapp|ads)/v\\d/ads?/preload url reject-dict ^https?://api\\.gotokeep\\.com/op-engine-webapp/v\\d/ad url reject-dict ^https://store\\.gotokeep\\.com/api/v\\d/mypage/egg url reject-dict ^https?://api\\.gotokeep\\.com/homepage/v\\d/tab url script-response-body Keep.js ^https://api\\.gotokeep\\.com/athena/v\\d/people/my url script-response-body Keep.js  Furthermore, we can even modify responses. script-response-body enables us to write javascripts to decide what content to show. It\u0026rsquo;s very useful to filter misc data appearing on the page, such as promotions, egg hunts or ads. For example, I wrote a simple Zhihu.js to change the page mode from mobile to PC and remove the annoying login modal, since you cannot use Zhihu unless you download their app.\n// PC version of zhihu: remove login modal and guidance for downloading app let body = $response.body identifier = \u0026#39;\u0026lt;/script\u0026gt;\u0026lt;/head\u0026gt;\u0026#39; insert_pos = body.indexOf(identifier) if (insert_post\u0026gt;-1){ work = \u0026#39;setTimeout(function(){ const openInAppButton = document.querySelector(\u0026#34;.OpenInAppButton\u0026#34;); if (openInAppButton) { openInAppButton.style.display = \u0026#34;none\u0026#34;; } const closeBtn = document.querySelector(\u0026#34;.Modal-closeButton\u0026#34;); if (closeBtn) { closeBtn.click() }; const btns = document.querySelectorAll(\u0026#34;.ModalExp-modalShow .ModalWrap-itemBtn\u0026#34;); if (btns \u0026amp;\u0026amp; btns.length \u0026amp;\u0026amp; btns[1]) { btns[1].click(); }}, 2500)\u0026#39;; body = body.slice(0, insert_pos) + work + body.slice(insert_pos) } $done({ body: body }) Get Free From it I have sort out several rules for commonly used APPs — just for personal use (You can find it on my Github). Hopefully, it can help you get free from the annoying splash ads.\n"
            }
    
        ,
            {
                "id": 30,
                "href": "https://ixiaopan.github.io/blog/post/metrics-classification/",
                "title": "Metrics for classification",
                "section": "post",
                "date" : "2021.10.26",
                "body": "In linear regression, we choose $R^2$ as our metrics to measure how good our algorithms are. But how about classification? In this post, we are going to talk about several commonly used metrics for classification.\nMetrics We first consider the binary classification, where an example must be classified as either A or B. Thus, there are four possible cases,\n the ground truth is A, and the predicted lable is A the ground truth is A, but the predicted lable is B the ground truth is B, and the predicted lable is B the ground truth is B, but the predicted lable is A  Often, we use a confusion matrix to contain all these conditions, as Table 1 shows.\nConfusion Matrix    predicted\\ground truth P N     P TP FP   N FN TN    Based on the confusion matrix, we can derive some useful metrics to measure model performance from various aspects.\nAccuracy Accuracy tells us how well a classifier is in general by considering both positive and negative examples that are correctly classified.\n$$ \\text{Accuracy} = \\frac{TP + TN}{ TP + FP + FN + TN} $$\nThe drawbacks of accuracy are obvious. When we have imbalanced data where the number of negative examples is much more than positive examples, TN tends to be dominant, making no sense in most cases. For example, the number of O is more than the number of named entities in NER, resulting 99% accuracy. However, it does not indicate our classifier works well. By contrast, it may not be able to recognize any entity at all!\nPrecision Precision tells us how many true positive examples are in all predicted positive examples, which is calculated as follows,\n$$ \\text{ Precision } = \\frac{TP}{ TP + FP} $$\nFrom the above equation, we can see that larger TP and smaller FP will lead to higher precision. That makes sense because more examples that belong to either the class of interest or the negative are identified correctly. (PS: FP is also known as Type-1 Error.)\nSpam detection is a classical binary classification. It\u0026rsquo;s fine if several spam emails are escaped from our classifier. However, if some non-spam (or ham) emails are identified as spam, that\u0026rsquo;s really a stupid decision — we would miss some important emails! Similarly, precision are more important in recommender systems, where wrong recommendation would bring bad experience to customers.\nTPR/Recall Sometimes, we would prefer to identify all cases from the actual positive classes even at a cost of lower precision. That is, we seek a higher recall (sensitiviy, TPR), which is shown below.\n$$ \\text { Recall } = \\frac{TP}{ TP +FN } $$\nFor example, it does not cause much serious impacts if people who are not pregnant were diagnosed as pregnant. However, if women who are pregnant were not identified, well, you are in a big trouble. (PS: FN is also called Type-II Error.)\nI\u0026rsquo;m often confused with the two metrics. Which one should I focus more on ? Precision or recall?\n FP is a higher concern, then it\u0026rsquo;s precision; otherwise, it\u0026rsquo;s recall  You can also consider in this way — whether the examples of interest are not identified (or recalled) result in little or no consequence,\n if yes, then it\u0026rsquo;s precision; otherwise, it\u0026rsquo;s recall.  One or two spam emails in our inbox is acceptable but one neglected patient with illness can cause a big flu!\nSpecificity Like Recall, we can also define specificity — out of all the negative classes, how much we predicted correctly.\n$$ \\text { Specificity } = \\frac{TN}{ FP + TN } $$\nF1-Score F1-Score is the harmonic mean of precision and recall. F1-Score is preferable if we want to balance the precision and recall.\n$$ \\text {F1-score} = 2 * \\frac{ \\text {precision} * \\text {recall} }{\\text {precision} + \\text {recall}} $$ It is an effective metric in the following cases:\n FP and FN are equally cost True Negative is high Adding more data doesn\u0026rsquo;t effectively change the outcome  ROC-AUC The ROC (Receiver Operator Characteristic) curve is plotted with TPR (True Positive Rate or Recall) against the FPR (False Positive Rate) at various threshold values, where TPR is on the y-axis and FPR is on the x-axis.\n A greater FPR means higher the number of FP than TN A smaller TPR means higher the number of FN than TP  Thus, the ROC curve depicts the trade-off between Type-I and Type-II Error. Below are code snippets of the implementation of ROC, and the plot is shown in Figure 1.\ndef ROC(yp1, yp2): pmin = np.min(np.array( (np.min(yp1), np.min(yp2)) )) pmax = np.max(np.array( (np.max(yp1), np.max(yp2)) )) print(f\u0026#39;min: {pmin}, max:{pmax}\u0026#39;) iters = 50 thRange = np.linspace(pmin, pmax, iters) ROC = np.zeros((iters, 2)) accuracy = np.zeros(iters) for i in range(iters): threshold = thRange[i] # C_2 is positive while C_1 is negative, and if y \u0026gt; threshold, then it\u0026#39;s positive TP = np.sum(yp2 \u0026gt; threshold) / len(yp2) FP = np.sum(yp1 \u0026gt; threshold) / len(yp1) ROC[i, :] = [ TP, FP ] accuracy[i] = np.sum(yp2 \u0026gt; threshold) + np.sum(yp1 \u0026lt; threshold) # auc = -1 * np.trapz(ROC[:, 0], RPC[:, 1]) return ROC, accuracy roc, accuracy = ROC(yp1, yp2) fig, ax = plt.subplots(figsize=(6,6)) ax.plot(roc[:,1], roc[:,0], c=\u0026#39;m\u0026#39;) # FP vs TP ax.set_xlabel(\u0026#39;False Positive\u0026#39;) ax.set_ylabel(\u0026#39;True Positive\u0026#39;) # ax.set_title(f\u0026#39;ROC curve, AUC = {auc:.2f}\u0026#39; ) ax.grid(True)   Figure 1: The ROC Curve.  AUC (Area Under Curve) is the area under the ROC curve. The value is between 0 and 1. The greater the AUC, the better is our classifier.\n  If two classes are well separated, the auc is close to 1\n   Usually, two classes overlap, so we get ROC like the one below\n   If they overlap further so that the AUC is approximately 0.5, our model has no ability to distinguish them\n   The worst case is that the model gives the contrary prediction as shown below\n   Multiclass In the case of the multiclass classification, we have the corresponding confusion matrix for each class based on the one-vs-all methodology, as Figure 2 shows. Then, there are two choices we\u0026rsquo;re facing\n macro-averaging  combine all classes and average them a more balanced overview   micro-averaging  pool the corresponding group from each class the result would be dominated by frequent classes      Figure 2: Metrics for multiclass classification  References  Classification Metrics Every Data Scientist Must Know The philosophical argument for using ROC curves "
            }
    
        ,
            {
                "id": 31,
                "href": "https://ixiaopan.github.io/blog/post/dl-06-rnn/",
                "title": "Deep Learning - RNN",
                "section": "post",
                "date" : "2021.09.26",
                "body": "When I first learned RNN, I found lots of articles about it. That\u0026rsquo;s good news, but most of them focus on the design of the architecture.The network itself is not difficult to understand, the real problem is why it works. What\u0026rsquo;s the intuition behind it? Why do we need \u0026ldquo;recurrence\u0026rdquo;? Unfortunately, few articles explain it clearly. Luckily, the book Deep Learning gives the answers.\nRNN Sequence Modeling Recurrent neural networks (RNN) is another kind of neural networks, which are good at processing sequential data, such as stock prices, text, and audio. Machine translation and speech recognition are typical applications. So, what is special about sequential data? Why not still use MLP? Why do we bother designing another type of network to suit it?\nQ1: the characteristics of sequential data Sequential data consists of a series of data points across time, which have a dependency on each other. In CNN, we feed a single image into the network each time, but the order of these images doesn\u0026rsquo;t matter. Each image is an independent individual. However, when we process text, the order of words matters. For example, \u0026ldquo;I like running\u0026rdquo; is a correct sentence while \u0026ldquo;Running like I\u0026rdquo; is wrong. More importantly, there are dependencies between words. We understand the meaning of a piece of text or audio based on the understanding of the previous words.\nQ2: Why not MLP? Remember that there are two main reasons why we use CNN rather than MLP\n MLP cannot capture spatial information  Flattening will cause the loss of spatial information of image pixels, especially there are multiple channels (depth). For example, the colour of a pixel can be estimated by considering the nearest neighbouring pixel\u0026rsquo;s value   Repeated pattern is hard to detect  The object to be detected (e.g. Cats) can appear anywhere in the image. Actually, we do not care about locations but the pattern.    Well, the reasons why MLP is not suitable for sequential data are similar.\n Spatial information in sequential data means the order of data and dependencies among data.  Each word is considered as an input feature when using MLP, but the representation of each word does not contain dependencies between words.   A sentence can be written in different ways using the same words.  For example, \u0026ldquo;Today, I am going to watch a movie.\u0026rdquo; is the same as \u0026ldquo;I am going to watch a movie today.\u0026rdquo; When we feed each word into MLP, \u0026ldquo;Today\u0026rdquo; will be assigned with different weights.   Sentence length is variable.  Q3: What kind of networks is suitable for sequential data? From the above analysis, we conclude that a network that is capable of modeling sequential data should be able to\n handle with variable sequence length retain the order of data and dependencies between data detect repeated patterns  Architecture So, how RNN works? Figure 1 shows that RNN is somehow a kind of state machine, which only accepts two inputs:\n the previous system state $h_{t-1}$ the current input $x_t$    Figure 1: The architecture of RNN.  Specifically, RNN is defined as\n$$ h_t = \\sigma (W_x x_t + W_h h_{t-1} + b_h) \\\\ o_t = W_y h_t + b_y \\\\ \\hat y_t = \\sigma(o_t) $$ First, unlike the Feed Forward Networks that requires fixed length of the inputs, RNN can accormadate variable sequence length by looping. Second, the input data are fed into the network sequentially to maintain the order. At each time $t$, the system will reach a new state $h_t$ by receiving the previous state $h_{t-1}$ and the current input $x_t$. And the new state will be fed into the network recursively. In other words, at each time $t$, the system has already known what happend before through $h_{t-1}$. Finally, the weights of RNN are shared ( $W_t, W_h, W_y, b_h, b_y$ ) each time step to capture the repeated pattern.\nBPTT Traning a RNN is nothing special. First, let\u0026rsquo;s define the loss function. For a sequence whose length is $T$, the loss is computed as\n$$ L = \\sum_{t=1}^T L_t (\\hat y_t, y_t) = \\sum_{t=1}^T -y_t \\text{log}\\hat y_t = \\sum_{t=1}^T -y_t \\text{log} \\sigma(o_t) $$ where $\\sigma$ is the softmax function. The computation graph is shown below.\n  Figure 2: Computation Graph of RNN  $W_y$ First, we compute the gradient of $L$ w.r.t $W_y$. From Figure 2, we can see that $W_y$ has three parents $o_0, o_1, o_2$. We can generalize it to $T$ time steps $$ \\frac {\\partial L}{\\partial W_y} = \\sum_{t=1}^T \\frac {\\partial o_t}{\\partial W_y} \\frac {\\partial L}{\\partial o_t} = \\sum_{t=1}^T \\frac {\\partial o_t}{\\partial W_y} \\frac {\\partial \\hat y_t}{\\partial o_t} \\frac {\\partial L}{\\partial \\hat y_t} $$\nNow the question is — what\u0026rsquo;s the derivative of $L$ w.r.t $o_t$?\n$$ \\frac {\\partial L}{\\partial o_t} = -y_t \\frac{1}{\\sigma(o_t)} \\frac{\\partial \\sigma(o_t)}{\\partial o_t} - \\sum_{i \\neq t} ^N y_i \\frac{1}{\\sigma(o_i)} \\frac{\\partial \\sigma(o_i)}{\\partial o_t} $$\nGenerally, $\\sigma$ is the softmax function, so the derivative of $\\sigma$ w.r.t $o_i$ is given as\n$$ \\frac{\\partial \\frac{e^{o_t}}{\\sum_{k=1}^K e^{o_k}}}{\\partial o_i} = \\frac{ \\frac{\\partial e^{o_t}}{\\partial o_i} \\sum_{k=1}^K e^{o_k} - e^{o_i} e^{o_t} }{ [\\sum_{k=1}^K e^{o_k} ]^2} $$\nHere, we use the derivative of $f(x) = \\frac{g(x)}{h(x)}$ directly.\n$$ f'(x) = \\frac{g'(x)h(x) - h'(x)g(x)}{h^2(x)} $$\nFor $i = t$, we have\n$$ \\frac{\\partial \\sigma(o_t)}{\\partial o_t} = \\sigma(o_t) (1 - \\sigma(o_t)) $$ For $i \\ne t$, we have\n$$ \\frac{\\partial \\sigma(o_i)}{\\partial o_t} = - \\sigma(o_i) \\sigma(o_t) $$ Thus,\n$$ \\frac {\\partial L}{\\partial o_t} = -y_t \\frac{1}{\\sigma(o_t)} \\sigma(o_t) (1 - \\sigma(o_t)) + \\sum_{i \\neq t} ^N y_i \\frac{1}{\\sigma(o_i)} \\sigma(o_i) \\sigma(o_t) = -y_t (1 - \\sigma(o_t)) + \\sum_{i \\neq t} ^N y_i \\sigma(o_t) \\\\ = -y_t + y_t \\sigma(o_t) + \\sum_{i \\neq t} ^N y_i \\sigma(o_t) = \\sigma(o_t) - y_t $$\nFinally, we have $$ \\frac {\\partial L}{\\partial W_y} = \\sum_{t=1}^T \\frac {\\partial o_t}{\\partial W_y} \\frac {\\partial L}{\\partial o_t} = \\sum_{t=1}^T (\\hat y_t - y_t) \\otimes h_t $$\nwhere $\\otimes$ is the outer product of vectors.\n$W_h$ Similarly, the gradient of $L_2$ w.r.t $W_h$ is the sum of gradients of all nodes that are the parents of $W_h$.\n$$ \\frac {\\partial L_2}{\\partial W_h} = \\frac {\\partial h_0}{\\partial W_h} \\frac {\\partial L_2}{\\partial h_0}+ \\frac {\\partial h_1}{\\partial W_h} \\frac {\\partial L_2}{\\partial h_1} + \\frac {\\partial h_2}{\\partial W_h} \\frac {\\partial L_2}{\\partial h_2} \\\\ \\ \\\\ = \\frac {\\partial h_0}{\\partial W_h} \\frac {\\partial h_1}{\\partial h_0} \\frac {\\partial h_2}{\\partial h_1}\\frac {\\partial L_2}{\\partial h_2} + \\frac {\\partial h_1}{\\partial W_h} \\frac {\\partial h_2}{\\partial h_1} \\frac {\\partial L_2}{\\partial h_2} + \\frac {\\partial h_2}{\\partial W_h} \\frac {\\partial L_2}{\\partial h_2} \\\\ \\ \\\\ = \\sum_{i=0}^2 \\frac {\\partial h_i}{\\partial W_h} (\\prod_{j=i}^2 \\frac {\\partial h_{j+1}}{\\partial h_j} ) \\frac {\\partial L_2}{\\partial h_2} \\\\ \\ \\\\ = \\sum_{i=0}^2 \\frac {\\partial h_i}{\\partial W_h} (\\prod_{j=i}^2 \\frac {\\partial h_{j+1}}{\\partial h_j} ) \\frac {\\partial \\hat y_2}{\\partial h_2} \\frac {\\partial L_2}{\\partial \\hat y_2} $$\nSo, the gradient of $L$ w.r.t $W_h$ is\n$$ \\frac {\\partial L}{\\partial W_h} = \\sum_{t=0}^T \\sum_{i=0}^t \\frac {\\partial h_i}{\\partial W_h} (\\prod_{j=i}^t \\frac {\\partial h_{j+1}}{\\partial h_j} ) \\frac {\\partial \\hat y_t}{\\partial h_t} \\frac {\\partial L_t}{\\partial \\hat y_t} $$\n$W_x$ Similarly, the gradient of $L$ w.r.t $W_x$ is defined as follows,\n$$ \\frac {\\partial L}{\\partial W_x} = \\sum_{t=0}^T \\sum_{i=0}^t \\frac {\\partial h_i}{\\partial W_x} (\\prod_{j=i}^t \\frac {\\partial h_{j+1}}{\\partial h_j} ) \\frac {\\partial \\hat y_t}{\\partial h_t} \\frac {\\partial L_t}{\\partial \\hat y_t} $$\nTruncated BPTT Vanishing/Exploding Gradient Problem The main problem of RNN is the well-known vanishing/exploding gradient. Why? From the above equations, we can see that it could be due to the repeated multiplication of $\\frac {\\partial h_{j+1}}{\\partial h_j}$.\nRemember that $h_{j+1}$ is derived from\n$$ h_{j+1} = \\text{tanh} (W_h h_j + W_x x_{j+1} + b_h) $$ Thus, the derivative is\n$$ \\frac {\\partial h_{j+1}}{\\partial h_j} = \\text{diag} [\\text{tanh}'(o_j)] W_h $$\nSo, the gradient is determined by the weights and the derivatives of the activation functions. (PS: the result is a Jacobian matrix because we are taking the derivative of a vector function w.r.t a vector.)\nFirst, the activation function is usually tanh or sigomid funtion, and they have the following properties:\n the derivatives are always less than 1 the derivatives tend to be saturated and close to zero when the input are far away from zero.  Thus, with small values in the matrix and multiple multiplications, the gradient will shrink quickly.\nSecond, if $W_h$ overpowers $\\text{tanh}'(o_j)$, the gradient value can be inferred by the eigenvalues of $W_h$, as the below equation shows,\n$$ \\prod_{j=i}^t \\frac {\\partial h_{j+1}}{\\partial h_j} = Q \\Lambda^{t-i} Q^{-1} $$\n If the largest eigenvalue of $W_h$ is greater than 1, the gradient will grow quickly and go to infinity On the contrary, if it is less than 1, the gradient will shrink exponentially  Identify vanishing/exploding So, there are some signals that might indicate that we are suffering from vanishing or exploding\nvanishing\n the parameters of the deepest layers changes greatly but the parameters of the front layers change little the model learns slowly  exploding\n the parameters changes exponentially the parameters might be NaN  Solution Gradient Clipping There is a specific solution to mitigate the exploding gradient —— gradient clipping. Since the gradient is too huge, it\u0026rsquo;s likely to move out of the parameter plane. An intuitive way is to control the size of the gradient (threshold), which is denoted by $\\eta$. The intuition is to take a step in the same directions, but a smaller step.\n$$ \\text{if } ||g|| \\ge \\eta: g = \\eta \\frac{g}{||g||} $$\nNon-saturating Activation Function Instead of using sigmoid and tanh functions that prone to be saturated, we use non-saturating functions, such as ReLu.\nWeight Initialization Batch Normalization The purpose of normalization is to transform data into a fixed range by translation and scale. It is a common technique used in data preprocessing. Min-max and z-score are two common normalization methods. Batch normalization means normalization performed on a batch of data rather than the whole data as we train data by group in NN. Below is the algorithm of batch normalization,\n$$ \\mu = \\frac{1}{|B|} \\sum x_i \\\\ \\sigma^2 = \\frac{1}{|B|}\\sum(x_i - \\mu)^2 \\\\ x_i' = \\frac{x_i - \\mu}{\\sqrt { \\sigma^2 + \\epsilon}} \\\\ y_i = r x_i' + \\beta $$\nFigure 3 shows what batch normalization does. Before normalization, $x$ could be anywhere in the input space. After normalization, we restrict the input space to a small range (the shadow area), preventing $x$ from reaching the edges of the sigmoid function. In doing so, the gradient of the sigmoid function is unlikely to be small value.\n  Figure 3: Sigmoid function with normalized inputs  Pros\n avoid vanishing gradient  Cons\n depends on the batch size, small batch size may cause unstable results not suitable for dynamic networks, e.g. RNN (different time steps)  Re-design Networks Some effective new networks are residual networks or gated RNNs.\nGated RNN Intuition The introduce of gate in gated RNNs can be considered in this way: the input data at each time step are not of equal importance. For example, in a sentence, the first word might be more important than the second word, so we\u0026rsquo;d better to keep the previous system stable and not change too much at the second time step. Thus, we need some controls on the current system state and the current input data, determining how much they could affect on the system.\nStrictly speaking, the purpose of the gates is to ensure the integrity of information, as Written Memories: Understanding, Deriving and Extending the LSTM said. In RNN, $h_t$ is paricipated in the creation of new information at each time step. However, the new state will be ultimately fed into the non-linear function, such as tanh, causing the problem of information morphing. Therefore, it is difficult to decode the past information even if it is included in the current system state due to the distortion of information.\nHow to solve it? We explicitly add and subtract information from the system. We also need a read interface to avoid information overload. This is because the system contains so much information that not all of them are useful to the current time step. Moreover, we should read something first before writing, because we know nothing about the current system (we shall know something before writting). Otherwise, we run the risk of overwriting the system without having the old information (break the incremental change).\nAt the very beginning, we can read from the initial state. After that, we read the previous state ($s_{t-1}$), then decide what new information to write based on it. The new information is known as the candidate state, denoted by $\\widetilde s_t$. We selectively forget some useless information (sbstraction), and update the system using the candidate state (addition). In doing so, we can ensure that the change in the system state is incremental, $$ s_t = s_{t-1} + \\Delta s_t $$\n  Figure 4: The Prototype LSTM. Source: Written Memories.  Figure 4 depicts the internal structure of the initial LSTM. Below are the complete equations from Written Memories\n$$ f_t = \\sigma (W^fs_{t-1}+ U^fx_t + b^f) \\\\ i_t = \\sigma (W^i s_{t-1} + U^ix_t + b^i) \\\\ o_t = \\sigma (W^o s_{t-1} + U^o x_t + b^o) \\\\ \\ \\\\ \\widetilde s_t = \\phi (W(o_t \\odot s_{t-1}) + Ux_t + b) \\\\ s_t = f_t \\odot s_{t-1} + i_t \\odot \\widetilde s_t $$\nLSTM LSTM, short for long short-term memory, is designed to retain long-time dependencies. The main differences with vanilla RNNs are the cell state and the design of three gates, as shown in Figure 5.\n  Figure 5: The internal structure of LSTM   There are three gates in LSTM, namely, forget gate, input gate and output gate. The value of gates is between 0 and 1, which is often achieved using the sigmoid function. 0 means nothing will flow through the gate, while 1 indicates all information will pass.  $$f_t = \\sigma (W_x^fx_t + W_h^fh_{t-1} + b^f)$$ $$i_t = \\sigma (W_x^ix_t + W_h^ih_{t-1} + b^i)$$ $$o_t = \\sigma (W_x^ox_t + W_h^oh_{t-1} + b^o)$$   The new information at the current step is the same as the vanilla RNN  $$g_t = \\text{tanh} (W_x^gx_t + W_h^gh_{t-1} + b^g)$$   The cell state, denoted by $C_t$, is the horizontal line across LSTM. It contains the whole information of the system and is visible to LSTM only. As mentioned above, we can control the extent to which we want to forget the previous information and how much the new information to be added. For example, if $f$ is close to 1 and $i$ is close to 0, the previous information retains, which means the inputs at the current step might not be so important (the first word might be more important than the second word)  $$C_t = f_t \\odot C_{t-1} + i_t \\odot g_t$$   The current hidden state (used for making decision) is derived from the cell state and the current inputs.  $$ h_t = o_t \\odot \\text{tanh} (C_t) $$\nUnlike the prototype LSTM, the real LSTM in practice has several differences.\n the state to be used for writting has already been ready, that is the hidden state $h_{t-1}$ ( $h_{t-1} = o_{t-1} \\odot s_{t-1}$) we can immediately obtain the candidate write $g_t$, and then update the main state $c_t$ finally, we obtain the next hidden state from $c_t$ using $o_t$  So why does LSTM mitigate the problem of vanishing gradient? The reason why gradient vanishes is the recursive multiplication of weight and derivative of the activation functions, as explained above. However, in LSTM, gradient is calculated by addition instead of multiplication, as shown below.\n$$ \\frac{\\partial C_t}{\\partial C_{t-1}} = \\frac{\\partial f_t \\odot C_{t-1}}{\\partial C_{t-1}} + \\frac{\\partial i_t \\odot g_t}{\\partial C_{t-1}} \\\\ = \\frac{\\partial f_t }{\\partial C_{t-1}} \\odot C_{t-1} + \\frac{\\partial C_{t-1} }{\\partial C_{t-1}} \\odot f_t + \\frac{\\partial i_t \\odot g_t}{\\partial C_{t-1}} \\\\ = f_t + C_{t-1} \\odot \\frac{\\partial f_t}{\\partial h_{t-1}}\\frac{\\partial h_{t-1}}{\\partial C_{t-1}} + g_t \\odot \\frac{\\partial i_t}{\\partial h_{t-1}}\\frac{\\partial h_{t-1}}{\\partial C_{t-1}} + i_t \\odot \\frac{\\partial g_t}{\\partial h_{t-1}}\\frac{\\partial h_{t-1}}{\\partial C_{t-1}} \\\\ = f_t + C_{t-1} \\odot \\sigma' W_h^f o_{t-1} \\odot \\text{tanh}'(C_{t-1}) + g_t \\odot \\sigma' W_h^i o_{t-1} \\odot \\text{tanh}'(C_{t-1}) + i_t \\odot \\sigma' W_h^g o_{t-1} \\odot \\text{tanh}'(C_{t-1}) $$\nLet\u0026rsquo;s compare it with the vanilla RNN\n$$ \\frac {\\partial h_{j+1}}{\\partial h_j} = \\text{diag} [\\text{tanh}'(o_j)] W_h $$\nFrom the above equations, we can see that the gradient in LSTM is more flexible than RNN in two aspects\n $f_t, i_t, g_t$ are learned from the current time step gradient can vary at each time step in LSTM (at step 1, it could be greater 1; at step 2, it might be less than 1), while it\u0026rsquo;s either less than 1 or greater than 1 in RNN  In this way, it is possible for LSTM to retain gradient for a long time.\nGRU There are many variants of LSTM, and one of them is GRU, as Figure 6 shows.\n  Figure 6: The internal structure of GRU  From Figure 6, we see that there are 2 gates and only one state in GRU.\n the reset gate $r_t$, which is also the read gate in the prototype LSTM the update gate $z_t$, which is the concise version of forget ($z_t$) and update ($1-z_t$) gates in the prototype LSTM  Thus, the equations in GRU can be derived as follows,\n$$ r_t = \\sigma (W^rh_{t-1}+ U^rx_t + b^r) \\\\ z_t = \\sigma (W^z h_{t-1} + U^zx_t + b^z) \\\\ \\ \\\\ \\widetilde h_t = \\phi (W(r_t \\odot h_{t-1}) + Ux_t + b) \\\\ h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\widetilde h_t $$\nBiRNN References  Exploding and Vanishing Gradients Backpropagation Through Time for Recurrent Neural Network Fancy RNN Written Memories: Understanding, Deriving and Extending the LSTM The Vanishing Gradient Problem Why LSTM Solves the Gradient Vanishing Problem of RNN And of course, LSTM — Part II Deriving the backpropagation equations for a LSTM Deriving LSTM Gradient for Backpropagation "
            }
    
        ,
            {
                "id": 32,
                "href": "https://ixiaopan.github.io/blog/post/dl-05-cnn/",
                "title": "Deep Learning - CNN",
                "section": "post",
                "date" : "2021.09.23",
                "body": "Convolutional Neural Networks (CNN) are widely used for image classification, object detection and other tasks related to images or videos in the field of computer vision. Plenty of architectures based on convolution operation have been proposed in recent years, such as AlexNet and ResNet. So, how do CNNs work? Why not use MLP? In this post, we will go through the fundamentals of CNN and several ConvNets to develop a big picture of CNN.\nConvolution Convolution Operation As its name suggests, CNN employes a special mathematical operation called convolution rather than matrix multiplication in the networks. So, what\u0026rsquo;s the convolution? Mathematically, the convolution operation is defined as\n$$ s(t) = (x \\ast w)(t) = \\int x(a) w(t - a)da $$ where $x$ is the real valued function of $t$, $w$ is a weighting function that measures the weight of $x$ at the point of $t$. So, convolution is a weighted average operation. For example, $x$ returns the real-time position of a plane, but the returned positions have some noises. A solution is to average the positions of recent moments. Obviously, the most recent moment has the highest weight. For functions that return discrete values, the discrete convolution operation is defined as\n$$ s(t) = (x \\ast w)(t) = \\sum_{a = -\\infin}^{\\infin} x(a) w(t - a) $$\nCross-correlation The above equation considers only one variable. However, in machine learning, we often deal with multi-dimensional data. In this case, the convolution is simply the sum of the products from all dimensions,\n$$ S(i, j) = (I \\ast K)(i, j) = \\sum_{m}\\sum_{n} I(m, n)K(i - m, j - n) $$\nwhere $I$ is a two-dimensional input array, and $K$ is a two-dimensional weighting matrix, also known as a kernel. Careful people will notice that $K$ is flipped when multiplying $I$. Suppose $I$ represents an image whose width is $m$ and height is $n$, as $m$ increases, the index of $I$ increases while the index of $K$ decreases. That is, the left part of the image multiplies the right part of the kernel.\nIn deep learning, flipping a kernel or not is unnecessary, since the kernel is obtained through learning. If it should be flipped, then the learned kernel is flipped. Thus, there is no need to explicitly specify a flipped kernel, let alone we do not know what it is exactly. In fact, many deep learning frameworks implement a similar function and call it as convolution. This function is cross-correlation, which is the same as convolution but without flippling\n$$ S(i, j) = (I \\ast K)(i, j) = \\sum_{m}\\sum_{n} I(i+m, j+n)K(m, n) $$ In CNN, $I$ and $K$ are often referred to as the input and the kernel, respectively. The output is often referred to as the feature map.\nReceptive Field In CNN, the scalar obtained from the cross correlation is referred to a neuron. The area enclosed by relevant inputs is called the receptive field of that neuron. So, convolution is simply an element-wise multiplication of learned weights (kernel) across a receptive field, which is repeated at various positions across the input. Normally, we also add a bias term for each filter.\n  Figure 1: Receptive Field of a neuron  Why CNN Okay, but we still do not know why to use CNN instead?\nSparse connectivity In MLP, all input variables are connected to a hidden neuron. Suppose there are $m$ features and $h$ hidden units, we have $m * h$ parameters to learn. If we restrict the number of connections, say $k$, then there are only $k * h$ parameters. Generally, $k$ is far smaller than $m$, which greatly improves the efficiency of learning. This technique can be easily achieved in CNN through a kernel whose size is smaller than the input. So, sparse connectivity in CNN means\n reduce memory space to store the parameters reduce the number of computation operations increase the efficiency of learning  Parameter sharing Another difference is that kernels in CNN are shared. Instead of different $k$ parameters for each hidden neuron in the traditional neural networks, there are only $k$ shared parameters for each element in each feature map (for now, we only consider one channel).\nTranslation equivariance Translation equivariance indicates that CNN should get the same result from the same pattern, no matter where the pattern is. Mathematically, it can be expressed as,\n$$ I(T(x)) = T(I(x)) $$ where $T$ represents a transformation function, and $I$ is the mapping function. If the object of interest in the inputs moves, the corresponding representation should move the same amount in the feature map due to parameter sharing.\nThis works based on assumption that we want to detect a specific pattern in an image, irrelevant to locations. Suppose there are two of the same cats but in different positions in an image, it is redundant to learn the same parameters to classify them as cats. As a human, we know the second one is also a cat, because we see that it has whiskers, furs, ears, tails and other characters of being a cat. Thus, we hope the kernels or convolutions will also be able to extract these features, regardless of positions. As long as an area contains these features, it will be classified as cats.\nPadding and Stride Now let\u0026rsquo;s see how a kernel really works. Figure 2 depicts the convolution operation between a two-dimensional array and a kernel with the size of $2 \\times 2$ .\n  Figure 2: Convolution Operation  The output tensor is derived as follows\n$$ 1 * 1 + 2 * 3 + 4 * 2 + 5 * 4 = 35 \\\\ 2 * 1 + 3 * 3 + 5 * 2 + 6 * 4 = 45 \\\\ 4 * 1 + 5 * 3 + 7 * 2 + 8 * 4 = 36 \\\\ 5 * 1 + 6 * 3 + 8 * 2 + 9 * 4 = 75 $$\nWe also noticed that the size of the output tensor is $2 \\times 2$, which is smaller than the input size ($3 \\times 3$). In fact, the output size is determined by the shape of the input and kernel, as shown below\n$$ ( I_h - K_h + 1 ) \\times ( I_w - K_w + 1 ) $$\nwhere $I$ is the input size, and $K$ is the size of the kernel. In this example, the output size is $ (3 - 2 + 1) \\times ( 3 - 2 + 1) = 2 \\times 2 $.\nWhy padding Padding and stride are two commonly used techniques in CNN. But why?\nIf we further apply another kernel to the output, we will get a scalar in the end, which is not always the desired result. Sometimes we want to preserve the shape of the input. So, we often use zero-padding to retain the size.\nBesides, each time we perform convolution operation, we will lose pixels on the boundaries of the image as we only scan the border only once. In the above example, we can see that pixels on the top and bottom border of the image ($1, 3, 7, 9$) appear only once in the convolution operation, while the inner part of the image (2,4,5,6,8) present twice.\nWhy stride On the other hand, the step that the kernel moves each time either horizontally or vertically is 1, which is expensive. Sometimes we might want to quickly obtain a reduced output, a solution is to increase step.\nWith padding and stride, the output shape is given as\n$$ \\lfloor (I_h + P_h - K_h) / S_h + 1 \\rfloor \\times \\lfloor (I_w + P_w - K_w) / S_w + 1 \\rfloor $$\nwhere $P$ and $S$ indicates the padding and stride, respectively. From the above equation, we can see that\n the output size increases as $P_h$ and $P_w$ increases when $S$ stays the same if $P_h = K_h - S_h$ and $P_w = K_w - S_h$, the shape of output is $I_h/S_h \\times I_w / S_w$ if $S_h = 1$ and $S_w = 1$  an odd value of $K_h$ will pad the same rows in both sides of the height; an even value of $K_h$, well, one solution is to pad $\\lfloor P_h/2 \\rfloor$ rows on the top of the image and $\\lceil P_h/2 \\rceil$ rows on the bottom    That\u0026rsquo;s why you often see odd kernels in CNN (stride is 1 by default). Besides, in practice, we often set $P_h = P_w$ and $S_h = S_w$.\nMultiple Channels So far, we only considered a single-layer input. As we know, an image has three channels —— R, G, B. That is, we need to perform convolution at each channel and then add the respective feature map together to obtain the final output, which is depicted in Figure 3.\n  Figure 3: Convolution with multi-layer inputs  Figure 3 shows that the number of kernels are the same as the number of channels in the input data. To make a clear distinction between one kernel and multiple kernels, we note that a kernel is a two-dimensional array (a slice of a filter) while a set of kernels are called a filter. In fact, many articles use them interchangeably. So far so good. But we still get one output. How do we get multiple outputs? —— The answer is to use multiple filters.\nNow let\u0026rsquo;s focus on the number of parameters. Let $C_I$ and $C_O$ be the number of input and output channels, and $K_w$ and $K_h$ be the width and height of each kernel, the total number of parameter needed in convolution layer is given as (bias is ignored)\n$$ C_O * C_I * K_w * K_w $$\nIn summary, the core parameters of a convolution layer are\n the dimensionality of the input the spatial extent of the kernel the number of kernels ( output channels )  Data Types Convolution can be applied to many dimensionalities and types of data, for example\n    Single Channel Multiple Channels     1D Audio multiple sensor data over time   2D greyscale images Colour image data   3D Volumetric data Colour Video Data    1 x 1 Kernel The minimum size of a kernel is $1 \\times 1$. It seems meaningless at first, since such a kernel does not correlate neighbouring pixels (capture local spatial information). Well, perhaps the only reason we\u0026rsquo;d like to use it is to change the dimension of the input channel ( the number of feature maps ).\n$1 \\times 1$ kernel will map each input element, so the output size is the same as the size of the input. However, the number of feature map depends on the number of filters we apply.\nPooling A typical CNN contains three layers\n convolution layer activation layer pooling layer  Max/Avg Pooling We have introduced the previous two layers, so what does pooling do? Pooling acts more like a window, where it reports some statistics from a group of data enclosed by that window, such as the maximum value or the average value, as shown in Figure 4 and Figure 5.\n  Figure 4: Max Pooling    Figure 5: Average Pooling  Unlike kernels, the pooling layer does not contain any parameter. But, why Pooling?\n decrease output dimension  reduce memory space for storing parameters improve computational efficiency   make feature representation approximately invariant to small translations of the input   In all cases, pooling helps to make the representation approximately invariant to small translations of the input. Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.\n— Deep Learning, P336\n Local VS Global Pooling The above pooling operations are local pooling, because they result in a smaller feature map. On the contrary, global pooling reduces a feature map to a scalar, which often used near the end of networks to flatten feature mapts into a feature vector that can be fed into a MLP.\nConvNet LeNet LeNet was the earliest CNN proposed for handwritten digit recognition in 1998. As the oldest CNN architecture, it is a good starting point to learn how CNNs work before we move on to more complex networks.\nArchitecture   Figure 6: Architecture of LeNet-5. Source: GradientBased Learning Applied to Document Recognition  Figure 6 shows that LeNet has 7 layers consisting of\n 3 convolutional layers 2 sub-sampling layers 2 fully-connected layers  which are denoted by $C_x$, $S_x$, and $F_x$, respectively. Based on the formulas discussed above, we can derive the shape of the intermediate ouputs and learning parameters in each layer in LeNet, which is shown below.\n   Layer Kernel/Padding/Pooling Output Num of Paramaters       1@32$\\times$32    C1 6@5 $\\times$ 5, 0,1 6@28$\\times$28 5*5*1*6+6 = 156   S2 2$\\times$2,0,2 6@14$\\times$14 6*2=12   C3 16@5 $\\times$ 5,0,1 16@10$\\times$10 (5*5*3*6 + 6) + (5*5*4*9 + 9)+(5*5*6*1 + 1) =1516   S4 2$\\times$2, 0, 2 16@5$\\times$5 16*2=32   C5 120@5 $\\times$ 5 120 5*5*16*120 + 120 = 48120   F6 NaN 84 120*84+84 = 10164   Output NaN 10 84 * 10 + 10 = 850      60,850    Table 1: The shape of the output and learning parameters in LeNet.\nLeNet differs from modern CNNs in several ways\n the activation function is sigmoid function rather than reLU LeNet used subsampling (similar to average pooling) to reduce output dimentionality while modern CNNs use max pooling  Implementation class LeNet(nn.Module): def __init__(self, num_class): super(LeNet, self).__init__() # in_channel, output_channel, kernel_size, stride, padding self.conv1 = nn.Conv2d(1,6,5) self.conv2 = nn.Conv2d(6,16,5) # self.conv3 = nn.Conv2d(16, 120, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, num_class) def forward(self, x): x = F.sigmoid(self.conv1(x)) x = F.avg_pool2d(x, 2) x = F.sigmoid(self.conv2(x)) x = F.avg_pool2d(x, 2) x = torch.flatten(x, 1) x = F.sigmoid(self.fc1(x)) x = F.sigmoid(self.fc2(x)) logits = self.fc3(x) probs = F.softmax(logits, dim=1) return logits, probs AlexNet ImageNet is a publicly large image database for computer vision and deep learning research. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was an annual image classification competition held from 2010 to 2017 using 1.3 million images in 1000 classes from the ImageNet dataset. AlexNet was the winner of ILSVRC 2012, achieving a top-5 error rate of 15.3%.\nArchitecture   Figure 7: Architecture of AlexNet. Source: [ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3065386)  As shown in Figure 7, AlexNet consists of 8 layers\n 5 convolutional layers 3 fully connected layers  Each layer and the corresponding number of learning parameters are shown below,\n   Layer Kernel/Padding/Pooling Output Num of Paramaters       3@227$\\times$227    C1 96@11 $\\times$11, 0, 4 96@55$\\times$55 (=(227-11)/4+1) 11*11*3*96+96 =   P2 3$\\times$3, 0, 2 96@27$\\times$27 (=(55-3)/2+1) 0   C3 256@5 $\\times$ 5,2,1 256@27$\\times$27 (=(27-5+2*2)/1 + 1) 5*5*96*256 + 256 =   P4 3$\\times$3, 0, 2 256@13$\\times$13 (=(27-3)/2+1) 0   C5 384@3 $\\times$ 3, 1, 1 384@13$\\times$13 (=(13-3+1*2)/1+1) 3*3*256*384 + 384 =   C6 384@3 $\\times$ 3, 1, 1 384@13$\\times$13 (=(13-3+1*2)/1+1) 3*3*384*384 + 384 =   C7 256@3 $\\times$ 3, 1, 1 256@13$\\times$13 (=(13-3+1*2)/1+1) 3*3*384*256 + 256 =   P8 3$\\times$3, 0, 2 256@6$\\times$6 (=(13-3)/2+1) 0   F9 NaN 4096 6*6*256*4096 + 4096   F10 NaN 4096 4096*4096 +4096   Output NaN 1000 4096*1000 + 1000      62,378,344    Table 1: The shape of the output and learning parameters in LeNet.\nThe main differences with LeNet includes\n reLU is used as the activation function overlapping max pooling is used to downsample the output GPU optimization  Avoid overfitting  Data Augmentation Dropout  Implementation PyTorch AlexNet - Github\nVGG VGG is short for Visual Geometry Group, which aimed to investigate the effect of convolutional network depth on large-scale image recognition. They increased the depth of network by using a small filter (3 $\\times$ 3) instead of 5 $\\times$ 5 or 7 $\\times$ 7, showing significant improvements by adding the depth to 16-19. Typically, when we speak of VGG, we are talking about VGG-16 or VGG-19. The complete experimental ConvNet configurations are shown in Figure 8.\nArchitecture   Figure 8: Architecture of VGG. Source: Very Deep Convolutional Networks for Large-Scale Image Recognition  The major difference to the previous networks is that the authors employed a stack of 3 $\\times$ 3 filters to have receptive fields of 5 $\\times$ 5, 7 $\\times$ 7 or 11 $\\times$ 11 rather than using larger filters that were the same size as the receptive fields. So, there are two questions,\n How it works? Why do we use it this way?  Figure 9 shows that we can use two 3 $\\times$ 3 filters instead of a 5 $\\times$ 5 filter to have the same receptive field. Similarly, three such filters will have a 7 × 7 effective receptive field.\n  Figure 9: Replacing the 5X5 filter with a small 3X3 conv filter.   What do we gain from the smaller filters?\n more non-linear activations  each filter is followed by a non-linear activation function, which makes the decision function more discriminative   decrease the number of parameters  suppose both the input and output have $C$ channels, a single 5 $\\times$ 5 filter has $5*5*C^2= 25C^2$ while the stacked two 3$\\times$3 filters has $2*3*3*C^2=18C^2$    Drawbacks Though VGG outperfoms other networks in ILSVRC, it\u0026rsquo;s difficult to train VGG from scratch because of\n long traning time huge number of weights  much deeper network depth more fully-connected layers    ResNet ResNet, short for Residual Network, was the winner of the ILSVRC 2015 classification challenge. Like VGG, there are many variants of ResNet, such as ResNet-18, ResNet-34, ResNet-50 and so on. Similar to other ConvNets, ResNet also has Conv layer, pooling, activation and FC layers. The difference is that ResNet introduced the identity connection, which is shown in Figure 10.\n  Figure 10: The core block of ResNet.  Motivation In my opinion, the main idea of ResNet is similar to gradient boosting. We know that the idea of gradient boosting is to learn residuals. Similarly, ResNet tries to learn the residual function rather than the mapping function directly. What does this mean? First, neural networks are function approximators. Normally, we build a neural network to learn a specific function, say $h(x)$. Now, we suppose that $h(x)$ can be decomposed into two parts\n$$ f(x) + x = h(x) $$\nFrom Figure 10, we see that $f(x)$ is the target function that we want to learn while $x$ is the identity connection, If the output is $x$ ($h(x) =x$), then $f(x) = 0$. In other words, as learning progresses, $f(x)$ should approach $0$. That\u0026rsquo;s why $f(x)$ is called the residual function.\nThe effect of Identity Connection VGG has demenstrated that as the depth of networks increases, the generalisation becomes better. However, the gradients are prone to shrink to zero if the network is too deep due to the chain rule, so some weights are never updated during learning. This is known as vanishing gradients. With ResNet, gradients can flow directly through the identity connection to the previous layers because of the addition operation, even if the graident in the residual block is very small.\nArchitecture   Figure 11: ResNet 34 from original paper[Deep Residual Learning for Image Recognition]    Figure 12: The size of building blocks in different layers of ResNet. Source: Deep Residual Learning for Image Recognition  One problem is that $f(x)$ might have different shape with $x$. In this case, we employ 1 $\\times$ 1 kernel to change the input shape, which is depicted by the dotted line in Figure 11. Mathematically, it can be expressed as\n$$ f(x) + Wx = h(x) $$\nReferences  CNN Explainer Chapeter 9 Convolutional Networks - Deep Learning Simple Introduction to Convolutional Neural Networks A Gentle Introduction to 1×1 Convolutions to Manage Model Complexity An Intuitive Explanation of Convolutional Neural Networks CNNs and Equivariance Translation Invariance in Convolutional Neural Networks Understanding AlexNet VGG-16 Architecture: A Complete Guide Understading and visualizing ResNets ResNets K. He, X. Zhang, S. Ren and J. Sun, “Deep Residual Learning for Image Recognition,” in CVPR, 2016. https://arxiv.org/abs/1512.03385 "
            }
    
        ,
            {
                "id": 33,
                "href": "https://ixiaopan.github.io/blog/post/model-selection/",
                "title": "Model Selection",
                "section": "post",
                "date" : "2021.09.19",
                "body": "Model Complexity What is model complexity? Perhaps it mainly refers to the number of parameters that the model has. However, it does not mean that a model is complex if it has more parameters, although it is often the case. Generally, a simple model could result in underfitting, while a complex model could lead to overfitting. So, what factors affect the complexity of a model? Well, it is hard to say. Often,\n a model with more parameters a model whose parameters take a wider range of values a model that takes more training iterations  Model Selection In machine learning, we care about two factors that affect model performance\n model training data  The process of selecting the best model is known as model selection. In practice, it includes two parts\n select the best model from a list of candicate models search the best parameters for the best model  Validation Dataset To select the best model, we need another dataset to validate our hypothesis, which is the validation dataset. Why? Training data are used for training models, and test data are used to measure the performance of generalisation, which can only be used once. If we use test data to do model selection and then test the generalizability of the best model, we face data leakage since the selected model has already seen test data. Thus, we need to hold out a new dataset for validation only. In practice, data will be split into 70% for training, 10% for validation, and 20% for testing.\nK-Fold Cross-validation But what if we have scarce data? Taking out some data for validation and testing will further result in fewer training data. To tackle this, we employ cross-validation to resample data. Specifically, we split data into several equal-sized groups, and leave out one group as validation dataset once a time. That is, if there are $K$ groups, we will repeat $K$ times. Then, we take the average of the $K$ validation scores. In doing so, we can make the most of training data.\n  Figure 1: Cross-validation  Learning Curve Validation Curve Reference  How to use Learning Curves to Diagnose Machine Learning Model Performance Diagnosing Model Performance with Learning Curves Model Complexity Optimization  "
            }
    
        ,
            {
                "id": 34,
                "href": "https://ixiaopan.github.io/blog/post/dl-02-architecture/",
                "title": "Deep Learning - Neural Networks",
                "section": "post",
                "date" : "2021.09.18",
                "body": "Neural Networks Neural networks are simply function compositions.\n Feedforward networks  $$ y = f(g(x, \\theta_g), \\theta_f) $$\n Recurrent networks  $$ y_t = f(y_{t-1}, x_t, \\theta) $$\nGenerally, you can think of neural networks as function approximation machines, where we want to find appropriate $\\theta$ to make $f^{*}$ as similar as possible to the true function $f$.\nIn practice, there are lots of things to consider when designing a neural network. Basically, we need to consider the following factors,\n Loss function Architecture of the network  How many layers? How many units in each layer? How are these layers connected?   Optimisation Algorithm  Loss Function In classification tasks, target variables are categorical, so we need to find a method to transform them into numerical values. Typically, we use one-hot encoding to encode each category into a binary value. If an example belongs to class $j$, then the true target value is $C_j = 1$, otherwise $C_j = 0$. When we make predictions, we\u0026rsquo;d like to predict the probability of an example belonging to a class. For example, we may have such a prediction $(0.1,0.7,0.2)$. Since class 2 has the highest probability, we say that this example is classified as class 2.\nBut how to quantify the difference between predictions and the ground truth so as to find the best parameters? In Machine Learning, we have learned mean squared error (MSE) and cross-entropy as the loss functions. So, which one to use?\nMSE Let\u0026rsquo;s take the derivative of MSE w.r.t $w$. The equation below shows that the gradient of MSE w.r.t $w$ is the product of the difference between the target value $y$ and predicted value $\\sigma(z)$ and the gradient of the activation function.\n$$ \\frac {\\partial }{\\partial w}\\frac{1}{2}(y - \\sigma(z))^2 = (y - \\sigma(z))\\sigma'(z)x $$\nA widely used activation function in binary classification is the sigmoid function. From Figure 1, we see that when the predicted value is close to 1/0, the curve gets very flat, indicating $\\sigma'(z)$ becomes very small. If the ground truth is opposite to the predicted value, our machine will learn slowly as the gradient of MSE w.r.t. $w$ is small.\n  Figure 1: Sigmoid function  Cross-Entropy Entropy is an important concept in information theory. Basically, it tells us the average information a random variable conveys. Cross-entropy measures the difference between two distributions, where $f(x)$ is considered as the ground truth. Thus, a smaller cross-entropy indicates better learning.\n$$ E = -\\sum_{x \\in X} f(x) \\text{log} g(x) $$\nDoes it have the problem of slow learning? Let\u0026rsquo;s take the derivative of $E$ w.r.t $w_j$ (suppose it\u0026rsquo;s binary classification)\n$$ \\frac{\\partial L}{\\partial w_j} = - \\frac{1}{n} \\sum_x (\\frac{y}{\\sigma(z)} - \\frac{1 - y}{1 - \\sigma(z)})\\sigma'(z)x_j = - \\frac{1}{n} \\sum_x (\\sigma(z) - y) x_j $$\nWe see that the gradient of $w_j$ is determined by the error in the output. In other words, cross-entropy punishes the wrong classification heavily. Figure 2 also shows that misclassified point conveys much information.\n  Figure 2: Negative logarithm between 0 and 1.  Activation Function Why Why do we need an activation function?\nDeep learning works by mimicking how a human brain learns. Every second, our brains receive a lot of information, but not all of them are useful. Some information will be ignored, otherwise our brains will be overloaded. In deep learning, we need to find a method to do the same thing. Activation functions, as their name suggests, decide whether to fire neurons or not.\nOn the other hand, since neural networks are composed of multiple layers and each layer can be expressed as $Y = XW$, the final output is given as $ y = XW_1 W_2..W_n = XW$, where $W = W_1W_2\u0026hellip;W_n$. If all functions are linear, there is no need to add the depth of the network as we can just use a single layer to achieve this. To capture the features, we must use a nonlinear function, also known as an activation function, which is applied to each hidden unit following an affine transformation controlled by parameters.\nWhat So, what should a valid activation function look like?\n differentiable  because deep learning is gradient-based learning, we need to know in which direction and how much to move in the parameter space   monotonic  Binary Step function A simple way to fire a neuron is to set a threshold. If the output of a neuron is greater than zero, the step function will return 1 and this neuron will be activated, otherwise it will be deactivated.\n Equation  def step(x): if x \u0026gt; 0: return 1 return 0  Range: $\\{0, 1\\}$ Pros  simple   Cons  only suitable for binary classificaition the gradient is zero, so the parameters will not updated via backprop      Figure 4: Binary step activation function.  Identity Identity function is a linear function that returns the multiple of the input.\n Equation  $$ f(x) = ax $$\n  Range: $(-\\infin, \\infin)$\n  Pros\n The gradient is non-zero    Cons\n However, the gradient is a constant value that does not depend on the inputs, so learning will not be improved from errors since the gradient is always the same.      Figure 3: Identity activation function.  Sigmoid  Equation  $$ f(z) = \\frac{1}{1 + e^{-z}} $$\n Range: $[0, 1]$ Pros  Smooth S-shaped and continuously differentiable It is suitable for predicting the probability as an output   Cons  The gradient is small as $z$ is far away from 0, which will cause \u0026lsquo;vanishing gradient\u0026rsquo;      Figure 5: Sigmoid function and its derivative  x = torch.arange(-5, 5, 0.1) # F.sigmoid is deprecated y = torch.sigmoid(x) Tanh  Equation  $$ f(x) = \\text{tanh} (x) = \\frac{2}{1+e^{-2x}} - 1 $$\n  Range: $[-1, 1]$\n  Pros\n Similar to sigmoid, but it\u0026rsquo;s symmetric around 0 The gradient is steeper than sigmoid    Cons\n vanishing gradient      Figure 6: Tanh function  x = torch.arange(-5, 5, 0.1) # F.tanh is deprecated y = torch.tanh(x) ReLU  Equation  $$ f(x) = \\text{max} (0, x) $$\n  Range: $[0, \\infin)$\n  Pros\n sparse network and efficient simpler mathematical operation    Cons\n dying reLU, since neurons with negative values will be deactivated, and thus the weights for them will not be updated due to the zero gradient      Figure 6: ReLU function  x = torch.arange(-5, 5, 0.1) y = F.relu(x) Leaky ReLU A solution to solve dying ReLU problem is to make the horizontal line into non-horizaontal line. The leak coefficient that controls the slope, also known negative_slope in PyTorch, is a learned parameter, typically, it\u0026rsquo;s $0.01$.\n$$ f(x) = \\text{max} (0, x) + \\text{negative\\_slope} * \\text{min}(0, x) $$\nx = torch.arange(-5, 5, 0.1) y = F.leaky_relu(x) Parametric ReLU $$ f(x) = \\text{max} (0, x) + \\text{a} * \\text{min}(0, x) $$\nx = torch.arange(-5, 5, 0.1) # weight must be a tensor y = F.prelu(x, torch.tensor(0.25)) Reference  Understanding Activation Functions in Neural Networks Activation Functions in Neural Networks  "
            }
    
        ,
            {
                "id": 35,
                "href": "https://ixiaopan.github.io/blog/post/dataviz/",
                "title": "Data Visualisation",
                "section": "post",
                "date" : "2021.09.11",
                "body": "Data visualisation is one of the modules in the first semester. Though I worked with web design and UI for several years, I\u0026rsquo;d never think about them deeply. As I learned more about design principles and the use of different charts, my aesthetic sense improved greatly. Here I listed some useful resources just for future reference.\nDLKW Pyramid https://allthingy.com/data-information-knowledge-wisdom/\nChart Junk Data story Below are key points when you structure a data story\n Who is your audience? What do you want your audience to know? Pose a question to the audience Lead them through the data in a way they understand Give Context, and highlight key trends or events Conclude by answering the question  Planning Idea: the story you want to tell\nData: the evidence that support your idea\nHook: draw readers' attention\nContext \u0026amp; Detail Why are you interested in this story?\nQuestion What do you want your audience to know?\nToulmin Argumentation   Figure 1: Toulmin Argumentation.  For example: House prices of London is higher than other areas in England.\n Claim: House prices of London is higher. Grounds: London is the capital of England. Warrant: Usually, capital of a country has better economy, since Lodon is the capital of England, the prices are higher. Backing: GDP, salary, history reason Rebuttal: Other areas out of London have lower prices Qualifiers: So, generally this is true in most time.  Aim or Action At last, what action do you want to your audience to take?\nNarrative Pattern http://napa-cards.net/\nCharts Basic Chart Interactive Chart Eliminate Clutter Design Principles Visual Perception Draw Attention Rules Colors Misleading Books  Information Dashboard Design  Design Principles and the application of various charts   Storytelling with Data  Guidance to write a data report   Show Me the Numbers The Functional Art The Visual Display of Quantitative Envisioning Information Data Visualization Book Reviews  A list of resources    Tools   From Data To Viz\n  The Data VIsualisation Catalogue\n  Chart Guide\n  Char Chooser\n When you don\u0026rsquo;t know which chart to use, you should come here    Junk Chart\n Discuss junk charts and how to make them better    Tableau\n  D3\n  RAW - an online visualisation tool\n  Data  Kaggle Makeover Monday Google Dataset Search  Reference  Toulmin Argumentation - OWL "
            }
    
        ,
            {
                "id": 36,
                "href": "https://ixiaopan.github.io/blog/post/misc-leetcode-02/",
                "title": "Leetcode 102",
                "section": "post",
                "date" : "2021.09.10",
                "body": "Practice! Practice! Practice!\nTraverse a Tree Three strategies to visit a tree depends on the order of visiting the root\n pre-order in-order post-order  Two methods to implement this\n Recursive Iterative  The idea is first-in-last-out, so we use stack to keep nodes.    Pre-order Traversal class Solution: def preorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if root is None: return [] # recursive l_n = self.preorderTraversal(root.left) r_n = self.preorderTraversal(root.right) return [ root.val ] + l_n + r_n # iterative val = [] stack = [ root ] while len(stack) \u0026gt; 0: node = stack.pop() val.append( node.val ) if node.right: stack.append( node.right ) if node.left: stack.append( node.left ) return val In-oder Traversal class Solution: def inorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if root is None: return [] # recursive l_n = self.inorderTraversal(root.left) r_n = self.inorderTraversal(root.right) return l_n + [ root.val ] + r_n # iteratively stack = [] val = [] while True: while root: stack.append(root) root = root.left if len(stack) == 0: return val # once end, it must be the local root node without left children # then we pop it out to visit this node # next we visit its right child # if the right child is None, we pop up its parent node node = stack.pop() val.append(node.val) root = node.right Post-order Travesal class Solution: def postorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if root is None: return [] # recursive l_n = self.postorderTraversal(root.left) r_n = self.postorderTraversal(root.right) return l_n + r_n + [ root.val ] Level-order Traversal BFS Breadth-First Search visit nodes level by level. We use a queue to keep nodes.\n Each time we are goingt to visit a node, we pop it out of the queue At the same time, we add its left node and right node into the queue    Figure 1: We use queue to store nodes in BFS ($\\text{LeetCode}^{[1]}$)  def BFS(root): if root is None: return [] ret = [] queue = [ root ] while len(queue) \u0026gt; 0: ret.append( [ n.val for n in queue ] ) next_level = [] for n in queue: next_level += [ n.left, n.right ] queue = [ n for n in next_level if n ] DFS Tree problem Top-down Solution  visit a node and get some values pass the values to its children  return specific value for null node left_ans = top_down(root.left, some_val_of_node) right_ans = top_down(root.right, some_val_of_node) Bottom-up Solution  get the answer from the children nodes then update the answer for the parent node  class Solution: def maxDepth(self, root: Optional[TreeNode]) -\u0026gt; int: if root is None: return 0 left_depth = self.maxDepth(root.left) right_depth = self.maxDepth(root.right) return max(left_depth, right_depth) + 1 Reference  [1] Binary Tree - LeetCode "
            }
    
        ,
            {
                "id": 37,
                "href": "https://ixiaopan.github.io/blog/post/dl-01-pre/",
                "title": "Deep Learning - Preliminary",
                "section": "post",
                "date" : "2021.09.09",
                "body": "So far, we\u0026rsquo;ve covered most of the things that we should know about machine learning, including concepts, optimization, and popular models under the hood. Yet, some advanced techniques, such as the Gaussian Process and MCMC, are not mentioned. We will talk about them later. From now on, we will move to Deep Learning. But before that, we are going to revisit some math knowledge.\nLinear Algebra Tensor We introduced tensors in the previous article. A tensor is a n-dimensional array of numbers.\n If $n=0$, it\u0026rsquo;s a scalar \u0026lt;=\u0026gt; a number mathmatically If $n =1$, it\u0026rsquo;s a vector \u0026lt;=\u0026gt; a list of numbers If $n=2$, it\u0026rsquo;s a matrix \u0026lt;=\u0026gt; a 2-dimensional array with two axises If $n\u0026gt;2$, it\u0026rsquo;s a tensor \u0026lt;=\u0026gt; an array with more than two axises  In linear algebra, we can add vectors, multiply a vector by a scalar, or do both of them.\nVector Addition $$ \\bold v=\\begin{bmatrix}v_1\\\\ v_2\\end{bmatrix} \\bold w=\\begin{bmatrix}w_1\\\\ w_2\\end{bmatrix} \\bold v+\\bold w=\\begin{bmatrix}v_1+w_1\\\\ v_2+w_2\\end{bmatrix} $$\nScalar multiplication $$ a \\bold v=\\begin{bmatrix}av_1\\\\ av_2\\end{bmatrix} $$\nLinear Combination $$ c\\bold v + d\\bold w $$\nUnit Vector Vectors have both directions and length. The length is defined as follows,\n$$ || \\bold v|| = \\sqrt{\\bold v^T\\bold v} $$ The vector whose length equals one is known as the unit vector,\n$$ \\bold v^T \\bold v = 1 $$\nHere are some unit vectors,\n$$ \\bold i=\\begin{bmatrix}1\\\\ 0\\end{bmatrix} \\bold j=\\begin{bmatrix}0\\\\ 1\\end{bmatrix} \\bold u=\\begin{bmatrix}cos\\theta\\\\ sin\\theta\\end{bmatrix} $$\nA vector can be transformed into a unit vector by dividing it by its length, as shown below.\n$$ \\bold u = \\frac{\\bold v}{ || \\bold v ||} $$\nDot product $$ \u0026lt;v, w\u0026gt; = v \\cdot w = v^Tw = v_1 * w_1 + v_2 * w_2 + \u0026hellip; + v_n * w_n $$\n The order of $v$ and $w$ makes no difference. $v \\cdot w$ is zero when $v$ and $w$ are prependicular.  For any real matrix 𝐴 and any vectors 𝐱 and 𝐲 , we have\n$$ ⟨𝐴𝐱,𝐲⟩=⟨𝐱,𝐴^T𝐲⟩ $$\nAx = b Mathematically, machine learning is all about $\\bold A \\bold x= \\bold b$, where $A, b$ are known, and $x$ is unknown. We can interpret $Ax=b$ from two aspects\n $\\bold A \\bold x$ is a linear combination of the columns of A ( column picture )  The system has solution only if the target $b$ lies in the column space of $A$   $\\bold A \\bold x$ is dot products ( row picture )  $m$ equations    Rank To solve $x$ or to solve $m$ equations, we need $m$ pivots ($\\bold A$ is an $m \\times n$ matrix). Pivots are the diagnoal elements of $\\bold A$. To find the pivots, we transform $A$ to an upper triangle matrix. Besides, we define the number of pivots of $A$ as the rank. In fact, rank contains the following meanings:\n the number of pivots $r$ independent rows or columns $r$ is the dimension of the column space or row space  However, it is not always lucky to obtain $m$ non-zero pivots. The possible solution could be either of the following cases\n no solution  $$ x - 2y = 1 \\\\ 0y = 8 $$\n infinitely many solutions  $$ x - 2y = 1 \\\\ 0y = 0 $$\n exactly one solution  $$ 3x - 2y = 5 \\\\ 2y = 4 $$\n$\\bold A$ in case 1 and 2 is called singular —— there is no second pivot.\n Singular equations have no solution or infinitely many solutions. Pivots must be non-zero becaue we have to divide by them.  Inverse Matrix It is also possible to multiply the inverse of $\\bold A$ to obtain $\\bold x$ directly (closed-form solution), as shown below. $$ x = A^{-1}b $$ However, $A^{-1}$ is not always exist.\nIs A invertible?\n $A$ must have $m$ non-zero pivots $A$ has $m$ linearly independent columns $det A \\neq 0$  If $A$ is invertible,\n  the only solution to $Ax=b$ is $x=A^{-1}b$\n  $x=0$ must be the only solution for $Ax = 0$\n  n \u0026gt;= m In conclusion, there are two ways to determine whether the system has a solution. If $A$ is invertible, there is only one solution. Otherwise, it depends on the rank of A.\n  Figure 1: The four possibilities for linear equations depned on the rnak (Introduction to Linear Algebra, Chapter 3)  In order to ensure that the system has a solution for all values of $b \\in R^m$, $A$ must contain $m$ linearly independent columns and $n \\ge m$, as shown in Figure 1.\nMatrix Multiplication $$ AB = A \\begin{bmatrix}b_1 \u0026amp;b_2\u0026amp; b_3\\end{bmatrix} = \\begin{bmatrix} Ab_1 \u0026amp;Ab_2\u0026amp; Ab_3\\end{bmatrix} $$ Associative Law is true\n$$ A(BC) = (AB)C \\\\ A(B + C) = AB + AC \\\\\\ (A + B)C = AC + BC $$\nHowever, Commulative Law is not always true\n$$ AB \\neq BA $$\nNorms In the previous section, we introduced how to compute the length of a vector. In fact, it is a kind of $L^p$ norm, which is used to measure the size of vectors, as defined below.\n$$ || \\bold x||_p = ( \\sum_i |x_i|^p)^{\\frac{1}{p}} $$\nThus, the previous measurement of the size of a vector is called $L^2$ norm, which is also known as the Euclidean norm. Intuitively, it is the distance from the origin to the point $x$.\n It is also common to use the squared $L^2$ norm, which equals to $x^Tx$. However, the squared $L^2$ norm increases slowly near the origin. In several machine learning applications, it is important to discriminate between elements that are exactly zero and elements that are small but nonzero. In these cases, we turn to a function that grows at the same rate in all locations, but that retains mathematical simplicity: the L 1norm.\n\u0026ndash; Deep Learning, p37\n $L^1$ norm is defined as follows,\n$$ || \\bold x||_1 = \\sum_{i}|x_i| $$\nAnother commonly used norm is the max norm, which returns the largest absolute value of the component of $x$\n$$ ||x||_{\\infin} = \\text{max}_i |x_i| $$\nThe size of a matrix is known as Frobenius norm, which is similar to the $L^2$ norm of a vector. We sum up the squared value of each element, as shown below\n$$ ||A||_F = \\sqrt{\\sum_{i,j}A_{ij}^2} $$\nDeterminant Mathematially, the determinant is a scalar value calculated from a square matrix. Geometrically, it indicates the scale factor by which the unit space changed by $A$. The unit space is determined by the basis of a space.\nFor example, the determinant of $\\begin{bmatrix}1\u0026amp;0\\\\0\u0026amp;1\\end{bmatrix}$ is 1. We know that each column of the matrix is the basis of the 2D space. The area of the parallelogram defined by the two columns is 1. Similarly, the determinant indicates the volume of the parallelepiped in the 3D space.\nWhen multiplying by $A$, we are using another basis of the same space. The corresponding change in the area of the unit space is measured by determinant. Specifically, if determinant is 0, then the unit space is contracted completely along at least one dimension. In other words, the new unit space could be a plane instead of parallelepiped if we are in 3D space. Since anything multiplies 0 is 0, we are not able to cancel the transformation from zero. Thus, $A$ is invertible if and only if the determinant is not zero.\nCalculus Derivative The derivative of $f(x)$ with respect to $x$ is defined as\n$$ f'(x) = \\text{lim}_{h\\rarr0} \\frac{f(x+h) - f(x)}{h} $$\nThe python implementation of numerical differentiation is shown below\ndef numericala_diff(f,x): h = 1e-10 return ( f(x + h) - f(x) ) / h Gradient The previous equation involves only one variable, what if we have multiple variables? For example, $f(x, y) = x^2 + 2y$. In this case, we calculate the partial derivative of $f$ with respect to each variable while keeping other variables as constants\n$$ \\frac{\\partial f} {\\partial x_i} = \\text{lim}_{h \\rarr 0} \\frac{f(x_1,\u0026hellip;,x_i+h,\u0026hellip;,x_n) - f(x_1,\u0026hellip;,x_i,\u0026hellip;,x_n)}{h} $$\nThe vector of partial derivative of $f$ at a point ($\\bold x = x_1, x_2,\u0026hellip;,x_n$) is the gradient of $f$ at $\\bold x$\n$$ \\nabla_x f(x) = [\\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2}, \u0026hellip;, \\frac{\\partial f(x)}{x_n}]^T $$\nThe python implementation of the gradient of $f$ at $x$ is shown below\ndef numerical_gradient(f, x): h = 1e-4 # 0.0001 grad = np.zeros_like(x) for idx in range(x.size): tmp_val = x[idx] x[idx] = tmp_val + h fxh1 = f(x) x[idx] = tmp_val - h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # revert  return grad Chain Rule Consider a function $f$ that relys on $z$ which depends on $x$, for instance, $y = f(g(x))$, where $z =g(x)$. Then the derivative of $f$ w.r.t $x$ is defined as\n$$ \\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx} $$\n $dz/dx$ means how much $z$ will change due to the unit change in $x$ $dy/dz$ means how much $y$ will change due to the tiny change in $z$  Therefore, the total change in $f$ due to the change in $x$ equals the product of all these changes. This is known as the \u0026ldquo;chain rule.\u0026rdquo;\nWhat if we have a vector-valued function? Suppose we have\n$$ \\bold z = g(\\bold x) \\text{ and } y = f(\\bold z) $$\nwhere $x \\in R^m, z \\in R^n, y \\in R $. Consider the gradient of $g$ with respect to $x_i$, for a specific $x_i$, it will cause change in $z_1, z_2, \u0026hellip; , z_n$, and then the change in $\\bold z$ will cause change in $y$ in the end. Therefore, we have\n$$ \\frac{\\partial y}{\\partial x_i} = \\sum_{j}^n\\frac{\\partial y}{\\partial z_j} \\frac{\\partial z_j}{\\partial x_i} $$\n $\\frac{\\partial z_j}{\\partial x_i}$ represents how a small change in $x_i$ influences the intermediate output $z_j$ $\\frac{\\partial y}{\\partial z_j}$ represents how a small change in $z_j$ influences the output $y$  We can rewrite it in vector notation\n$$ \\nabla_{\\bold x} y = (\\frac{\\partial \\bold z}{\\partial \\bold x})^T \\nabla_{\\bold z} y $$\nwhere $\\frac{\\partial \\bold z}{\\partial \\bold x}$ is the $n \\times m$ Jacobian matrix of $g$, as shown below\n$$ \\begin{bmatrix} \\frac{z_1}{x_1} \u0026amp; \\frac{z_1}{x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{z_1}{x_m} \\\\ \\\\ \\frac{z_2}{x_1} \u0026amp; \\frac{z_2}{x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{z_2}{x_m} \\\\ \\\\ \u0026amp; \u0026amp; \u0026hellip; \u0026amp; \\\\ \\frac{z_n}{x_1} \u0026amp; \\frac{z_n}{x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{z_n}{x_m} \\end{bmatrix}^T \\begin{bmatrix} \\frac{y}{z_1} \\\\ \\frac{y}{z_2} \\\\ \u0026hellip; \\\\ \\frac{y}{z_n} \\end{bmatrix} $$\nD = XW In a one-layer-hidden network, suppose we have $N$ examples with $M$ features defined as $X \\in R^{n \\times m}$ and $H$ hidden neurons defined as $W \\in R^{m\\times h}$, the objective function $L = f(D) = f(XW)$ is some scalar function of $D$ that we want to optimise. So, what are the derivatives of $L$ w.r.t. $W$?\nIf we are familiar with matrix derivatives, it\u0026rsquo;s easy to find the answer, which is $\\bold X^T \\frac{\\partial L}{\\partial D}$. But how do we derive it step by step? The derivative of $f(\\bold X) \\in R$ w.r.t $\\bold X \\in R^{m\\times n}$ is defined as\n$$ \\frac{\\partial f}{\\partial \\bold X} = \\begin{bmatrix} \\frac{\\partial f}{x_{11}} \u0026amp; \\frac{\\partial f}{x_{12}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial f}{x_{1n}} \\\\ \\\\ \\frac{\\partial f}{x_{21}} \u0026amp; \\frac{\\partial f}{x_{22}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial f}{x_{2n}} \\\\ \\\\ \u0026amp; \u0026amp; \u0026hellip; \u0026amp; \\\\ \\frac{\\partial f}{x_{m1}} \u0026amp; \\frac{\\partial f}{x_{m2}} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial f}{x_{mn}} \\end{bmatrix} = \\sum_{i,j}E_{ij} \\frac{\\partial f}{\\partial x_{ij}} $$\nLet\u0026rsquo;s start by considering a specific weight $W_{uv}$, the derivative of $L$ w.r.t $W_{uv}$ is given as\n$$ \\frac{\\partial L}{\\partial W_{uv}} = \\sum_{ij}\\frac{\\partial D_{ij}}{\\partial W_{uv}} \\frac{\\partial L}{\\partial D_{ij}} $$\nwhere $D_{ij}$ is the output of the $j_{th}$ neuron for the $i_{th}$ example, and $W_{uv}$ is the $u_{th}$ weight of the $v_{th}$ neuron. Thus, if $j \\neq v$, $D_{ij}$ has nothing to do with $W_{uv}$, so $\\frac{\\partial D_{ij}}{\\partial W_{uv}} = 0$. Therefore, we can simply the summation\n$$ \\frac{\\partial L}{\\partial W_{uv}} = \\sum_{i}\\frac{\\partial D_{iv}}{\\partial W_{uv}} \\frac{\\partial L}{\\partial D_{iv}} = \\sum_{i} \\frac{\\partial L}{\\partial D_{iv}} X_{iu} $$\nSince $W$ is an $m \\times h$ matrix, $\\frac{\\partial L}{\\partial \\bold D}$ is an $n \\times h$ matrix, and $X$ is an $n \\times m$ matrix, we have\n$$ \\frac{\\partial L}{\\partial \\bold W} = X^T \\frac{\\partial L}{\\partial \\bold D} $$\nSo what does it mean? Well, the gradient of the loss with respect to a parameter tells you how much the loss will change with a small perturbation to that parameter.\nProbability Since we have already talked about Probabilistic Model, we will skip this section.\nNumerical Computation Rounding Errors Underflow occurs when numbers are very close to zero. Overflow, on the other hand, occurs when numbers are so large that computers return infinity or negative infinity. Both are due to the limited precision of numerical representation.\n# underflow and overflow in Python math.exp(-10000) # 0.0 math.exp(10000) # OverflowError: math range error Rounding errors can easily happen when dealing with softmax function, which is defined as\n$$ \\sigma(z_i) = \\frac{e^{z_i}}{\\sum^K_{j=1}e^{z_j}} $$\n $z_i \u0026raquo; 0$ , then $\\text{exp}(z_i) \\approx \\infin $, =\u0026gt; overflow $z_i \u0026laquo; 0$ , then $\\sigma(z) \\approx 0$ =\u0026gt; underflow =\u0026gt; $\\text{log }\\sigma(z) = -\\infin$  Log-sum-exp How to solve this to obtain a stable value? The trick is the log-sum-exp.\n$$ y = \\text{ log } \\sum^K_{j=1}e^{z_j} \\\\ e^y = \\sum^K_{j=1}e^{z_j} \\\\ e^{-a}e^y = \\sum^K_{j=1}e^{z_j}e^{-a} \\\\ y-a = \\text{log} \\sum^K_{j=1}e^{z_j-a} \\\\ y = a + \\text{log} \\sum^K_{j=1}e^{z_j-a} $$\nTypically, we set $a$ to the maximum value of $z$. Thus,\n$$ \\text{ log} \\frac{e^{z_i}}{\\sum^K_{j=1}e^{z_j}} = z_i - \\text{ log } \\sum^K_{j=1}e^{z_j} \\\\ = z_i - a - \\text{log} \\sum^K_{j=1}e^{z_j-a} $$\nFor example, suppose we have $z=[10000, 10000]$, then $\\sigma(z_0) = 0.5$. If we compute the softmax function directly, it will cause overflow error. Instead, let\u0026rsquo;s plug $10000$ into the above equation using the log-sum-exp trick, and we have\n$$ \\text{log} \\sigma(z_0) = 10000 - 10000 - \\text{log}2 = \\text{log}0.5 $$\nOverparameterisation One interesting characteristic of the softmax function is overparameterisation — subtracting $\\psi$ from every weight $\\theta^k$ does not affect the final result, as shown in the following equation. This means that there are multiple parameter settings that satisfy the same function.\n$$ \\sigma(x^{i}) = \\frac{\\exp((\\theta^{k}-\\psi)^\\top x^{i})}{\\sum_{j=1}^K \\exp( (\\theta^{j}-\\psi)^\\top x^{i})} \\\\ = \\frac{\\exp(\\theta^{(k)\\top} x^{(i)}) \\exp(-\\psi^\\top x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{i}) \\exp(-\\psi^\\top x^{i})} \\\\ = \\frac{\\exp(\\theta^{(k)\\top} x^{i})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{i})}. $$\nOn the other hand, we can eliminate $\\theta^k$ by replace $\\theta^k$ with $\\theta^k - \\psi = 0$ without affecting prediction. In doing so, we only need to optimise over $(K - 1)*n$ parameters rather than $Kn$ parameters ($\\theta^1, \\theta^2, \u0026hellip;, \\theta^K, \\text{where } \\theta^i \\in R^n$).\nAutomatic Differentiation Computation Graph Sum  Multiplication   Forward AD There are three ways to compute gradient. The simplest one is to apply common rules directly, for example,\n$$ f(x) = x^2 \\\\ f'(x) = 2x $$\nFor composite functions, the gradient is computed step by step, for instance,\n$$ f(x) = g(x) + h(x) \\\\ f'(x) = g'(x) + h'(x), \\\\ f(x) = g(x)\\cdot h(x) \\\\f'(x) = h(x) g'(x) + h'(x) g(x) $$\nHowever, it is difficult for complex functions to obtain such an equation. In this case, we solve it by applying numerical differentiation\n$$ f'(x) = \\text{lim}_{h\\rarr0} \\frac{f(x+h) - f(x)}{h} $$\nWell, it seems perfect, but in practice, we do not know exactly what a function is composed of and the input variable could be dynamic. Ideally, we\u0026rsquo;d like to obtain the value of a function and the corresponding gradient simultaneously. This technique is known as automatic differentiation.\nForward mode AD is an intuive and simple way to accomplish this, which is similar to how computer works the equations out. For example, we have a funciton $z = xy + sin(x)$, and then computers will compute step by step as follows,\n$$ x = ? \\\\ y = ?\\\\ a = xy \\\\ b = sin(x) \\\\ z = a + b $$\nLet\u0026rsquo;s differentiate the each expression w.r.t some variable $t$\n$$ \\frac{dx}{dt} = ? \\\\ \\frac{dy}{dt} = ? \\\\ \\frac{\\partial a}{\\partial t} = x \\frac{dy}{dt} + y \\frac{dx}{dt} \\\\ \\frac{\\partial b}{\\partial t} = cos(x) \\frac{dx}{dt} \\\\ \\frac{\\partial z}{\\partial t} = \\frac{ \\partial a }{\\partial t} + \\frac{ \\partial b }{\\partial t} $$\nTo get the derivative of $f$ w.r.t a specific variable, we simply set $t$ as the given varaible. For example, we set $t = x$ to compute $\\frac{\\partial z}{\\partial x}$. Besides, we can see that the operation of excuating the function and computing the derivative can be done at the same time. Dual numbers is one way to achieve this concisely.\nDual number Dual number is similar to complex number, which is defined as,\n$$ a + b\\epsilon $$\nwhere $\\epsilon^2 = 0$. Below are some basic rules of dual number\n$$ (a + b\\epsilon) + ( c + d\\epsilon ) = a + c + (b + d)\\epsilon \\\\ (a + b\\epsilon)(c+d\\epsilon) = ac + bc\\epsilon + ad\\epsilon $$\nSo\u0026hellip; what is the use of dual number? We\u0026rsquo;ve known that Talyer expansion around a point $a$ is defined as\n$$ f(x) = f(a) + f'(a) (x - a) + \\sum_{n=2} \\frac{f^n(a)}{n!} (x - a)^n $$\nLet\u0026rsquo;s replace $x = a + b\\epsilon$, then we have\n$$ f(x) = f(a) + f'(a) b\\epsilon + \\epsilon^2 \\sum_{n=2} \\frac{f^n(a)}{n!} b^n\\epsilon^{n-2} = f(a) + f'(a) b\\epsilon $$\nFrom this, we see that by calculating $x = a + b\\epsilon$, we can obtain the value of the function $f(x)$ and the derivative at $x$ simultaneously. The implementation of forward mode AD by dual numbers is shown below,\nclass DualNumber: def __init__(self, value, dvalue): self.value = value self.dvalue = dvalue def __repr__(self): return repr(self.value) + \u0026#39; + \u0026#39; + repr(self.dvalue) + \u0026#34;ε\u0026#34; def __str__(self): return str(self.value) + \u0026#34; + \u0026#34; + str(self.dvalue) + \u0026#34;ε\u0026#34; def __mul__(self, other): if isinstance(other, DualNumber): return DualNumber(self.value * other.value, self.dvalue * other.value + other.dvalue * self.value) return DualNumber(self.value * other, self.dvalue * other) def __add__(self, other): if isinstance(other, DualNumber): return DualNumber(self.value + other.value, self.dvalue + other.dvalue) return DualNumber(self.value + other, self.dvalue) # test x = 2 y = 5 a=DualNumber(x, 1) b=DualNumber(y, 0) # w.r.t y z = a * b + a # df/da = b + 1 print(z) # it should be 6 # 12 + 6ε Reverse AD(backprop) Forward mode AD is intutive and simple, but we need to repeat $n$ times if there are $n$ variables. In other words, for each variable $x_i$, we need to set $t = x_i$ while keep others constant. If the process of computing deriative is complex and the number of parameters is huge (which is often the case in deep learning), it would take much time to obtain the whole derivatives. A solution is reverse mode AD, which enables us to obtain the derivatives of all variables in one go.\nAs previously mentioned, the chain rule is defined as\n$$ \\frac{\\partial z}{\\partial x_i} = \\sum_{j}^n\\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} $$\nWe first consider each component $y_j$ that influence $z$, and then the change in each $y_i$ due of the change in an input variable $x_i$. Thus, $\\frac{\\partial y_j}{\\partial x_i}$ could be zero for any $y_j$ that does not invovle $x_i$.\nHowever, the chain rule can also be expressed as\n$$ \\frac{\\partial z}{\\partial x_i} = \\sum_{j}^n \\frac{\\partial y_j}{\\partial x_i} \\frac{\\partial z}{\\partial y_j} $$\nIn this case, we first consider what output variables a given input variable $x_i$ can affect, and then how each output variable influence $z$. If $y_i$ has no contribution to $z$, $\\frac{\\partial z}{\\partial y_j}$ is zero.\nTherefore, suppose the yet-to-be-given output variable is $s$, we have\n$$ \\frac{\\partial s}{\\partial z} = ? \\\\ \\frac{\\partial s}{\\partial b} = \\frac{\\partial z}{\\partial b} \\frac{\\partial s}{\\partial z} \\\\ \\frac{\\partial s}{\\partial a} = \\frac{\\partial z}{\\partial z} \\frac{\\partial s}{\\partial z} \\\\ \\frac{\\partial s}{\\partial y} = \\frac{\\partial a}{\\partial y} \\frac{\\partial s}{\\partial a} \\\\ \\frac{\\partial s}{\\partial x} = \\frac{\\partial a}{\\partial x} \\frac{\\partial s}{\\partial a} + \\frac{\\partial b}{\\partial x} \\frac{\\partial s}{\\partial b} = (y + cos(x)) \\frac{\\partial s}{\\partial z} $$\nWe can see the dependency between variables by drawing a tree, as shown in Figure 2. Besides, we also see that backprop follows breadth-depth first strategy. In other words, we compute gradients level by level.\n  Figure 2: Visualisation of reverse mode AD.  When implementing reversel mode AD, we need to track the children nodes for each input variable and the derivative w.r.t each input input variable at this step, which are used to apply chain rule. Some example codes are shown below.\nimport math class Var: def __init__(self, value): self.value = value self.children = [] self.grad_value = None def grad(self): if self.grad_value is None: self.grad_value = sum(weight * var.grad() for weight, var in self.children) return self.grad_value def __repr__(self): return repr(self.value) def __str__(self): return str(self.value) def __mul__(self, other): z = Var(self.value * other.value) self.children.append((other.value, z)) # (derivative, output) other.children.append((self.value, z)) return z def __add__(self, other): z = Var(self.value + other.value) self.children.append((1, z)) other.children.append((1, z)) return z def sin(x): z = Var(math.sin(x.value)) x.children.append((math.cos(x.value), z)) return z x = Var(2.0) y = Var(3.0) z = x*y + sin(x) z.grad_value = 1.0 # see the gradient of z to 1 print(x.grad()) References  Dual Numbers \u0026amp; Automatic Differentiation Reverse-mode automatic differentiation from scratch, in Python Matrix Calculus Deep Learning - Computation \u0026amp; Optimization "
            }
    
        ,
            {
                "id": 38,
                "href": "https://ixiaopan.github.io/blog/post/misc-leetcode-01/",
                "title": "Leetcode 101",
                "section": "post",
                "date" : "2021.09.06",
                "body": "Practice! Practice! Practice!\nMath Reverse an integer  the key is to multiply each remainder by 10 in each division $a*10*10+b*10+c$  reverse_x = 0 while x \u0026gt; 0: mod_x = x % 10 # mod operation x = x // 10 # floor division  # reverse reverse_x = reverse_x * 10 + mod_x Plus one  if $d==9$ ( $v==0$ ), we need to increment 1 for the next digit otherwise we are done  def plusOne(digits): d = digits[-1] v = (d + 1) % 10 if v == 0: return self.plusOne(digits[:-1]) + v else: digits[-1] = v return digits Binary addition  elementwise addition between numbers with variable length addition starts from right to left  inc = 0 while l \u0026gt;= 0 and r \u0026gt;= 0: # a,b should have the same length t = a[l] + b[r] + inc v = t % 2 # the result of the current position if t \u0026gt;= 2: inc = 1 else: inc = 0 l -= 1 r -= 1 # which is equivalent to  inc = 0 while l \u0026gt;= 0 or r \u0026gt;= 0 or inc \u0026gt; 0: if l \u0026gt;= 0: # no need to ensure that len(a) == len(b) inc +=a[l] l -= 1 if r \u0026gt;=0: inc += b[r] r -= 1 v = inc % 2 inc = inc // 2 The Euclidean Algorithm # find the greatest common divider of A and B def findGCD(A, B): while B \u0026gt; 0: A, B = B, A % B return A Array The second largest number # the largest number is unique max_v, sec_max_v = -999, -999 for n in nums: if n \u0026gt; max_v: sec_max_v = max_v max_v = n elif n \u0026gt; sec_max_v: sec_max_v = n String Two pointer approach The same direction  one slow-runner and one fast-runner, $+=1$ the slow-runner points at the placeholders the fast-runner is used to loop each element the key is to determine the movement strategy  def removeDuplicate(nums): start = 0 end = 1 while end \u0026lt; len(nums): if nums[end] == nums[start]: end += 1 else: if end - start != 1: nums[start + 1], nums[ end ] = nums[ end ], nums[start + 1] start += 1 end += 1 return start + 1 The opposite direction  the left pointer $+=1$ the right pointer $-=1$ This technique is often used in a sorted array  def reverseString(s): l, r = 0, len(s) - 1 while l \u0026lt; r: s[l], s[r] = s[r], s[l] l += 1 r -= 1 return s Two sum (sorted array)\ndef twoSum(nums): l, r = 0, len(nums) - 1 while l \u0026lt; r: t = nums[l] + nums[r] if t \u0026gt; target: r -= 1 elif t \u0026lt; target: l += 1 else: return [l, r] Binary Search An effective way to find an element in a sorted array.\n if not sorted, we can always sort the collection and apply binary search divide search space in two spaces each time converge at some position  Template 1  basic form of binary search Loop ends when the two pointers meet $ \\text{left} \u0026lt;= \\text{right} $  def findElement(nums, target): i, j = 0, len(nums) - 1 while i \u0026lt;= j: # mid = (i + j) // 2 mid = i+ (j - i) // 2 # better in this way due to overflow if target \u0026gt; nums[mid]: i = mid + 1 elif target \u0026lt; nums[mid]: j = mid - 1 else: return mid return -1 Template 2  Loop ends when there is only one element left search left: $right = mid$ search right: $ left = mid + 1$ $ \\text{left} \u0026lt; \\text{right}$  class Solution: def firstBadVersion(self, n, pick): \u0026#34;\u0026#34;\u0026#34; :type n: int :rtype: int \u0026#34;\u0026#34;\u0026#34; l, r = 1, n while l \u0026lt; r: mid = (l+r)//2 if isBadVersion(mid, pick) is False: # search right l = mid + 1 else: # search left r = mid # if l \u0026lt;= n and isBadVersion(l, pick) is True: return l # return -1 Template 3 Hash Table Keep the corresponding value of a specified key in advance\nTwo sum\n if the next value is the desired value, then it must be in the dict otherwise, we update the dict  hashmap = {} for i in range( len(nums) ): if nums[i] not in hashmap: hashmap[ target - nums[i] ] = i else: return [ hashmap[ nums[i] ], i ] Refer  Array and String - LeetCode Binary Search - LeetCode The Euclidean Algorithm "
            }
    
        ,
            {
                "id": 39,
                "href": "https://ixiaopan.github.io/blog/post/misc-sql/",
                "title": "Daily SQL Practice",
                "section": "post",
                "date" : "2021.07.30",
                "body": "Practice! Practice! Practice!\nOrder of execution  from where group by having select order by limit  Leetcode Q1 Delete Duplicated Emails\n delete all duplicate email entries in a table named Person, keeping only unique emails based on its smallest Id.\n -- query all duplicated emails SELECT email FROM Person GROUP BY email HAVING COUNT(*) \u0026gt; 1 -- solution 1: 1414ms -- got an error: You can\u0026#39;t specify target table \u0026#39;Person\u0026#39; for update in FROM clause DELETE FROM Person WHERE Id NOT IN (SELECT MIN(Id) uid FROM Person GROUP BY Email) -- fixed - delete duplicated emails DELETE FROM Person WHERE Id NOT IN (SELECT a.uid FROM (SELECT MIN(Id) uid FROM Person GROUP BY Email) a ) -- solution 2: 1719ms DELETE p1 FROM Person p1, Person p2 WHERE p1.Email = p2.Email and p1.Id \u0026gt; p2.id What I learned\n  You can't specify target table 'Person' for update in FROM clause\n You cannot update a table and select directly from the same table in a subquery. You can work around this by using a multi-table update in which one of the tables is derived from the table that you actually wish to update, and referring to the derived table using an alias. More detail\n   DELETE statement can specify which table to delete from combining JOIN see here\n  Q2 Rising Temperature\n find all dates' id with higher temperature compared to its previous dates (yesterday).\n SELECT w2.id FROM Weather w1,Weather w2 WHERE DATEDIFF(w2.recordDate, w1.recordDate) = 1 and w2.Temperature \u0026gt; w1.Temperature What I learned\n MySQL - DATEDIFF  Hackerrank Q1 weather-observation-station-5\n Query the two cities in STATION with the shortest and longest CITY names, as well as their respective lengths (i.e.: number of characters in the name). If there is more than one smallest or largest city, choose the one that comes first when ordered alphabetically.\n /* CITY: DEF, ABC, PQRS and WXY. outputs: ABC 3 PQRS 4 */ -- v1 (SELECT city, CHAR_LENGTH(city) from station ORDER by CHAR_LENGTH(city), city LIMIT 1) UNION (SELECT city, CHAR_LENGTH(city) from station ORDER by CHAR_LENGTH(city) desc, city LIMIT 1) -- v2 (SELECT city, CHAR_LENGTH(city) from station where CHAR_LENGTH(city) = (SELECT MAX(CHAR_LENGTH(city)) from station) ORDER BY city asc LIMIT 1) UNION (SELECT city, CHAR_LENGTH(city) from station where CHAR_LENGTH(city) = (SELECT MIN(CHAR_LENGTH(city)) from station) ORDER BY city asc LIMIT 1); What I learned\n UNION  combine multiple result sets, the selected columns must have the same size, data type, and order removes one duplicate row; To retain the duplicate row, use UNION ALL   CHAR_LENGTH()  return the length of a string, measured in characters   MIN(expr)/MAX(expr)  return the maximum/minimum value of expr    Q2 weather-observation-station-6\n Query the list of CITY names starting with vowels (i.e., a, e, i, o, or u) from STATION.\n -- v1 select distinct city from station where city LIKE \u0026#39;a%\u0026#39; or city LIKE \u0026#39;e%\u0026#39; or city LIKE \u0026#39;i%\u0026#39; or city LIKE \u0026#39;o%\u0026#39; or city LIKE \u0026#39;u%\u0026#39; where city LIKE \u0026#39;%a%\u0026#39; or city LIKE \u0026#39;%e%\u0026#39; or city LIKE \u0026#39;%i%\u0026#39; or city LIKE \u0026#39;%o%\u0026#39; or city LIKE \u0026#39;%u%\u0026#39; -- v2 select distinct city from station where city REGEXP \u0026#39;^[aeiou]\u0026#39; -- start with vowels where city REGEXP \u0026#39;a|e|i|o|u\u0026#39; -- contains vowels where city NOT REGEXP \u0026#39;^[aeiou]\u0026#39; -- not start with vowels where city REGEXP \u0026#39;^[aeiou].*[aeiou]$\u0026#39; -- start\u0026amp;end with vowels  -- v3 select distinct city from station where LEFT(city, 1) IN (\u0026#39;a\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;u\u0026#39;) -- start with vowels where LEFT(city, 1) IN (\u0026#39;a\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;u\u0026#39;) and RIGHT(city, 1) IN (\u0026#39;a\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;u\u0026#39; -- start\u0026amp;end with vowels  What I learned\n REG_EXP / NOT REG_EXP  match substring   LEFT(str, n)  return the leftmost n characters    Q3 The Blunder\n Write a query calculating the amount of error (i.e.: average monthly salaries), and round it up to the next integer.\n SELECT CEIL(AVG(o.salary) - AVG(t.salary_wo_zero)) from (SELECT id, REPLACE(salary, 0, \u0026#39;\u0026#39;) as salary_wo_zero from employees) t JOIN employees as o on o.id = t.id What I learned\n Every derived table must have its own alias REPLACE()  return the string with all occurences of from_str replaced by to_str   Specify table name when two tables have the same columns  Q4 The Score Report\n Generate a report containing three columns: Name, Grade and Mark.\n in descending order by grade oder by name alphabetically if there is more than one students with the same grade Print \u0026ldquo;NULL\u0026rdquo; as the name if the grade is less than 8 (1-7) oder by marks if there is more than one students with the same grade (1-7)   SELECT CASE WHEN grade \u0026gt; 7 THEN name ELSE NULL END name, grade, marks FROM Students JOIN Grades WHERE Marks between Min_Mark and Max_Mark ORDER BY grade desc, name, marks What I learned\n join/inner join without on is the same as cross join on is required for left/right join but optional for join/inner join BETWEEN  used in where clause to find values within a range   CASE WHEN  evaluate a list of conditions and return one of the possible results    Q5 earnings-of-employees\n find the maximum total earnings for all employees as well as the total number of employees who have maximum total earnings.\n SELECT months * salary as earnings, COUNT(*) from employee GROUP BY 1 ORDER BY 1 desc LIMIT 1 What I learned\n In MySQL, you can use the column alias in the group by/order by/having to refer to that column  Q6 draw-the-triangle\n--- * * * * * --- * * * * --- * * * --- * * --- *  create table Test(id integer, title varchar(10)); insert into Test(id, title) values(1, \u0026#34;Hello\u0026#34;); insert into Test(id, title) values(2, \u0026#34;Hello\u0026#34;); insert into Test(id, title) values(3, \u0026#34;Hello\u0026#34;); insert into Test(id, title) values(4, \u0026#34;Hello\u0026#34;); insert into Test(id, title) values(5, \u0026#34;Hello\u0026#34;); SELECT *, @NUMBER:=@NUMBER-1 FROM Test, (SELECT @NUMBER:=5) t ; --- id\ttitle\t@NUMBER:=5 @NUMBER:=@NUMBER-1 --- 1\tHello\t5 4 --- 2\tHello\t5 3 --- 3\tHello\t5 2 --- 4\tHello\t5 1 --- 5\tHello\t5 0  --- solution 1 SET @counter = 3; SELECT *, REPEAT(\u0026#39;* \u0026#39;, @counter := @counter - 1) FROM Test WHERE @counter \u0026gt; 2; --- id\ttitle\tREPEAT(\u0026#39;* \u0026#39;, @number := @number - 1) --- 1\tHello\t* *  --- solution 2 SET @counter = 3; SELECT *, REPEAT(\u0026#39;* \u0026#39;, @counter := @counter - 1) FROM Test LIMIT 1 --- solution 3 SELECT REPEAT(\u0026#39;* \u0026#39;, @NUMBER := @NUMBER - 1) FROM information_schema.tables, (SELECT @NUMBER:=21) t LIMIT 20; What I learned\n SELECT a number or string without FROM clause, the query will return one row with that value SELECT a number or string from a table, the query will return that value for every row in the table := is an assignment operator SET @var = val declar a variable  Q7 median\nSELECT ROUND(AVG(lat_n), 4) from ( SELECT lat_n, ROW_NUMBER() OVER(ORDER BY lat_n) AS row_num, COUNT(lat_n) OVER() AS t_row_num from station ) t2 WHERE row_num in (FLOOR((t_row_num-1)/2+1), CEIL((t_row_num-1)/2+1)) What I learned\n median = ( floor(N/2) + ceil(N/2) ) / 2 where N is a 0-based counter OVER()  ROW_NUMBER() to obtain the positional index for each row COUNT() to calculate total number of a subset    Q8 symmetric-pairs\n Two pairs (X1, Y1) and (X2, Y2) are said to be symmetric pairs if X1 = Y2 and X2 = Y1.\nWrite a query to output all such symmetric pairs in ascending order by the value of X. List the rows such that X1 ≤ Y1.\n SELECT f1.X, f1.Y from Functions f1 JOIN Functions f2 on f1.Y = f2.X and f1.X = f2.Y GROUP BY f1.X, f1.Y HAVING COUNT(f1.X) \u0026gt; 1 or f1.X \u0026lt; f1.Y ORDER BY f1.X What I learned\n HAVING COUNT(f1.X) \u0026gt; 1 to filter own mirrored pairs - there is only one row (2, 2), which should be excluded f1.X \u0026lt; f1.Y to filter duplicated pairs - (2, 5), (5, 2), then (5, 2) should be discarded  Q9 Pivot\n Pivot the Occupation column in OCCUPATIONS so that each Name is sorted alphabetically and displayed underneath its corresponding Occupation. The output column headers should be Doctor, Professor, Singer, and Actor, respectively.\n --- Doctor, Professor Singer, Actor --- Jenny Ashley Meera Jane --- Samantha Christeen Priya Julia --- NULL Ketty NULL Maria  SELECT MIN(Doctor), MIN(Professor), MIN(Singer), MIN(Actor) FROM (SELECT Name, Occupation, RANK() OVER(PARTITION BY Occupation ORDER BY Name) AS row_num, CASE WHEN Occupation = \u0026#39;Doctor\u0026#39; THEN Name ELSE NULL END AS Doctor, CASE WHEN Occupation = \u0026#39;Professor\u0026#39; THEN Name ELSE NULL END AS Professor, CASE WHEN Occupation = \u0026#39;Singer\u0026#39; THEN Name ELSE NULL END AS Singer, CASE WHEN Occupation = \u0026#39;Actor\u0026#39; THEN Name ELSE NULL END AS Actor FROM OCCUPATIONS) t GROUP BY row_num What I learned\n MIN(expr) can be used with numeric, char, datetime datatypes, ignoring NULL  e.g. MIN(['AA', 'AB', 'AC']) = 'AA'   The idea is to  construct a one-hot encoding table group by occupation and rank by name within each group aggregate each row by grouping by within-group row number using MIN(str)    References  join without on SQL Alias sql-using-alias-in-group-by SQL-Hackerrank-Draw-the-triangle SQL := "
            }
    
        ,
            {
                "id": 40,
                "href": "https://ixiaopan.github.io/blog/post/misc-interview/",
                "title": "Software Engineer Interview",
                "section": "post",
                "date" : "2021.07.29",
                "body": "From the beginning of July, I started to look for job opportunities in Data Science, Front-End, or both. The whole process is time-consuming and tedious. Now I have achieved my goal ( I mean the experience in seeking a job, I still have no offer for now ), so I am going to share my idea and recent thoughts about job seeking.\nWhat I learned Just a moment ago, I finished my final technical interview, which should have gone well, but it didn’t. I failed at the code part. The reason is that I was too confident in my coding skills, but the fact is that my mind was blank when I wrote code. So what happened? There are several reasons leading to this.\nOverconfident I have to admit that my programming skills have indeed become poorer.\nThe prime reason is that I was overconfident in my programming skills. In fact, I have not written codes for almost one year. ( I mean writing code every day ) I have been accustomed to googling and looking up official docs so that I cannot remember some common APIs. In short, I focus more on utilising previous work experience to solve problems, instead of paying too much attention to specific functions. Anyway, anyone can write these functions from scratch.\nAnother reason is that I’m a bit evasive in programming. Why? Because of the heavy workload. So I somewhat avoid programming subconsciously. I hope I could have a life without work-related things during my study, totally relaxing myself.\nLack of preparation The second reason is that I was not fully prepared. In part because I was not familiar with the interview pattern/process. Well, I had never attended a video interview before. I didn\u0026rsquo;t know how it runs. But basically, I was blindly confident in myself.\nOn the other hand, I applied for a new position (software engineer), which is a mix of frontend, machine learning and database. To be honest, SQL is not my cup of tea. I rarely use it during my work. I am not sure what questions will be asked to me in the interview. I had no previous interview experience to learn from.\nEnglish Poor English listening and communication skills. That\u0026rsquo;s the hard part. On the one hand, I am not familiar with some technical terms in English (Machine Learning is okay, but some frontend terms are different from my understandings), though I am always reading technical documents in English. Well, that\u0026rsquo;s another thing, you know. On the other hand, I am not able to express my thoughts completely and fluently due to poor vocabulary. Sounds very frustrating.\nWhat to do  Strengthen the understanding of basic machine learning concepts  Some concepts e.g. FP, FN are still not clear. I know the definition, but I am still confused when/how to use it and express it clearly. I need more hands-on experience.   Enhance programming skills  SQL Python, NumPy and Pandas ( I often mixed the syntax of Python and JavaScript, oppps\u0026hellip;)   English??  no idea, since I am not sure whether I will stay abroad    Why Data Science Almost everyone will ask me this question — Why data science? I\u0026rsquo;m still asking myself, too.\nShort answer  Do what I want - experience different jobs\n I am still interested in frontend, but I would like to try something new. Life has no boundaries. Just do what you want if you are able to. Maybe the worst case is that I would go back to Frontend. Well, it\u0026rsquo;s not too bad, after all, I don\u0026rsquo;t hate frontend at all.\nLong answer First, I had a strong desire to study last year. Meanwhile, I started to become interested in data, so after doing research, I chose data science as my master\u0026rsquo;s degree.\nSecond, I believe that machine learning and AI are the future trends of technology since we are generating more and more data and companies have realised the importance and value of data. Big data applications are everywhere. So I think it\u0026rsquo;s an exciting job which could bring big changes to the world and have a bright career prospect.\nThird, as a developer, I dream to be a full-stack developer. At least I don\u0026rsquo;t limit myself to frontend or backend only. As a compound developer with the knowledge of both machine learning and frontend, I believe I have more choices.\nBesides, I like to try something new. I also expect how far I can go.\n"
            }
    
        ,
            {
                "id": 41,
                "href": "https://ixiaopan.github.io/blog/post/dl-00-pytorch/",
                "title": "Deep Learning - PyTorch",
                "section": "post",
                "date" : "2021.07.19",
                "body": "The most popular deep learning frameworks nowadays are Tensorflow and PyTorch. Well, during my study, I use PyTorch more often. Recently, I am building the classic BiLSTM-CRF model using PyTorch. It\u0026rsquo;s a bit hard for me when operating matrices since it uses various advanced techniques about indexing and slicing. I think it\u0026rsquo;s necessary to explain these amazing functions. Therefore, I am going to write this post to revisit the most important aspects of PyTorch for future references.\nTensor Tensor is the primary data structure in PyTorch. Simply put,\n A tensor refers to a multi-dimensional array where you can access an individual element by indexing A tensor\u0026rsquo;s rank tells us how many indexes are needed to refer to a specific element within the tensor. It can be zero, one, two, three and so on. If we have a rank-2 tensor, it means that this tensor have 2 axes or it\u0026rsquo;s a matrix. An axis of a tensor is a specific dimension of a tensor. In practice, we often do computation along a specifc axis.  torch.tensor(1.) torch.tensor([1, 2, 3]) torch.arange(12).reshape(3, 4) torch.arange(12).reshape(2, 3, 2) A tensor\u0026rsquo;s shape is very important because it contains all the information about the tensor like rank and the length of each axis. For example, in image classification, the data typically have the following shape. It\u0026rsquo;s common to see NCHW rather than BCHW, where B is replaced by N. Common orderings are show below.\n NCHW NHWC CHWN  [Batch, Channels, Height, Width] storage In fact, the tensor declared above is just a view of the underlying data. View means a kind of way to look at data. For example, you can look at an image from the top direction or the right direction. In the case of data, you could look at data in the original order or you could skip a fixed number of elements.\nNo matter how you view it, the data in memory stay the same. In fact, the real values are stored in a contiguous block of memory. In PyTorch, we can access it by invoking tensor.storage(). For example,\nx = torch.tensor([[0, 1, 2], [3, 4, 5]]) x.storage() The result is shown below. You might find that the elements are sorted along the rows of the tensor $x$. Tthe difference is that they are stored in a one-dimensional array.\n  Figure 1: The underlying data beneath the tensor x  Here comes a question: How does the indexing operation $x[0, 1]$ work?\nstride In oder to index an element, PyTorch needs to know meta data about a tensor, such as stride, which indicates the number of elements to be skipped along each dimension. In the above example, the stride is\nx.stride() # (3, 1) $3$ means we need to skip 3 elements to get to the next row, 1 means we just move one step so as to reach the next column. Mathematically, the indexing operation in 2D tensor are described as follows, where offset is usually zero\n$$ x[i, j] = i * \\text{stride}[0] + j * \\text{stride}[1] + \\text{offset} $$\nWhy is it designed like this? It\u0026rsquo;s less expensive for some operations like transpose since there is no need to reallocate memory space. Instead, we just modify the stride.\nFor instance, $x^T$ is shown below\n0 3 1 4 2 5 If we look at the underlying data of $x^T$, we will find that $x^T$ and $x$ have the same storage shown in Figure 1.\nx.data_ptr() == xt.data_ptr() # True Let\u0026rsquo;s have a look at the stride of $x^T$\nxt.stride() #(1, 3) In this case, we simply move one step forward if we want to reach the next row while we need to move 3 steps to get to the next column.\nApart from this, knowing how storage and stride work will also help understand other PyTorch functions' behaviour. This article discusses the difference between torch.expand() and torch.repeat(). Accoding to the PyTorch document\n torch.expand()\n​\tExpanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.\ntorch.repeat()\n​\tUnlike expand(), this function copies the tensor’s data.\n x = torch.arange(3) x.stride() # 1 y = x.expand(2, 3) y.stride() # (0, 1) z=x.repeat(2, 1) z.stride() # (3, 1) From the results of strides, we can see that torch.tensor.expand() does not require extra memory space. This is essential when working with large data set.\ncontiguous In Pytorch, there is a concept called contiguous, indicating whether a tensor has the same values as the storage when counting from the innermost dimension. In other words, when moving along the rows in 2D tensors, the values sorted in this way are exactly the order of the underlying data. Visually, such an order is more comfortable and consistent.\nFor example, if we flatten $x$ along the innermost dimension, we will get $$ 0, 1, 2, 3, 4, 5 $$\nIf we flatten $x^T$ along the innermost dimension, we will get\n$$ 0, 3, 1, 4, 2, 5 $$\nwhich is different from the storage shown in Figure 1. Therefore, we say $x$ is contiguous while $x^T$ is not. We can also check it in Pytorch shown below.\nx.is_contiguous() # True xt.is_contiguous() # False Why But wait, why do we need contiguous tensor?\nOne reason is that we can exploit the benefit of cache to improve the speed of fetching data. In short, when fetching an element of a matrix, we can also get the neighboring elements at the same time, requiring less memory accesses.\nAnother reason is that some functions only work in the contiguous tensor, such as view()\nxt.view(1, 6) # RuntimeError: view size is not compatible with input tensor\u0026#39;s size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. There are two approaches to fix it: call contiguous() to make the tensor contiguous or call reshape()\nxt.contiguous().view(1, 6) xt.reshape(1, 6) However, reshape() could return a view (if the tensor is contiguous) or a new tensor (if not).\nHow How do we make $x^T$ contiguous too?\nxtc = xt.contiguous()   Figure 2: The underlying data beneath the tensor $x^T$  Figure 2 shows the new storage. Thus, contiguous() reallocates a new memory and copy the original tensor into that memory space. We can check it using the code below.\nxt.data_ptr() == xtc.data_ptr() # False Meanwhile, the stride has also been changed to adapt to this new storage.\nxT.stride() # (2, 1) torch.nn nn.module torch.nn is the submodule that contains everything needed to build neural networks in PyTorch. We know that a neural network is composed of a stack of layers. In PyTorch, we call these layers modules and they are all subclasses of nn.Module.\nI think we call them modules because a single layer is a layer while multiple layers can also be considered as a big layer. Thus, we refer to both of them as modules regardless how many layers there are. Moreover, a module can also have other submodules (subclasses of nn.Module) as their attributes and thus track their parameters automatically.\nWhen I built the BiLSTM-CRF for NER, I split LSTM and CRF apart. However, I noticed that the BiLSTM model can still retrieve the parameters of the CRF layer. Now I found the reason.\nFirst, both the nn.Linear() and CRF() are moules in PyTorch. Second, when the instance of such a module is assigned to an attribute of another nn.Module ( in this case, it\u0026rsquo;s BiLSTM ), this module will be automatically registered as the submodule of BiLSTM, and thus BiLSTM have access to the parameters of its submodules.\nclass CRF(nn.Module): def __init__(self): super(CRF, self).__init__() class BiLSTM(nn.Module): def __init__(self): super(BiLSTM, self).__init__() self.fc = nn.Linear(10, 2) self.crf = CRF(self.num_of_tag) forward When I first used PyTorch, I often confused why this model doesn\u0026rsquo;t invoke forward(). For example, I have a simple forward network, I have seen two options shown below to move forward.\nself.fc = nn.Linear(5, 4) self.fc(inputs) # option 1, the right way self.fc.forward(inputs) # option 2, wrong Well, the reason is simple. In fact, the built-in PyTorch subclasses of nn.Module allows themselves to be called as a simple function call. In the __call__, it calls forward(). So what\u0026rsquo;s the difference? The difference is that PyTorch will do other operations by provided hooks before calling forward(). Thus, it\u0026rsquo;s possible to call forward() directly, but it shouldn\u0026rsquo;t do this unless we don\u0026rsquo;t provide any hooks. ( More details can be seen here )\nparameter We have built our models, so how to access to the parameters? PyTorch provides parameters() function, which allows us to collect all parameters from the first layer to the last layer.\nfor param in model.parameters(): print(param) However, it\u0026rsquo;s a bit hard to distinguish them when you have many layers. Thus, PyTorch provides another function named_parameters()\nfor name, param in model.named_parameters(): print(name, param) With the name of each layer, it\u0026rsquo;s easy to retrieve individual parameter by accessing the corresponding name directly, such as\nmodel.out_linear.weight model.out_linear.bias So far so good. But where are the names from?\nsequential PyTorch provide two ways to create neural networks. The first one is to use nn.Sequential(), as shown below\nmodel = nn.Sequential( nn.Linear(5, 4), nn.Tanh(), nn.Linear(4, 2) ) \u0026#39;\u0026#39;\u0026#39; Sequential( (0): Linear(in_features=5, out_features=4, bias=True) (1): Tanh() (2): Linear(in_features=4, out_features=2, bias=True) ) \u0026#39;\u0026#39;\u0026#39; Well, the names in this example are just numbers. It would be fine if there are few layers. To build a more semantic model structure, we can pass name for each layer,\nfrom collections import OrderedDict model = torch.nn.Sequential(OrderedDict([ (\u0026#39;hidden_layer\u0026#39;, torch.nn.Linear(5, 4)), (\u0026#39;active_func\u0026#39;, torch.nn.Tanh()), (\u0026#39;out_layer\u0026#39;, torch.nn.Linear(4, 2)) ])) model \u0026#39;\u0026#39;\u0026#39; Sequential( (hidden_layer): Linear(in_features=5, out_features=4, bias=True) (active_func): Tanh() (out_layer): Linear(in_features=4, out_features=2, bias=True) ) \u0026#39;\u0026#39;\u0026#39; As mentioned earlier, we can retrieve the parameters for an individual layer by accessing its name, such as\nmodel.hidden_layer.bias \u0026#39;\u0026#39;\u0026#39; Parameter containing: tensor([ 0.1177, -0.2876, -0.3861, -0.3367], requires_grad=True) \u0026#39;\u0026#39;\u0026#39; Though nn.Sequential() is handy, the order of layers is fixed. That\u0026rsquo;s where nn.Module comes into play. We rewrite the above code using nn.Module\nclass SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.hidden_layer = nn.Linear(5, 4) self.out_layer = nn.Linear(4, 2) def forward(self, inputs): outs = self.hidden_layer(inputs) outs = torch.tanh(outs) outs = self.out_layer(outs) return outs net = SimpleNet() \u0026#39;\u0026#39;\u0026#39; SimpleNet( (hidden_layer): Linear(in_features=5, out_features=4, bias=True) (out_layer): Linear(in_features=4, out_features=2, bias=True) ) \u0026#39;\u0026#39;\u0026#39; Basic Techniques Verify torch print(\u0026#39;torch version:\u0026#39;, torch.__version__) # verify GPU cuda_enabled = torch.cuda.is_available() print(\u0026#39;GPU available:\u0026#39;, cuda_enabled) print(\u0026#39;GPU version:\u0026#39;, torch.version.cuda) # using CUDA t = torch.tensor([1, 2, 3]) if cuda_enabled: t = t.cuda() Creating Tensor With Data torch.Tensor(x) torch.tensor(x) torch.from_numpy(x) torch.as_tensor(x) What\u0026rsquo;s the difference between them? Let\u0026rsquo;s see an example.\nx = np.arange(3) # construct a toy data using numpy print(x, x.dtype) # [0 1 2] int64 o1 = torch.Tensor(x) o2 = torch.tensor(x) o3 = torch.from_numpy(x) o4 = torch.as_tensor(x) # tensor([0., 1., 2.]) torch.float32 # tensor([0, 1, 2]) torch.int64 # tensor([0, 1, 2]) torch.int64 # tensor([0, 1, 2]) torch.int64 x[1] = 5 print(o1) print(o2) print(o3) print(o4) # tensor([0., 1., 2.]) # tensor([0, 1, 2]) # tensor([0, 5, 2]) # tensor([0, 5, 2]) After running the above code, you might have the following questions,\n What\u0026rsquo;s the difference between torch.Tensor and torch.tensor? inconsistent data type — torch.Tensor() returns float32 while others return int. Why? Why do different methods behave so weird after changing the original data?  Q1 difference between torch.Tensor() and torch.tensor()\n torch.Tensor() is the constructor of the torch.Tensor  class torch.tensor() is a factory function that returns tensor object  Q2 Why different dtypes?\n torch.Tensor() uses the default dtype when creating the tensor. You can see the default dtype by invoking the following code.  torch.get_default_dtype() # float32   others use the dtype inferred from the input data.\n  You cannot specify a dtype in the torch.Tensor() constructor, but dtype can be explicitly passed as an argument when using other methods\n  Q3 memory sharing\nThis is because torch.tensor() and torch.Tensor() copy the input data while the latter ones use the same data in memory. We see that x.numpy() also shares the same memory with the original tensor.\nx = torch.randint(5, (3,)) # tensor([3, 2, 2]) a = x.numpy() x[1]=9 print(x) # tensor([3, 9, 2]) print(a) # array([3, 9, 2]) Without Data torch.eye(2) torch.zeros(3, 4) torch.ones(3, 4) Loss function MSE y_pred = torch.randn(3, 5, requires_grad=True) y_true = torch.randn(3, 5) F.mse_loss(y_pred, y_true) # or mse_loss = torch.nn.MSELoss() mse_loss(y_pred, y_true) Cross Entropy # Cross-entropy Loss # Example of target with class indices input = torch.randn(3, 5, requires_grad=True) target = torch.randint(5, (3,), dtype=torch.int64) loss = F.cross_entropy(input, target) # typically, either 1 or 0 loss.backward() # Example of target with class probabilities input = torch.randn(3, 5, requires_grad=True) target = torch.randn(3, 5).softmax(dim=1) loss = F.cross_entropy(input, target) loss.backward() BCE # Binary Cross-Entropy Loss (predicted prob, true prob [0,1]) bce_loss=nn.BCELoss() loss = bce_loss(torch.sigmoid(z), y_true) Reshape Advanced Techniques torch.scatter_ This function is often used to implement one-hot encoding. As its name suggests, scatter means to distribute a list of values over another tensor.\nIn the case of one-hot encoding, the matrix is a parse matrix whose element value is either zero or one. So one is the scalar to be scattered at the corresponding column index for each observation.\nlabel = torch.tensor([2, 3, 4, 0]).view(4, 1) one_hot = torch.zeros(len(label), 5) one_hot.scatter_(1, label, 1) \u0026#39;\u0026#39;\u0026#39; tensor([[0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [1., 0., 0., 0., 0.]]) \u0026#39;\u0026#39;\u0026#39; Further, the scatterd item could be any multi-dimensional array rather than a simple scalar, for example,\nlabels = torch.tensor([[2, 3], [4, 0], [1, 5]]) # 3 observations one_hot = torch.zeros(len(labels), 6).long() # 6 labels in total source = torch.arange(1, 7).long().view(3, 2) one_hot.scatter_(1, labels, source) \u0026#39;\u0026#39;\u0026#39; tensor([[0, 0, 1, 2, 0, 0], [4, 0, 0, 0, 3, 0], [0, 5, 0, 0, 0, 6]]) \u0026#39;\u0026#39;\u0026#39; From the above examples, we conclude that\n labels and source should have the same size along the specified dimension (the first argument) All tensors (one_hot, lables, source) should keep the same shape except the specified dimension  torch.squeeze() squeeze() and unsqueeze() often appear in pairs to expand dimension of a tensor for broadcasting.\n  squeeze() remove all dimensions with the value of 1\nx = torch.arange(8).view(1,2,4) \u0026#39;\u0026#39;\u0026#39; tensor([[[0, 1, 2, 3], [4, 5, 6, 7]]]) \u0026#39;\u0026#39;\u0026#39; x.squeeze(0) \u0026#39;\u0026#39;\u0026#39; tensor([[0, 1, 2, 3], [4, 5, 6, 7]]) \u0026#39;\u0026#39;\u0026#39;   unsqueeze() add one dimension along the specified dimension\n​\nx = torch.tensor([1, 2, 3]) x.unsqueeze(dim=0) # tensor([[1, 2, 3]]) x.unsqueeze(dim=1) \u0026#39;\u0026#39;\u0026#39; tensor([[1], [2], [3]]) \u0026#39;\u0026#39;\u0026#39;   torch.gather() I happened to meet this function when building BiLSTM+CRF NER models. It was hard to implement batch training for CRF layers until I found torch.gather(). Well, it\u0026rsquo;s a bit similar to torch.scatter_() except that torch.gather() aims to fetch data while toch.scatter_() is used to write values.\nFor BilSTM+CRF models, we need to calculate the sentence score as follows,\n$$ \\text{Score} (D_j) = \\sum_{i=0}^{|D_j|} T(y^{w_i} \\rarr y^{w_{(i+1)}}) + E(w_{i+i}|y^{w_{(i+1)}}) $$\nwhere emission scores are the outputs of BiLSTM and the transition scores are the parameters of the CRF layer. For each batch training, we need to calculate each sentence score in this batch.\nemission score: (batch_size, max_seq_len, num_of_tag) [ [ O B I [ w00 0.1 0.2 0.7 ] [ w01 0.2 0.4 0.4 ] ... ], # sentence 1 [ O B I [ w10 0.1 0.2 0.7 ] [ w11 0.2 0.4 0.4 ] ... ] # sentence 2 ] tags: (batch_size, max_seq_len) [ [ w00=\u0026gt;B, w01=\u0026gt;I, ...], # sentence 1 [ w10=\u0026gt;B, w11=\u0026gt;O, ...], # sentence 2 ... ] It\u0026rsquo;s easy to fetch data from the specified index along the desired dimension using torch.gather(). In this case, we want to fetch data from the index that the true tag of each word belongs to along the innermost dimension of emission_score. For example, sentence 1 has two words with the labels B and I, so the corresponding scores in the emission score are emission_score[0][0][1] and emission_score[0][1][2].\nemission_score = torch.tensor([ [ [ 0.1, 0.2, 0.7 ], [ 0.2, 0.3, 0.5 ] ], [ [ 0.35, 0.25, 0.4 ], [ 0.6, 0.25, 0.15 ] ] ]) true_labels = torch.tensor([ [ 1, 2], [ 1, 0], ]) torch.gather(emission_score, 2, true_labels.unsqueeze(-1)).squeeze(-1).sum(-1) \u0026#39;\u0026#39;\u0026#39; tensor([[0.2000, 0.5000], [0.2500, 0.6000]]) =\u0026gt; tensor([0.7000, 0.8500]) \u0026#39;\u0026#39;\u0026#39; which is exactly the second term of $\\text{Score} (D_j) $.\nReferences  https://zhang-yang.medium.com/explain-pytorch-tensor-stride-and-tensor-storage-with-code-examples-50e637f1076d PyTorch - Deeplizard "
            }
    
        ,
            {
                "id": 42,
                "href": "https://ixiaopan.github.io/blog/post/misc-sort/",
                "title": "Sorting",
                "section": "post",
                "date" : "2021.07.11",
                "body": "Data structure and algorithms are two important courses in Computer Science. Knowing how to utilise appropriate data structures and algorithms to fit our problems can greatly decrease memory space and improve performance. Besides, the ideas behind the classical algorithms are also worth learning to strengthen our logical thinking skills. Well, this series of articles are simply quick notes to help refresh my knowledge for future reference. So let\u0026rsquo;s start from sorting.\nBubble Sort Idea Bubble sort is the first sorting algorithm I learned in the module \u0026lsquo;Data Structure\u0026rsquo;. The idea is simple,\n each time, we compare numbers in pairs and make an exchange until we put the largest number at the rightmost position\n Say we have $n = 6$ numbers shown in Figure 1,\n  Figure 1: Toy data used throughout this post  For the first iteration, there are n = 6  items left to sort, so we need to compare n - 1 = 5 pairs of elements consecutively.\n(9, 4), 2, 16, 10, 12 4, (9, 2), 16, 10, 12 4, 2, (9, 16), 10, 12 4, 2, 9, (16, 10), 12 4, 2, 9, 10, (16, 12) Similarly, for the second loop, there are n - 1 = 5 items left to sort, and we need to compare n - 2 = 4 pairs of elements.\n(4, 2), 9, 10, 12, 16 2, (4, 9), 10, 12, 16 2, 4, (9, 10), 12, 16 2, 4, 9, (10, 12), 16 From this, we can conclude that, for the $i\\text{th}$ loop,\n there are $n - i$ items left to sort consequently, we need to compare $n - i - 1$ times  Since we place the largest number at the rightmost position for each loop, so there are $n-1$ loops in total. The following table shows the above procedure.\n   iterations times we need to compare     0 n - 1   1 n - 2   2 n - 3   \u0026hellip;    n - 2 1    Code def bubbleSort(nums): n = len(nums) for i in range(n - 1): # each loop for j in range(n - i - 1): if nums[j] \u0026gt; nums[j+1]: nums[j], nums[j+1] = nums[j+1], nums[j] return nums bubbleSort([9, 4, 2, 16, 10, 12]) # [2, 4, 9, 10, 12, 16] Analysis Stable - Yes\nSince we only swap values when the previous item is greater than the latter item, the items with the same values will keep their order.\nComplexity - O($n^2$)\nFrom the table above, we can see that,\n in the worst case, we need to run comparison $1 + 2 + \u0026hellip; + (n - 1) = \\frac{(n-1)n}{2}$ times in the best case, we still need to compare $\\frac{(n-1)n}{2}$ times  In this example, we notice that 2, (4, 9), 10, 12, 16 have been in order, but the comparison continues. One way is to set a flag to notice it. We can do this because Bubble Sort is an in-place sorting, which means the list has sorted in ascending order while comparing.\ndef bubbleSort(nums): n = len(nums) has_sorted = False for i in range(n - 1): # loop passes if has_sorted: break for j in range(n - i - 1): if nums[j] \u0026gt; nums[j+1]: nums[j], nums[j+1] = nums[j+1], nums[j] has_sorted = False else: has_sorted = True return nums Now the complexity of the enhanced Bubble Sort for the bset case is $O(n)$ while the worset case still stays the same.\nSelection Sort Idea Selection sort is similar to Bubble sort. It first finds the index of the largest value in the remaining list, then makes an exchange between the largest item and the rightmost item for each loop. Therefore, the exchange happens only once in each iteration.\nLike Bubble sort, there are $n - 1$ iterations. For each iteration, we need to find the largest value from the remaining $n -i$ items.\nCode def selectionSort(nums): n = len(nums) for i in range(n - 1): max_index = 0 for j in range(1, n - i): if nums[j] \u0026gt; nums[max_index]: max_index = j nums[n-i-1], nums[max_index] = nums[max_index], nums[n-i-1] return nums Analysis Stable - No\nAfter each iteration, we swap two items, so the order of the items are not ensured.\nComplexity\nObviously, no matter the best case or the worst case, we need $O(n^2)$ comparisons.\nInsertion Sort Idea As the name suggests, we insert an item into the proper position of the well-sorted sublist each time. In other words, we maintain the sublist sorted in each iteration.\nBut how? For each $x_i$ in the sorted list, we compare $x_i$ with the being sorted item $x'$,\n if $x' \u0026gt;= x_i$,  simply put $x'$ behind $x_i$ end   if $x' \u0026lt; x_i$,  move $x_i$ a step forward (in the right direction) then move on to the next $x_i$ in the sorted list repeat the above comparison   or we reach the start point, then we know that $x'$ is the smallest number so far. For example, $2$ is placed at the beginning of the list during the third iteration shown in Figure 2.    Figure 2: Insertion sort  Code def insertionSort(nums): n = len(nums) for i in range(1, n): cur_elem = nums[i] cur_index = i while cur_index \u0026gt; 0 and cur_elem \u0026lt; nums[cur_index - 1]: nums[cur_index] = nums[cur_index - 1] cur_index -= 1 nums[cur_index] = cur_elem return nums Analysis Stable - Yes\nComplexity\nFor the best case where all items are sorted well and the corresponding sublist is sorted, there is no need to do a move operation. Therefore, the complexity is $O(n)$.\nFor the worst case, we still need $O(n^2$) comparisons.\nQuick Sort Idea Quick sort adopts the idea of divide and conquer. The main strategy is that we do a partition at the pivot value, dividing the list into two parts\n the left part contains items that are smaller than the pivot value the right part contains items that are greater than the pivot value  How? Well, we use two pointers: the left pointer and the right pointer that point to the first and the last of the remaining list respectively.\n Move the left pointer forward(towards the right direction) until the pointed element is greater than the pivot value Move the right pointer backward(towards the left direction) until the pointed element is smaller than the pivot value  But how to choose the pivot? Well, you could use the first element of a list.\nLet\u0026rsquo;s take an example. In Figure 3, we choose 9 as our pivot, after some steps we stop at the second row. We do a swap between pointers and then continue moving on until the two pointers pass by each other (the forth row). Well, since the two pointers have met, we cannot move further towards either the right or the left direction, which means that we are done. Obviously, the pivot value should be between the right pointer and the left pointer, so we do an exchange again.\n  Figure 3: Quick Sort  Analysis Stable - No\nWe do a swap between pointers, so the order of elements are not maintained.\nReferences  Problem Solving with Algorithms and Data Structures using Python "
            }
    
        ,
            {
                "id": 43,
                "href": "https://ixiaopan.github.io/blog/post/nlp-02-text-representation/",
                "title": "NLP - Text Representation",
                "section": "post",
                "date" : "2021.07.10",
                "body": "In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven\u0026rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation.\nThere are two aspects to consider: the level of representation and the meaning of numbers. We know that a sentence is composed of words and each word consists of a group of characters. This means we can represent text at sentence level, character-level, or both. As for numbers, the simplest way is to count the number of occurrences of each word. The most common methods based on this idea includes one-hot encoding, bag-of-word and TF-IDF.\nFrequency-Based One-hot One-hot representation indicates whether a word/character is present in a sentence/word. If true, we assign the value of 1 to that word, otherwise 0.\ncharacter-level Figure 1 shows a simple word representation at character level. The whole vocabulary contains 26 English letters. For each word, for example, the word impossible\n each row represents a character in impossible, so there are 10 rows The corresponding value of each row is a vector whose element value is either 0 or 1, indicating whether the corresponding letter is present in the given word  Thus, we will get a (10, 26) matrix for the word impossible.\n  Figure 1: character-vocabulary occurrence matrix with the shape of (|word|, |vocabulary|)  Below are simple codes to construct such a matrix.\nline = \u0026#39;Impossible Mr Bennet impossible when I am not acquainted with him\u0026#39; line = \u0026#39;\u0026#39;.join(line.lower().split()) line, len(line) # (\u0026#39;impossiblemrbennetimpossiblewheniamnotacquaintedwithhim\u0026#39;, 55) # each row represent a character in the sentence alphabet = \u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39; letter_tensor = torch.zeros(len(line), len(alphabet) for i, letter in enumerate(line): idx = alphabet.index(letter) letter_tensor[i][idx] = 1 word-level From the view of sentences, the vocabulary is all the unique words in the corpus and each row represents each word. The implementation of one-hot encoding at word-level is similar to the above codes.\nline = \u0026#39;Impossible Mr Bennet impossible when I am not acquainted with him\u0026#39; # build vocabulary vocabulary = set(line.lower().split()) word2idx = {w: idx for idx, w in enumerate(vocabulary)} len(vocabulary), word2idx[\u0026#39;impossible\u0026#39;] # 10, {\u0026#39;impossible\u0026#39;: 0, \u0026#39;not\u0026#39;: 1, \u0026#39;i\u0026#39;: 2, \u0026#39;with\u0026#39;: 3, \u0026#39;acquainted\u0026#39;: 4, \u0026#39;bennet\u0026#39;: 5, \u0026#39;mr\u0026#39;: 6, \u0026#39;him\u0026#39;: 7, \u0026#39;am\u0026#39;: 8, \u0026#39;when\u0026#39;: 9} # each row represent a word in the sentence word_vector = np.zeros((len(line.split()), len(vocabulary))) for idx, w in enumerate(line.lower().split()): j = word2idx[w] word_vector[idx][j] = 1 word_vector.shape # (11, 10) The results are shown below, we can see that this sentence can be represented as a 11 x 10 matrix.\n# (|sentence|, |vocabulary|) Impossible [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] Mr [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] Bennet [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] Pros  simple and intuitive to understand and implement  Cons  The size of a ont-hot vector is proportional to the size of vocabulary, resulting in a sparse representation when we have a large corpus The representation matrix doesn\u0026rsquo;t have fixed size. The dimension varies in sentences or words with different lengths. Too naive to capture the similarity between words It cannot deal with out-of-vocabulary(OOV) problem  Bag of Word The idea of Bag-of-word(BOW) is that all the words in the corpus are in a bag without considering the orders and context. The intuition is similar to the concepts introduced in LDA — a topic is characterized by a small specific set of words. Therefore, BOW is commonly used to classify documents. If two documents are similar (have the same words), they are likely to be classified into the same group. Each document is represented as a vector of |V| dimensions, where the element value of this vector is the frequency of the word in the corresponding doc. Say we have the following corpus,\ncorpus = [ \u0026#39;cat eats meat and dog eats meat\u0026#39;, \u0026#39;cat eats fish\u0026#39;, \u0026#39;dog eats bones\u0026#39; ] The vocabulary of this small corpus and the doc-term matrix are\ndef bagofWord(corpus): vocab = set(\u0026#39; \u0026#39;.join(corpus).lower().split()) word2idx = {word: idx for idx, word in enumerate(vocab)} doc_term = [] for doc in corpus: doc_v = [0]*len(vocab) for word in doc.lower().split(): if word in vocab: doc_v[ word2idx[word]] += 1 doc_term.append(doc_v) print(word2idx) for i, doc_v in enumerate(doc_term): print(corpus[i], list(doc_v)) # # {\u0026#39;eats\u0026#39;: 0, \u0026#39;and\u0026#39;: 1, \u0026#39;dog\u0026#39;: 2, \u0026#39;fish\u0026#39;: 3, \u0026#39;cat\u0026#39;: 4, \u0026#39;meat\u0026#39;: 5, \u0026#39;bones\u0026#39;: 6} # cat eats meat and dog eats meat [2, 1, 1, 0, 1, 2, 0] # cat eats fish [1, 0, 0, 1, 1, 0, 0] # dog eats bones [1, 0, 1, 0, 0, 0, 1] one-hot Sometimes, we don\u0026rsquo;t care about the number of occurrence of words. Just like one-hot encoding, we only want to know whether a word is present in the sentence or not, which would be useful for sentiment analysis. Well, that\u0026rsquo;s easy to implement using Sklearn\n# occurrence vectorizer = CountVectorizer(binary=True) X_train_dtm = vectorizer.fit_transform(X_train) X_test_dtm = vectorizer.transform(X_test) Pros  Simple and intutive to understand and implement Captures the semantics of documents, if two docs have similar words, they will be close to each other in the word space Fixed matrix representation no matter how long a sentence is  cons  It ignores the word order(context), so Cat bites man and Man bites cat have the same representation The size of the matrix is proportional to the size of vocabulary It doesn\u0026rsquo;t deal with out-of-vocabulary(OOV) problem It doesn\u0026rsquo;t capture the similarity between different words that have the same meaning, e.g cat eats, cat ate, BOW will treat them as different vectors though they convey the same semantics. As mentioned earlier, the most frequent words are often function words like pronouns, determiners and conjuctions. However, they are of no help for classification.  Bag of N-gram Basically, n-gram is a sequence of N tokens. Generally, we treat each word as an independent unit, in this case, a word is 1-gram or unigram. Similarly, a two-word sequence of words is 2-gram (bigram), three words is 3-gram (trigram), and so on so forth. A simple implementation is shown below.\ndef create_ngrams(tokens, n=2): ngrams = zip(*[tokens[i:] for i in range(n)]) return [\u0026#39; \u0026#39;.join(gram) for gram in ngrams] So what\u0026rsquo;s the use of n-gram?\n estimate the proability of the last word of an n-gram given the previous words assign probabilities to entire sequences  corpus = [ \u0026#39;Dog bites man\u0026#39;, \u0026#39;Man bites dog\u0026#39;, \u0026#39;Dog eats meat\u0026#39;, \u0026#39;Man eats food\u0026#39; ] count_vect = CountVectorizer(ngram_range=(1, 2)) bow_rep = count_vect.fit_transform(corpus) count_vect.vocabulary_ # {\u0026#39;dog\u0026#39;: 3, \u0026#39;bites\u0026#39;: 0, \u0026#39;man\u0026#39;: 10, \u0026#39;dog bites\u0026#39;: 4, # \u0026#39;bites man\u0026#39;: 2, \u0026#39;man bites\u0026#39;: 11, \u0026#39;bites dog\u0026#39;: 1, \u0026#39;eats\u0026#39;: 6, \u0026#39;meat\u0026#39;: 13, # \u0026#39;dog eats\u0026#39;: 5, \u0026#39;eats meat\u0026#39;: 8, \u0026#39;food\u0026#39;: 9, \u0026#39;man eats\u0026#39;: 12, \u0026#39;eats food\u0026#39;: 7} Pros and Cons\n  captures word order and context in a way\n  dimentionality increases as $n$ increases\n  it still have OOV problem\n  TF-IDF So far, we have learned that one-hot encoding focuses more on the occurrence of words in text while BOW pays more attention to word frequency. In both cases, they consider each word in the corpus euqally (with the same weight).\nIn contrast, TF-IDF allows us to measure the importance of each word relative to other words in the doc and the corpus. This is useful for information retrieval systems, where we expect that the most relevant documents should appear first.\nHow does TF-IDF work? As the name suggests, it calculates two quantities:\n  term frequency(TF), the normalized frequency of each token $w_i$ in a given doc $d_j$\n$$ TF(w_i, d_j) = \\frac{|w_i^{d_j}|}{|d_j|} $$\n The intution is that the more frequent a word appears in a doc, the more important it is. Thus, we need to increase its importance    inverse document frequency(IDF), the logarithm of the inverse normalized frequency of each token across all documents\n​ $$ IDF(w_i) = \\text {log} \\frac{|D|}{|w_i^D|} $$\n  The intuiton is fair straightforward — if a word appears across all the docs, for instance, stop words like a, is, and, it\u0026rsquo;s unlikely to capture the characteristics of the doc it belong to. That is, they are more common compared to other less frequent words in the same doc. In other words, we need to reduce its importance, that\u0026rsquo;s why we invert the calculation.\n  we use logarithm to further punish terms that appear more frequently across all the docs\n    Putting it together, the TF-IDF is defined as\n$$ TF\\_IDF = TF(w_i, d_j) * IDF(w_i) $$\nDistributed Representation Continuous Bag Of Words (CBOW) uses context words to predict the center word while Skip-Gram use the current word to predict its neighbouring words. The number of context words is determined by a parameter called \u0026ldquo;window size\u0026rdquo;.\nCBOW Basically, CBOW is a multi-class classification, which can be descibed as follows,\n words are encoded in one-hot format each one-hot encoded vector is fed into the first layer in order to get the embedding of that word combine the above real valued vectors in some way such that it captures the overall context finally, the linear layer and softmax layer are used to predict probability distribution over vocabulary. The largest prob indicates the most likely target word  class CBOW(nn.Module): def __init__(self, voc_size, embd_size): super(CBOW, self).__init__() self.embedding = nn.Embedding(voc_size, embd_size) self.fc = nn.Linear(embd_size, voc_size) def forward(self, x): out = self.embedding(x).sum(1) out = self.fc(out) out_prob = F.softmax(out) return out_prob However, there are two serious problems as one-hot matrix becomes sparse due to increasing vocabulary increases\n the calculation between one-hot and Embedding layer the calculation between Embedding layer and the linear layer the calculation of softmax layer  Vectorization The first problem is easy to solve because there is no need to store the entire one-hot matrix. After all, it\u0026rsquo;s just used to extract the embedding of the corresponding token, which corrsponds the row of the embdding matrix $E$. In other words, we can just store the row index of the words in one dimensional array.\nclass CBOW(nn.Module): ... def one_hot(self, context): \u0026#39;\u0026#39;\u0026#39; context: \u0026#39;thank very much\u0026#39; target: you \u0026#39;\u0026#39;\u0026#39; indices = [ lookup[token] for token in context.split() ] context_vec = np.zeros(len(indices)) # instead of one-hot (sparse matrix), we keep the row index of the embedding matrix context_vec[:len(indices)] = indices context_vec[len(indices):] = mask # in case of the number of tokens in the context is less than the window size return context_vec One thing we should notice is that same word could occur many times. For example, \u0026ldquo;Like\u0026rdquo; occurs twice in the sentence \u0026ldquo;Like (Sunday) Like Rain\u0026rdquo;. When performing backpropagation, we should accumulate the gradients for word \u0026ldquo;Like\u0026rdquo;. This is because that we aggerate the embeddings of all the context words for each instance, and then fed it into a linear layer to make a prediction in the forward process as shown below.\n$$ E_{Like} * 2 + E_{Rain} = E_{aggerate} \\\\ Out = E_{aggerate} * W_{fc} $$ where $E_{Like}$ is the embedding of word \u0026ldquo;Like\u0026rdquo;.\nout = self.embedding(x).sum(1) # (batch, |D|, |E|) out = self.fc(out) Skip-gram Negative Sampling The idea is to transform a multi-class problem into many binary classification problems by sampling the positive example and several negative examples (typically 10 ~ 50) according the frequency distribution of the vocabulary. Frequent words tend to be sampled and rare words is likely not to be sampled, since rare words barely occur in real life and more frequent words would lead to a better generalisation.\nSubsampling The idea of subsampling is that the vecot representation of frequent words do not change significantly, so we do not need to include them all for each training example. For example, \u0026ldquo;the\u0026rdquo; is frequently used in almost every sentence, so this is no need to train \u0026ldquo;France\u0026rdquo; and \u0026quot; the\u0026quot;. Word $w_i$ in the training set will be discarded with a higher probability computed by the formula\n$$ P(w_i) = 1 - \\sqrt {\\frac{t}{f(w_i)}} $$ where $f(w_i)$ is the frequency of word $w_i$ and $t$ is a chosen threshold (typically around $10^{-5}$). If a word appears very frequently, then $P(w_i)$ is close to $1$, which means it will not be used as the context word and target word. This technique could also be applied to discard less frequent phrases, see this post.\nEvaluation King - man + woman = Queen\nRefer  https://kelvinniu.com/posts/word2vec-and-negative-sampling/ Skip-Gram Paper "
            }
    
        ,
            {
                "id": 44,
                "href": "https://ixiaopan.github.io/blog/post/nlp-01-text-preprocessing/",
                "title": "NLP - Text Preprocessing",
                "section": "post",
                "date" : "2021.06.22",
                "body": "From now on, we will focus on a specific domain — Natural Language Processing(NLP), in part because my summer project is about Named Entities Recognition(NER). Therefore, I need to know some text preprocessing techniques and have a good understanding of the state-of-art NLP models, particularly BiLSTM + CRF. The very first step in NLP is text preprocessing, so I am going to start from here.\nConverting text to lowercase For some letter-based languages such as English, letters could be either lowercase or uppercase, e.g. book , Book or BOOK. Since they represent the same word book with the same semantics, there is no need to encode them three times. The common way is to convert all words into a consistent written style — lowercase because of its readable and concise style.\nHowever, this method sometimes could affect semantics of some specific words, for instance, May and may are two different words. Well, for tasks that involve parts-of-speech analysis, it\u0026rsquo;s a matter.\nIn python, it\u0026rsquo;s easy to convert a string into lowercase,\ntext = text.lower() Removing Punctuation For some tasks, punctuations such as , . \u0026quot; @ # are meaningless to us, so we should remove them. In Python, we can do this using the sub() method of the re module to replace any matched punctuation with an empty character\nre.sub(\u0026#39;[,\\\u0026#39;.!\u0026#34;]\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;\u0026#34;20.2 dollars! That\\\u0026#39;s impossible\u0026#34;, he said.\u0026#39;) #\u0026gt;\u0026gt; 202 dollars Thats impossible he said From the above code, we can see some issues,\n Abbreviation, That's -\u0026gt; Thats Prices, 20.2 dollars -\u0026gt; 202 dollars  Maybe we can write some specific rules for a corpus to remove particular punctuations, but it will be time-consuming and inflexible.\nTokenization Tokenization is a technique to split a piece of text into a smaller unit. The unit could be\n a single word a character a combination of several words  Why do we tokenize text? The answer is that the ultimate goal of tokenization is to build vocabulary for a corpus. After all, words in each sentence are ultimately from the vocabulary.\nOkay, so how do we perform tokenization to get a sequence of words? A naive way is to split a string by whitespace shown below.\ntext = text.split(\u0026#39; \u0026#39;) However, there are some issues. For example, United Kindom will be split into two words, United and Kindom. A common practice is to use the popular natural language toolkit (NLTK) to do this\nfrom nltk.tokenize import word_tokenize tokens = word_tokenize(text) With these tokens, how to build vocabulary? Usually, there are two ways to do this,\n include each uniqe token in the vocabulary include only the top K frequently occurring tokens; the idea is that repetition usually conveys the most important information  Stemming \u0026amp; Lemmatization In grammatical aspect, a word could have several variants to express tense, mood or something. For example,\n go, goes, went, gone are different tenses of go higher, highest are comparative and superlative form of high  In other words, these variants are originated from their root words. Generally, most words follow a general rule of to generate the corresponding forms, though there are some rare cases, as shown below,\n eat, ate, eaten good, better, best  Stemming and Lemmatization are two common methods to convert each word back to its original form or the root.\nStemming Stemming is a simple and crude method that simply chopps off letters from the end of a word until a common root is found, which is the idea of Potter Stemming algorithm.\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer words = [\u0026#39;run\u0026#39;, \u0026#39;runner\u0026#39;, \u0026#39;running\u0026#39;, \u0026#39;ran\u0026#39;, \u0026#39;runs\u0026#39;, \u0026#39;easily\u0026#39;, \u0026#39;saw\u0026#39;] p_stemmer = PorterStemmer() for word in words: print(word + \u0026#39;--\u0026gt;\u0026#39; + p_stemmer.stem(word)) # run--\u0026gt;run # runner--\u0026gt;runner # running--\u0026gt;run # ran--\u0026gt;ran # runs--\u0026gt;run # easily--\u0026gt;easili # saw--\u0026gt;saw  We can see that the result of stemming may not be a word, e.g. easili, and that the tense forms of some special words like ran, saw are not processed correctly.\nLemmatization On the contrary, lemmatization returns the true root or lemma of a word by considering the whole vocabulary of a language and the context of that word in the sentence. In the case of ran and saw, the true root words or lemmas are run and see respectively.\nNLTK provides a popular lemmatizer for us — WordNet Lemmatizer, which is an large lexical database of English. But before using it, we need to know the parts of speech of each word, which can also be done using pos_tag() method of NLTK.\n# we mannually specify the parts of speech for each word words = [ (\u0026#34;grows\u0026#34;, \u0026#39;v\u0026#39;), (\u0026#39;running\u0026#39;, \u0026#39;v\u0026#39;), (\u0026#34;better\u0026#34;, \u0026#39;a\u0026#39;), (\u0026#34;cats\u0026#34;, \u0026#39;n\u0026#39;), (\u0026#39;quickly\u0026#39;, \u0026#39;r\u0026#39;) ] lemmatizer = WordNetLemmatizer() [ lemmatizer.lemmatize(w, p) for w, p in words ] # [\u0026#39;grow\u0026#39;, \u0026#39;run\u0026#39;, \u0026#39;good\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;quickly\u0026#39;] Removing stopping words It seems that we\u0026rsquo;ve obtained the tokens as desired. But wait, let\u0026rsquo;s plot the number of occurrence of each word.\n  Figure 1: The top 10 frequently tokens  Figure 1 shows the frequency of each token in the built-in NLTK corpus named 1789-Washington.txt. It can be seen that most tokens are determiners, prepositions, conjuctions or other function words that have little lexical meaning. Obvisously, we need to remove them, which can be done easily using NLTK,\nfrom nltk.corpus import stopwords stop_words_en = stopwords.words(\u0026#39;english\u0026#39;) tokens = [ w for w in tokens if w not in stop_words_en ] Putting it together So far, we\u0026rsquo;ve introduced some basic and necessary text preprocessing techniques. Now let\u0026rsquo;s put it together to build the whole text preprocessing pipeline.\nimport string, re import nltk from nltk.corpus import wordnet from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from nltk.tag import pos_tag from nltk.stem.wordnet import WordNetLemmatizer def get_wordnet_pos(tag): if tag.startswith(\u0026#34;J\u0026#34;): return wordnet.ADJ elif tag.startswith(\u0026#34;R\u0026#34;): return wordnet.ADV elif tag.startswith(\u0026#34;V\u0026#34;): return wordnet.VERB else: return wordnet.NOUN lem = WordNetLemmatizer() stop_words_en = stopwords.words(\u0026#39;english\u0026#39;) def clean_sentence(text): # lower text = text.lower() # remove punctuation text = re.sub(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;,text) text = text.translate(str.maketrans(\u0026#39;\u0026#39;,\u0026#39;\u0026#39;,string.punctuation)) # tokenization # text.split(\u0026#39; \u0026#39;) # naive methods, e.g. New York will be splitted into [\u0026#39;New\u0026#39;, \u0026#39;York\u0026#39;] tokens = word_tokenize(text) # lemmatize, [a, go, c, ...] tokens = pos_tag(tokens) # [(a, \u0026#39;NN\u0026#39;), (went, \u0026#39;VB\u0026#39;), (c, \u0026#39;NN\u0026#39;)] tokens = [ lem.lemmatize(w, get_wordnet_pos(tag)) for w, tag in tokens ] # remove stopping words tokens = [ w for w in tokens if w not in stop_words_en ] tokens = [ w for w in tokens if len(w) \u0026gt; 1 ] return \u0026#39; \u0026#39;.join(tokens) "
            }
    
        ,
            {
                "id": 45,
                "href": "https://ixiaopan.github.io/blog/post/naive-bayes/",
                "title": "Naive Bayes Classification",
                "section": "post",
                "date" : "2021.06.16",
                "body": "In the previous articles, we introduced several classification algorithms like logistic regression. These models are often called discriminative models since they make prediction by calculating $P(Y|X)$ directly. Sometimes it might be hard to compute. Another way to think of this is that samples are generated from the existed distributions. And one of the most popular models is Naive Bayes classification.\nModel Given a sample $\\bold x = [x_1, x_2, \u0026hellip;, x_d]$, where $x_i$ represent each individual feature, we want to know the probability of $\\bold x$ belonging to each class $C_k$. With the aid of Bayes' inference, we can inverse this problem into the forward problem\n$$ P(Y=C_k|X= \\bold x) = \\frac{P(X= \\bold x|Y=C_k)P(Y=C_k)}{P(X= \\bold x)} $$\nThen, Naive Bayes makes the following assumptions\n all features are independent — one paticular feature doesn\u0026rsquo;t affect another, though it\u0026rsquo;s not real in practice. That\u0026rsquo;s why it\u0026rsquo;s called naive (lack of experience) all features have equal importance  Based on the above assumptions, we can rewrite Bayes' theorem as follows,\n$$ P(Y=C_k|X=\\bold x) = \\frac{P(x_1|Y=C_k)P(x_2|Y=C_k)\u0026hellip;P(x_d|Y=C_k)P(Y=C_k)}{P(X=\\bold x)} $$\nSince $P(X=\\bold x)$ is the evidence,\n$$ P(Y=C_k|X=\\bold x) \\propto P(Y=C_k) \\prod_{i=1}^d P(x_i|Y=C_k) $$\nNow the question is that where $P(x_i|Y=C_k)$ comes from. What distribution does the class $C_k$ follow? In Naive Bayes, there are three typical distributions\n Bernoulli Multinonimal Gaussian  Bernoulli Naive Bayes In Bernoulli distribution, the outcome of an event, denoted by $y_i$, is either 1 or 0. Let $p$ be the probability of being 1, so we have\n$$ P(Y=y_i) = y_ip + (1- y_i )(1 - p) $$\nSince 0 or 1 can encode the occurrence of an event, we can use this to filter spam email. We don\u0026rsquo;t care how many suspect words occur in an email. Instead, once they occur, this email should gain more attention. Hence, each email can be represented by a word vector with binary element taking value 1 if the corresponding word appears in the email and 0 otherwise,\n$$ \\bold x = (x_1, x_2, \u0026hellip;, x_{|v|}), \\text{ where } x_i \\in {1, 0} $$\nwhere $V$ is the vocabulary of the training emails. In other words, we do $|V|$ Bernoullis trials for each email. For each word in $V$, if it\u0026rsquo;s present in that email, the outcome is 1, otherwise 0. This repeats $|V|$ times until all words in $V$ have been visited.\nNow suppose there are $D$ emails, including $D_s$ spam emails and $D_h$ ham emails, given a new email $\\bold x $, we want to know whether it\u0026rsquo;s a spam email or not. Based on the model defined above, we need to calculate two things\n the probability of being each class, $P(Y = C_k)$ given that class $C_k$, the probability of sampling the new data $x$, $P(X=\\bold x|Y=C_k) = \\prod_{i=1}^V P(x_i|Y=C_k)$  The first question can be solved by the following equation,\n$$ P(Y=S) = \\frac{|D_s|}{|D|}, P(Y=H) = 1 - P(Y=S) $$\nBased on the assumption of Naive Bayes, we consider each word in the vocabulary as a single independent random variable that follows Bernoulli distribution with probability $p$, where $p$ is the probability of a word $x_i$ occurring in an email of class $C_k$ and can be computed as follows,\n$$ p(x_i | Y=C_k) = \\frac{|D_s(w_i)|}{|D_s|} $$\nwhere $|D_s(w_i)|$ represent the number of emails that belong to class $C_i$ and contain the word $x_i$. Therefore, the probability of $\\bold x$ being classified into the spam category is\n$$ P(Y = S| X = \\bold x ) \\propto P(Y=S) \\prod_{i=1}^V P(x_i|Y = S) $$\n$$ = \\frac{|D_s|}{|D|} \\prod_{i=1}^V \\text{ } y_i P(x_i|Y=S) + (1- y_i )(1 - P(x_i|Y=S)) $$\nMultinomial Naive Bayes In multinomnial distribution, there are $K$ possible outcomes, so the probability is\n$$ p(x_k) = \\prod_{k}^K \\mu_k^{I_{k}}, \\text{ where } _{k} = 1 \\text{ if } x_k = 1, \\text{otherwise } 0 $$\nIf we do $N$ trials, the likelihood of the observations is\n$$ P(m_1, m_2, \u0026hellip;, m_k) = \\prod_{n=1}^N \\prod_{k}^K \\mu_k^{I_{nk}} = \\frac{N!}{m_1!m_2!\u0026hellip;m_k!}\\prod_{k}^K \\mu_k^{m_k} $$\nwhere $m_k=\\sum_n^N I_{nk}$ represents the number of $I_{nk} = 1$ and $\\sum_{i=1}^k m_i = N$. Through MLE, $\\mu_k$ is esimated as\n$$ \\mu_k = \\frac{m_k}{N} $$\nSometimes we are more interested in the frequency of a word, for example, sentiment classification. If words about happy occur more frequently, then we\u0026rsquo;re more likely to classify the text as the positive category. In this case, each text can be represented by a word vector with element whose value is the frequency of that word.\n$$ \\bold x = (x_1, x_2, \u0026hellip;, x_{|V|}), \\text{ where } x_i \\in Z $$\nwhere $V$ is the vocabulary of the training texts. Thus, we can see that a text can be generated by doing $|N_x|$ multinominal trials.\nSuppose there are $D$ texts, including $D_p$ positive texts and $D_n$ negative texts, given a new text $\\bold x $, we want to know whether the sentiment that that text shows is positve or negative.\nAgain, the probability of being a class $C_k$ is given by,\n$$ P(Y=\\text{Positive}) = \\frac{|D_p|}{|D|}, P(Y=\\text{Negative}) = 1 - P(Y=\\text{Positive}) = \\frac{|D_n|}{|D|} $$\nThe likelihood of generating the sample $\\bold x$ is given by,\n$$ P(X=\\bold x|Y=C_k) = P(x_1, x_2, \u0026hellip;, x_{|V|}|Y=C_k) \\propto \\prod_{k}^V \\mu_k^{x_k} $$\nwhere $x_k$ represents the frequency of that word that occurs in the text $\\bold x$ and $\\sum_{i=1}^V x_i = N_x$.\nFor words that not occur in the text, $\\mu_k^0 = 1$. Thus, the posterior probability can be rewritten as follows,\n$$ P(Y = \\text{Positive} | X = \\bold x ) \\propto P(Y= \\text{Positive} ) \\prod_{k=1}^{N_x} \\mu_k $$\nGaussian Naive Bayes Similarly, Gaussian Naive Bayes assumes that data from each class follows a Gaussian distribution.\nZero Probability If a new sample contains a word that is unseen in the training data, the probability is zero.\n$$ \\mu = \\frac{m_k}{N} = 0 $$\nIf one of the term $\\mu_k$ is zero, the whole probability will be zero too. One way to avoid this is to use add-one smoothing, adds a count of one to each unique word.\n$$ \\mu = \\frac{m_k + 1} {N + |V|} $$\nReferences  https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html https://medium.com/atoti/how-to-solve-the-zero-frequency-problem-in-naive-bayes-cd001cabe211 "
            }
    
        ,
            {
                "id": 46,
                "href": "https://ixiaopan.github.io/blog/post/svm/",
                "title": "Support Vector Machine",
                "section": "post",
                "date" : "2021.06.15",
                "body": "Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \\in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,\n$$ y_i d_i \\ge \\Delta $$\nwhere $d_i$ is the distance from $x_i$ to the separating plane and $\\Delta$ is the margin. Our goal is to find a separating plane that maximises $\\Delta$. This is known as Support Vector Machine.\nDistance to hyperplanes How to calculate the distance from a point to a hyperplane ?\n  Figure 1: Distance from a point to a hyperplane.  The black line in Figure 1 represents a hyperplane defined by an orthogonal vector $w$ and a bias $b$\n$$ w^Tx - b||w|| = 0 $$\nThe distance from the origin to the hyperplane is given by\n$$ \\frac{w^Tx_0}{||w||} = \\frac{ b||w||}{||w||} = b $$\nThus, the distance from a point $x_i$ ($x_i \\neq 0$) to the hyperplane is given by\n$$ d_i = \\frac{w^Tx_i}{||w||} - b $$\nPrimal Problem Our goal is to find $w$ and $b$ to maximise $\\Delta$ subject to the following constraints\n$$ y_i ( \\frac{w^Tx_i}{||w||} - b) \\ge \\Delta \\text{ for all i = 1, 2, 3, \u0026hellip;, n} $$\nDivide both sides by $\\Delta$,\n$$ y_i(\\frac{w^Tx_i}{||w||\\Delta} - \\frac{b}{\\Delta}) \\ge 1 $$\nThen define $w' = w/(||w||\\Delta)$ and $b'= b/\\Delta$,\n$$ y_i(w'^Tx_i - b') \\ge 1 $$\nNote\n$$ ||w'|| = ||\\frac{w}{||w||\\Delta}|| = \\frac{1}{\\Delta} $$\nTherefore, minimising $||w'||^2$ is equivalent to maximising the margin $\\Delta$. Now the problem becomes a quadratic programming problem defined below,\n$$ \\text{min}_{(w',b')} \\frac{||w'||^2}{2} \\text{ }\\text{ }\\text{ subject to $y_i(w'^Tx_i - b') \\ge 1$ for all data points} $$\nExtended Feature Space However, data are not always linearly separated, such as data in Figure 2.\n  Figure 2: Non-linearly separable data  Maybe we could do some data transformation and work in a new feature space where data are linearly speparable. Yea, in fact, SVM maps all feature vectors to an extended feature space by defining a special $\\phi(x)$, which is a function of $x$\n$$ x \\rarr \\phi(x) $$\nBelow are some examples of $\\phi(x)$. You can choose any mapping you like.\n$$ \\phi(x) = x_1^2, \\phi(x) = x_2^2, \\phi(x) = \\sqrt {x_1x_2} $$\nDual Form In the extended feature space, we substitute $x$ for $\\phi(x)$. So the question is defined as follows,\n$$ \\text{min}_{(w,b)} \\frac{||w||^2}{2} \\text{ }\\text{ }\\text{ subject to $y_i(w^T \\phi(x_i) - b) \\ge 1$ for all data points} $$\nAs mentioned earlier, we can use Lagrange multiplier to solve constrained optimisation. The Lagrangian function or the primal problem is given by\n$$ \\text{min}_{w, b}\\text{max}_{\\alpha} \\mathcal{L} (w, b, \\alpha) $$ where\n$$ \\mathcal{L} (w, b, \\alpha) = \\frac{||w||^2}{2} - \\sum_{i=1}^N \\alpha_i (y_i(w^T \\phi(x_i) - b) - 1) $$\nsubject to $\\alpha_i \\ge 0$ (because we have inequality constraints).\nThen we tranform the primal problem into the dual problem. We first minimise Lagrange function w.r.t $w, b$ then maximise with respect to $\\alpha$\n$$ \\text{max}_{\\alpha} \\text{min}_{w, b} \\mathcal{L} (w, b, \\alpha) $$\nTaking the derivative of $\\mathcal{L} (w, b, \\alpha)$ w.r.t. $w, b$ respectively,\n$$ \\nabla_w \\mathcal{L} = w - \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) = 0 $$\n$$ \\nabla_b \\mathcal{L} = \\sum_{i=1}^N \\alpha_i y_i = 0 $$\nSubstituing back to the Lagrangian function\n$$ \\text{max}_{\\alpha} \\frac{||w||^2}{2} - \\sum_{i=1}^N \\alpha_i (y_i(w^T \\phi(x_i) - b) - 1) $$\n$$ = \\text{max}_{\\alpha} \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) - \\sum_{i=1}^N (\\alpha_i y_iw^T \\phi(x_i) - \\alpha_i y_ib - \\alpha_i) $$\n$$ = \\text{max}_{\\alpha} \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) - \\sum_{i=1}^N (\\alpha_i y_i (\\sum_{j=1}^N \\alpha_j y_j\\phi(x_j)^T) \\phi(x_i) - \\alpha_i) $$\n$$ = \\text{max}_{\\alpha} \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) - \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{j=1}^N \\alpha_j y_j \\phi(x_j) + \\sum_{i=1}^N \\alpha_i $$\n$$ = \\text{max}_{\\alpha} \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{j=1}^N \\alpha_j y_j \\phi(x_j) $$\n$$ = \\text{max}_{\\alpha} \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j) $$\nThe dual problem is to find $\\alpha$ that maximise $\\mathcal{L}$, subject to constraints\n$$ \\sum_{i=1}^N \\alpha_i y_i = 0 $$\nThe Hessian of $\\mathcal{L}$ is $-\\frac{1}{2} X^TX$ where $X _{ik}$= $y_k \\phi_i(x_k)$, so it\u0026rsquo;s negative seim-definite, and thus there is a unique maximum.\nSoft Margin Okay, let\u0026rsquo;s just leave the dual problem alone for a while(we will go back in the next section). Now we look at another situation where we want to relax the margin constraints. In other words, we allow some samples to appear in the margin area. This is called soft margin. We do it by introducing a slack variable $s_i$ shown in Figure 3.\n  Figure 3: Soft margin   Now the constraint is\n$$ y_i(w'^Tx_i - b') \\ge 1 - s_i $$\nwhere $s_k \\ge 0$. Obviously, the value of $s_i$ includes three situations\n For samples that lies far away from the hyperplane, $s_i = 0$, because they are far enough For $0 \\lt s_i \\le 1$, there exist some samples that lie between margin and on the correct side of hyperplane For $s_i \\gt 1$, samples are misclassified  Objective The objective function becomes\n$$ \\text{min}_{(w',b')} \\frac{||w'||^2}{2} + C\\sum_{i=1}^N s_i $$\nwhere $C$ controls the degree of misclassification we desire\n a small C allows more freedom to relax the margin, which means it\u0026rsquo;s acceptable to tolerate some misclassifications a large C means we want to correctly classify as many samples as possbile  The Lagragian function with slack variables is\n$$ \\mathcal{L} = \\frac{||w||^2}{2} + C\\sum_{i=1}^N s_i - \\sum_{i=1}^N \\alpha_i (y_i(w^T \\phi(x_i) - b) - 1 + s_i) - \\sum_{i=1}^N \\beta_is_i $$\nwhere $\\beta_i$ are Lagrange multipliers that satisfy $\\beta_i \\ge 0$ (KKT condition)\nAgain, we minimise $\\mathcal{L}$ w.r.t $s_i$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial s_i} = C - \\alpha_i - \\beta_i = 0 $$ So we have\n$$ \\alpha_i = C - \\beta_i $$\nSince $\\beta_i \\ge 0$,\n$$ 0 \\le \\alpha_i \\le C $$\nHinge Loss By the way, the objective function can also be written in the following form\n$$ \\text{min}_{(w',b')} \\frac{||w'||^2}{2} + C\\sum_{i=1}^N \\text{max} (0, 1-y_i f(x_i)) $$\nwhere the second term is hinge loss function. So,\n $y_i f(x_i) \\gt 1 $,  $x_i$ is outside margin and on the right side of the hyperplane, so there is no contribution to loss   $y_i f(x_i) = 1 $,  $x_i$ is on the margin. Again, there is no contribution to loss   $y_i f(x_i) \\lt 1 $  $x_i$ lies between the margin or it\u0026rsquo;s misclassified, so it increases the loss    Kernel Trick Kernel Kernel is simply the inner product in feature space,\n$$ K(x, y) = \\phi(x)^T \\phi(y) = K(y, x) $$\nwhere\n $\\phi(x) = [\\phi_1(x), \\phi_2(x), \u0026hellip;, \\phi_k(x)]$ $\\phi_i(x)$ are real valued functions of $x$ ( $\\phi_i(x)$ is just a real number ) $k$ is the number of $\\phi_i(x)$; it could be a specific number or INFINITE( that\u0026rsquo;s crazy! )  so kernel tells us the closeness or similarity between $x$ and $y$. The space spanned by $\\phi(x)$ is called feature space or kernel space\n$$ \\mathcal{K} = \\text{span } [ \\phi(x) | x \\in \\mathcal{X} ] \\in R^k $$\nSymmetric Given a kernel $K: \\mathcal{X} \\times \\mathcal{X} \\rarr R$ and a training data set $\\mathcal{X} = [x_1, x_2, \u0026hellip;, x_n]$, we can construct a kernel matrix or Gram matrix $G \\in R^{n \\times n}$\n$$ G_{ij} = K(x_i, x_j) $$\nObviously, $G$ is a symmetric matrix and can be decomposed into $G = X^TX$, where\n $X$ is a $k \\times n$ matrix each column of $X$ is $\\phi(x_i)$  Positive Semi-Definite Consider a new vector $w = X v \\in R^k$, we have\n$$ ||w||^2 = v^TX^T Xv = v^TGv \\ge 0 $$\nHence, $G$ is positive semi-definite and all eigenvalues of $G$ are equal or greater than zero, i.e. $\\lambda_i \\ge 0$.\nEigenfunction Remember the eigenvector of $G$ is defined as follows\n$$ Gv = \\lambda v $$\nThis means,\n$$ \\sum_{j=1}^n G_{ij} v_j = \\lambda v_i $$\nIf we extend the dimention of $G$ to INFINITE, as shown in Figure 4, we have\n$$ \\sum_{j=1}^{\\infin} G_{ij} v_j = \\sum_{j=1}^{\\infin} K(x_i, x_j) v_j = \\lambda v_i $$\n  Figure 4: G could have infinite dimension (Ref[3])  Then we define an eigenfunciton $\\psi(x)$, which is a real funtion of $x$\n$$ \\sum_{j=1}^{\\infin} K(x_i, x_j) \\psi(x_j) = \\lambda \\psi(x_i) $$\nWith the help of the integral operator, we can rewrite it as follows,\n$$ \\int_{y \\in \\mathcal{X}} K(x, y) \\psi(y) dy = \\lambda \\psi(x) $$\nThus, $Gv = \\lambda v$ is just a special case of this, when $\\mathcal{X}$ is a finite set.\nMercer’s theorem In general there will be a denumerable set of eigenfunctions $[ \\psi_1(x),\\psi_2(x), \u0026hellip; ] $ and the corresponding $[\\lambda_1, \\lambda_2, \u0026hellip;]$ where\n$$ K(x, y) = \\sum_i^{\\infin} \\lambda_i \\psi_i(x) \\psi_i(y) $$\nThis is known as Mercer’s theorem. And we find that $G_{ij} = \\sum_{k=1}^n \\lambda v_i^k v_j^k$ is just a special case of this, when $\\mathcal{X}$ is a finite set.\nIf we define $\\phi_i(x) = \\sqrt \\lambda_i \\psi_i(x) $ then\n$$ K(x, y) = \\sum_i^{\\infin} \\sqrt \\lambda_i \\psi_i(x) \\sqrt \\lambda_i\\psi_i(y) = \\sum_i \\phi_i(x) \\phi_i(y) = \\phi(x)^T \\phi(y) $$\nAn immediate consequence is that for any real $\\psi(x)$,\n$$ \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} K(x, y) \\psi(y) \\psi(x) dy dx $$\n$$ = \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} (\\sum_i^{\\infin} \\lambda_i \\psi_i(x) \\psi_i(y)) \\psi(y) \\psi(x) dy dx $$\n$$ = \\sum_i^{\\infin} \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} [\\sqrt\\lambda_i \\psi_i(y) \\psi(y)] [\\sqrt\\lambda_i \\psi_i(x) \\psi(x) ] dydx $$\n$$ = \\sum_i^{\\infin} \\int_{x \\in \\mathcal{X}}[\\sqrt\\lambda_i \\psi_i(x) \\psi(x) ] (\\int_{y \\in \\mathcal{X}} [\\sqrt\\lambda_i \\psi_i(y) \\psi(y)] dy) dx $$\n$$ = \\sum_i^{\\infin} (\\int_{x \\in \\mathcal{X}}[\\sqrt\\lambda_i \\psi_i(x) \\psi(x) ] dx)^2 \\ge 0 $$\nThus, $v^TGv \\ge 0$ is just a special case of this, when $\\mathcal{X}$ is a finite set.\nPutting it together, the following statements are equivalent,\n  $K(x, y)$ is positive semi-definite\n  The eigenvalue of $K(x, y)$ are non-negative\n  The kernel can be written $$ K(x, y) = \\sum_i^{\\infin} \\lambda_i \\psi_i(x) \\psi_i(y) = \\sum_i \\phi_i(x) \\phi_i(y) $$ where $\\phi(x)$ are real functions\n  For any real function $\\psi(x)$ $$ \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} K(x, y) \\psi(y) \\psi(x) dy dx \\ge 0 $$\n  Properties of Kernels Adding Kernels If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) + K_2(x, y)$ is also a kernel\n$$ \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} K_3(x, y) \\psi(y) \\psi(x) dy dx $$\n$$ = \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} (K_1(x, y) + K_2(x, y)) \\psi(y) \\psi(x) dy dx \\ge 0 $$\nSimilarly, If $K(x, y)$ is a valid kernel so is $cK(x, y)$ for $c \\gt 0$.\nProduct of Kernels If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) K_2(x, y)$ is also a kernel\n$$ K_3(x, y) = K_1(x, y) K_2(x, y) = \\sum_i \\phi_i^1(x) \\phi_i^1(y) \\sum_j \\phi_j^2(x) \\phi_j^2(y) $$\n$$ = \\sum_i \\sum_j \\phi_i^1(x) \\phi_j^2(x) \\phi_i^1(y) \\phi_j^2(y) $$\nDefine $\\phi_k(z) = \\phi_i^1(z)\\phi_j^2(z)$, and the number of $\\phi_k(z)$ is $i * j$\n$$ K_3(x, y) = \\sum_k \\phi_k(x) \\phi_k(y) $$\nExponentiating Kernels $\\text{exp} (K(x, y))$ is a valid kernel since\n$$ exp(K) = 1 + K + \\frac{1}{2} K^2 + \u0026hellip; $$\nsince the addition and multiplication of kernels yield valid kernels, each term is also a kernel and therefore the exponential of a kernel is a kernel.\nCommon Kernels TODO\nLinear Gaussian Quadratic Comments  SVM relys on distances between data points, so it would be better to normalise data if we don\u0026rsquo;t know what features are important. Different C can make a great difference in performance. We can use cross-validation to choose the optimal C. A linear decision boundary doesn\u0026rsquo;t always exist, especially we obtain a poor performance. If so, we might try a non-linear boundary with Kernel. There are many kernel functions designed for particular data types. Often, Kernel comes with its parameters, so fine-tunining them is also important to improve model\u0026rsquo;s performance. We don\u0026rsquo;t need to explicitly know what $\\phi(x)$ are. All we need to know is that there exist a hyperplane that can separate the data linearly in a higher dimensional space. Though we are in the extended feature space, we do computation of the inner product in the original feature space.  References [1] https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n[2] http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/\n[3] https://www.cs.cmu.edu/~bapoczos/other_presentations/kernel_methods_01_10_2009.pdf\n"
            }
    
        ,
            {
                "id": 47,
                "href": "https://ixiaopan.github.io/blog/post/kmeans/",
                "title": "K-means",
                "section": "post",
                "date" : "2021.06.14",
                "body": "So far, we\u0026rsquo;ve talked much about supervised learning. Aside from it, there exist many other learning types such as unspervised learning, semi-supervised learning and so on. This post will introduce one of the most widely used unsupervised clustering algorithms — K-means. We will cover the implementation of K-means algorithm, the limitation of this algorithm as well as its applications.\nProblem Formulation Given an unlabelled dataset $D =[x_1, x_2, \u0026hellip;, x_N] $, we want to explore whether there is a pattern in the data and what pattern is. In the case of clustering, we want to know which samples are close to each other so as to form a cluster. To achieve this, we assign to each sample a membership indicator denoted by $r_{nk}=1$ if the sample is in cluster $k$, otherwise $r_{nj}=0$ for $j \\ne k$. Thus,\n there could be $N$ clusters, i.e. each sample forms a cluster. there could be only one cluster, i.e. the whole dataset forms a cluster.  So what\u0026rsquo;s the optimal value for $K$?\nObjective Function The idea behind K-means is intutive to understand. Similar people or people with similar interests tends to form a group. Such a group also has a small variation. Based on this idea, the aim of K-means is to find $r_{nk}$ to minimise the within cluster variation,\n$$ J = \\sum^N_n\\sum^K_k r_{nk} || x_n - \\mu_k ||^2 $$\nwhere $\\mu_k$ is the center of the cluster that a sample $x_n$ belongs to.\nWe should notice that this objective also limits the ability of K-means somehow. You can imageine $\\mu_k$ as the center of a circle, and the average within cluster variation is the maximum radius of that circle. Any point outside that circle doesn\u0026rsquo;t belong to this cluster. So K-means works better for circular clusters.\n  Figure 1: K-means works better for data with circular clusters (Ref[2])  Approach Since $\\mu_k$ and $r_{nk}$ are both unknown, how to solve it? Well, we solve this by repeating the following two steps,\n Step 1: Randomly choose $K$ cluster centers, and then minimise $J$ w.r.t $r_{nk}$. In this step, we assign to each sample a membership indicator given the fixed cluster centers.  $$ r_{nk} =\\begin{cases} 1 \u0026amp; \\text{if $ k =\\text{argmin}_j ||x_n - \\mu_j||^2$} \\\\ 0 \u0026amp; \\text{otherwise}\\end{cases} $$\n  Step 2: Keep $r_{nk}$ fixed and then calculate the new cluster centers — $\\mu_k$ is equal to the mean of all samples assigned to that cluster $k$. This is because we minimise $J$ w.r.t $\\mu_k$ by taking the derivative of $J$ and setting it to zero\n$$ \\frac{\\partial J}{\\partial \\mu_k} = 2 \\sum^N_{x_n \\in C_k} r_{nk} || x_n - \\mu_k || =0 $$\n$$ =\u0026gt; \\mu_k = \\frac{\\sum_{x_n \\in C_k} r_{nk} x_n}{\\sum_{x_n \\in C_k} r_{nk}} $$\n  So when will this process stop? There are some stopping criterions\n The centroids remain the same Within cluster variance reaches below the threshold you\u0026rsquo;ve set Fixed number of iterations have been reached  Choosing the optimal K Like the method used in K-NN, we plot the metric as a function of $k$ shown below.\n  Figure 2: The optimal K lies at the elbow point (Hands-on machine learning)  We can see that inertia decreases greatly as k increases up to 4, after that, it decreases slowly. On the other hand, increasing k indeed decrease the sum of squared distance from each sample to its closest centroid. This is because the more clusters k there are, the more cohesive each cluster is, and therefore the lower the within cluster variation is.\nLimits of K-means Specify K explicitly Obvisously, we have to explicitly specify $K$. But more often, we don\u0026rsquo;t what the right $K$ is.\nSensitive to shape As said earlier, K-means performs poorly when the clusters vary in sizes or densities, or have nonspherical shape. Figure below shows how K-means deals with a data set containing three ellipsoida clusters with different densities and orientation. Though the solution on the right has a lower inertia, it obviously breaks the inner structure of real clusters. Mabybe we could try Gaussian mixture models for this case.\n Centroids initialization Although K-means algorithm is guaranteed to converge, it might not coverge to the right solution. And this depends on the initialization of the centroids, which can also be seen in above Figure. To avoid this, we\u0026rsquo;d better run the algorithm several times and select the solution that has the minimum within cluster variation.\nHard classification Clustering assignment used in K-means is called hard classification since each sample is assigned to a cluster indicator. But sometimes we would expect probability to estimate uncertainty in clustering assignment. For example, if two clusters overlap, it\u0026rsquo;s hard to have 100% confidence to say that some sample belongs to one of the two clusters.\nReferences [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2]\tJ. VanderPlas, Python Data Science Handbook. Sebastopol, CA: O’Reilly Media, 2016.\n"
            }
    
        ,
            {
                "id": 48,
                "href": "https://ixiaopan.github.io/blog/post/logistic-regression/",
                "title": "Logistic Regression",
                "section": "post",
                "date" : "2021.06.14",
                "body": "We\u0026rsquo;ve known that linear regression can be used to predict a continuous value, but sometimes the target variable might be categorical, i.e. whether tomorrow is sunny or someone has a cancer. Can we still use linear regression to solve this classification problem? The answer is Yes and the algorithm that we will introduce in this article is known as logistic regression. Well, we can also try Perceptron since it\u0026rsquo;s also a linear classifier. PS: Don\u0026rsquo;t mix it up with linear regression. Logistic regression is a classification algorithm.\nLogistic Function Given an input dataset $D = [x_1, x_2, \u0026hellip;, x_n]$ with the corresponding lable $Y = [y_1, y_2, \u0026hellip;, y_n]$ where $x_i \\in R^d$ and $y_i \\in {0, 1}$, we want to construct a linear classifer that gives us a value $z_i$ as a weighted sum of all features\n$$ z_i = w^T x_i + b $$\nRange issue Now the question is how $z_i$ relate to $y_i$ since the range of $z_i$ is from $-\\infin$ to $\\infin$ while $y_i$ is either 0 or 1. Any value that is larger then 1 or lower than 0 has no meaning because we interpret $y_i$ as probability.\nLog-odds Obviously, we need to come up with a way to transform $z_i$ so as to obtain a value that lies between 0 and 1. But how? What about $f(x) = \\text{log} x$ ? No, it fails since the domain of the log function can only be positive. Maybe we could make a modification about the log function like this,\n$$ z_i = \\text{log } \\frac{p_i}{1 - p_i} $$\nwhere $p/(1-p)$ is called odds and $z$ is often called the logit. The inverse of the log-odds function, also known as sigmoid function, is defined as follows,\n$$ \\hat y_i = p_i = \\frac{1}{1 + e^{-z_i}} $$\nFigure 1 shows what sigmoid function looks like. We can see that log-odds helps us to solve the range problem.\n  Figure 1: sigmoid activation function  Parameter Estimation We\u0026rsquo;ve defined the logistic model, the next step is to estimate the parameters, i.e. $w$. Minimising the loss function and MLE are two common ways to solve this. In fact, minimising the loss is equivalent to MLE in logistic regression.\nBinominal In the case of binary classification, $y_i$ follows Bernoulli Distribution, i.e. $y_i \\sim Bern(n, p_i)$\n$$ p(y_i = 1 |x_i) = p_i $$\n$$ p(y_i = 0 |x_i) = 1 - p_i $$\nThe likelihood of a single sample is,\n$$ p(y_i|x_i) = p_i^{y_i} (1-p_i)^{1-y_i} $$\nSo the likelihood function of the whole training data is,\n$$ L = \\prod_{i=1}^N p(y_i|x_i) $$\nThe log likelihood function is given by\n$$ \\text{log} L = \\sum_{i=1}^N \\text{ log } p(y_i|x_i) = \\sum_{i=1}^N y_i\\text{log} p_i + (1-y_i) \\text{log} (1-p_i) $$\n$$ = \\sum_{i=1}^N \\text{log} (1-p_i) + \\sum_{i=1}^N y_i \\text{log} \\frac{p_i}{1-p_i} $$\n$$ = \\sum_{i=1}^N \\text{log} (1-p_i) + \\sum_{i=1}^N y_i (w^Tx_i + b) $$\n$$ = \\sum_{i=1}^N \\text{log} (1- \\frac{1}{1 + e^{-z_i}}) + \\sum_{i=1}^N y_i (w^Tx_i + b) $$\n$$ = \\sum_{i=1}^N -\\text{log} (1 + e^{(w^Tx_i + b)}) + \\sum_{i=1}^N y_i (w^Tx_i + b) $$\nOur goal is to maximise the log likelihood function, so we take the derivative of $L$ w.r.t $w$\n$$ \\frac{\\partial L}{\\partial w_j} = -\\sum_{i=1}^N \\frac{e^{(w \\cdot x_i)}}{1+e^{(w \\cdot x_i)}} x_{ij} + \\sum_{i=1}^N y_i x_{ij} $$\n$$ = \\sum_{i=1}^N (y_i - z_i) x_{ij} $$\nHowever, this is a transcendental equation(an equation containing the variables being solved for), which means we cannot solve analytically, and thus there is no closed-form solution. But we can still find the estimation of $w$ through gradient descent using the above derivative.\nThe loss function used in logistic function is defined as follows,\n$$ L = - \\sum_{i=1}^N y_i \\text{log} p_i - (1-y_i) \\text{log} (1-p_i) $$\nwhich is often called the \u0026lsquo;Binary Cross Entropy\u0026rsquo;. Obviously, the loss function is exactly the negative log likelihood function. Thus, maximising likelihood is equal to minimising the loss.\ndef crossEntropyLoss(X, y, theta): epsilon = 1e-12 p = logistic(X @ theta) cost = -np.mean(y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon)) return cost Multinominal What if we have more than one target category? It\u0026rsquo;s not a simple yes or no classification since now we have many possible classes. Suppose there are $k$ classes, and each sample must belong to one class, so we have\n$$ \\sum_{j=1}^k p_{i, j} = 1 $$\nJust like the binominal case, we choose one class as the reference model and estimate the \u0026lsquo;odds\u0026rsquo;\n$$ z_{i,j} = \\text{log } \\frac{p_{i, j}}{p_{i, J}} = \\bold w_j ^Txx_i + b_j $$\n$$ p_{i, j} = p_{i, J} e^{z_{i, j}} = p_{i, J} e^{\\bold w_j ^Tx + b_j} $$\nTo obtain $p_{i, j}$, we need to eliminate $p_{i, J}$. Combing the two equations, we have\n$$ \\sum_{j=1}^k p_{i, j} = p_{i, J} \\sum_{j=1}^k e^{z_{i, j}} = 1 $$\n$$ \\text{=\u0026gt; } p_{i, J} = \\frac{1}{\\sum_{j=1}^k e^{z_{i, j}} } $$\nplug $p_{i, J}$ into $p_{i,j}$, we have\n$$ p_{i, j} = \\frac{ e^{\\bold w_j ^Tx + b_j} }{\\sum_{j=1}^k e^{z_{i, j}} } $$\nwhich is known as softmax function.\nAgain, we use MLE to estimate $\\bold w$. The difference from binominal case is that now there are $k$ estimates we need to calculate. The likelihood of a single sample is,\n$$ p(y_i|x_i) = \\prod_{j=1}^{k} p_{ij}^{I_{ij}} $$\nwhere $I_{ij} = 1$ if $x_i$ belong to class $j$, otherwise $I_{ij}=0$. The likelihood function of the whole training data is,\n$$ L = \\prod_{i=1}^N \\prod^k_{j=1} p_{ij}^{I_{ij}} $$\nThe log likelihood function is given by\n$$ \\text{log } L = \\sum_{i=1}^N \\sum_{j=1}^k I_{ij} \\text{log }(p_{ij})= \\sum_{i=1}^N \\sum_{j=1}^k I_{ij} \\text{log } \\frac{ e^{\\bold w_j ^Tx_i + b_j} }{\\sum_{m=1}^k e^{z_{i, m}} } $$\n$$ = \\sum_{i=1}^N \\sum_{j=1}^k I_{ij} (z_{ij} - \\text{log } \\sum_{m=1}^k e^{z_{i,m}}) $$\nNow let\u0026rsquo;s take the derivative of $L$ w.r.t the coefficients from the $j\\text{th}$ class $\\bold w_j$\n$$ \\frac{\\partial L}{\\partial \\bold w_j} = \\sum_{i=1}^N (I_{ij} x_i -\\sum_{j=1}^k I_{ij}\\frac{e^{z_{ij}} x_i}{\\sum_{m=1}^k e^{z_{i,m}}}) = \\sum_{i=1}^N (I_{ij} - p_{i, j}) x_i $$\nThe matrix form can be written as\n$$ \\frac{\\partial L}{\\partial \\bold w_j} = \\bold X^T (I_j - p_j) $$\nwhere $\\bold X = [x_1, x_2, \u0026hellip; x_N]^T $ is a $N \\times D$ matrix and\n$$ I_j = \\begin{bmatrix}I_{1j}\\\\ I_{2j}\\\\ \u0026hellip;\\\\ I_{nj} \\end{bmatrix}, p_j = \\begin{bmatrix}p_{1j}\\\\ p_{2j}\\\\ \u0026hellip;\\\\ p_{nj} \\end{bmatrix} $$\nTherefore,\n$$ \\frac{\\partial L}{\\partial \\bold W} = \\bold X^T (\\bold I - \\bold P) \\in R^{D \\times K} $$\nwhere $\\bold I = [I_1, I_2, \u0026hellip;, I_k] \\in R^{N \\times K}, \\bold P = [p_1, p_2, \u0026hellip;, p_k] \\in R^{N \\times K}$\nComments Logistic regression is a traditional classification model that often works effectively. However, it is problematic if data are linearly separable. If $w, b$ perfectly separates data linearly, so does cb, cw with $c \\gt 0$, so there is no parameter vector that maximises likelihood.\nReferences  https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c "
            }
    
        ,
            {
                "id": 49,
                "href": "https://ixiaopan.github.io/blog/post/perceptron/",
                "title": "Perceptron",
                "section": "post",
                "date" : "2021.06.13",
                "body": "Perceptron is a traditional classification algorithm and it is the basis of the neural network. Though it\u0026rsquo;s out of date, it plays a historical role in the development of neural network. Knowing how it works will help us lay the foundations for the future study of the neural network.\nModel Perceptron is a linear binary classifier, where linear means that it separates the input space linearly and binary means that it classify a sample into one of the two categories. Let $\\bold x_1, \\bold x_2, \\bold x_3, \u0026hellip;, \\bold x_n$ be observations and $y_n \\in -1, 1$ be target classes, Perceptron model is defined as follows,\n$$ \\hat y_n = f( \\bold w^T \\bold x_n) $$\nwhere $f(x)$ is an active function\n$$ f(x) = \\begin{cases} 1 \u0026amp; \\text{if $x \\ge 0$} \\\\ -1 \u0026amp; \\text{otherwise}\\end{cases} $$\nLoss Function If we classifiy correctly, $\\hat y_n $ will have the same sign as $y_n$. Otherwise, it will have the opposite sign to $y_n$.\n$$ g(x) = \\begin{cases} \\hat y_n y_n \\ge 0\u0026amp; \\text{if classified correctly} \\\\ \\hat y_n y_n \\lt 0 \u0026amp; \\text{otherwise}\\end{cases} $$\nAt the same time, we find that the sign of $\\hat y_n$ is determined by $w^Tx$, so we have\n$$ g(x) = \\begin{cases} y_n (\\bold w^T \\bold x_n) \\ge 0\u0026amp; \\text{if classified correctly} \\\\ y_n (\\bold w^T \\bold x_n) \\lt 0 \u0026amp; \\text{otherwise}\\end{cases} $$\nWe don\u0026rsquo;t care about what value of $w^tx_n$ is, instead, we are more interested in its sign. Naturally, the loss function is defined as follows,\n$$ L = -\\sum_{n \\in M} y_n (\\bold w^T \\bold x_n) $$\nwhere $M$ is the set of misclassified points. It\u0026rsquo;s clear that the loss function will reach the minimum point if we classify all samples correctly.\nParameter Estimation Though we could take the derivative of $L$ w.r.t. $\\bold w$\n$$ \\frac{\\partial L}{\\partial \\bold w} = -\\sum_{n \\in M} \\bold x_n y_n $$\nObviously, we cannot set this equal to 0 and get a solution for $\\bold w$. A common way to find the optimal parameter is to use gradient descent, as shown below\n$$ \\bold w^{t+1} = \\bold w^t + \\eta \\sum_{n \\in M} \\bold x_n y_n $$\nwhere $\\eta$ is the learning rate. The updating of $\\bold w$ could be iteratively — update $w$ when a sample is misclassified each time. Since $y_n \\in -1, 1$, we can decompose the above equation into the below two equations\n$$ \\bold w^{t+1} = \\begin{cases} \\bold w + x_n \u0026amp; \\text{if $y_n = 1$} \\\\ \\bold w - x_n \u0026amp; \\text{if $y_n = -1$}\\end{cases} $$\nIf a point belongs to positive category, i.e. $y_n = 1$, but our model misclassifies it as a negative category, then we would move $\\bold w$ along the direction towards this point because of $w + x_n$ and vice versa.\nAlgorithm Putting it together, the whole algorithm of Perceptron is\n Randomly initialise $w$ Until convergence or some stopping rules reached,  for $x_1, x_2, \u0026hellip;, x_n$  compute $\\hat y_n = f(w^T x_n)$ if $\\hat y_n y_n \u0026lt; 0$,  if $y_n = 1$, update $w' = w + \\eta x_n$ if $y_n = -1$, update $w' = w - \\eta x_n$       "
            }
    
        ,
            {
                "id": 50,
                "href": "https://ixiaopan.github.io/blog/post/knn/",
                "title": "K-nearest neighbours",
                "section": "post",
                "date" : "2021.06.12",
                "body": "K-Nearest Neighbors(KNN) is a distance-based algorithm in machine learning used for both classification and regression. It\u0026rsquo;s simple and intutive to understand because it doesn\u0026rsquo;t require extra training step and the idea behind it is simple enough — similar points are tends to be close to each other. In this article, we will learn how KNN works.\nHow KNN works Suppose we have a dataset and a new data point, we want to know which class this new example belong to or what the predicted value is if it\u0026rsquo;s a regression task. KNN works in this way,\n Choose a value for $K$ Calculate distances between the new point and each sample of the training data Sort the distances from the nearest to farthest and get the first $K$ nearest samples For classification  Majority vote among the $K$ observations. The new sample belongs to class that has the highest votes.   For regression  Average the value of the $K$ observations    def majority_vote(labels): votes = Counter(labels) label, _ = votes.most_common(1)[0] return label def knn_classify(inputs, labels, x, k, distance): distances = sorted([ (distance(x, point), label) for point, label in zip(inputs, labels)], key=lambda: lp:lp[0]) k_nearest_labels = [ lp[1] for lp in distances[:k] ] return majority_vote(k_nearest_labels) What if we have the same votes for two different categories?\n Pick one of them randomly Calculate weighted voting Reduce k until we find a unique answer  Distance From above, we can see that the main problems are how to find an appropriate $K$ and how to measure distance. There are several common distance measures in machine learning, such as Euclidean and Manhattan. They are applied in different situations, so knowing how and when they can be best used is important to improve our model performance.\nEuclidean Distance Euclidean distance is defined as follows,\n$$ D = \\sqrt{\\sum_i^d (x_i - y_i)^2} $$\nIt\u0026rsquo;s simply the length of the straight line connecting two points. We should be careful with it because it\u0026rsquo;s sensitve to the scale of features. So we need to normalize data first. Besides, it is not suitable for high-dimensional space. Anyway, it\u0026rsquo;s still the most common and straightforward distance measure.\nCosine Distance Cosine distance measures the closeness between two points by calculating the angle between them.\n$$ D = \\text{cos}\\theta = \\frac{x \\cdot y}{||x|| \\text{ } ||y||} $$\nSo this method doesn\u0026rsquo;t care about the length of a vector. If we are more interested in the direction of a vector rather than the length, it would be a better choice. For example, two documents represented by word-document vector could be similar even they vary in the number of the same words. This might be because one document is just much longer than another one, but they are all related to the same topic.\nManhattan Distance Manhattan distance is defined as the sum of steps moving from one point to another point. You can only move along the horizontal direction or vertical direction. Imagine you stand on a chess board, how many cells do you need to move?\n$$ D = \\sum_i^d |x_i - y_i| $$\nThis method originated from how to calculate the distance between source and destination in a city. You cannot always walk through a building or something, so Euclidean distance is not helpful. Instead, a distance based on streets would be optimal.\nThrough Manhattan distance could work well in a high-dimensional space, it\u0026rsquo;s less intutive than Euclidean distance. Also, the distance measured by Manhattan is a bit larger than Euclidean because of the Triangle Inequality Theorem.\nFinding the best K How do we find the optimal value for $K$? If $K$ is too small, the model would have a high variance and a poor generalization. If $K$ is too large, the model would have a high bias.\n If we have some domain knowledge, for example, we want to classify whether a new flower belong to a specific species among given all species. $K$ could be the number of that specific species. Typically, we may have little domain knowledge, one way to find the best $K$ is cross-validation. We choose the \u0026lsquo;elbow\u0026rsquo; point as the optimal $K$ from all these scores returned by cross-validation, as shown below. As a rule of thumb, we could choose $k = \\sqrt N$, where $N$ is the number of samples    Figure 1: The red point in the figure represents the \u0026#39;elbow\u0026#39; point. In this example, the best k is 13.   Pros and Cons Pros  Easy, simple and straightforward to understand no need to train, it\u0026rsquo;s a non-parametric method Suitable for both classification and regression  Cons  More suitable for a small number of samples because distance is computed throughout nearly every data in the training set. Thus computation might be a matter if the dataset is large. Sensitive to outliers. If one category has some outliers, a new sample might be classified into this category since they might be more close to the new sample even if the new sample should belong to another category. Tends to be biased if samples are unbalanced. One category would dominate the majority voting of the new sample because it has a great number of samples.  References  https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa https://medium.com/machine-learning-101/k-nearest-neighbors-classifier-1c1ff404d265 "
            }
    
        ,
            {
                "id": 51,
                "href": "https://ixiaopan.github.io/blog/post/information-theory/",
                "title": "Information Theory",
                "section": "post",
                "date" : "2021.06.07",
                "body": "I remembered that I learned a little stuff about information when I was in my undergraduate course. Well, maybe I was wrong. It’s been almost ten years. But when it comes to information theory, I know that we will talk about a person — Shannon. So what’s the information theory? What problems does it help to solve?\nInformation First, let\u0026rsquo;s understand what information is. Consider a guessing game where Alice randomly chooses a number between 1 and 100, and then we need to guess what that number is. An intuitive method is to use a dichotomy search. We may ask, \u0026lsquo;is it larger or smaller than 50?\u0026rsquo;. If it\u0026rsquo;s less than 50, we continue to ask, \u0026lsquo;is it larger or smaller than 25?\u0026rsquo; We do this iteratively until we find the right number. Mathematically, the number of searches is $log_2(100)$. If the number Alice chooses is small, say 3, then it would take many steps to get the correct answer.\nIf we know that Alice has some preferences for some particular numbers, say she likes 88 most. Then the first question we ask at this time would be \u0026lsquo;is it 88?\u0026rsquo;. If we are lucky enough, we will get the number once. On the other hand, since 88 is the number that Alice likes most, there is no doubt that she is likely to choose 88 as the riddle. Conversely, if Alice is going to surprise us, she would choose a number that she doesn\u0026rsquo;t like very much. If so, it increases the uncertainty of our guess, i.e. we would need to ask more questions.\nThe number of questions we ask or the degree of surprise is information, which is measured by\n$$ I(X=x) = -\\text{log}P(X=x) $$\nwhere $P(x)$ is a probability of occurence of x. When using the natural logarithm, the information unit is nat. One nat is the amount of infomration gained from the event with the probability of $\\frac{1}{e}$. Besides, we also use bit as the unit if the log base is 2.\nIn short, information quantifies how uncertain an event is. A rare event will cause much surprise and consequently contain much information.\nEntropy Suppose we have a probability distribution $P(x)$ over a random variable $X$, and the value of each outcome represents information associated with it. The entropy of $X$ is then defined as the expectation of information,\n$$ H(X) = -\\sum_{x \\in X} P(X=x) \\text{log} P(X=x) $$\nIf a random variable $X$ follows binominal distribution with probability $p$, then its entropy is computed as follows,\n$$ H(X) = -p\\text{In}p - (1-p)\\text{In}(1-p) $$\n  Figure 1: Plot of binary entropy (Wikipedia)  Figure 1 shows how $H(X)$ changes as $p$ changes. We can see that $H(X)$ tends to be zero if $p$ lies on the end of x-axis, which means that it tells us rare information since we know what to expect. On the contrary, $H(X)$ reaches the highest point if $p=0.5$, this is because we don\u0026rsquo;t know what to expect and anything could happen. From this view, entropy is also a measure of how uncertain $X$ is. The higher the entropy is, the higher the uncertainty is.\nKL Divergence Given two probability distribution $f(x)$ and $g(x)$ for a random variable $x$, the difference between them is defined as KL divergence, which is sometimes called relative entropy\n$$ D(f||g) = \\sum_{x \\in X} f(x) \\text{log} \\frac{f(x)}{g(x)} = -\\sum_{x \\in X} f(x) \\text{log} g(x) + f(x) \\text{log} f(x) $$\nCross-entropy Combing the definition of entropy, we have\n$$ H(f)= -\\sum_{x \\in X} f(x) \\text{log} f(x) $$\n$$ H(f, g) = -\\sum_{x \\in X} f(x) \\text{log} g(x) $$\n$$ D(f||g) = H(f, g) - H(f) $$\nwhere $H(f, g)$ is cross-entropy, which is the expected information of $X$ sampled from a probability distribution $f(x)$ with outcomes encoded using another distribution $g(x)$. We can find that KL divergence can be decomposed into two parts: cross-entropy and the entropy of $X$ sampled from $f(x)$. Usually $f(x)$ is considered as \u0026lsquo;true\u0026rsquo; probability distribution, so miniming KL divergence against a fixed reference distribution $f$ is equivalent to minimising cross-entropy.\nCross-entropy is a common loss function used in classification tasks. Suppose there are 3 classes, the class distribution for a given $x$ is $0, 1, 0$ and our model might give us a predicted class distribution, say $0.2, 0.5, 0.3$. Then, we calculate entropy for this observation, $H(f, g) = -(0 + 1 *\\text{log}0.5 + 0) = 1$. After a period of learning, the model could give us a better predicted label distribution say $0.05,0.9,0.05$, and this time, we find that the entropy decreases to $0.15$. It shows that the difference between our model and true distribution is decreasing.\nConditional Entropy Like conditional probability, we define conditional entropy as the uncertainty about a random variable $C$ after we observe one feature $x$ in $X$\n$$ H(C|X=x) = -\\sum_{c \\in C} p(c|X=x) \\text{log} p(c|X=x) $$\nThe difference between the entropy $H(C)$ and the conditional entropy $H(C|X=x)$ is realized information\n$$ I[C; X=x] = H(C) - H(C|X=x) $$\nHence, $I(C;X=x)$ measures how much uncertainty of $C$ changes due to observering $x$. Here, we use \u0026lsquo;change\u0026rsquo; because realized information is not always reduced.\nMutual Information Mutual information defined below tells us the expected reduction in uncertainty of $C$ that a feature gives us\n$$ I(C; X) = H(C) - \\sum_x p(X=x) H(C|X=x) $$ Here are some important notes about mutual information:\n it is always positive it is zero if $C$ and $X$ are statistically independent it is symmetric in X and C  In fact, we have met mutual information before — information gain in decision trees.\n"
            }
    
        ,
            {
                "id": 52,
                "href": "https://ixiaopan.github.io/blog/post/constrained-optimisation/",
                "title": "Constrained Optimisation",
                "section": "post",
                "date" : "2021.06.03",
                "body": "When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I\u0026rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key.\nAs we all know, the main effort in machine learning is to find a loss function and optimise it, i.e. find the minimum or maximum point, and this is the question of optimisation. However, we may only find local optimisation because of some constraints. Even without constraints, there is still a chance that we would reach local optimisation only. In short, there are two main situations we need to consider: unconstrained optimisation and constrained optimisation. And constrained optimisation further falls into two categories, equality constraints or inequality constraints.\nOn the other hand, the extreme value of a function typically relates to some property of that function. That means if we know a function has some particular property, then we know that it must have an extreme value or not. This property can be characterised by convexity. As you can see, this post will be very mathematical. Seems a bit scary, ummm\u0026hellip;\nEquality Constraints Suppose we want to minimise a function $f(x)$ subject to an equality constraint, $g(x) = 0$. This can be solved by introducing Lagrange multiplier $\\alpha$\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nThen set the gradient of $\\mathcal{L}$ equal to the zero vector\n$$ \\nabla_x \\mathcal{L} = \\nabla_x f(x) - \\alpha \\nabla_xg(x) = 0 $$\n$$ \\frac{\\partial \\mathcal L}{\\partial \\alpha} = -g(x) = 0 $$\nFinally, solving the above equations will give us the minimum point we are seeking.\nMultiple Constraints If we have multiple constraints, then multiple Lagrange multipliers are introduced,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\sum_i^m \\alpha_i g_i(x) $$ And the corresponding solutions are given by,\n$$ \\nabla_x f(x) = \\sum_i^m \\alpha_i \\nabla_xg_i(x) $$\n$$ \\frac{\\partial \\mathcal L}{\\partial \\alpha_i} = -g_i(x) = 0 $$\nLagrange But what\u0026rsquo;s the rationale behind these formulas? Though we are not required to know everything, basic understanding of Lagrange is still needed. Remember that the question is to find a point $x^*$ that satisfies both $g(x^*) = 0$ and $f(x^*) =m $, where $m$ is the minimum value of $f(x)$ given the constraint.\nThe line represented by $f(x) = c$ is known as a contour line. Since $f(x)$ can have many values, we can plot many contour lines with equal intervals between lines in ascending or descending order from inner to outer. Graphically, we want to find a point that lies on the line of $g(x) = 0$ and the line of $f(x)$ with the minimum value simultaneously as shown in Figure 1. The green point is the point we are looking for.\n  Figure 1: Illustration of equality constraints  But we still need to figure out equations to calculate the position of the gree point. Let\u0026rsquo;s start from $x_0$, the magenta point in Figure 1. Mathematically, the above process of finding $x^*$ can be described as follows,\n$$ f(x_0 + \\delta x) \u0026lt; f(x_0) $$\n$$ g(x_0) = g(x_0 + \\delta x) = 0 $$\nSo in which direction should we move at $x_0$? With the aid of Taylor expansion, we have $$ g(x_0 + \\delta x) = g(x_1) = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) + \\frac{1}{2} (x_1 - x_0)^T H (x_1 - x_0) $$\n$$ = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) + O(||x_1 - x_0||^2) $$\nwhere $x_1 = x_0 + \\delta x$ and $H$ is a matrix of second derivative of $g(x)$ known as Hessian. The third term tends to be zero if $\\delta x$ is small enough, then we are left with\n$$ g(x_0 + \\delta x) = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) = g(x_0) $$\nThus,\n$$ (x_1 - x_0)^T\\nabla_xg(x_0) = 0 $$\nwhich means that the moving direction from $x_0$ should be perpedicular to $\\nabla_xg(x_0)$. But we are not done, because not all $(\\delta x = x_1 - x_0)$ point to the right direction along which $f(x)$ decreases. In order to ensure this, we require that $\\delta x$ must satisfy\n$$ (x_1 - x_0)^T ( - \\nabla_x f(x)) \u0026gt; 0 $$\nwhere $- \\nabla_x f(x)$ indicates the descent direction. Therefore, as long as the value of the dot product is greater than zero, the moving of point will continue unless the value becomes zero. If so, it means that the direction of $\\nabla_x f(x)$ is parallel to $\\nabla_x g(x)$, i.e.\n$$ -\\nabla_x f(x) = \\lambda \\nabla_x g(x) $$\nWe can rewrite this by replacing $\\lambda$ with $\\alpha = -\\lambda $\n$$ \\nabla_x f(x) = \\alpha \\nabla_x g(x) $$\nFinally, we come to the method of Lagrange multipliers.\nInequality Constraints Now we consider another situation where we have inequality constraints, i.e. $g(x) \\ge 0$. It looks a little complicated, but if we think about it for a while, we can find that only two things could happen, as shown in Figure 2,\n either the optimum point satisfies $g(x) \\gt 0$ or the (local) optimum point lies on the boundary, $g(x) = 0$    Figure 2: Two possible outcomes of inequality constraints  Again, we use Lagrange to solve it,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nCase 1 In the first case where the optimum point lies in the interior of the constraint, i.e. $g(x) \u0026gt; 0$, which is filled in red in Figure 2 a). We set $\\alpha = 0$, which means the constrains has no influence on $f(x)$.\nCase 2 As for the second case, it is exactly the same as the equality constraints, i.e.\n$$ \\nabla_x f(x) = \\alpha \\nabla_xg(x) $$\nbut with an additional constraint $\\alpha \u0026gt; 0$. So why do we set $\\alpha \u0026gt; 0$ here?\n$\\alpha \\ge 0$ Visually, it can be seen From Figure 2 b) that both the magenta and blue points seem to be the right point we are seeking. But in fact, only the bule one is in a lower position. And we find that $\\nabla_x f(x) $ and $\\nabla_x g(x)$ point to the same direction at the blue point. Thus, $\\alpha$ is positive.\nIn theory, if we are at a point where $-\\nabla_x f(x)$ points to the feasible region, which is the area defined by $g(x) \u0026gt; 0$, i.e. any value of $x$ inside this region is valid, it means that a point with a smaller value of $f(x)$ could be found in the feasible region. But it contradicts the assumption that we can only move along the boundary of the region. In other words, this is not the optimal point.\nIf $-\\nabla_x f(x)$ at some point points to the exterior of the feasible region, then we are in the right position because the outer of the feasible region is invalid and we cannot move forward any further(we are already on the border of the region).\nIt is noticeable that $\\nabla_x g(x)$ points in towards the feasible region. Therefore, we conclude that $\\nabla_x f(x)$ and $\\nabla_x g(x)$ have the same direction. Thus, $\\alpha$ is positive.\nFrom above, we can also draw another conclusion shown below\n$$ \\alpha g(x) = 0 $$\nKKT Conditions Putting it together, we want to minimise $f(x)$ subject $g(x) \\ge 0$, and the Lagrangian function is defined as follows,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nThen we can find a local mininum $x^*$ s.t.\n $\\nabla_x f(x) = \\alpha \\nabla_xg(x)$ $\\alpha \\ge 0$  $\\alpha = 0$, the solution is in the interior or $\\alpha \\gt 0$ and $g(x) = 0$, i.e. the solution is on the boundary   $\\alpha g(x) = 0$  These are the Karush-Kuhn-Tucker (KKT) conditions.\nMany Inequalities Once again, if we have many ineuqality constraints, then Lagrangian function is given by,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\sum_i^m \\alpha_i g_i(x) $$\nAnd the corresponding solutions are given by,\n$$ \\nabla_x f(x) = \\sum_i^m \\alpha_i \\nabla_xg_i(x) $$\nplus the constraints that\n either $\\alpha_i = 0$ or $\\alpha_i \\gt 0$ and $g_i(x) = 0$ $\\alpha_i \\nabla_x g_i(x) = 0$  Duality Let\u0026rsquo;s revisit the above problem again. Consider minimising a function $f(x)$ subject to $g(x) \\ge 0$, the lagrangian function is given by\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nPrimal Problem Now consider maximising $\\mathcal{L}(x, \\alpha)$ w.r.t $\\alpha$,\n$$ \\text{max}_\\alpha \\mathcal{L}(x, \\alpha) = \\begin{cases} f(x) \u0026amp; \\text{if $g(x) \\ge 0$} \\\\ \\infin \u0026amp; \\text{otherwise}\\end{cases} $$\n  For any $x'$ that satisfies the constraint $g(x) \\ge 0$, we conclude that\n$$ f(x') \\ge \\mathcal{L}(x', \\alpha) $$\ni.e. the upper bound of $\\mathcal{L}(x, \\alpha)$ is $f(x)$. Thus, the maximum value of $\\mathcal{L}$ we can obtain is to set $\\alpha = 0$\n  On the contratry, if the constraint is not satisfied, then there exists some $x'$ that satisfies $g(x) \\lt 0$. If so, then we can make $\\mathcal{L}$ infinite by taking $\\alpha \\rarr \\infin$.\n  Next we take the minimum of the maximum of $\\mathcal{L}$, i.e.\n$$ \\text{min}_\\text{x} \\text{max}_\\alpha \\mathcal{L}(x, \\alpha) $$\nwhich is equivalent to the problem of minimising $f(x)$ subject to $g(x) \\ge 0$. We call the original problem as primal problem.\nDual Problem Yet we still don\u0026rsquo;t find a solution to $x$. How about reversing the order of max and min like the below formula?\n$$ \\text{max}_\\alpha \\text{min}_\\text{x} \\mathcal{L}(x, \\alpha) $$\nWe solve $\\text{min}_\\text{x} \\mathcal{L}(x, \\alpha)$ using the method of Lagrange, and find that $x$ is a function of $\\alpha$. Then we plug $x$ into $ \\mathcal{L}(x, \\alpha)$ and obtain a new function of $\\alpha$, say $h(\\alpha)$. So the problem becomes to maximise $h(\\alpha$), also known as dual problem,\n$$ \\text{max}_\\alpha h(\\alpha) $$\nHowever, is the solution to dual problem the same as the primal problem? Why do we bother solving a dual problem rather than the original problem?\nSince $\\alpha \\ge 0$ and $g(x) \\ge 0$, we have,\n$$ \\text{min}_\\text{x} \\mathcal{L}(x, \\alpha) \\le \\text{min}_x f(x) = p^* $$\n$$ d^* = \\text{max}_\\alpha \\text{min}_\\text{x} \\mathcal{L}(x, \\alpha) \\le p^* $$\nwhere $p^*$ and $d^*$ are the optima of the primal and dual problem respectively.\nIt can be seen that the solution of dual problem gives us a lower bound on the primal problem. If possible, we can also have the same solution.\nThe reason why we solve the dual problem is that the primal problem works in a feature space that may have high dimensions while the dual problems depends on the number of constraints, which is much smaller than the dimensitionality of $x$.\nLinear Programming In linear programming, we minimise a linear function $c^Tx$ subject to a series of linear constraints $g(x) = Mx - b \\ge 0$. The primal problem is described as follows,\n$$ \\mathcal{L} (x, \\alpha) = c^Tx - \\alpha^T(Mx - b) $$\n$$ \\text{minimise } c^T x $$\n$$ \\text{subject to } Mx \\ge b $$\nand the dual problem is defined as,\n$$ \\mathcal{L} (x, \\alpha) = b^T\\alpha - x^T(M^T \\alpha - c) $$\n$$ \\text{maximise } b^T \\alpha $$\n$$ \\text{subject to } M^T \\alpha\\le c $$\nLet\u0026rsquo;s see and example.\nPrimal problem\n$$ \\text{minimise } z = 15x_1 + 12x_2\\\\ \\text{subject to } x_1 + 2x_2 \\ge 3, 2x_1 - 4 x_2 \\ge 5 $$\nDual problem\n$$ \\text{maximise } w = 3y_1 + 5y_2\\\\ \\text{subject to } y_1 + 2y_2 \\le 15, 2y_1 - 4 y_2 \\le 12 $$\nQuadratic Programming In quadratic programming, we minimise a quadratic function $x^TQx$ subject to a series of linear constraints $g(x) = Mx - b \\ge 0$. The primal problem is described as follows,\n$$ \\text{max}_\\alpha \\text{min}_\\text{x} \\mathcal{L} (x, \\alpha) = x^TQx - \\alpha^T(Mx - b) $$\nUsing the method of Lagrange, we have $x^* = \\frac{1}{2}Q^{-1}M^T\\alpha$, and we substitue it into $\\mathcal{L}$\n$$ \\text{max}_\\alpha -\\frac{1}{4} \\alpha^TMQ^{-1}M^T\\alpha + \\alpha^Tb $$\nConvexity Quadratic Form  Quadratic form is a polynominal function with terms all of degree of two. For example, $4x^2 + 2xy - 3y^2$. — Wikipedia\n Here \u0026ldquo;the degree of two\u0026rdquo; means that the sum of exponents for each term is 2. A general quadratic form of $n$ variables is defined below, where $M$ could be chosen symmetric.\n$$ Q(\\bold x) = \\bold x^T \\bold M \\bold x = \\sum_{i,j}^d \\bold M_{ij} \\bold x_i \\bold x_j $$\nIn this example $4x^2 + 2xy - 3y^2$, the quadratic form is given by,\n$$ Q(\\bold x) = \\bold x^TM\\bold x = \\displaystyle{\\begin{bmatrix}x\u0026amp;y\\end{bmatrix} \\begin{bmatrix}4\u0026amp;1\\\\1\u0026amp;-3 \\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix} } $$\nBasically, quadratic form is a mapping from $R^d$ to $R$. Without doubtness, for any quadratic form, we have $Q(\\bold 0) = 0$. But is $x=\\bold 0$ is the minimum or maximum point for $Q$ ? The answer is determined by the definiteness of $Q$ described below,\n $Q(\\bold x) \\gt0$,  positive definite   $Q(\\bold x) \\ge 0$,  positive semi-definite   $Q(\\bold x) \\lt 0$ ,  negative definite   $Q(\\bold x) \\le 0$,  negative semi-definite   $Q(\\bold x)$ could be both positive and negative  indefinite    Clearly,\n if $Q$ is positive definite, then $x = 0$ is global minimum if $Q$ is negative definite, then $x = 0$ is global maximum  Furthermore, quadratic form can also be characterised in terms of eigenvalues:\n positive definite if and only if the eigenvalues of $M$ are positive, negative definite if and only if the eigenvalues of $M$ are negative, indefinite if and only if $M$ has bothe positive and negative eigenvalues  Here are some proofs. First, we should know that any two eigenvectors from different eigenspaces of a symmetric matrix are orthogonal and any symmetric matrix can be orthogonally diagonalizable.\nLet $\\bold P = [\\bold v1, \\bold v2, \u0026hellip;, \\bold v_n]$ be eigenvectors that correspond to different eigenvalues $\\Lambda= \\lambda_1, \\lambda_2, \u0026hellip;, \\lambda_n$ of a symmetric matrix $A$. To show that $v_1 \\cdot v_2 = 0$, compute\n$$ \\lambda_1 v_1 \\cdot v_2 = (A v_1)^T v_2 = v_1^T A v_2 = \\lambda_2 v_1^Tv_2 $$\n$$ (\\lambda_1 - \\lambda_2) v_1^T v_2 = 0 $$\nBut $ \\lambda_1 \\ne \\lambda_2$, so $v1 \\cdot v2=0$. Furthermore, $\\bold P ^T \\bold P = I$, so $P^{-1} = P^T$. Then we have\n$$ AP = PD\\\\A = PDP^{-1} = PDP^T $$\nThe quadratic form of $A$ can be written as follows,\n$$ x^T A x= x^T PDP^Tx = (P^Tx)^T D (P^Tx) = y^TDy = \\lambda_1y_1^2 + \\lambda_2y_2^2 + \u0026hellip; + \\lambda_ny_n^2 $$\nIf the eigenvalues of $A$ are positive, then $x^TAx \u0026gt; 0$ and $A$ is positive definite. On the other hand, if $A$ is positive definite, then $x^TAx \u0026gt; 0$ for any $x \\ne 0$. If we substitute $x$ with an eigenvector $v_1$, we have\n$$ v_1^T A v_1 = v_1^T \\lambda v_1 = \\lambda v_1^Tv_1 = \\lambda ||v_1||^2 \\gt 0 $$\nThus, $\\lambda \u0026gt; 0$.\nConvex Region Before we talk about convex function, let\u0026rsquo;s start with convex region and convex set.\nA region $R$ is said to be a convex region if any two points $x$ and $y$ in that region plus any $a \\in [0, 1]$ satisfy,\n$$ z = a x + ( 1 - a )y \\in R $$\n  Figure 3: Convex region and non-convex region  Convex Set Similarly, for any set of points $S$, if for any two points $x, y \\in S$ and any $a \\in [0, 1]$ satisfy $$ z = a x + ( 1 - a )y \\in S $$ then $S$ is a convex set. We can prove that the set of positive semi-definite matrices form a convex set. Let $A_1, A_2 \\in S$, compute\n$$ x^T z x = x^T (a A_1 + ( 1 - a)A_2)x = x^TaA_1x + x^T ( 1-a) A_2 x \\ge 0 $$\nThus, $z$ is positive semi-definite and $z \\in S$.\nConvex Function Any function is said to be convex if any two points $x$ and $y$ plus $a \\in [0, 1]$ satisfy\n$$ af(x) + (1 - a) f(y) \\ge f(a x + (1 -a ) y) $$\n  Figure 4: Convex function  Conversely, if the condition doesn\u0026rsquo;t meet, the function then is said to be a convex-down or concave function. These two functions are symmetric — everything true for convex functions is also true for concave functions.\nAt the beginning, I often mixed up them. I couldn\u0026rsquo;t tell which figure is convex or concave. But later, I found a simple way to distinguish them correctly. First we find the lowest point or the highest point of a figure. Then we observe the direction along which the curve expands. If the direction is toward down, the function is concave. Otherwise, it\u0026rsquo;s convex. By the way, the area enclosed by the curve (lies on or above the curve ) is defined as epigraph. The epigraph of a convex function forms a convex region, and if the epigraph of a function forms a convex region then the function is convex.\nLinear Functions Now let\u0026rsquo;s take a look at a special case — equality. A function that we are quite familar with satisfies the equality, which is linear function.\n$$ f(x) = mx + c $$ It\u0026rsquo;s easy to proove it.\n$$ m(ax + (1 - a)y) + c = max + my - may + c = af(x) - ac + my(1 -a) + c \\\\= af(x) + my(1-a) + c(1-a) = af(x) + (1-a)f(y) $$\nSo is it a convex or concave function? The answer is both.\nStrictly Convex What about the condition without euqality? Well, such a condition is called strict inequality and functions that satisfy the strict inequality is said to be strictly convex/concave.\nSums of Convex functions If we have a set of convex functions, then it\u0026rsquo;s easy to prove that the sum of the multiplication of positive factors and these functions is also a convex function using the property that the second derivative is equal or greater than zero. Below is the proof.\n$$ g(x) = \\sum_i \\alpha_i f_i(x) $$\n$$ g''(x) = \\sum_i \\alpha_i f''_i(x) \\ge 0 $$\nSecond derivative One thing we should remember is that any tangent line of a convex funtion lies on or below the function. Let $t, z$ be two points on the graph of a convex function, then we can derive the following conditions,\n $ t \\lt z$  $$ f(t) + f'(t) (z - t)\\le f(z) $$\n$$ f'(t) \\le \\frac{f(z) - f(t)}{z - t} $$\n $ t \\gt z$  $$ f(t) - f'(t)(t - z) \\le f(z) $$\n$$ \\frac{f(t) - f(z)}{(t - z)} \\le f'(t) $$\nThe two conditions can be combined as a single condition for any two points $a, b$ that satisfies $a \u0026lt; b$ as follows,\n$$ f'(a) \\le \\frac{f(a) - f(b)}{b-a} \\le f'(b) $$\nHence, $f''(a) \\ge 0$.\nIn high dimension, the second derivative of a function is known as Hessian. And a necessary and sufficient condition for that function to be convex is that its Hesssian must be positive semi-positive at all points.\nUnique Minimum As said early, convexity can help to find the extreme value of a function. How does it work? Let $x^*$ be a local minimum of a function, suppose there exists another points $\\hat x$ such that $f(\\hat x) \u0026lt; f(x^*)$. By the definition of convexity, we have\n$$ f(a \\hat x + (1-a)x^*) \\le af(\\hat x) + (1-a) f(x^*) \\le af(x^*) + (1-a) f(x^*) = f(x^*) $$\nIf we set $ a \\rarr 0 $, it means that there exist points around $x^*$ with a smaller value than $f(x*)$, which is a contradiction to the definition of local minimum.\nThus, we can see that any local minimum of a convex funtion is a global minimum. Besides,\n there could be many local minimum for a convex function. In other words, the minimum of a convex function will form a convex set. a strictly convex function has at most one global minimum  Putting it together, the whole process of determining whether a function would have a minimum is shown in Figure 5\n  Figure 5: Using Hessian to determine whether a function is a convex function   Inverse of Convex Funtions Let $f(x)$ be a convex function, how about the convexity of the inverse of it, i.e. $g(x) = f^{-1}(x)$?\nFirst, the second derivative of a composite function is given by\n$$ \\frac{d^2f(g(x))}{dx^2} = \\frac{d f'(g(x)) g'(x)}{dx} = f''(g(x)) [g'(x)]^2 + f'(x) g''(x) $$\nBesides, if $f^{-1}(x)$ is the invese of $(x)$, we have\n$$ f(f^{-1}(x)) = x $$\n$$ f''(f^{-1}(x)) = 0 $$\nThus, we conclude that\n$$ g''(x) = - \\frac{f''(g(x)) [g'(x)]^2}{f'(g(x))} $$\nSince $f''(x) \\ge 0$ and $[g'(x)]^2 \\ge 0$, the sign of $g''(x)$ is determined by $-f'(g(x))$.\nHere is an example. Let $f(x) = x^2$ , so that $f''(x) = 2 \\gt 0$ and $f'(x) = 2x$. Since $g(y) = f^{-1}(y) = \\sqrt y \\ge 0$, $f'(g(x)) \\ge 0$ and consequently $\\sqrt x$ is concave.\nJensen’s inequality Jensen’s inequality involves inequality of convex function, which states that for any convex function $f(x)$,\n$$ E[f(x)] \\ge f(E[x]) $$\nand for any concave function,\n$$ E[f(x)] \\le f(E[x]) $$\nIt\u0026rsquo;s easy to prove using the fact that a convex function must lie on or above its tangent line at any point $x'$\n$$ f(\\hat x) \\ge f(x') + (\\hat x - x')^T \\nabla f(x) $$\nso this is true for $x' = E[x]$\n$$ f(\\hat x) \\ge f(E[x]) + (\\hat x - E[x])^T \\nabla f(x) $$\nthen taking expectations of both sides\n$$ E[f(\\hat x)] \\ge f(E[x]) + (E[\\hat x] - E[x])^T \\nabla f(x) = f(E[x]) $$\nA typical example is $f(x) = x^2$, Jensen\u0026rsquo;s inequality shows that\n$$ E[x^2] - E^2[x] \\ge 0 $$\nWell, it\u0026rsquo;s the formula of variance, and variance are non-negative.\nConclusion Finally, we\u0026rsquo;re here. I spent several days writing up this article. To be honest, I am not a math person, and it was a struggle to explain these mathematical concepts and formulas clearly and accurately. But in doing so, I had a better understanding about optimisation. But knowing these equations only is not enough, the key point is to learn to apply them in machine learning to solve real-world problems. Anyway, we are done for now.\nReferences  https://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf "
            }
    
        ,
            {
                "id": 53,
                "href": "https://ixiaopan.github.io/blog/post/probabilistic-model/",
                "title": "Probabilistic Model",
                "section": "post",
                "date" : "2021.05.26",
                "body": "The goal of machine learning is to infer an unknown pattern from data. However, both parameters and predictions are estimated values, so how confident are we in these values? To measure uncertainty, we use probability. On the other hand, Bayes' theorem provides a framework for us to invert the problem into a forward process, where we observe data from parameters instead of making inferences from data.\nProbability Random Variable At the beginning, let\u0026rsquo;s have a quick refresh on probability.\n A random variable is variable that can take on different values randomly.\nOn its own, it is a description the states that are possible; it must be coupled with a probability distribution that specifies how likely each of these states are.\n— Deep Learning, p54\n In short, a random variable covers two aspects: possible value and the likelihood of taking that value. Conventionly, we use a capital letter, such as $X$ or $Y$, to represent a random variable.\nConditional Probability Suppose we have two random variables of interest, $X$ and $Y$,\n  The joint probability of $X$ that takes the value of $x$ and $Y$ that takes the value of $y$ is written as $P(X = x, Y=y)$, which means that the probability of $x$ and $y$ happening at the same time\n  Given $Y=y$, the conditional probability of $X$ given $Y=y$ is denoted by $P(X|Y=y)$\n  There are two major rules of probability that we should remember,\n the sum rule, i.e. the marginal probability of $X$ that takes the value of $x$, irrespective of the value of $Y$  $$ P(X=x) = \\sum_{y \\in Y} P(X=x, Y=y) $$\n the product rule, i.e. the joint probability of $X$ and $Y$ can be written as the product of the conditional probability and the marginal probability  $$ P(X, Y) = P(Y|X) P(X) = P(X|Y)P(Y) $$\nFrom the above formula, we can deduce the following equation, which is also known as Bayes' Rule,\n$$ P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)} $$\nExpectation The expectation of the function $f(x)$ of a random variable $x$ is the mean value of $f(x)$\n$$ E_{x \\sim P(x) } f(x) = \\sum_x P(x) f(x) $$\nThe variance measures how much a new sample drawn from $P(x)$ deviate from the mean value\n$$ \\text{Var(x)} = E[ (f(x) - E[f(x)])^2 ] $$\nBayes' Inference In machine learning, our goal is to learn a model $\\Theta$ from a given data set $D$, but $\\Theta$ is uncertain. One way to measure uncertainty is probability, so the question becomes how to compute the probablity of $\\Theta$ given the data $D$, i.e. $P(\\Theta|D)$. But the only thing we know is the observed data, so how can we do it? The answer is the Bayes' Rule, which helps us convert this into a forward problem, where parameters are known and thus we can draw data from the distribution determined by that paramaters. The formula is defined as follows,\n$$ P(\\Theta|D) = \\frac{P(D|\\Theta)P(\\Theta)}{P(D)} $$\n $P(\\Theta)$ is called the prior probability, i.e. the best guess about $\\Theta$ before we see the data. You might have a good estimate on it or simply have no idea at all $P(D|\\Theta)$ is the likelihood of the data given the parameters $\\Theta$ $P(D)$ is the evidence or marginal probability denoted by $P(D) = \\sum_{\\theta \\in \\Theta }P(D, \\theta)$ $P(\\Theta|D)$ is the posterior probability, i.e. the updated probability of $\\theta_i$ after we see the data  Pros and cons Pros\n Bayes' rule provides us a full probabilistic description of the parameters It doesn\u0026rsquo;t overfit since we are not choosing the best parameters that fit the data perfectly  Cons\n However, we need to compute $P(D)$, whichsometimes is not reasonable, e.g. there are too many possible values of $\\Theta$ Posterior might not be described as a nice probability function  MAP If we ignore the evidence $P(D)$ (after all, it\u0026rsquo;s just a constant for any $\\theta$), we are only left with the numerator. An easy way to compute $P(\\Theta|D$) is to find the maximum value shown below, though it\u0026rsquo;s not a strictly probability\n$$ {\\argmax}_{\\Theta} log (P(D|\\Theta)) + log (P(\\Theta)) $$\nThis method is called Maximum A Posterior(MAP). However, it can overfit because we are finding the parameters that maximise the likelihood of the observed data. Thus, we are likely to get a model that fit the data with no errors.\nMLE Furthermore, if we ignore the prior $P(\\Theta)$, the MAP is just maximising the likelihood(MLE), which is widely used statistics in machine learning.\nConjugate Prior In some cases, the likelihood of the observed data $D$ is simple to compute, and the posterior would have the same form as the prior if we could find a right prior. Such a likelihood and prior distribution are said to be \u0026lsquo;conjugate\u0026rsquo;. Here we consider two common distributions that a prior might follow: Bernoulli and Poisson.\nBernoulli Distribution Suppose we have a binary random variable $X \\in \\{0, 1 \\}$, and $X_i=1$ if the ith trial is a head and 0 otherwise. Then the likelihood of $X_i$ given the probability of a head $\\mu$ is\n$$ P(X_i = 1|\\mu) = \\mu $$\n$$ P(X_i = 0|\\mu) = 1 - \\mu $$\nOr we can write it in this form\n$$ P(X_i|\\mu) = \\mu^{X_i} (1-\\mu)^{1-X_i} $$\nSuppose we have a data set $ D = \\{ x_1, x_2, \u0026hellip;, x_n \\}$, where $x_i \\in \\{0, 1 \\}$, assuming these observations are drawn independently, then the likelihood of $D$ can be computed as follows,\n$$ L(D;\\mu) = \\prod_{i=1}^N P(X_i|\\mu) = \\prod_{i=1}^N \\mu^{X_i} (1-\\mu)^{1-X_i} = \\mu^{N_h} (1-\\mu)^{N-N_h} $$ where $N_h = \\sum_i X_i$, i.e number of heads.\nBeta The next step is to choose the prior $P(\\Theta)$. In the case of Bernoulli, it would be better if we can find a function that has a simliar exponential parts that appeared in the above likelihood function to model the probability of every single value of $\\mu$. Luckily, Beta distribution shown below is the right function we are looking for.\n$$ P(\\mu) = Beta(\\mu|a, b) = \\frac{\\mu^{a-1} (1-\\mu)^{b-1}}{B(a, b)} $$\nwhere $B(a, b)$ is a normalisation constant\n$$ B(a, b) = \\int_0^1 \\mu^{a-1} (1-\\mu)^{b-1} d\\mu $$\nSo how to choose a, b? It depends. If we have no idea about $\\mu$, it\u0026rsquo;s natural to assume that there are equal chances to take all vaules of $\\mu$. This corresponds to a beta distribution with $a = b = 1$.\nThe last step is to plug the prior and likelihood into the Bayes' rules,\n$$ P(\\mu|D) = \\frac{P(D|\\mu)P(\\mu)}{P(D)} = \\frac{\\mu^{N_h} (1-\\mu)^{N-N_h} \\mu^{a-1} (1-\\mu)^{b-1}}{P(D) B(a, b)} = \\frac{\\mu^{N_h+a-1} (1-\\mu)^{N+b-N_h-1}}{P(D) B(a, b)} $$\nand\n$$ P(D) = \\int_0^1 \\frac{\\mu^{N_h+a-1} (1-\\mu)^{N+b-N_h-1}} {B(a, b)} d\\mu = \\frac{B(N_h +a, N+b-N_h)}{B(a, b)} $$\nSo we have\n$$ P(\\mu|D) = Beta(\\mu| N_h + a, N+b-N_h) $$\nIn summary, before we see data, we have some beliefs about $\\mu$ governed by $Beta(\\mu|a, b)$. After seeing the data, the probability of $\\mu$ now is updated via $Beta(\\mu| N_h + a, N+b-N_h)$, which can be served as the prior for the next new observations.\nIncremental Updating For independent data we can update the hyperparamters incrementally, we consider an individual data at a time so that,\n$$ P(\\mu|X_1) = \\frac{P(X_1|\\mu)P(\\mu)}{P(X_1)} $$\n$$ P(\\mu|X_2, X_1) = \\frac{P(X_2, X_1|\\mu)P(\\mu)}{P(X_2, X_1)} = \\frac{P(X_2|\\mu)P(X_1|\\mu)P(\\mu)}{P(X_2)P(X_1)} = \\frac{P(X_2|\\mu)P(\\mu|X_1)}{P(X_2)} $$\nIt\u0026rsquo;s clear that the previous posterior now becomes the prior for the next piece of data.\nPoisson Distribution Poisson distribution measures the probability of a given number of events occuring in a specific time range, which is given by,\n$$ Pois(N;\\theta) = \\frac{e^{-\\theta}\\theta^N}{N!} $$\nwhere $N$ is the number of occurences in a time slot and $\\theta$ is the parameter of interest. For example, we want to know the rate of traffic along a road between 8am and 9am, so $N$ is the number of cars and $\\mu$ is the rate of traffic per hour.\nGamma Then we have Gamma distribution as our prior,\n$$ P(\\theta) = \\Gamma(\\theta|a, b) = \\frac{b^a \\theta^{a-1} e^{-b\\theta}}{\\Gamma(a)} $$\nso the posterior after seeing the first piece of data is\n$$ P(\\theta|N_1) = \\frac{P(N_1|\\theta)P(\\theta)}{P(N_1)} = \\frac{b^a}{\\Gamma(a) N_1! P(N_1) }e^{-(b+1)\\theta} \\theta^{N+a-1} \\propto e^{-(b+1)\\theta} \\theta^{N_1+a-1} $$\nwe can see that the posterior is also a Gamma distribution with $a_1 = a + N_1$ and $b_1 = b + 1$.\nMultinominal Distribution In the case of Bernoulli, we only consider two outcomes: 0 or 1. But what if we have 3 or more outcomes? Well, we use multinominal distribution, which is the generalization of the binominal distribution. Suppose we have a $k$-sided dice with the probability of $\\mu_k$ for $x^k = 1$, and we roll the dice $N$ times, so there are $N$ independent observations $x_1, x_2, \u0026hellip;, x_n$, the multinominal distribution are given by,\n$$ M(m_1, m_2, \u0026hellip;, m_k|\\mu, N) = \\frac{N!}{m_1!m_2!\u0026hellip;m_k!} \\prod_{k=1}^K \\mu_k^{m_k} $$\nwhere $m_k=\\sum_n^Nx_n^k$ represents the number of $x_n^k = 1$.\nDirichlet TODO\nGaussian Distribution In the case of a single variable $x$, the Gaussian distribution is defined as,\n$$ N(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2} $$\nFor a vector-valued random variable $\\bold x \\in R^d$ , We can extend it to the Multivariate Gaussian distribution\n$$ N(\\bold x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{(2\\pi)^d}\\sigma_1\\sigma_2\u0026hellip;\\sigma_d} e^{\\frac{1}{-2{\\sigma_1}^2}(x_1 - \\mu_1)^2 + \\frac{1}{-2{\\sigma_2}^2}(x_2 - \\mu_2)^2 + \u0026hellip; + \\frac{1}{-2{\\sigma_d}^2}(x_d - \\mu_d)^2} $$\nThe exponential part of $e$ can be re-written in a matrix form, as shown below, where\n$$ \\begin{bmatrix} x_1 \u0026amp;x_2 \u0026amp; \u0026hellip; \u0026amp; x_d \\end{bmatrix} \\begin{bmatrix} \\sigma_1^2 \\\\ \u0026amp; \\sigma_2^2 \u0026amp; \\\\ \u0026amp; \u0026hellip; \\\\ \u0026amp; \u0026amp; \\sigma_d^2 \\end{bmatrix}^{-1} \\begin{bmatrix} x_1 \\\\x_2 \\\\ \u0026hellip; \\\\ x_d \\end{bmatrix} \\\\ = (x - \\mu)^T {\\sum}^{-1}(x-\\mu) $$\nThus,\n$$ N(\\bold x; \\mu, \\sum) = \\frac{1}{\\sqrt {(2 \\pi)^d |\\sum}|} e^{-\\frac{1}{2} (x - \\mu)^T\\sum^{-1}(x-\\mu)} $$\nThe number of free parameters A general symmetric covariance matrix has $ 1 + 2 + 3 + \u0026hellip; + D = D(D+1)/2$ independent parameters, and there are another $D$ independent parameters in $\\mu$ , giving $D(D+3)/2$ parameters in total.\nIf we consider diagonal covariance matrix $\\sum = diag(\\sigma_i^2)$, then we have a total of $D + D = 2D$ independent parameters.\nIf we restrict the covariance matrix to be proportional to the identity matrix, $\\sum=\\sigma^2I$, giving $D + 1 $ independent parameters, in this case, the $PDF$ is $\\frac{1}{\\sqrt{2\\pi}^D\\sigma^D} e^{\\frac{1}{-2{\\sigma}^2}\\sum_{i=1}^D(x_i - \\mu_i)^2}$, which means the density is only related to the distance to the mean from the $x$ (different $x$ with the same distance to the mean has equal density)\n Maximum likelihood Estimate Given a data set $\\bold X = (\\bold x_1 , . . . , \\bold x_N )^T $ in which the observations ${ x_n }$ are assumed to be drawn independently from a multivariate Gaussian distribution,\nEsimation for $\\mu$ Step 1: construct the likelihood function $$ L(\\mu,\\sum|D) = \\prod_{i=1}^N N(\\mu, \\sum) $$\nStep 2: The log likelihood function is given by $$ \\text{In} L = \\sum_{i=1}^N {\\frac{-D}{2} \\text{In}2\\pi - \\frac{1}{2} \\text{In}\\sum - \\frac{1}{2}(x_i-\\mu)^T{\\sum}^{-1} (x_i-\\mu)} $$\nStep 3: the derivative of the log likelihood with respect to $\\mu$ is given by\n$$ \\frac{ \\partial L }{\\partial \\mu} = \\sum_{i=1}^N{\\sum}^{-1}(x_i - \\mu) $$ Here, we use a bit trick\n$$ x^TMx = 2Mx $$ Since the element of $\\sum^{-1}$ is positive，we have\n$$ \\mu_{ML} = \\frac{1}{N} \\sum^Nx_n $$\nEstimation for $\\Sigma^{-1}$ $$ trace[ABC] = trace[BAC] = trace[CAB] \\\\ x^TAx = tr[x^TAx] = tr[xx^TA] \\\\ \\frac{\\partial}{\\partial A} tr[AB] = B^T \\\\ \\frac{\\partial}{\\partial A} log |A| = A^{-T} \\\\ \\frac{\\partial}{\\partial A} x^TAx = xx^T $$\nThe derivative of the log likelihood with respect to $\\sum^{-1}$ is given by\n$$ \\frac{ \\partial L }{\\partial \\sum^{-1}} = \\sum_{i=1}^N \\frac{1}{2} \\sum - \\frac{1}{2} (x_i - \\mu) (x_i - \\mu)^T $$ Thus,\n$$ \\sum_{ML} = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu) (x_i - \\mu)^T $$\nDiscriminal vs Generative Models Take the problem mentioned in the introduction as an example, suppose we have 2 classes, \u0026lsquo;cat\u0026rsquo; and \u0026lsquo;dog\u0026rsquo;, and we want to know which class the new image belong to. The problem is to find the probability $P(C=cat|x)$ and $P(C=dog|x)$, and then we classify the image into the class with the largest probability.\nUsually, we think of our observations as given and the predictions as random variables, and we model the target variable as a function of the predictors. This is known as discriminal model. In this example, we could use logistic regression to make a classification, and the corresponding probability can be computed using a sigmoid function\n$$ P(C=cat|X)=\\frac{1}{1 + e^{-(wx + b)}} $$\nHowever, we can also consider both features and target variables at the same time. Using the Bayes' rule below, we can calculate $P(Y|X)$ in another way,\n$$ P(C=cat|X) \\propto P(X, C=cat)= P(X|C=cat)P(C=cat) $$\nThis is known as generative model, where we use Bayes' rule to turn $P(X|Y)$ into $P(Y|X)$.\nIn discriminal model, we don\u0026rsquo;t need the prior of classes, so there are less parameters to be determined. Also, it might have a better performance if the estimate for that prior is far away from the true distribution.\nGraphical Models Given 2 random variables, $X$ and $Y$, they could be dependent directly or not. Even if they are not dependent directly, typically there is still correlation between them. If they are correlated, then\n X could affect Y Y could affect X X has no direct effect on Y, but they could be both affected by another random variable Z  We can describe the above relationships using a graph, where each node represents a random variable and the links between nodes show that relationship. The above three relationships are captured by the following figure,\n  Figure 1: Directed graphical models representing the above three conditions   Such a graph is known as Bayesian networks where we use a directed graph to show causal relationships between random variables. Specifically, we add directed links between $X$ and $Y$ if $X$ directly influences $Y$ shown in the left graph of Figure 1.\nConditional Independence The left and middle graphs of Figure 1 are easy to understand since they only have two variables, and they are denoted by $P(Y|X), P(X|Y)$ respectively. However, the last one with three variables, $X, Y, Z$, is a bit tricky. Let\u0026rsquo;s first consider the conditional distribution of $X$ given Z and Y, we have\n$$ P(X|Z, Y) = P(X|Z) $$\nIf we further consider the joint distribution of X and Y conditioned on Z, then we have\n$$ P(X, Y|Z) = \\frac{P(X, Y, Z)}{P(Z)} = \\frac{P(Z) P(Y|Z)P(X|Z, Y)}{P(Z)} = P(X|Z)P(Y|Z) $$\nThis is called conditional independence, which means that X and Y are statistically independent, given Z. From the view of a graphical model, we can see that there is no direct link between X and Y shown in the right graph of Figure 1.\nLatent Dirichlet Allocation Now let\u0026rsquo;s learn a topic modeling method that utilises the knowledge we\u0026rsquo;ve talked about so far, Latent Dirichlet Allocation(LDA). Note this is not linear discriminant analysis, which is also abbrivated to LDA. Here, LDA is an unsupervised learning method that is used to model topics within a set of documents. Speaking of \u0026lsquo;topic\u0026rsquo;, it means that when you find a group of words occuring many times in an article, such as \u0026lsquo;banana, apple, fruit, vegetable\u0026rsquo;, you will relate them to an area, like \u0026lsquo;Food\u0026rsquo; in this case.\nLatent In LDA, documents are represented as a fixed group of topics, which are unknown as latent variables. And these topics are characterized by a small specific set of words. For example, words like \u0026lsquo;teacher, student, school, exam, marks\u0026rsquo; should occur more frequently in the area of Education than topics like Sports. This means different topics have different word distribution, as shown in Figure 2.\n  Figure 2: Word distribution varies in different topics  If a document contains more words like \u0026lsquo;teacher, school\u0026rsquo;, it\u0026rsquo;s likely to be identified as \u0026lsquo;Education\u0026rsquo;. But it could contain other topics. For example, if this is a document regarding a sports contest held in a school, then it\u0026rsquo;s much likely to be associated with \u0026lsquo;Sports\u0026rsquo; rather than \u0026lsquo;Education\u0026rsquo;. Thus, we can see that a document can also contain different topics with different weights, i.e. each document has its own topic distribution.\nFrom above, we know that a document consists of a group of topics, and each topic has its special words. To generate a document, we can randomly choose N topics for N words in a document and then randomly choose a word from the corresponding topic. Of course, such an article contains little meaning since we ignore the order and semantics of words. But it\u0026rsquo;s reasonable for LDA since LDA treats documents just as a bag of words(BOW).\nDirichlet So the topic-word distribution and doc-topic distribution are the unknown parameters we want to find, but there are many possible values for them. Again, we need a right prior to describe this uncertainty of the values, but which prior should we use?\nRemember that we draw a topic from $K$ topics for each word in a document with $N$ words, it means there are $K$ possbile outcomes available for each word, and we repeat this process $N$ times, so it\u0026rsquo;s a multinominal distribution. And this is true for drawing a word from that topic with $V$ words. Furthermore, we\u0026rsquo;ve known that the cojugate prior of the multinominal distribution is Dirichlet distribution, so Dirichlet distribution is chosen as our prior, where\n doc-topic distribution follows $\\theta^d \\sim Dir(\\alpha)$ topic-word distribution follows $\\phi^t \\sim Dir(\\beta)$  Allocation To sum up, the whole process of generating a document in LDA is illustrated in Figure 3,\n  Figure 3: Graphical model for LDA  The figure looks a bit scary, well, let\u0026rsquo;s start with some notations first,\n $M=|D|$, the number of documents  $D={d_1, d_2, \u0026hellip;, d_M}$, where $d_i$ represents $i_{th}$ document   $V=|W|$, the number of vocabulary appeared in all documents  $W={w_1, w_2, \u0026hellip;, w_V}$, where $w_i$ represents $i_{th}$ word   $K = |T|$, the number of topics  $T={t_1, t_2, \u0026hellip;, t_K}$, where $t_k$ represents $k_{th}$ topic   $N_{d}$, the number of words in a document $d$ $\\bold w^{d}={w_1^{d}, w_2^{d}, \u0026hellip;, w_{N_{d}}^{d}}$, where $w_i^{d}$ represents $i_{th}$ word in a document $d$ $\\theta^d$ is a probability vector, which represents the distribution of topics in a document $d$  $\\Theta = (\\theta^d|d \\in D)$   $\\phi^t$ is a probability vector, which represents the distribution of words associated with a topic $t$  $\\Phi = (\\phi^t|t \\in T)$   $\\bold t^{d} = t_1^{d}, t_2^{d}, \u0026hellip;, t_{N_{d}}^{d}$, where $t_i^{d}$ represents a topic drawn from $\\theta^d$  From Figure 3, the joint distribution of the hidden and observed variables for a single document is given by,\n$$ p(\\bold t^d, \\bold w^d, \\theta^d, \\Phi|\\alpha, \\beta) = P(\\theta^d|\\alpha) P(\\Phi|\\beta) P(\\bold t^d, \\bold w^d|\\theta^d, \\Phi) = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P( t_n^d, w_n^d|\\theta^d, \\Phi) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d |\\theta^d, \\Phi) P(w_n^d|t_n^d, \\theta^d, \\Phi) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\n$$ = P(\\theta^d|\\alpha) \\prod_t^TP(\\phi^t|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\nOur goal is to maximise this equation, but before that we need to eliminate the hidden variable $\\bold t^d$,\n$$ p(\\bold w^d, \\theta^d, \\Phi|\\alpha, \\beta) = \\int_{t^d} P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} \\sum_{t_n^d=t_1}^{t_K} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\nThere are two common ways to compute this: Gibbs sampling and variational inference.\nGibbs Sampling TODO\nVariational inference TODO\nReferences  http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/  "
            }
    
        ,
            {
                "id": 54,
                "href": "https://ixiaopan.github.io/blog/post/semanticweb/",
                "title": "Semantic Web",
                "section": "post",
                "date" : "2021.05.08",
                "body": "In semester 2, I took a module called Semantic Web Technologies. There are some reasons why I choose this module. At first glance, the name itself sounds not appealing. Actually, it does. But it\u0026rsquo;s still on my shortlist because I\u0026rsquo;ve been working on the web for many years and I wondered what the semantic web was.\nOn the other hand, I was not interested in modules like Advanced Databases since I\u0026rsquo;ve learnt database during my undergraduate degree in Computer Science. Besides, I hope I could have more free time to do other things, and I don\u0026rsquo;t think I can deal with a more difficult module like Reinforcement Learning though it\u0026rsquo;s indeed useful and interesting. Maybe I could learn it later when needed.\nOkay, let\u0026rsquo;s go back to this post. The goal of this post is to go through the most important parts of this module for the final exam preparation.\nWeb for machine We all know that the current web pages are written in HTML. You can use \u0026lt;p\u0026gt; to define a paragraph like this.\n\u0026lt;p\u0026gt;Hello world\u0026lt;/p\u0026gt; HTML looks more like a natural language as it is designed to be readable and understandable for human, not machines. However, we hope machines can understand information as well. But why? Because we want machines to do reasoning about data rather than save data only.\nWeb pages can be linked using the markup \u0026lt;a\u0026gt;, but not all relevant documents are connected. Anyone can publish a new web page anytime. Moreover, not everyone uses the same language to describe the same thing. Thus, the interconnected web pages are simply a limited bundle of documents. More importantly, the data are not connected, and we don\u0026rsquo;t have explicit schemas that enable machines to understand documents.\nOn the contrary, the goal of Semantic Web is to make data readable and understandable for machines. To achieve this, the first step is to encourage people to share their data. Then things are denoted by their unique identifiers. Thus, data from various contexts can be connected through the identifiers, making a huge net finally (also known as Linked Data). Therefore, in Semantic Web, machines learn from data directly instead of documents that are written for people.\nWhat are the applications of Semantic web? Well, we can utilise the vocabulary of a domain to build a knowledge graph, which encodes the semantics of domain knowledge in this network. Search engines can augment their search results from such knowledge graphs since they provide domain-specific knowledge.\nRDF RDF is short for the Resource Description Framework. It\u0026rsquo;s a triple-based data model for knowledge representation. Figure 1 shows a triple composed of 3 components, which means Bob has a friend named Alice.\n  Bob is the subject\n  Alice is the object\n  hasFriend is the predicate that connects them\n    Figure 1: A triple-based data model  The above model is an abstract framework. Subjects and predicates can be anything. To describe the specific subjects and the relationship between them, we need to find a way to identify them explicitly. We also need a data format that defines and parses this model.\nURI In the World Wide Web, URI is used to define a unique object. Here are some examples,\nhttps://www.example.com mailto:aaa@xxx.com urn:isbn:123456 Well, URI can also be followed by an optional fragment identifier like this,\nhttps://www.example.com/index.html#introduction Namespace The above method has one problem. A term may have different meanings in different domains. If we include them all in a file, it will cause name conflicts. We can solve it using namespace. Specifically, a resource can be represented by a namespace, a colon and a local name.\n\u0026lt;http://example.org/ontology#hasFriend\u0026gt; // is equivalent to @prefix d: \u0026lt;http://example.org/ontology#\u0026gt; . d:hasFriend Turtle The last thing is to define a data structure so that machines can parse and serialize. One of the popular data formats is Turtle. The following are some syntaxes defined by Turtle,\n Resource URI are written in angle brackets Literal values are written in double quotes Triples end with a full stop  \u0026lt;http://example.org/data#Bob\u0026gt; \u0026lt;http://example.org/ontology#hasFriend\u0026gt; \u0026lt;http://example.org/data#Alice\u0026gt; . RDF/XML Apart from Turtle, we can also express RDF using RDF/XML, which is similar to XML. But it\u0026rsquo;s not easy for humans to read compared to Turtle.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;rdf:RDF xmlns:rdf=\u0026#34;http://www.w3.org/1999/02/22-rdf-syntax-ns#\u0026#34; xmlns:ns0=\u0026#34;http://example.org/ontology#\u0026#34;\u0026gt; \u0026lt;rdf:Description rdf:about=\u0026#34;http://example.org/data#Bob\u0026#34;\u0026gt; \u0026lt;ns0:hasFriend rdf:resource=\u0026#34;http://example.org/data#Alice\u0026#34;/\u0026gt; \u0026lt;/rdf:Description\u0026gt; \u0026lt;/rdf:RDF\u0026gt; Tools The following are some useful tools for understanding RDF.\n EasyRDF Converter  Tool for converting between different RDF syntaxes   W3C RDF Validator  Validator for RDF/XML, which can also visualise RDF as a graph    Generally, we are required to know how to read them and write some simple statements in this module.\nRDFS RDF only tells us some specific instances. For example, Bob knows Alice, Bob works for Google, etc. However, they don\u0026rsquo;t define the vocabulary used in those triples. From the perspective of object-oriented programming, we want to define the classes that generate these instances. Strictly speaking, we want to define the ontology of a domain. Moreover, if we define some rules in this domain, say Person worksFor Company, then we can infer that Bob is a person and Google is a company under the defined rule.\nRDFS and OWL are two common ontology languages to do this. We will talk RDFS first in this section. RDFS allows us to define classes and properties as well as the characteristics of classes and properties.\nclass definition In OOP, we use keyword class to declare a class, for example,\nclass Person {} In RDFS, we use predefined classes, such as rdfs:Class, rdfs:Property, to declare the corresponding object.\nex:Person rdf:type rdfs:Class .   Figure 2: Declare a class  Furthermore, we use rdfs:subClassOf to declare that a class is a subclass of another class. It\u0026rsquo;s clear that rdfs:subClassOf is transitive. For example, if Teacher is a subclass of UniStaff and UniStaff is a subclass of Person , then Teacher is a subclass of Person, as shown in Figure 3.\nex:Teacher rdf:type rdfs:Class ; rdf:subClassOf ex:UniStaff . ex:UniStaff rdf:type rdfs:Class ; rdf:subClassOf ex:Person .   Figure 3: RDFS class semantics   On the other hand, rdf:type distributes over rdf:subClassOf. This means that if C rdf:type A and A rdfs:subClassOf B , then C rdf:type B. For instance, Figure 4 illustrates that if Bob is an instance of Teacher, then Bob is also an instance of Unistaff and Person.\n  Figure 4: rdf:type distributes over rdfs:subClassOf  property definition Property declaration is similar to class declaration. The following code declares a property named ex:hasStreet.\nex:hasStreet rdf:type rdfs:Property . But not every object has an addresss. Besides, we sometimes want to limit the domain and range of a property. In this case, we would say, only people can have an address.\nex:hasStreet rdf:type rdfs:Property ; rdf:domain ex:Person . We also notice that ex:hasStreet is a more specific attribute of ex:hasAddress since addresses are usually composed of several components like towns and streets. So we define ex:hasStreet as a subproperty of ex:hasAddress.\nex:hasStreet rdf:type rdfs:Property ; rdf:domain ex:Person ; rdf:subPropertyOf ex:hasAddress . Figure 5 illustrates that if Bob lives in a street named West Road, it implies that West Road is Bob\u0026rsquo;s address. Furthermore, we have already stated that only Person can have street names, which means Bob must be a person under this semantics.\n  Figure 5: RDF Schema property semantics   SPARQL So far, we are able to represent data. But how do we retrieve it from databases? Well, SPARQL is a SQL-like query language that allows us to make a query.\nbasic syntax SELECT \u0026lt;variable\u0026gt; FROM \u0026lt;graph\u0026gt; WHERE { \u0026lt;triple patterns\u0026gt; }  variables are prefixed with ? triple patterns are expressed in Turtle  For example, the following codes return the names of the people who have street addresses.\nSELECT ?name WHERE { ?name ex:hasStreet ?street . } FILTER We can also query the names of the people who live on the streets only containing \u0026lsquo;Baker\u0026rsquo;.\nSELECT ?name WHERE { ?person ex:hasStreet ?street . FILTER regex(?street, \u0026#34;Baker\u0026#34;) ?person ex:hasName ?name . } Another example is to filter all the items whose prices are lower than 5 pounds.\nSELECT ?name WHERE { ?item ex:hasPrice ?price . FILTER (?price \u0026gt;= 5) ?item ex:hasName ?name . } UNION UNION allows us to either match this condition or that one, which is equivalent to the operation of OR\nSELECT ?name WHERE { { ?person ex:hasStreet \u0026#34;Baker Street\u0026#34; . } UNION { ?person ex:hasStreet \u0026#34;West Road\u0026#34; . } ?person ex:hasName ?name . } We won\u0026rsquo;t dive into SPARQL deeper because SPARQL has quite similar syntaxes to SQL. If you are familiar with SQL, you\u0026rsquo;ll know how to use SPARQL instantly.\nDescription Logic (DL) TODO\n"
            }
    
        ,
            {
                "id": 55,
                "href": "https://ixiaopan.github.io/blog/post/svd/",
                "title": "Singular Value Decomposition",
                "section": "post",
                "date" : "2021.05.04",
                "body": "Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into the multiplication of three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post.\nChange of Basis Suppose there is a point in the 2D space, how do you describe it? The common way is to use the Cartesian coordinate system, which is composed of two fixed perpendicular oriented axes, measured in the same unit of length. The two perpendicular axes are just a special set of vectors served as the basis of the 2D space. Actually, there are many other sets of vectors that can be the basis for the 2D space. For example, in Figure 1, the position of the red point is (-4, 1) when using the standard basis (1, 0), (0, 1) (colored in grey). If we change the basis to (2, 1), (-1, 1) (colored in blue), the position is (-1, 2).\n  Figure 1: The same point with different coordinates in two different coordinate spaces  From Figure 1, we can see that the absolute position of the red point always stay the same. However, the relative positions to the different bases are different. Mathematically, the red point can be described from the perspective of basis as follows,\n$$ x = P_b[x]_b = c_1 \\bold b_1 + c_2 \\bold b_2 + \u0026hellip; + c_n \\bold b_n $$\n$$ P_b = [\\bold b_1, \\bold b_2, \u0026hellip; , \\bold b_n ] $$\n$$ [x]_b = [c_1, c_2, \u0026hellip; c_n] $$\nwhere\n $[x]_b$ is a set of scalars, which represent the length of projection onto each axis of the current coordinate system $P_b$ is the corresponding basis of the current coordinate system  Let\u0026rsquo;s plug the above point and the basis (1, 0), (0, 1) (colored in grey) into the equation,\n$$ P_b = [ (1, 0), (0, 1)] $$\n$$ [x]_b = (-4, 1) $$\n$$ x_b = -4 \\begin{bmatrix}1\\\\ 0 \\end{bmatrix} + 1 \\begin{bmatrix}0\\\\ 1 \\end{bmatrix} = \\begin{bmatrix}-4\\\\ 1 \\end{bmatrix} $$\nLet\u0026rsquo;s do the same calculation with another basis (colored in blue),\n$$ P_b = [ (2, 1), (-1, 1)] $$\n$$ [x]_b = (-1, 2) $$\n$$ x_b = -1 \\begin{bmatrix}2\\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix}-1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix}-4\\\\ 1 \\end{bmatrix} $$\nAs expected, they yield the same result. And the second one essentially changes the basis of $R^2$ from (2,1),(-1,1) to(1,0),(0,1), which is the standard basis of $R^2$.\nActually, this example is a special case of the change of basis, where the new basis is the standard basis. More generally, $P_{c \\larr b}$ is known as the change of coordinate matrix from the old basis $b$ to the new basis $c$, which we are going to switch to in $R^n$.\nSay we are in the basis $b$ and $[x]_b$ is known, the corresponding coodinates of $x$ under the new basis $c$ can be computed as follows,\n$$ x_c = P_{c \\larr b} x_b $$\nSince $P_{c \\larr b}$ is invertible, we have\n$$ (P_{c \\larr b})^{-1}x_c = x_b $$\nwhich is the inverse operation of change of basis from $b$ to $c$. We can generalize this to any number of points and dimensions.\n$$ A=US\\\\(D,N)= (D, M) \\times (M, N) $$\n$$ U^{-1} A = S\\\\(M, D) \\times (D, N) = (M, N) $$\nwhere\n $A$ is a $D\\times N$ matrix with $D$ dimensions and $N$ points $S$ is a $M\\times N$ matrix with $M$ dimensions and $N$ points described in a new vector space decided by another basis $U$ is the change of coordinate matrix from $S$ to $A$ $U^{-1}$ is the the change of coordinate matrix from $A$ to $S$  If we do some transformation for a point in the standard coordinate system, what\u0026rsquo;re the new coordinates of the same point in another system? This problem can be solved by the following equation,\n$$ x_s' = U^{-1}TUx_s $$\nwhere $T$ represents the transformation matrix. If $T=I$, $x_s'$ is exactly $x_s$.\nSVD SVD is a technique in linear algebra that can be used to decompose any $N \\times P$ matrix\n$$ X = U S V^T $$\n $U$ is an $N \\times N$ orthogonal matrix, where the columns of $U$ are the eigenvectors of $XX^T$ $S$ is an $N \\times P$ diagonal matrix whose diagonal entries are the sorted singluar values, which are square roots of eigenvalues of $XX^T$ or $X^TX$ $V$ is a $P \\times P$ orthogonal matrix, where the columns of $V$ are the eigenvectors of $X^TX$  $$ C = X^TX = (USV^T )^T USV^T = VS^TU^T USV^T = VSS^TV^T $$\n$$ D = XX^T = USV^T (USV^T )^T = USS^TU^T $$\n$$ =\u0026gt; C [\\bold v_1, \\bold v_2, \u0026hellip;, \\bold v_p] = [\\lambda_1 \\bold v_1, \\lambda_2\\bold v_2, \u0026hellip;, \\lambda_p \\bold v_p] $$\n$$ =\u0026gt; D [\\bold u_1, \\bold u_2, \u0026hellip;, \\bold u_n] = [\\lambda_1 \\bold u_1, \\lambda_2\\bold u_2, \u0026hellip;, \\lambda_n \\bold u_n] $$\nTherefore, $V$ and $U$ are matrices of eigenvectors for $X^TX$ and $XX^T$.\nIf we look at the formula of SVD from the view of \u0026lsquo;the change of basis\u0026rsquo; discussed above, we will find that\n $V^T$ is the change of matrix from, say basis A, to the standard basis $S$ is a scaling matrix $U$ is another change of matrix from the standard basis to basis A  Geometrally, the multiplication between a matrix $A$ and a vector $x$ represents a linear transformation, where we using another basis of $R^n$ to represent the same point and $A$ is the change of matrix between bases. By decomposing $A$, we can clearly see that this transformation is composed of three transformations:\n $ V^T$: rotation $S$: scaling $U$: rotation  Economical Forms of SVD Since $S$ is an $r \\times r$ diagonal matrix for some $r$ not exceeding the smaller of $N$ and $P$, we could simplify $S$ and the correspoding $V$ and $U$. Specifically, for $X$ with more samples than features $N \u0026gt; P$ (tall and thin), we ignore the last $N - P$ columns of U\n and for $X$ with more features than examples $N \u0026lt; P$ (short and fat), we ignore the last $P - N$ rows of $V^T$\n Linear Regression Revisited In the previous post Linear Regression 02, we introduced pseudo-inverse $A^+$\n$$ \\bold {\\hat w} = A^+ \\bold y = (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y = V (S^TS)^{-1}S^TU^T \\bold y = VS^+U^T \\bold y $$\nwhere the elements of $S^+$ are the reciprocal of the singular values,\n$$ S^+ = (S^TS)^{-1}S^T = \\begin{bmatrix}s_1^{-1}\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ 0\u0026amp;s_2^{-1} \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ \u0026hellip; \\\\ 0\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; s_p^{-1} \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\end{bmatrix} $$\nThis means if any of the singular values of $X$ are small, then $S^{-1}$ will magnify component in that direction. Thus, little change in $\\bold y$ will lead to a greatly different model and eventually a poor generalization.\nOne way to tackle this is regularisation. Ridge Regression is one variant of linear regression by adding a regulariser $\\lambda ||\\bold w||^2$ to the loss function,\n$$ L = ||\\bold X \\bold w - \\bold y||^2 + \\lambda ||\\bold w||^2 $$\nThe estimate $\\bold w$ is given by\n$$ \\bold {\\hat w} = V(S^TS + \\lambda I)^{-1} S^TU^T \\bold y $$\nwhere\n$$ (S^TS + \\lambda I)^{-1} S^T = \\begin{bmatrix}\\frac{s_1}{s_1^2+\\lambda}\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ 0\u0026amp;\\frac{s_2}{s_2^2+\\lambda} \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ \u0026hellip; \\\\ 0\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; \\frac{s_p}{s_p^2+\\lambda} \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\end{bmatrix} $$\n if $s_i = 0$, then $\\frac{s_i}{s_i^2 + \\lambda} =0$ and the \u0026lsquo;inverse\u0026rsquo; is defined if $s_i \u0026laquo; \\lambda$, then $\\frac{s_i}{s_i^2 + \\lambda} \\simeq \\frac{1}{\\lambda }$ if $si \u0026raquo; \\lambda$, then $\\frac{s_i}{s_i^2 + \\lambda}\\simeq s_i^{-1}$  Therefore, adding a regulariser can make our model much more stable.\nPCA Principal component analysis(PCA) is often used to reduce dimentionality. The idea of PCA is to find directions along which data has the largest variation. The variation can be computed by projecting data onto that direction. Mathematically, we want to find a vector $v$ with $||v||=1$ to maximise\n$$ \\sigma^2 = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 $$\nThere are two things to notice here:\n $v$ is an unit vector since we are care about the direction only data are centralized first for simple computation; centralizing data doesn\u0026rsquo;t change the distribution of data  We can solve the above equation by introducing Lagrange multiplier\n$$ L = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 - \\lambda (||v||^2 - 1) $$\n$$ = \\frac{1}{n-1}\\sum_i^n \\bold v^T (\\bold x_i - \\bold \\mu)(\\bold x_i - \\bold \\mu)^T\\bold v - \\lambda (||v||^2 - 1) $$\n$$ = \\bold v^T \\bold C \\bold v - \\lambda (\\bold v^T\\bold v - 1) $$\nwhere $\\bold C$ is the covariance matrix of $X$. Then we take the derivative of $L$ w.r.t $\\bold v$ , and then set it to $0$\n$$ \\frac{\\partial L}{\\partial \\bold v} = 2(C \\bold v - \\lambda \\bold v) = 0 $$\nso $\\bold v$ is the eigenvector of $\\bold C$, and the variance along this direction is,\n$$ \\sigma^2 = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 = \\bold v^T \\bold C \\bold v = \\lambda \\bold v^T \\bold v = \\lambda $$\nProperties of Covariance Matrix We know that the quadratic form of a vector and a matrix is defined as\n$$ x^T M x $$\nThus, the quadratic form of $C$ is\n$$ x^TCx = x^T XX^Tx=u^Tu \\ge 0 $$\nso $C$ is positive semi-definite, which means all eigenvalues of $C$ are greater than or equal to zero. Why? Suppose $\\mu$ is an eigenvector of $C$, since $\\mu^TC\\mu\\ge0$ and $||\\mu||\u0026gt;0$, then we have\n$$ \\mu^TC\\mu = \\mu^T \\lambda \\mu =\u0026gt; \\lambda = \\frac{\\mu^TC\\mu }{||\\mu||^2 } \\ge 0 $$\nzero eigenvalue What if $C$ has a zero eigenvalue? Well, a zero eigenvalue means that there is no variation in the direction of the corresponding eigenvector. So when will we have zero eigenvalues? Based on the definition of eigenvalue, we have\n$$ Cx = 0x = 0 $$\nSince $x$ is nonzero vector, $C$ is singular or non-invertible. And this will inevitably happen if the number of features is much more than the number of examples. Conversely, if $C$ is invertible, it has no zero eigenvalues and is said to be positive definite (since all eigenvalues are greater than 0).\nGeometry of PCA Since $C$ is a $p \\times p$ symmetric matrix, it can be orthogonally diagonalized as $C = PDP^T$, where $P$ is an orthogonal matrix and $D$ is a diagonal matrix. Thus, the above formula can also be written as follows,\n$$ \\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 = \\sum_i^n \\bold v^T (\\bold x_i - \\bold \\mu)(\\bold x_i - \\bold \\mu)^T\\bold v = \\bold v^T C \\bold v $$\n$$ = \\bold v^T PDP^T \\bold v = \\bold y^T D \\bold y $$\nwhere $\\bold y = P^T \\bold v$. Mathematically, $\\bold v^T C \\bold v$ and $\\bold y^T D \\bold y$ yield the same result, which represents the sum of deviation of each sample from the original point along the direction determined by this vector as shown in Figure 2.\n  Figure 2: Change of variable in $x^T A x$ (Introduction to Linear Algebra[1])  Geometrically, $P^T_{y \\larr v}$ is the change coordinate matrix from $v$ to $y$, which finally transform the shape of the quadratic form $\\bold v^TC\\bold v$ into the standard position, such as the shapes in the figure below.\n  Figure 3: An ellipse and a hyperbola in standard position (Introduction to Linear Algebra[1])  Projection For a data set with $p$ features, we want to reduce its dimension to $k$, there are a few steps to follow\n  Construct a $p\\times p$ covariance matrix $C$ using centralized data\n  Find all the eigenvectors $v_i$ and eigenvalues $\\lambda_i$ of $C$\n  Construct a $p \\times k$ projection matrix $P$ with $k$ eigenvectors determined by the $k$ largest eigenvalues (principal components)\n  Project the original data into the space spanned by the principal components\n  $$ \\bold z = P^T(\\bold x - \\bold \\mu) $$\nwhere $\\bold z$ is our new inputs.\nUsually, there are two common ways to find the eigenvalues:\n eigendecomposition SVD  Eigen-decomposition follows the above steps, and we can see it from the following code,\ndef pca(X): # Data matrix X, assumes 0-centered P, N = X.shape # Compute covariance matrix, where X is a P by N matrix C = np.dot(X, X.T) / (N-1) # Eigen decomposition eigen_vals, eigen_vecs = np.linalg.eig(C) # Project X onto eigen space X_pca = np.dot(eigen_vecs.T, X) return X_pca, eigen_vals, eigen_vecs Instead, SVD decomposes $X$ directly. Besides, we don\u0026rsquo;t need to centralize data first in SVD, though most people will do.\ndef svd(X): N, P = X.shape # In practice, we usually subtract the mean from data and then perform SVD X_c = X - np.mean(X, axis=0) # the columns of Vt are the eigenvalues of X^TX U, Sigma, Vt = np.linalg.svd(X_c) # Project X onto eigen space X_pca = np.dot(X, Vt.T) return U, Sigma, Vt So why do we use SVD? Simply put, If the number of features are much more than the number of example, i.e. $P\u0026raquo;N$, it\u0026rsquo;s not easy to compute eigenvalues using eigen-decomposition. But in SVD, the algorithm will compute eigenvalues by choosing the smaller of two matrice $X^TX$ and $XX^T$. In this case, $X^TX$ has less elements( $N * N$ ) than $XX^T$($ P * P$). More details can be found in the following section \u0026lsquo;PCA for image\u0026rsquo;.\nReconstruction From the perspective of projection, the projection onto a vector $\\bold v_j$ can be seen as approximating the inputs by\n$$ \\hat {\\bold x_i} = \\bold \\mu + \\sum_j^k z_j^i \\bold v_j $$\n$$ z_j^i = \\bold v_j^T(\\bold x_i - \\bold \\mu) $$\nSo our goal is to minimize the error\n$$ E[ ||\\hat {\\bold x_i} - \\bold x_i ||^2] = \\frac{1}{n} \\sum_{n=1}^n ||\\sum_{i=1}^k \\bold u_i^T\\bold x_n \\bold u_i + \\sum_{i=k+1}^p \\bold u_i^T\\bold x_n \\bold u_i - \\sum_{j=1}^k \\bold v_j^T\\bold x_n \\bold v_j = \\frac{1}{n} \\sum_{n=1}^n \\sum_{i=k+1}^p ||\\bold u_i^T\\bold x_n \\bold u_i||^2 $$\n$$ = \\frac{1}{n}\\sum_{n=1}^n\\sum_{i=k+1}^p (\\bold u_i^T\\bold x_n) \\bold u_i^T \\cdot (\\bold u_i^T\\bold x_n) \\bold u_i =\\frac{1}{n} \\sum_{n=1}^n\\sum_{i=k+1}^p (\\bold u_i^T\\bold x_n)^2 $$\n$$ = \\frac{1}{n} \\sum_{n=1}^n\\sum_{i=k+1}^p \\bold u_i^T\\bold x_n \\bold x_n^T \\bold u_i= \\sum_{i=k+1}^p \\bold u_i^T (\\frac{1}{n} \\sum_{n=1}^n\\bold x_n \\bold x_n^T )\\bold u_i $$\n$$ \\sum_{i=k+1}^p \\bold u_i^T S \\bold u_i = \\sum_{i=k+1}^p \\lambda_i \\bold u_i^T \\bold u_i = \\sum_{i=k+1}^p \\lambda_i $$\nWe can see that the error is exactly the sum of the eigenvalues in the directions that are discarded.\nPCA for images Suppose we have an image with the size of $256 \\times 256$, so it has nearly $64K$ pixels or features. Then we could create a covariance matrix $C$ with more than $4\\times10^9$ elements using PCA. However, this huge matrix is not easy to compute eigenvalues. To make this problem tractable, we usually work in a dual space instead of a feature space. The dual space we choose is spanned by $n$ vectors, which are exactly the sample images. Specifically, if we have $n$ images, the subpace of $R^n$ has at most $n-1$ dimensions, and usually $n$ is much smaller than $p$. But how do we find the eigenvalues of $C$ in this dual space?\nWe\u0026rsquo;ve known that $C = XX^T$ is a $p\\times p$ matrix, where $X$ is a $p \\times n $ matrix. Now we construct another matrix $D=X^TX$ with $n \\times n$ elements, which is also a symmetric matrix. Suppose $v$ is the eigenvalue of $D$, then we have\n$$ Dv = \\lambda v $$\n$$ XX^TXv = \\lambda Xv $$\n$$ CXv = \\lambda Xv $$\n$$ Cu = \\lambda u $$\nwhere $u = Xv$. We find that $C$ and $D$ has the same eigenvalues. Thus, we can use the dual $n \\times n$ matrix $D$ to find eigenvalues and eigenvectors of $C$.\nFind K Components The last question is how to decide the number of components. Instead of guessing the number of dimensions that we want to keep, we choose the right $k$ components along which the sum of the explained variance ratio is greater than a threshold. The explained variance ratio of each component indicates how much the variance explained along component. In Sklearn, it can be accessed via explained_variance_ratio_.\nfrom sklearn.decomposition import PCA pca = PCA(n_components=2) X2D = pca.fit_transform(X) pca.explained_variance_ratio_ The following code shows how to find $k$ components with a variance ratio of $0.95$ using Sklearn.\nfrom sklearn.decomposition import PCA pca = PCA() pca.fit(X_train) cumsum = np.cumsum(pca.explained_variance_ratio_) d = np.argmax(cumsum\u0026gt;0.95) + 1 # or simply pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_train) pca.n_components_ Also, we can plot the explained variance ratio as a function of the number of dimensions, and the elbow in the curve is where the appropriate $k$ lies.\n  Figure 4: Explained variance as a function of k (Hands-on machine learning, 2019)  References [1]\tG. Strang, Introduction to Linear Algebra, 5th ed. Wellesley, MA: Wellesley-Cambridge Press, 2016.\n[2] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n"
            }
    
        ,
            {
                "id": 56,
                "href": "https://ixiaopan.github.io/blog/post/end2end-project-01/",
                "title": "An E2E Project - EDA",
                "section": "post",
                "date" : "2021.04.25",
                "body": "So far, we have discussed many algorithms, such as linear regression and ensemble methods. It\u0026rsquo;s time to learn how to build a real machine learning project to enhance our understanding of these models.\nOverview The book \u0026ldquo;Hands-on Machine Learning\u0026rdquo; has summarised the below 7 steps as a guidance to do an end-to-end machine learning project,\n Frame the problem Get the data Explore the data Prepapre the data Explore many different models Fine-tune the model Deploy the system  In this post, we will cover the first 3 parts. The complete code can be found here.\nFrame the problem The first step is to define you problem. People are unlikely to do things without reasons, right? So ask yourself, what problem do you want to solve? Why are you interested in them? What\u0026rsquo;s your objective? Are there any solutions out there?\nAs an example, we are going to predict used car prices because accurate prices prediction can help both buyers and sellers. For buyers, it can ensure the money that customers invest on used cars to be worthy. For used car dealers, they might want to know which factors influence car prices most so as to adjust sales strategy and offer a better prediction to customers. Therefore, there is a necessity for building a used car price prediction system.\nGet the data There are many ways to get the desired data. You can write a web crawler to download the data from related websites or you can get data freely from some public data platforms. Among them, Kaggle is one of the most popular data platforms. In this toy project, the data is downloaded from Used Cars Dataset - Kaggle.\n!kaggle datasets download -d austinreese/craigslist-carstrucks-data -p data !unzip data/craigslist-carstrucks-data.zip -d data Explore the data Exploring the data or EDA is the very first and important step when doing a machine learning project. The goal of EDA is to get insights from the data so as to clean and prepare data for building models. Generally, it involves the following steps,\n Identify variables Examine data quality Univariate analysis Bivariate analysis  Identify variables After getting data, we should have a peek at data first. Specifically, we need to answer the following questions,\n  How many observations and variables do we have?\n  What variables are your predictors and target?\n  How about data types and memory usage?\n  What\u0026rsquo;s the descriptive statistics about data?\n  Luckily, Pandas provides convenient functions to answer these questions.\ndf_vehicles.shape df_vehicles.info() df_vehicles.describe() df_vehicles.head() The following tables shows the distribution of data types and some toy data.\n   Data Type Variable Name     Numerical (6) id, year, price, odometer, lat, long   Object (19) url, image_url, region_url, manufacturer, model, condition, cylinders, fuel, title_status, transmission, drive, type, paint_color, region, state, posting_date, description, size, VIN      Table 1: Some example data  Comments:\n There are 426,880 rows and 25 attributes. The variable price is our target variable and the remaining are our predictors. id, url, region_url, image_url, post_date, and VIN have nothing to do with car prices. Thus, we need to remove them. There are numerous categorical variables plus 6 numerical variables.  Examine data quality The raw data is unlikely to use directly because it\u0026rsquo;s inevitable to introduce errors like duplication, null values or extreme values when collecting data. The higher the data quality is, the better model\u0026rsquo;s performance is. So we need to identify and solve these problems to obtain a clean data set. Below are some common techniques to check whether we have redundant data, missing values or outliers.\n1) duplication # return true if we have duplicate rows df.duplicated() # remove the duplicated rows, keep the first row by default df.drop_duplicates() Fortunately, there are no duplicated rows. But it depends.\n2) missing value Heatmap enables us to identify the distribution of missing values visually.\nsns.heatmap(df.isna())   Figure 1: A heatmap for visualising the distribution of null values  Figure 1 shows that size, condition, VIN, cylinders, drive and paint_color have a significant number of null values. To quantify the number of missing values, we can use a table shown below.\n  Table 2: A table that shows the percentage of null values  3) outlier In the previous post Descriptive Statistics, we talked about the descriptive statistics like mean, variance, and range. These values can be obtained easily using the following code,\ndf_vehicles.describe()   Table 3: A table that shows descriptive statistics  From above table, we can see that the lowest car price is 0 while the highest car price is up to 3,600,000,000. The mean of the car prices is greater than the median, which means the distribution of prices is not symmetrical and there are abnormal values in prices. And the same goes for odometer. To verify our belief further, we can plot boxplots for year and odometer shown in Figure 2.\n  Figure 2: Boxplots for year and odometer  Okay, let\u0026rsquo;s put it all together. From the above initial examination, we can conclude that,\n There are 426,880 rows and 25 attributes in our data set, and price is our target variable. A great deal of categorical variables need to be encoded. There are no redundant rows. There are some useless columns to be dropped, such as url, image_url, region_url, id, post_date, and VIN . Nearly all features contain a great deal of missing values. We can remove features directly or find ways to impute them. price and odometer have extreme values.  Univariate analysis In univariate analysis, we will look at variables one by one. The statistics and visualization methods depend on the data types. Typically, we divide data into 2 types: numerical variable and categorical variable.\n1) numerical variables For numerical variables, we measure the central tendency and dispersion of the data, which are discussed earlier. To visualize data, we can use histogram, boxplot, or other suitable charts. You might find that this is also exactly what we did before for detecting missing values and outliers.\n   measurement statistics     central tendency mean, median, mode   dispersion range, quantile, variance, skewness   visualization histogram, boxplot, bar chart    For example, Figure 3 shows the distribution of the variable year. It can be seen that most cars were made after 2000. Besides, this distribution has a long left tail, which means the mean is lower than the median. From Figure 4, we can find that the mean is 2010 while the median is 2013. This is because the feature year contains some extreme small values.\n  Figure 3: The distribution of the year  2) categorical variables For categorical variables, we usually plot bar charts to understand the distribution of each category, as shown in Figure 4.\n  Figure 4: Bar charts for some categorical variables  Bivariate analysis Analyzing one predictor seems a bit monotonous. In fact, we are more interested in the relationships between predictors and our goal. Generally speaking, we can do conduct an analysis\n between numerical and numerical between numerical and categorical between categorical and categorical  1) numerical vs numerical Scatter plots provide a nice way to find the relationships between continuous variables. It\u0026rsquo;s easy to plot them with the help of seaborn. From Figure 5, we can see that there is a negative relationship between odometer and car prices, which means the car prices will decrease as the odometer increases.\n  Figure 5: The relationship between odometer and price  However, scatter plot cannot tell us how strong this relationship is. The solution is to calculate correlation. Figure 6 shows that the car prices have a moderate positive relation with year and a negative relation with odometer. On the contrary, latitude and longitude have a weak relation with car prices.\n  Figure 6: The correlation among continuous variables  2) categorical vs numerical For categorical features, we group data by category and compare them side by side using boxplot or line chart. Basically, we just divide the whole data into several groups and the following analysis is the same as we did before.\n3) categorical vs categorical Conclusion In summary, we introduced some common data analysis techniques to gain insights from your data in this post. Hopefully it can give you an idea of how to explore data. If you\u0026rsquo;d like to explore more excellent analysis methods and visualizations, Kaggle is a great place to enhance your skills.\nAs for the remaining parts, we will talk about them in the following posts.\nReferences [1] https://www.kaggle.com/austinreese/craigslist-carstrucks-data\n[2] https://www.scribbr.com/statistics/statistical-tests/\n"
            }
    
        ,
            {
                "id": 57,
                "href": "https://ixiaopan.github.io/blog/post/ensemble-methods/",
                "title": "Ensemble Methods",
                "section": "post",
                "date" : "2021.04.20",
                "body": "Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.\nOverview In real life, we often take advices from others. For example, suppose you want to know whether a movie is worthwhile to watch, you may ask your friends who\u0026rsquo;ve watched to give the movie a score(out of 5), say 3, 3, 2, 2, 2, 4. Since 5 people gave a score that is lower than 4, you may think about choosing another movie, let\u0026rsquo;s say Avatar. Then you ask your friends again and have some scores like this 5, 5, 5, 5, 5. Wow! All of your friends think that Avatar is an amazing movie that should certainly not be missed. You agree with their opinons and decide to watch Avatar finally. From this example, we can see that gathering plenty of opinions from different people are likely to make an informed decision.\nHere, I highlight the words different people. It makes little sense if we only asks for people who have the same interests. Therefore, the more diverse the people are, the more sensible our decisions are. Basically, the idea behind it is the wisdom of collaborating.\nVoting The method used to decide whether to watch a movie or not in this example is known as voting. For classification, there are 2 types of voting named hard voting and soft voting.\n Hard voting returns the most popular class shown in Figure 1. Soft voting averages the probability of each class and then return the class that has the maximum probability.  Hard Voting   Figure 1: Hard voting classifier predicitons (Hands-on machine learning, 2019)  Figure 1 can also be illustrated in a mathematical way,\n$$ y' = mode(C_1(x), C_2(x), \u0026hellip;, C_n(x)) $$\nFor example, {0, 1, 0, 1, 1} are the class labels predicted by our 5 different classifiers for a data point $x$. By hard voting, the final class label is class 1 .\nC1 -\u0026gt; 0 C2 -\u0026gt; 1 C3 -\u0026gt; 0 C4 -\u0026gt; 1 C5 -\u0026gt; 1 Weighted Hard Voting Hard voting works nice, but in some cases, some people might be more professional than others. Hence, their opinions are much more significant. How to distinguish professionals and common people?\nWe assign weights to them. Specifically, we assign higher weights to professionals while common people have lower weights. Then we calculate weighted sum of occurrence of each class label and find the class label that has the maximum value.\n$$ y' = \\operatorname*{argmax}_i w_j\\sum_j [C_j == i] $$\nwhere $[C_j == i] = 1$ if classifier $j$ predicts class label i and 0 otherwise.\nFor example, if we assign the following weights to the previous 5 classifiers, then we will have 0.7 for class 0 and0.5 for class 1. Thus, class 0 wins because 0.7 is greater than 0.5.\n0.4, C1 -\u0026gt; 0 0.1, C2 -\u0026gt; 1 0.3, C3 -\u0026gt; 0 0.2, C4 -\u0026gt; 1 0.2, C5 -\u0026gt; 1 Soft Voting Instead of predicting the class label directly, some classifiers like logistic regression can predict the probability of each class label that $x$ belongs to. Then we simply average these probabilities for each class label. Certainly, you can assign weights to classifiers.\n$$ y' = \\operatorname*{argmax}i \\frac{1}{n} \\sum_j^n w_j p{ij} $$\nwhere $p_{ij}$ is the probability of class label $i$ that $x$ belongs to when using classifier $C_j$.\nAverage for Regression We simply average the predictions of different machines for a regression task.\n$$ y' = \\frac{1}{n} \\sum_j^n w_j C_j $$\nBagging In order to make our models different from each other, we use various algorithms to train the same data, as discussed above. Another way to have a set of diverse models is to train the same model on different data sets. But usually we only have one training data set. Where do other data sets come from? Well, they are sampled with replacement from the original data set, which is known as bootstrapping.\n  Figure 2: The process of bagging (Hands-on machine learning, 2019)  Specifically, given a training data set $D=(x_i, y_i)_i^n$ of the size $N$, we build an ensemble model of size $m$ according to the following steps:\n  For $i=1, 2, 3, \u0026hellip;, m$\n draw $N'(N' \\le N)$ samples with replacement from $D$, which is denoted by $D^*_i$ build a model (e.g. decision tree) $T^*_i$ based on $D_i^*$    For an unseen data, aggregate the predictions of all $T^*$\n perform a majority vote for classification average the predictions for regression    from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier ensemble_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=300, max_sample=100, bootstrap=True) Random Forest Bagging can be used for any models. Among them random forest is the special one. As its name suggests, it is exclusively designed for decision trees. Besides, it introduces extra randomness when growing trees.\n  Figure 3: A random subset of features at each split for each tree (Reference [2])  Specifically, it randomly choose a subset of $m'$ of the features at each split instead of using all features shown in Figure 3. By doing so, all trees can have much different training data set further, so they are less similar to each other, which results in more significant predictions.\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier from sklearn.tree import DecisionTreeClassifier random_forest_clf = BaggingClassifier(splitter=\u0026#39;random\u0026#39;, DecisionTreeClassifier(), max_leaf_nodes=16,n_estimators=300, max_sample=100, bootstrap=True) ## is equivalent to this random_forest_clf=RandomForestClassifier(n_estimators=300, max_leaf_nodes=16) Extra-Trees TODO\nWhy Bagging works Take random forests as an example, each decision tree is a machine learned from a data set. Based on the theory of bias and variance, we know that the mean meachine can be expressed as $f'_m=E_D[f'(x|D)]$. Thus, $f'(x|D)$ can be interpreted as a random variable $X$, and $f'_m$ can be described as $\\mu = E(X)$.\nSince we have $N$ decision trees in a random forest, there are $N$ random variables $X_i$, where $\\mu = E(X_i)$. We can construct a new random variable $Z = \\frac{1}{n}\\sum_i^nX_i$, which represents the mean of $n$ random independent variables $X_i$.\nThe expected value of $Z$ is given by\n$$ E[Z] = E[ \\frac{1}{n}\\sum_i^nX_i ] =\\frac{1}{n} E[\\sum_i^nX_i] = \\frac{1}{n} nE[X_i] = \\mu $$\nThe variance is\n$$ E[(Z - E[Z])^2] = E[(\\frac{1}{n}\\sum_i^nX_i - \\mu)^2] = \\frac{1}{n^2} E[(\\sum_i^n (X_i - \\mu))^2] $$\n$$ = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2 + \\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] $$\nSince $X_i$ is independent of $X_j$ ( $i\\ne j$ ),\n$$ E[\\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] = 0 $$\nwe are left with\n$$ E[(Z - E[Z])^2] = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2] = \\frac{1}{n} \\sigma^2 $$\nwhere $E[(X_i-\\mu)^2]=\\sigma^2$.\nFrom the above euqation, we can see that ensemble methods reduce variances as $n$ increases when our models are uncorrelated.\nOn the contrary, if our models are correlated with the correlation coefficient $\\rho$\n$$ \\rho = \\frac{E[(X_i - \\mu)(X_j - \\mu)]} {\\sqrt{\\sigma^2(X_i)}\\sqrt{\\sigma^2(X_j)}} $$\nThe variance is\n$$ E[(Z - E[Z])^2] = \\frac{1}{n} \\sigma^2 + \\frac{n-1}{n} \\rho \\sigma^2 = \\rho \\sigma^2 + \\frac{(1 - \\rho)}{n} \\sigma^2 $$\nAs $n$ increases, the second term vanishes and we are left with the first term. Therefore, if we want the ensemble methods to do well, we need our models to be uncorrelated.\nRandom forests do a good job because of both the randomness of training data sets sampled via bootstrapping and the randomness of features considered at each split. The algorithm results in much less correlated trees, which trades a higher bias for a lower variance, generally yielding better results.\nFeature Importance Like decision tree, random forests can tell us the relative importance of each feature. It is measured by calculating the sum of the reduction in impurity over all the nodes that are split on that feature.\nrandom_forest_clf.feature_importances_ Boosting In boosting, we combine a group of weak learners into a strong learner.\nWhat\u0026rsquo;s the weak learners? They are the learning machines that do a little better than chance. Thus, they have high bias but low variance. The goal of boosting is to reduce the bias by combining them.\nHow to construct a weak learner? Well, one of the widely used types of weak learner are very shallow trees, for example, the stump with only one depth.\nAdaBoost and GradientBoost are two popular algorithms for boosting. Let\u0026rsquo;s lookt at AdaBoost first.\nAdaBoost AdaBoost is a classic algorithm for binary classification. Suppose we have a data set $D=(x^i,y^i)_i^N$ with $y^i \\in {-1, 1}$, and our weak learner $h(x)$ provides a prediction $h(x^i) \\in {-1, 1 }$. The goal of AdaBoost is to construct a strong learner by combining all the weak learners, which can be written as a weighted sum of weak learners,\n$$ C_n(x) = \\sum_i \\alpha_i h_i(x) $$\nHow to find $\\alpha_i$ and $h_i(x)$? Well, it\u0026rsquo;s difficult to find all the coefficients for one time, so we will solve this equation greedily,\n$$ C_n(x) = C_{n-1}(x) + \\alpha_n h_n(x) $$\nwhere $C_{n-1}(x)$ is the current ensemble model that fit the training data best and $h_n(x)$ is the weak learner we are going to add.\nExponential Loss The second step is to find an appropriate loss function to optimize. How do we measure the performance of a classification model? One of the most widely used loss functions is 0-1 loss,\n$$ L = \\sum_i^N [y_i \\ne y'_i] $$\nwhere $[y_i \\ne y'_i] = 1$ if $x_i$ is classified incorrectly and 0 otherwise. However, it\u0026rsquo;s not-convex and difficult to optimize shown in Figure 4. In AdaBoost, we use exponential loss.\n$$ L = \\sum_i^n e^{-y^iC_n(x^i)} $$\nFrom Figure 4, it can be seen that data that are classified correctly have lower value while misclassfication observations have much larger values, which means exponential loss punishes examples classified incorrecly much more than correct classifications.\n  Figure 4: Exponential loss in AdaBoost  Intuition To minimize the loss, we plug the previous $C_n(x)$ into the loss function\n$$ L = \\sum_i^n e^{-y^i (C_{n-1}(x^i) + \\alpha_n h_n(x^i))} = \\sum_i^n e^{-y^i C_{n-1}(x^i)} e^{-y^i \\alpha_n h_n(x^i)} =\\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} + \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} $$\n$$ = \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} - \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} $$\n$$ = e^{-\\alpha_n} \\sum_i^n w_n^i + (e^{\\alpha_n} - e^{-\\alpha_n}) \\sum_{y^i\\ne h_n(x^i) }^n w_n^i $$\nThen we find that minimizing the loss is equivalent to minimizing the sum of weights of each data that $h_n(x)$ misclassified, and that the value of weights depend on the current ensemble model $C_{n-1}(x)$.\n$$ \\sum_{y^i\\ne h_n(x^i) }^n w_n^i = \\sum_{y^i\\ne h_n(x^i) }^n e^{-y^i C_{n-1}(x^i)} $$\n If the data misclassified by $C_{n-1}(x)$ are still classified incorrectly by $h_n(x)$, then $w_n^i$ is extremely large. If the data classified correcly by $C_{n-1}(x)$ are misclassified by $h_n(x)$, then $w_n^i$ is small.  Simply put, misclassified data points will get high weights while correctly classified data points will get their weights decreased.\nTherefore, we are finding some weak learner that tries to correct the errors the previous learners made. Furthermore, we also notice that we need to update $w_n^i$ for the next weak learner $h_{n+1}(x)$,\n$$ w_{n+1}^i = e^{-y^i C_{n}(x^i)} = e^{-y^i (C_{n-1}(x^i) + \\alpha_nh_n(x^i))} = w_n^i e^{-y^i\\alpha_nh_n(x^i)} $$\nSo the new weight of each data depends on the last weight of that data, the weight of the previous weak learner $h_n(x)$ and itself. But wait, what\u0026rsquo;s the initial weight of each data? We simply initialize weights $w_1^i = \\frac{1}{N}$ for every training sample.\nOkay, now we are only left with $\\alpha_n$. To find $\\alpha_n$, we take the derivative of $L$ with respect to $\\alpha_n$\n$$ \\frac{\\partial L}{\\partial \\alpha_n} = e^{\\alpha_n} \\sum_{y^i\\ne h_n(x^i)}^n w_n^i - e^{-\\alpha_n} \\sum_{y^i= h_n(x^i) }^n w_n^i $$\nThat is\n$$ \\alpha_n = \\frac{1}{2} In\\frac{\\sum_{y^i= h_n(x^i) }^n w_n^i}{\\sum_{y^i\\ne h_n(x^i) }^n w_n^i} $$\nAlgorithm Let\u0026rsquo;s put it all together. The algorithm of AdaBoost can be summarised as below,\n  Given a data set $D=(x^i,y^i)_i^N$ with $y^i \\in \\{-1, 1\\}$ and a group of weak learners $h(x)$ of size $T$\n  Associate a weight $w_1^i = \\frac{1}{N}$ with every data point $(x^i, y^i)$\n  For $t = 1$ to $T$\n Train a weak learner $h_t(x)$ that minimises $\\sum_{y^i\\ne h_t(x^i) }^n w_t^i $ Update the weight of this learner, $\\alpha_t = \\frac{1}{2} In\\frac{\\sum_{y^i= h_t(x^i) }^n w_t^i}{\\sum_{y^i\\ne h_t(x^i) }^n w_t^i}$ Update weights for each training point, $w_{t+1}^i = w_t^i e^{-y^i\\alpha_th_t(x^i)}$    Make a prediction\n $C_n(x) = sign[\\sum_t^T \\alpha_t h_t(x)]$    Example Step 1: Initialisation\nHere we have 8 rows with 3 predictors chest_pain, blocked_arteries and weight and 1 target variable heart_disease. Each data point is initialised with an equal weight 0.125.\n  Figure 5: Toy data from \u0026#39;StatQuest with Josh Starmer\u0026#39;  Step 2: Find the weak learner\nHere, we use stump as our weak learner and Figure 6 shows the first optimal tree where we only misclassified one observation.\nfrom sklearn import tree X = df.drop([\u0026#39;heart_disease\u0026#39;, \u0026#39;weights\u0026#39;], axis=1) y = df[\u0026#39;heart_disease\u0026#39;] clf = tree.DecisionTreeClassifier(max_depth=1) clf = clf.fit(X, y) tree.plot_tree(clf.fit(X, y)) plt.show()   Figure 6: The first stump  Step 3: Update weights\nSince we have only one misclassification, the error rate is 1/8 and $\\alpha_1$ is 0.97. Then we update weights for each data point using $\\alpha_1$.\ndef cal_alpha(error): return 0.5*np.log((1 - error)/error) alpha_1 = cal_alpha(1/8) correct_samples = df[clf.predict(X) == y] df.loc[clf.predict(X) == y, \u0026#39;weights\u0026#39;] = correct_samples[\u0026#39;weights\u0026#39;] * np.exp(-alpha_1) misclassified_samples = df[clf.predict(X) != y] df.loc[clf.predict(X) != y, \u0026#39;weights\u0026#39;] = misclassified_samples[\u0026#39;weights\u0026#39;] * np.exp(alpha_1) print(alpha_1, df)   Figure 7: Updated weigts after training the first stump  Step 4: Go back to Step 2 until the desired number of learners is reached.\nAdjusted impurity Recall that Gini Index is written as\n$$ Q_m^g(L) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nand entropy discussed in Decision Tree as\n$$ Q_m^e(L) = -p_c(L)logp_c(L) $$\nwhere $p_c(L)$ is the fraction of the observations belong to class $c$. In order to use the weight of each data in AdaBoost, we need to change it slightly.\n$$ p_c(L) = \\frac{\\sum_{x^j \\in C} w_n^j I[y_j == C]}{\\sum_{x^i \\in L} w_n^i} $$\nWhy this works? Remember that the lower the impurity is, the better the split is. And a higher fraction leads to a lower impurity or entropy.\n If we classify the misclassified example in the node $L$ correctly, then the denominator of $p_c(L)$ becomes smaller and then $p_c(L)$ becomes larger. On the contratry, if this split works so bad, then we will have many observations that classified incorrectly, resulting in smaller $p_c(L)$ due to a small numerator and large denominator.  Thus, we are finding the best split that can correctly classify the examples that previous learners failed as much as possible.\nReferences [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2]\tT. Yiu, “Understanding random forest - towards data science,” Towards Data Science, 12-Jun-2019. [Online]. Available: https://towardsdatascience.com/understanding-random-forest-58381e0602d2. [Accessed: 23-Apr-2021].\n[3]\tJ. Rocca, “Ensemble methods: bagging, boosting and stacking - Towards Data Science,” Towards Data Science, 23-Apr-2019. [Online]. Available: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205. [Accessed: 23-Apr-2021].\n[4] Gradient Boosting Tutorial http://talks.albertauyeung.com/pycon2017-gradient-boosting/\n"
            }
    
        ,
            {
                "id": 58,
                "href": "https://ixiaopan.github.io/blog/post/decision-tree/",
                "title": "Decision Tree",
                "section": "post",
                "date" : "2021.04.19",
                "body": "The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as \u0026lsquo;Is it a fiction movie? Is it directed by David Fincher?\u0026rsquo;\n  Figure 1: The process of movie selection  From Figure 1, we can see that a decision tree builds a binary tree to partion the data. Each node is a decision rule based on a feature and the tree can grow endlessly. Two questions then arise:\n How is the decision rule made? How deep is the decision tree?  Decision Rule Since there are so many features and each feature can have different values, we have to loop over all of them and find the best split. Well, how to measure the performance of a split? There are 3 measures widely used — Gini Impurity, Entropy, and Information Gain.\nGini Index Gini Index, also known as Gini Impurity, measures how often a randomly selected element from the set is classified incorrectly.\n$$ Q_m^g(L) = \\sum_{c \\in C } p_c(L) (1 - p_c(L)) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nwhere $L$ is short for the children node and $p_c(L)$ is the faction of class $c$ in the leaf node $L$,\n$$ p_c(L) = \\frac{1}{|L|} \\sum_{x, y \\in L} [y == c] $$\nwhere $[y == c] = 1$ if $y$ belongs to class $c$ and 0 otherwise.\n  Figure 2: The plot of Gini Index  Figure 2 shows that the value of Gini Index is the lowest at the start and end of the x-axis and maximum at the middle of the x-axis. In other words,\n If the leaf node only has one class, then this node is a pure node and the gini impurity of this leaf node is 0. On the other hand, if all elements in this leaf node belong to an individual class, then the Gini Index of this node has the maximum value of $ 1 - 1/len(L) $.  Entropy Entropy is a concept of Information Theory. Before introducing the entropy, we should have a little understanding of information.\nInformation is related to the surprise in some way. For example, if I told you that you will go to work tomorrow. Well, that\u0026rsquo;s not surprising because you work every day. However, if I told you that tomorrow is the end of the world, you are likely to be shocked because it is an breaking news.\nWe can measure this surprise by the following equation\n$$ I(x) = -log(p(x)) $$\nIf an event is unlikely to happen, then $p(x)$ is close to 0 and $I(x)$ tends to be infinity, which means it conveys much information since impossible things happened and it must have a significant implication behind it.\nThen what\u0026rsquo;s the entropy? The previous equation calculate the information contained in one outcome. However, it\u0026rsquo;s quite often to have many outcomes for a random variable $X$. Therefore, the expected information over all outcomes is defined as the entropy.\n$$ H(X) = E_{x \\in X}[-log(p(x)] $$\nIn this case, the classes in the leaf node is the random variable $X$ and the probability of each outcome is the fraction of each class, which is expressed as\n$$ Q_m^e(L) = \\sum_{c \\in C}- p_c(L) log(p_c(L)) $$\nIn a word, entropy measures the randomness of a set. Lower entropy means a purer set while higher entropy means there are more other classes that is not the class $c$ in a set.\nInformation Gain Information Gain is simply the difference between the impurity of the parent node $A$ before splitting and the sum of impurity of all children nodes after splitting. It measures how much the impurity of a set $A$ were reduced after splitting. The larger the information gain is, the better the split is.\n$$ IG(A) = H(A) - \\sum_{L \\in children \\ nodes} p(L) H(L) $$\nIn summary, when a decisiton tree makes a split on a feature, it tries to achieve,\n a lower impurity a lower entropy a higher information gain  CART Algorithm CART is short for classification and regression tree algorithm, which is used to train Decisioin Trees. CART tries to find the best split that produces purest subsets by calculating the weighted Gini Impurity on each split made by each feature $k$ from the parent node\u0026rsquo;s training sample and corresponding threshold $t_k$.\nSpecifically, it tries to minimize the following loss function for a classification task,\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}Q_m(C_m^L) + \\frac{|C_m^R|}{|N_m|} Q_m(C_m^R) $$\nwhere $C_m^L$ and $C_m^R$ are children nodes splitted on node $N_m$.\nFor a regression task, it minimizes the sum of squred error\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}SSE(C_m^L) + \\frac{|C_m^R|}{|N_m|} SSE(C_m^R) $$\nwhere $SSE(subset)$ is defined as\n$$ SSE(subset) = \\sum_{n \\in subset} (y_n - \\overline y)^2 $$\n$$ \\overline y = \\frac{1}{|subset|} \\sum_{n \\in subset} y_n $$\nPrunning Now let\u0026rsquo;s address the second problem, i.e. when to stop growing the tree. You can either stop growing the tree when you build the tree or trim the tree after building, which are known as pre-pruning and post-pruning respectively.\nPre-pruning Scikit-learn provides several hyperparameters to do a pre-pruning:\n the maximum depth the minimum number of the samples a node must have to split the minimum number of the samples in a leaf node the maximum number of leaf nodes  All of these parameters can be tuned with cross-validation.\nTODO\nPost-pruning Though pre-pruning is straightforward, it is a bit short-sighted since it doesn\u0026rsquo;t build a full tree and there might be some splits works better later on. Therefore, it would be better to have a large and full-size tree and then we trim some less important branches to avoid overfitting. Cost complexity pruning, also known as weakest link pruning, is one way to achieve this.\nPros and Cons Advantages:\n Decision trees are simple and intutive to interpret, and we can easily visualize the process of decision making. They can be used for both classification and regression. There is no need to normalize or scale data. They can help to understand what features are most important.  Disadvantages:\n They can be easy to overfit. They are sensitive to variations of training data. If you rotate the same data, you will get a completely different tree because all splits are perpendicular to an axis.    Figure 3: senstivity to variations of training set (Hands-on machine learning 2019)  References [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2] arunmohan, “Pruning decision trees,” Kaggle.com, 02-Sep-2020. [Online]. Available: https://www.kaggle.com/arunmohan003/pruning-decision-trees. [Accessed: 22-Apr-2021].\n[3] “How to choose α in cost-complexity pruning?,” Stackexchange.com. [Online]. Available: https://stats.stackexchange.com/questions/193538/how-to-choose-alpha-in-cost-complexity-pruning. [Accessed: 22-Apr-2021].\n[4] “Cost-complexity pruning - ML wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Cost-Complexity_Pruning. [Accessed: 22-Apr-2021].\n"
            }
    
        ,
            {
                "id": 59,
                "href": "https://ixiaopan.github.io/blog/post/linear-regression-02/",
                "title": "Linear Regression 02",
                "section": "post",
                "date" : "2021.04.17",
                "body": "In the previous post, we talked about simple linear regression. However, we only considered one predictor. It\u0026rsquo;s quite common to have multiple predictors for real-world problems. For example, if we want to predict car prices, we should consider many factors like car sizes, manufacturers and fuel types. The simple linear regression is not suitable for this case. Therefore, we need to extend it to accommodate the multiple predictors.\nMultiple Linear Regression Suppose we have a data set with the size of $n$, and each data point has $d$ dimensions. Then the input data is denoted by $X \\in R^{n \\times d}$, and the parameters and targets are denoted by $\\bold w \\in R^d$, $\\bold y \\in R^n$ respectively. Thus, the loss function can be written by the following equation:\n$$ L = \\sum_i^{n} (\\bold x_i \\bold w - \\bold y_i)^2 = (\\bold X \\bold w - \\bold y)^T(\\bold X \\bold w - \\bold y) $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - \\bold y^T \\bold X \\bold w - \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - 2 \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\nThen we take the derivative of $L$ with respect to $\\bold w$ as simple linear regression before. Well, we need to know a little bit about the matrix calculus\n$$ \\frac{\\partial}{\\partial \\bold x} \\bold x^TA\\bold x = (A + A^T)\\bold x $$\n$$ \\frac{\\partial}{\\partial \\bold x} A^T \\bold x = A $$\nThe gradient of $L$ can be seen easily\n$$ \\frac{\\partial}{\\partial \\bold x } L = (2\\bold X^T\\bold X)\\bold w - 2 \\bold X^T\\bold y $$\nSetting this gradient to zero,\n$$ \\bold w= (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y $$\nHowever, this equation is unlikely to work if $\\bold X^T\\bold X$ is not invertible (singular), such as the number of features are more than the number of observations ($n \u0026lt; d$). One way to solve this equation is to use SVD.\nPseudoinverse SVD technique can decompose any matrix $A$ into the multiplication of three matrices, i.e. $U\\Sigma V^T$. Thus the above equation can be written in the following form,\n$$ \\bold w = A^+y $$\n$$ A^+ = (\\bold X^T\\bold X)^{-1} \\bold X^T = V\\Sigma^{-1}U^T $$\nIn practice, the algorithm will set the elements of $\\Sigma$ that less than a smaller threshold to zero, then take the inverse of all nozero values, and finally transpose the resulting matrix i.e. $(U\\Sigma V^T)^{-1}$\n When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Speciﬁcally, it provides the solution $x = A^+ y$ with minimal Euclidean norm $||x||_ 2$ among all possible solutions.\nWhen A has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the x for which Ax is as close as possible to y in terms of Euclidean norm $|| Ax − y ||_ 2$ .\n— Deep Learning, p44\n Comparison of algorithms Below are common methods to estimate LR.\n Analytical solution calculates $(X^TX)^{-1}$ directly, which is a $d \\times d$ matrix, so larger $d$ causes slow computation SVD is a dimension reduction method, again, smaller $d$ is fast GD-based methods tend to be affected by the scale of features      Large N Large D Scaling Required     Equation fast slow No   SVD fast slow No   Batch Gradient Descent slow fast Yes   SGD fast fast Yes    Probabilistic Interpretation Assumption Linear Regression works based on the following assumptions\n Y has a linear relationship to X. To check this, we can plot a scatterplot Y values are independent For each $x_i$, there is an error $\\epsilon_i$ following Gaussian distribution $\\epsilon_i \\sim N(0, \\sigma^2)$  Residual Plot A residual plot shown in Figure 1 is a graph that shows the relationship between residuals and $X$. If the residuals are randomly placed around the x axis, a linear model is appropriate. Otherwise, we would transform data to satisfy linearity.\n  Figure 1: Residual plots. [Source from [2]]  MLE We have found the parameters by minimizing the loss, but now we are going to use another method to derive the same result, which is known as maximum likelihood estimation(MLE).\nThe basic idea of MLE is that if the data were generated from some model, then what\u0026rsquo;s the parameters of the model were most likely to make this happen? In other words, we are finding the parameters that maximize the probability of the data $D$ that we\u0026rsquo;ve seen.\nSuppose we have a data set of inputs $X={x^{(1)}, x^{(2)}, \u0026hellip;, x^{(N)}}$ and corresponding target variables ${y_1, y_2, .., y_N}$ with a Gaussian noise $\\epsilon_i \\sim N(0, \\sigma^2)$, we obtain\n$$ y'_i \\sim N(b + ax_i, \\sigma^2) $$\nNext we construct the likelihood of all data points,\n$$ L(\\theta|D) = \\prod_{n=1}^N p(y_i|x_i, a, b, \\sigma^2) $$\nFor a target value $y_i$, the probability of $y_i$ given the parameters $a, b, \\sigma$ is\n$$ p(y_i|x_i, a, b, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_i - b - ax_i)^2}{2\\sigma^2}} $$ Our goal is to find the appropriate parameters $a, b, \\sigma$ to maximise all the likelihood of $y_i$. Usually, we take the log likelihood to make computation more simpler,\n$$ In(L(\\theta|D)) =\\sum_i^n In(\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_i - b - ax_i)^2}{2\\sigma^2}}) \\\\ = \\frac{N}{2}In\\sigma - \\frac{N}{2}In2\\pi - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(y_i - b - ax_i)^2 $$\nFrom above equation, we can see that maximizing the likelihood is equivalent to minimizing the sum of squared error.\nGeometry of LR In this section, we will look at the geometry of the linear regression. In $N$-dimensional space whose axes are the values of $y_1, y_2, \u0026hellip;, y_n$ , the least-squares solution is obtained by finding the orthogonal projection of the target vector $y$ onto the subspace spanned by the columns of $X$.\n  Figure 1: Geometry interpretation of the least-squares solution (PRML 2006)  From the following matrix form, we can see that the predicted value $\\bold y'$ lies the column space of $X$. If the true target value $\\bold y$ also lies in this space, then the loss of linear regression is zero, which is never the case in real life.\n$$ \\displaystyle{\\bold y' = \\bold X \\bold w = \\begin{bmatrix}1\u0026amp;x_{11} \u0026amp; x_{12} \u0026amp; \u0026hellip; \u0026amp; x_{1d}\\\\ 1\u0026amp;x_{21} \u0026amp; x_{22} \u0026amp; \u0026hellip; \u0026amp; x_{2d}\\\\ \u0026hellip; \\\\ 1\u0026amp;x_{n1} \u0026amp; x_{n2} \u0026amp; \u0026hellip; \u0026amp; x_{nd} \\end{bmatrix} \\begin{bmatrix}w_0\\\\ w_1\\\\ w_2\\\\ \u0026hellip;\\\\ w_d \\end{bmatrix} } $$\nWeighted Least Square We assume $\\epsilon_i$ share the same variance (homoskedasticity), however, it is not always the case (heteroskedasticity). In this case, we add a weight to each residual, as shown below\n$$ L_{min} = \\sum w_i(y_i - ax_i - b)^2 $$\nWe can see that large noise will be punished heavily, i.e. $w_i$ tends to small. The idea is that we\u0026rsquo;d like to pay more attention to data with little noise for estimation rather than to concerntrate all noisy part of the data.\nNon-linear Data Transformation    Model Predicted Value     Logarithmic model $y' = a \\text{log} (x) + b$   Reciprocal model $y' = 1/(ax + b)$   Quadratic model $ y' = (ax + b)^2$   Exponential model $y' = e^{ax+b}$    Fitting Polynomials Model Selection AIC, Akaike\u0026rsquo;s Information Criterion is an estimator for the K-L divergence.\n$$ AIC = 2K - 2 \\text{log}(L) $$\nwhere $K$ is the number of predictors and $L$ is the maximised likelihood value. The goal is to find the minimum AIC, so\n $K$ will punish models with many predictors $L$ rewards good fit models  References [1] C. Bishop, Pattern Recognition and Machine Learning. 2006.\n[2] \u0026ldquo;Interaction Effects in Regression\u0026rdquo;, Stattrek.com, 2021. [Online]. Available: https://stattrek.com/multiple-regression/interaction.aspx?tutorial=reg. [Accessed: 11- May- 2021].\n"
            }
    
        ,
            {
                "id": 60,
                "href": "https://ixiaopan.github.io/blog/post/bias-variance-dilemma/",
                "title": "Bias-Variance dilemma",
                "section": "post",
                "date" : "2021.04.15",
                "body": "When you learn more about machine learning, you must hear people talking about high bias, high variance or something like that. What do they mean by \u0026lsquo;high bias\u0026rsquo; or \u0026lsquo;high variance\u0026rsquo;?\nActually, when I first heard these terms, I was completely confused. Even though I tried to find the answer on Google, I still had no idea until I took the Advanced Machine Learning module in semester 2. Therefore, I\u0026rsquo;m writing this post to try to explain this. I hope this post can help people who are still struggling with them to understand the two most important concepts clearly.\nGeneralization Error The underlying assumption of machine learning is that there are some relationships between data. However, we are not able to know this true function, otherwise there is no need to learn it.\nSuppose we have a true realtionship denoted by $f(x)$ (the red dot in Figure 1), and we want to construct a machine denoted by $f'(x)$ to approximate the true function based on the data $D$ sampled from the population $\\chi$.\nThe training loss is defined by the following equation, where $f'(x|D)$ is the machine we learn from this particular data set $D$\n$$ L_T(D) = \\sum_{x\\in D}(f'(x|D) - f(x))^2 $$\nHowever, our goal is to know how well this machine works on unseen data, which is known as generalization. The generalization loss is expressed as\n$$ L_G(D) = \\sum_{x\\in \\chi} p(x) (f'(x|D) - f(x))^2 $$\nIf we have another data set $D_1$, then we will get another machine $f'(x|D_1)$ and another generalization loss $L_G(D_1)$ shown in Figure 1.\n  Figure 1: Generalisation error  We can see that the generalization loss is depend on our training data. Thus, the generalization loss for a particular data set doesn\u0026rsquo;t make much sense. Instead, the average generalization loss over all the data set with the same size of $n$ is what we expect.\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\nMean Machine We have already known that there is a different machine $f'(x|D)$ for a given data set $D$. Thus, for an unseen data $x$, we will have many predictions of many different machines, which are represented in blut dots shown in Figure 2.\n  Figure 2: Bias and variance  The average prediction for an unseen data is the mean prediction(the yellow dot in Figure 2).\n$$ f'_m(x) = E_D[f'(x|D)] $$\nBias Bias is the distance between the mean prediction(the yellow dot) and the true value(the red dot) shown in Figure 2. High bias implies that our model is too simple and the prediction value is much far away from the true value.\n$$ B = \\sum_{x \\in \\chi} p(x) (f\u0026rsquo;m - f(x))^2 $$\nVariance Variance measures the variation in the prediction of the machine when we change different data set we train on. If we have a complex machine, as mentioned earlier, the machine will try its best to match every data in training data set. In other words, the machine memorized the trainining data and a little change in data set will cause significant variation in prediction.\n$$ V = \\sum_{x \\in \\chi}p(x) E_D[ (f'(x|D) - f\u0026rsquo;m)^2 ] $$\nBias-Variance dilemma Now it\u0026rsquo;s time to decompose the average generalisation error. Let\u0026rsquo;s plug the $f'_m(x)$ into the previous equation\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x) (f'(x|D) - f_m' + f_m' - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x){(f'(x|D) - f_m')^2 + (f_m' - f(x))^2 + 2(f'(x|D) - f_m')(f_m' - f(x)) }] $$\nIt\u0026rsquo;s noticeable that the cross-term will cancel out because $f\u0026rsquo;m$ and $f(x)$ are constants no matter what data set $D$ is.\n$$ E_D[\\sum_{x\\in \\chi}p(x)2(f'(x|D) - f_m')(f_m' - f(x))] $$\n$$ = \\sum_{x\\in \\chi}p(x) (2E_D[f'(x|D)]-f\u0026rsquo;m)(f\u0026rsquo;m-f(x)) = 0 $$\nTherefore, we are left with\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi}p(x)(f'(x|D) - f_m')^2 + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2] $$\n$$ = \\sum_{x\\in \\chi}p(x) E_D[(f'(x|D) - f_m')^2] + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2 $$\n$$ = V + B $$\nIn summary,\n If our machine is too simple, then we might not be able to fit the training data. Since the machine knows little about the data, it\u0026rsquo;s unlikely to work well on unseen data. This means our model has a high bias. If our machine is too complex, then we might be able to fit the training data perfectly. It means that the machine knows too much about the data, even the noise that it should not learn. Thus, it\u0026rsquo;s too sensitive to training data so that a little change in data will cause a great variance. This means our model has a high variance.  Throughout the world of machine learning, we are always trying to find a balance between bias and variance.\n"
            }
    
        ,
            {
                "id": 61,
                "href": "https://ixiaopan.github.io/blog/post/linear-regression-01/",
                "title": "Linear Regression 01",
                "section": "post",
                "date" : "2021.04.14",
                "body": "There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand. Perhaps it is the first algorithm that most people learn in machine learning. So let\u0026rsquo;s go!\nProblem Statement Suppose you are a teacher, and you recorded some data about the hours students spent on study and the grades they achieved. Below are some sample data you collected. Then you want to predict the score for someone according to the hours he spent.\n   Hours 0.5 1 2 3 4     Grade 20 21 22 24 25    How to make a prediction? Since there are only two variables, let\u0026rsquo;s plot them to see if there is a relation betwee grade and hours.\n  Figure 1: A scatter plot of hours and grade  Figure 1 shows that grade is positively related to hours. For simplicity, we can use a straight line(the red line) to approximate this relation. And this is exactly our first linear model.\nSimple Linear Regression Remember the equation of a straight line is typically written as,\n$$ y = ax + b $$\nIn this example, $x$ is the variable hours and $y$ is the variable grade, which we\u0026rsquo;ve already known. So the problem is to find the parameter $a, b$ such that the red line is as close as possible to the blue data points. Technically, this is called parameter estimation. Usually, there are two ways to do this: minimising the loss and maximising the likelihood. Now we focus on the former.\nResidual So how to measure the closeness mentioned above? The most common way is to measure the residual, which is the difference between the estimated value and our true value shown in Figure 2. For a single data point, the residual is defined as below, where $y_i$ is the true value and $y_i'$ is our predicted value.\n$$ e = y_i - y'_i = y_i - ax_i - b $$\n  Figure 2: Residual/Error is the difference between the observed value and the predicted value. (Bradthiessen.com 2021)  error Since we have many data points, we need to sum them up to evaluate the overall errors. Unfortunately, some error terms will cancel out when you do the this calculation directly.\n$$ L = \\sum_i^n (y_i - y'_i) = \\sum_i^n (y_i - ax_i - b) $$\nthe absolute value of error One way to tackle this is to take the absolute value of the error terms. However, the absoulte value of $x$ is not differentiable at $0$.\n$$ L = \\sum_i^n |y_i - y_i'| $$\nthe squared value of error Instead of taking the absolute value, we square all the errors and sum them up, which is known as the Residual Sum of Squares(RSS) or Sum of Squared Error (SSE).\n$$ L = \\sum_i^n (y_i - y_i')^2 $$\nClosed-form solution Finally, we find a function to measure the loss. Next, we need to find the parameters that minimize the squared error. The good news is that our loss function is differentiable and convex! It means that we have the global minimum value and can be calculated directly by taking derivatives.\nLet\u0026rsquo;s take the first derivative w.r.t $b$\n$$ \\frac{\\partial L}{\\partial b} = \\sum_i^n -2(y_i - ax_i-b) $$\nand then set this equation to $0$,\n$$ -2(\\sum_i^ny_i -a\\sum_i^nx_i - \\sum_i^nb) = -2(n\\overline y-an\\overline x - nb) = 0 $$\n$$ \\hat b = \\overline y - \\hat a \\overline x $$\nSimilarly, let\u0026rsquo;s take the first derivative w.r.t $a$\n$$ \\frac{\\partial L}{\\partial a} = \\sum_i^n -2x_i(y_i - ax_i-b) $$\nplug $b$ into this equaiton and set this equation to 0 again,\n$$ \\sum_i^n -2x_i(y_i - ax_i-\\overline y+a\\overline x) = \\sum_i^n -2x_i[(y_i-\\overline y)- a(x_i -\\overline x)] $$\n$$ \\hat a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} $$\nHere, we use a slight algebra trick,\n$$ c\\sum_i^n(x_i - \\overline x_i) = 0 $$\nplug the above equation into the previous equation\n$$ \\hat a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} = \\frac{\\sum_i^nx_i(y_i-\\overline y) - \\sum_i^n\\overline x(y_i - \\overline y)}{\\sum_i^nx_i(x_i -\\overline x) - \\sum_i^n\\overline x(x_i - \\overline x)} $$\n$$ = \\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2} $$\n$$ = \\frac{Cov(x, y)}{Var(x)} $$\nFinally, we find the best estimators for our simple linear regression.\nTest statistics The above formulas give us the best estimation for the parameters $a, b$ of the linear regression model. If we generate different data sets from the population, we will have different linear models and different values of dependent variable. If we average these values, we\u0026rsquo;ll find that the average value is pretty close to the true value. Mathematically, this can be expressed as follows,\n$$ E_D(\\hat ax_i + \\hat b|D) \\simeq ax_i+b $$\nThe idea behind the above equation is analogous to the Central Limit Theorem for Sample Mean. The population mean of the random variable $Y_i$ (the true line) can be estimated by the expected value of the sample mean(the estimated line). CLT tells us the distribution of the sample mean follows the Gaussian Distribution with the mean of the population mean as the sample size gets larger.\nStandard error But in practice, we can only have one data set, so how accurate is the parameters $a, b$ calculated from the above equations? In other words, a single sample mean may overestimate or underestimate the population mean, but to what extent is this deviation? We use the standard error of sample mean to measure it, which can be obtained by the following equation, where $\\sigma$ is the population standard deviation and $n$ is the sample size.\n$$ SE^2(\\hat {\\overline \\mu}) = Var(\\hat {\\overline \\mu}) = \\frac{\\sigma^2}{n} $$\nSimilarly, we can calculate the standard error associated with the parameters $a$ and $b$.\n$$ SE^2(\\hat a) = Var(\\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2}) = Var(\\frac{\\sum_i^n(x_i-\\overline x)y_i - \\sum_i^n(x_i-\\overline x)\\overline y}{\\sum_i^n(x_i -\\overline x)^2}) $$\n$$ = Var(\\frac{\\sum_i^n(x_i-\\overline x)(ax_i + b + \\epsilon_i)}{\\sum_i^n(x_i -\\overline x)^2}) $$\n$$ = \\frac{\\sum_i^n(x_i-\\overline x)^2}{(\\sum_i^n(x_i -\\overline x)^2)^2} Var(ax_i + b + \\epsilon_i) = \\frac{\\sum_i^n(x_i-\\overline x)^2}{(\\sum_i^n(x_i -\\overline x)^2)^2} Var(\\epsilon_i) $$\n$$ = \\frac{Var(\\epsilon_i)}{\\sum_i^n(x_i -\\overline x)^2} $$\nSince $x_i, y_i$ are known and $a, b$ are the true parameters, the only thing unknown is $\\epsilon_i$. In other words, they are all indepentdent of $\\epsilon_i$. In addition, we use a bit tricks to derive the above formula,\n$$ Var(c) = 0 $$\n$$ Var(cX) = c^2Var(X) $$\n$$ Var(c + X) = Var(X) $$\nIn summary, what we should know is that the standard error of $\\hat a$ tells us how far away this estimate $\\hat a$ is from the true value $a$ or how far away the predicated value $\\hat y$ is from the observed value $y$.\nFurthermore, since the predicted value obtained from a given sample is different from the true value, we cannot say we are sure that the estimated value is exactly the true value. But we could say we are 90% confident that the true value lies somewhere around the predicted value. And this \u0026lsquo;somewhere\u0026rsquo; can be computed by confident intervals.\np-value To investigate whether the estimated parameters are statistically significant, we perform hypothesis tests. A statistical significant result means it\u0026rsquo;s unlikely to happen by chance. In the simple linear regression, the null and alternative hypotheses are defined as\n$H_0$: There is no relationship between $X$ and $Y$\n$H_a$: There is some relationship between $X$ and $Y$\nMathematically, it is equvalent to testing\n$H_0$: $a=0$\n$H_a$: $a\\ne0$\nSo in order to test the null hypothesis, we need to demonstrate that $\\hat a$ is sufficiently far away from $0$. Thus, we can be confident that $a$ is not equal to $0$ and reject the null hypothesis. To quantify the distance between $\\hat a$ and $0$, we calculate t-score\n$$ t = \\frac{\\hat a - 0}{SE(\\hat a)} $$\nThe higher the $t$ is, the farther the distance is. But wait, what\u0026rsquo;s the probability of getting this estimate $\\hat a$ or more extreme? In other words, what\u0026rsquo;s the p-value? How to interpret this probability? A higher p-value doesn\u0026rsquo;t provide much information. In contrast, a smaller p-value means it\u0026rsquo;s unlikely to observe this t-score due to chance under the assumption that $H_0$ is true. You can interpret that a small p-value indicates a strong evidence against the null hypothesis. But how small is enough? Typically, we set the threshold value of $0.05$. If p-value is smaller than $0.05$, we reject $H_0$, otherwise we accept it.\nModel Evaluation Next question is how to evaluate our model? How good is it? There are two common metrics to measure the quality of a linear regression model: RSE and $R^2$.\nRSE Residual standard error measures the average deviation between the estimated value and the true value. It can be calculated using the following fomula, where n-2 is the degree of freedom(df) of the residual.\nQ: Why do we divide by $n-2$ not $n$?\nA: This is because the latter tends to underestimate variance. Rememer we divide by $n-1$ when calculating the sample variance. This is the same reason here.\nQ: But why n-2?\nA: We know that 2 points determine a line, which means there is no other line fitting the data and the residual of each data point is fixed. If we add a third point, there could be many lines fitting these points and thus different residuals. That means the third point increases the flexiblity of the value of residuals. We say we have one free observation. Therefore, the degree of freedom of the residual is $n-2$ in simple linear regression.\n$$ RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - y_i')^2} $$\nA smaller RSE indicates that our model fit the data well. However, RSE is measured in the unit of $Y$. For some data sets, RSE would be small, e.g $3.2$. But for other data sets, it would be very large, e.g $1200$. So it\u0026rsquo;s a bit confusing. That\u0026rsquo;s why $R^2$ comes. $R^2$ measures the fraction of variance explained, which is independent of the unit of $Y$.\n$R^2$ Coefficient of determination or $R^2$ is another metric to measure the goodness of a linear regression model. Let\u0026rsquo;s rewrite the previous equation by multiplying both the denominator and numerator by $\\sqrt {\\sum_i^n(y_i-\\overline y)^2}$\n$$ a = \\frac{\\sum_i^n (x_i - \\overline x)(y_i - \\overline y) \\sqrt {\\sum_i^n(y_i-\\overline y)^2}}{\\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(y_i-\\overline y)^2}} $$\n$$ a = R\\frac{s_y}{s_x} $$\nwhere\n$$ R = \\frac{Cov(x, y)}{\\sqrt{var(x)} \\sqrt{var(y)}} $$\n$$ s_y = \\sqrt{Var(y)} $$\n$$ s_x = \\sqrt{Var(x)} $$\nRemember that the error is defined as $e_i = y_i' - y_i$, so the mean of $e$ is\n$$ E(e) = \\frac{1}{N} \\sum_i^n e_i = \\frac{1}{N} \\sum_i^n b + ax_i - y_i =b + a\\overline x - \\overline y = 0 $$\nand the variance is\n$$ var(e) = \\sum_i^n (e_i - \\overline e)^2 = \\sum_i^n (y_i - b - ax_i)^2 = \\sum_i^n (y_i - \\overline y + a\\overline x - ax_i)^2 $$\nLet\u0026rsquo;s plug $a$ into this equation\n$$ var(e) = \\sum_i^n [(y_i - \\overline y) - R\\frac{s_y}{s_x}( x_i - \\overline x)]^2 = var(y) (1-R^2) $$\nOr you might be more familiar with this equation\n$$ R^2 = 1 - \\frac{var(e)}{var(y)} = 1 - \\frac{RSS}{TSS} $$\nTherefore, $R^2$ tells us how much the variance of $y$ has been explained by our models. The higher the $R^2$ is, the better our model is.\nReferences [1] Bradthiessen.com, 2021. [Online]. Available: https://www.bradthiessen.com/html5/docs/ols.pdf. [Accessed: 14- Apr- 2021].\n[2] “Linear Regression - ML Wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Linear_Regression. [Accessed: 14-Apr-2021].\n[3] K. Base and S. statistics, \u0026ldquo;Standard Error | What It Is, Why It Matters, and How to Calculate\u0026rdquo;, Scribbr, 2021. [Online]. Available: https://www.scribbr.com/statistics/standard-error/. [Accessed: 12- May- 2021].\n"
            }
    
        ,
            {
                "id": 62,
                "href": "https://ixiaopan.github.io/blog/post/descriptive-statistics/",
                "title": "Descriptive Statistics",
                "section": "post",
                "date" : "2021.04.13",
                "body": "Statistics is one of the most important skills for data scientists. There are two main branches:\n  descriptive statistics tells us the statistics about data like mean, mode and standard deviation, which you\u0026rsquo;ve learned in high school.\n  inferential statistics, on the other hand, uses a random dataset sampled from the population to make inferences about the population.\n  In this post, we focus on descriptive statistics and other basic concepts in statistics.\nCentral tendency Central tendency measures the centre of the data. Mean, mode, and median are three mainly used metrics.\nMean Mean is the average of data and calculated by summing up all data values and then dividing them by the number of data. For instance, say we have data 1,2,3,4,5,6, the mean is 3.5.\n$$ \\overline x = \\frac{\\sum_i^nx_i}{n} $$\n# vanilla python def mean(x): return sum(x)/len(x) Median Though mean is widely used, it\u0026rsquo;s sensitive to outliers. Suppose you have a set of data 1,2,3,4,5,6,100, after some calculations, you find that the mean is 17.28. However, it seems not convincing as most of the data values are less than 10 except for one extreme value 100.\nTo get the median, firstly we need to sort the data in ascending order and then find the middle number that separates the data into two groups of the same size. In this example, the median is 4.\ndef median(x): mid_p = len(x) // 2 if len(x) % 2 == 0: # even return (x[mid_p] + x[mid_p-1])/2 else: # odd return x[mid_p] Mode Mode is the most frequent value. There could be one, two or more data values with the same frequency, and that frequency is the largest. For instance,\nfrom collections import Counter a = [1, 2, 4, 5, 6, 6, 7, 6, 4, 4, 3, 3, 3, 2, 2,2, 10, 2, 3, 3] Counter(a).most_common() # [(2, 5), (3, 5), (4, 3), (6, 3), (1, 1), (5, 1), (7, 1), (10, 1)] Dispersion Dispersion measures how spread out a given dataset is.\nRange Range is the distance between the maximum value and the minimum value. Again, the range is sensitive to outliers.\n$$ r=max - min $$\nQuantile Quantiles are used to divide data into several equal-sized groups. The most widely used cut points are 0, 25, 50, 75, 100, denoted by min, Q1, Q2, Q3, max, respectively. IQR (interquartile range) measures where the central 50% of the data is.\n$$ \\text{IQR} = Q3 - Q1 $$\nIQR can be used to detect outliers. Data that is greater than the upper boundary or less than the lower boundary can be considered as outliers.\n$$ \\text{upper boundary }= Q3 + 1.5*IQR $$\n$$ \\text{lower boundary} = Q1 - 1.5*IQR $$\nVariance The deviation is the distance between a given data point and the mean. Since there are many data points, the variance calculates the average deviation from the mean. But when you do this, you will always get a zero due to the definition of the mean.\n$$ \\frac{1}{n}\\sum_i^n d_i = \\frac{1}{n}\\sum_i^n (x_i - \\overline x) = \\frac{1}{n}\\sum_i^nx_i - n\\overline x = \\frac{1}{n} (n\\overline x - n\\overline x) = 0 $$\nLet\u0026rsquo;s try to ignore the sign and use the absolute value of the deviation.\n$$ \\frac{1}{n-1} \\sum_i^n d_i = \\frac{1}{n-1} \\sum_i^n |x - \\overline x| $$\nThough it works, the most widely used method is to calculate the square of the deviation.\n$$ s^2 = \\frac{1}{n-1} \\sum_i^n (x_i - \\overline x)^2 $$\nHowever, the squared value is a bit hard to interpret. For instance, below are 15 records of fish size measured in kilogram, and the variance is 30.97. It means that if we randomly catch a fish, its weight would be 30.97 squared kilograms far away from the average weight. Emm\u0026hellip;, a squared kilogram is an odd unit.\n2.1, 2.4, 2.4, 2.4, 2.4, 2.6, 2.9, 3.2, 3.2, 3.9, 4.5, 6.3, 8.2, 12.8, 23.5 Standard Deviation It\u0026rsquo;s simple to solve this problem by taking the square root of the variance, which is the standard deviation. The standard deviation in the previous example is 5.56kg, which makes more sense.\n$$ s = \\sqrt{\\frac{\\sum (x - \\overline x)^2}{n-1}} $$\nDistribution Skewness Skewness measures the symmetry of a distribution. It\u0026rsquo;s quite common to have non-symmetric distributions shown in Figure 1.\n  Figure 1: Non-symmetric distributions  A left-/negative-skewed distribution\n a long left tail mean \u0026lt; median  A right-/positive-skewed distribution\n a long right tail mean \u0026gt; median  A skewed distribution implies that some special values are much larger/smaller than the common values. Let\u0026rsquo;s see an example.\n Figure 3.2 shows the histogram of the fish sizes gathered from a fisherman. We can see that this distribution is right-skewed. The majority of fish sizes are between 0 and 3kg, but a few fishes weigh over 3.5kg. It also can be seen that the average weight(1.67kg) is a bit higher than the median(1.62kg) shown in Table 3.2.\nCorrelation So far we have introduced different methods of characterizing the distribution of a single variable. But what about two or more variables? You might want to find the relationships between them. An intuitive way is to visualise data using a scatterplot. For example, you might find that car prices tend to increase as car ages decrease.\nStatistically, we can measure both the direction and the strength of this tendency using correlation coefficients. And Pearson correlation coefficients is widely used to measure the strength of a linear relationship. It can be calculated as follows,\n$$ r = \\frac{Cov(X, Y)}{\\sqrt{V(X)}\\sqrt{V(Y)} } $$\nwhere $Cov(X, Y)$ is the covariance between $X$ and $Y$, and $V(X), V(Y)$ are the standard deviation of $X, Y$ respectively. The value of $r$ ranges between -1 and 1.\n a negative value represents a negative relationship a positive value represents a positive relationship $0$ means there is no relationship between $X$ and $Y$  But to what extent is the value of $r$ large means a strong relationship? Below are some values that could give you an insight into how strong your correlation $r$ is.\n 0 - 0.3: weak relationship 0.3 - 0.6: moderate relationship 0.6 - 0.9: strong relationship 0.9 - 1.0: very relationship  Data Types Now let\u0026rsquo;s look at some characteristics of data. Roughly, we classify data into two categories:\n numerical/quantitative data categorical/qualitative data  Categorical data refers to data that cannot be measured, for example, the color of car or the quality of service. There are two subcategories, namely nominal data and ordinal data. The difference between them is that ordinal data can have order, such as 0=bad, 1=good, 2=better, 3=excellent.\nNumerical data refers to data that can be measured. There are also two subcategories, namely discrete and continuous. Discrete data describes values that cannot be divided into smaller individual parts, e.g. the number of people. You cannot say, \u0026lsquo;There are 1.5 people\u0026rsquo;. However, we can say, \u0026lsquo;He is 1.87 meters tall\u0026rsquo;. So height is a continuous variable.\nMeasurement Scales At a lower level, we can also classify data from the perspective of measurement scales, which capture the characteristics of data used to determine the types of variables. There are four main levels of measurement scales:\n nominal ordinal interval ratio  Each of them satisfies one or more of the following properties of measurement.\n Identity  each value have a unique meaning $=, \\cancel{=}$ nominal scale, e.g. car color   Maginitue  values have an ordered relationship to one another $\\lt, \\le, \\gt, \\ge$ ordinal scale, e.g. service quality   Equal interval  the different between data points A and B will be equal to the different between data points C and D $+, -$ interval scale, e.g. time   A minimum value of zero  the scale has a true zero point, below which no value exists ratio scale, e.g. height    References [1] B. al., \u0026ldquo;Introduction to Statistics | Simple Book Production\u0026rdquo;, Courses.lumenlearning.com, 2021. [Online]. Available: https://courses.lumenlearning.com/introstats1. [Accessed: 14- Apr- 2021].\n[2]\u0026ldquo;Measurement Scales\u0026rdquo;, Stattrek.com, 2021. [Online]. Available: https://stattrek.com/statistics/measurement-scales.aspx?tutorial=reg. [Accessed: 11- May- 2021].\n"
            }
    
]
