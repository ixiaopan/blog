<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<link>https://ixiaopan.github.io/blog/</link>
		<description>Recent content </description>
		<generator>Hugo -- gohugo.io</generator>
		
  		<language>en</language>
		
		<managingEditor>Page(&#34;&#34;) (ixiaopan)</managingEditor>
    	
  		<lastBuildDate>Mon, 07 Jun 2021 00:00:00 +0000</lastBuildDate>
		
		<atom:link href="/blog/" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Information Theory</title>
			<link>https://ixiaopan.github.io/blog/post/data-mining/</link>
			<pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/data-mining/</guid>
			<description>&lt;p&gt;I am not sure what I&amp;rsquo;ve learned from the module &amp;lsquo;Data Mining&amp;rsquo; is truly useful to me. It seems that the lecturers tried to cover everything, but in fact, they only scratched the surface due to time limits. Besides, the content they taught were less integrated. Half of the lectures revolved around the graph theory, the left were a mix of various techniques of machine learning. I am not going to do works related to graph, so I won&amp;rsquo;t bother to learn such stuff. What I found really practical are information theory, association analysis, regression and recommender systems. And the first two are the main topics of this article.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Constrained Optimisation</title>
			<link>https://ixiaopan.github.io/blog/post/constrained-optimisation/</link>
			<pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/constrained-optimisation/</guid>
			<description>&lt;p&gt;When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&amp;rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Probabilistic Model</title>
			<link>https://ixiaopan.github.io/blog/post/probabilistic-model/</link>
			<pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/probabilistic-model/</guid>
			<description>In the previous post Descriptive Statistics, we focused on summary statistics on a particular data set. However, in machine learning, we usually attempt to make inference, i.e. infer the unknown parameters from the given data. For unknown things, we use probability to describe its uncertainty. For inference, we use Bayes&#39; rule to invert it into a forward process.
Probability At the beginning, let&amp;rsquo;s have a quick refresh on probability. Conventionly, we use a capital letter, such as $X$ or $Y$, to represent a random variable.</description>
		</item>
      	
		<item>
			<title>Semantic Web</title>
			<link>https://ixiaopan.github.io/blog/post/semanticweb/</link>
			<pubDate>Sat, 08 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/semanticweb/</guid>
			<description>&lt;p&gt;In semester 2, I took a module called &lt;strong&gt;Semantic Web Technologies&lt;/strong&gt;. There are some reasons why I choose this module. At first glance, the name itself sounds not appealing. Actually, it does. But it&amp;rsquo;s still on my shortlist because I&amp;rsquo;ve been working on the web for many years and I wondered what the semantic web was.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Singular Value Decomposition</title>
			<link>https://ixiaopan.github.io/blog/post/svd/</link>
			<pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/svd/</guid>
			<description>&lt;p&gt;Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into the multiplication of three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>An E2E Project - EDA</title>
			<link>https://ixiaopan.github.io/blog/post/end2end-project-01/</link>
			<pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/end2end-project-01/</guid>
			<description>&lt;p&gt;So far, we have discussed many algorithms, such as linear regression and ensemble methods. It&amp;rsquo;s time to kick off a project from scratch to learn how a real machine learning project works.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Ensemble Methods</title>
			<link>https://ixiaopan.github.io/blog/post/ensemble-methods/</link>
			<pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/ensemble-methods/</guid>
			<description>&lt;p&gt;Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Decision Tree</title>
			<link>https://ixiaopan.github.io/blog/post/decision-tree/</link>
			<pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/decision-tree/</guid>
			<description>&lt;p&gt;The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as &amp;lsquo;Is it a fiction movie? Is it directed by David Fincher?&amp;rsquo;&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Linear Regression 02</title>
			<link>https://ixiaopan.github.io/blog/post/linear-regression-02/</link>
			<pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/linear-regression-02/</guid>
			<description>&lt;p&gt;In &lt;a href=&#34;https://ixiaopan.github.io/blog/post/linear-regression-01/&#34;&gt;the previous post&lt;/a&gt;, we talked about simple linear regression. However, we only considered one predictor. It&amp;rsquo;s quite common to have multiple predictors for real-world problems. For example, if we want to predict car prices, we should consider many factors like car sizes, manufacturers and fuel types. The simple linear regression is not suitable for this case. Therefore, we need to extend it to accommodate the multiple predictors.&lt;/p&gt;</description>
		</item>
      	
		<item>
			<title>Bias-Variance dilemma</title>
			<link>https://ixiaopan.github.io/blog/post/bias-variance-dilemma/</link>
			<pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
			<guid>https://ixiaopan.github.io/blog/post/bias-variance-dilemma/</guid>
			<description>&lt;p&gt;When you learn more about machine learning, you must  hear people talking about high bias, high variance or something like that. What do they mean by &amp;lsquo;high bias&amp;rsquo; or &amp;lsquo;high variance&amp;rsquo;?&lt;/p&gt;</description>
		</item>
      	
	</channel>
</rss>
