<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Linear Regression 01</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Linear Regression 01"/>
<meta name="twitter:description" content="There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and possibly the first algorithm that most of people learn in the world of machine learning."/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script>

    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Linear Regression 01</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 14, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;5 min  read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand. Perhaps it is the first algorithm that most people first learn in the world of machine learning. So let&rsquo;s go!</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Suppose you are a teacher, and you recorded some data about the hours students spent on study and the grades they achieved. Below are some sample data you collected. Then you want to predict the score for someone according to the hours he spent.</p>
<table>
<thead>
<tr>
<th>Hours</th>
<th>0.5</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grade</td>
<td>20</td>
<td>21</td>
<td>22</td>
<td>24</td>
<td>25</td>
</tr>
</tbody>
</table>
<p>How to make a prediction? Since there are only two variables, let&rsquo;s plot them to see if there is a  relation between <code>grade</code> and <code>hours</code>.</p>
<p>
  <figure>
    <img src="/blog/post/images/simple-linear-grade.png" alt="scatter-plot">
    <figcaption>Figure 1: A scatter plot of hours and grade</figcaption>
  </figure>

</p>
<p>Figure 1 shows that  <code>grade</code> is positively related to <code>hours</code>. For simplicity, we can use a straight line(the red line)  to approximate this relation. And this is exactly our first linear model.</p>
<h2 id="simple-linear-regression">Simple Linear Regression</h2>
<p>Remember the equation of a straight line is typically written as,</p>
<p>$$
y = ax + b
$$</p>
<p>In this example, $x$ is the variable <code>hours </code> and $y$ is the variable <code>grade</code>, which we&rsquo;ve already known. So the problem is how to calculate the parameter $a, b$. Technically, this is called <strong>parameter estimation</strong>. Usually, there are two ways to do this: minimising the loss and maximising the likelihood. Now we focus on the former.</p>
<h2 id="residual">Residual</h2>
<p>The residual is the difference between the estimated value and our true value. Minimising the residual is to make the estimated value as close to the true value as possible.</p>
<p>
  <figure>
    <img src="/blog/post/images/linear-regression-residual.png" alt="simple-linear-grade">
    <figcaption>Figure 2: Residual/Error is the difference between the observed value and the predicted value. (Bradthiessen.com 2021)</figcaption>
  </figure>

</p>
<p>For a single data point, the residual is defined as below,</p>
<p>$$
error = y_i - y'_i = y_i - ax_i - b
$$</p>
<p>where $y_i$ is the true value and $y_i'$ is our predicted value.  Since we have many data points, we need to sum them up to evaluate the overall errors.</p>
<h3 id="error">error</h3>
<p>Unfortunately, some error terms will cancel out when you do this calculation directly.</p>
<p>$$
L = \sum_i^n (y_i - y'_i) = \sum_i^n (y_i - ax_i - b)
$$</p>
<h3 id="the-absolute-value-of-error">the absolute value of error</h3>
<p>One way to tackle this is to take the absolute value of the error terms. However, the absoulte value of $x$ is not differentiable at $0$.</p>
<p>$$
L =  \sum_i^n |y_i - y_i'|
$$</p>
<h3 id="the-squared-value-of-error">the squared value of error</h3>
<p>Instead of taking the absolute value, we will square all the errors, which is known as the <strong>Residual Sum of Squares(RSS)</strong>  or <strong>Sum of Squared Error (SSE)</strong>.</p>
<p>$$
L =  \sum_i^n (y_i - y_i')^2
$$</p>
<p>PS: We will revisit SSE later from the perspective of MLE.</p>
<h2 id="closed-form-solution">Closed-form solution</h2>
<p>Finally, we find a function to measure the loss. Next, we need to find the parameters that minimize the squared error. The good news is that our loss function is differentiable and convex! It means that we have the global minimum value and can be calculated directly by taking derivatives.</p>
<p>Let&rsquo;s take the first derivatve w.r.t $b$</p>
<p>$$
\frac{\partial L}{\partial b} = \sum_i^n -2(y_i - ax_i-b)
$$</p>
<p>and then set this equation to $0$,</p>
<p>$$
-2(\sum_i^ny_i -a\sum_i^nx_i - \sum_i^nb) = -2(n\overline y-an\overline x - nb) = 0
$$</p>
<p>$$
b = \overline y - a\overline x
$$</p>
<p>Let&rsquo;s take the first derivatve w.r.t $a$</p>
<p>$$
\frac{\partial L}{\partial a} = \sum_i^n -2x_i(y_i - ax_i-b)
$$</p>
<p>plug $b$ into this equaiton and set this equation to 0 again,</p>
<p>$$
\sum_i^n -2x_i(y_i - ax_i-\overline y+a\overline x) = \sum_i^n -2x_i[(y_i-\overline y)- a(x_i -\overline x)]
$$</p>
<p>$$
a = \frac{\sum_i^nx_i(y_i-\overline y)}{\sum_i^nx_i(x_i -\overline x)}
$$</p>
<p>Here, we use a slight algebra trick,</p>
<p>$$
a\sum_i^n(x_i - \overline x_i) = 0
$$</p>
<p>plug the above equation into the previous equation</p>
<p>$$
a = \frac{\sum_i^nx_i(y_i-\overline y)}{\sum_i^nx_i(x_i -\overline x)} = \frac{\sum_i^nx_i(y_i-\overline y) - \sum_i^n\overline x(y_i - \overline y)}{\sum_i^nx_i(x_i -\overline x) - \sum_i^n\overline x(x_i - \overline x)}
$$</p>
<p>$$
= \frac{\sum_i^n(x_i-\overline x)(y_i-\overline y)}{\sum_i^n(x_i -\overline x)^2}
$$</p>
<p>$$
= \frac{Cov(x, y)}{Var(x)}
$$</p>
<p>Finally, we find the best estimators for simple linear regression.</p>
<h2 id="r2">$R^2$</h2>
<p>So how to evaluate our model? In other words, how good is it? We can use $R^2$ to measure our model. Let&rsquo;s rewrite the previous equation by multiplying both the denominator and numerator by $\sqrt {\sum_i^n(y_i-\overline y)^2}$</p>
<p>$$
a = \frac{\sum_i^n (x_i - \overline x)(y_i - \overline y) \sqrt {\sum_i^n(y_i-\overline y)^2}}{\sqrt {\sum_i^n(x_i-\overline x)^2} \sqrt {\sum_i^n(x_i-\overline x)^2} \sqrt {\sum_i^n(y_i-\overline y)^2}}
$$</p>
<p>$$
a = R\frac{s_y}{s_x}
$$</p>
<p>where</p>
<p>$$
R = \frac{Cov(x, y)}{\sqrt{var(x)} \sqrt{var(y)}}
$$</p>
<p>$$
s_y =  \sqrt{Var(y)}
$$</p>
<p>$$
s_x = \sqrt{Var(x)}
$$</p>
<p>Remember that the error is defined as $e_i = y_i' - y_i$, so the mean of $e$ is</p>
<p>$$
E(e) = \frac{1}{N} \sum_i^n e_i =  \frac{1}{N} \sum_i^n b + ax_i - y_i =b + a\overline x - \overline y = 0
$$</p>
<p>and the variance is</p>
<p>$$
var(e) = \sum_i^n (e_i - \overline e)^2 = \sum_i^n  (y_i - b - ax_i)^2 = \sum_i^n (y_i - \overline y + a\overline x - ax_i)^2
$$</p>
<p>Let&rsquo;s plug $a$ into this equation</p>
<p>$$
var(e) = \sum_i^n [(y_i - \overline y) - R\frac{s_y}{s_x}( x_i - \overline x)]^2 = var(y) (1-R^2)
$$</p>
<p>Or you might be more familiar with this equation</p>
<p>$$
R^2 = 1 - \frac{var(e)}{var(y)} = 1 - \frac{RSS}{TSS}
$$</p>
<p>Therefore, $R^2$ tells us how much the variance of $y$ has been explained by our models. The higher the $R^2$ is, the better our model is.</p>
<h2 id="references">References</h2>
<p>[1] <em>Bradthiessen.com</em>, 2021. [Online]. Available: <a href="https://www.bradthiessen.com/html5/docs/ols.pdf">https://www.bradthiessen.com/html5/docs/ols.pdf</a>. [Accessed: 14- Apr- 2021].</p>
<p>[2] “Linear Regression - ML Wiki,” Mlwiki.org. [Online]. Available: <a href="http://mlwiki.org/index.php/Linear_Regression">http://mlwiki.org/index.php/Linear_Regression</a>. [Accessed: 14-Apr-2021].</p>



        
      </article>

      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#problem-statement">Problem Statement</a></li>
    <li><a href="#simple-linear-regression">Simple Linear Regression</a></li>
    <li><a href="#residual">Residual</a>
      <ul>
        <li><a href="#error">error</a></li>
        <li><a href="#the-absolute-value-of-error">the absolute value of error</a></li>
        <li><a href="#the-squared-value-of-error">the squared value of error</a></li>
      </ul>
    </li>
    <li><a href="#closed-form-solution">Closed-form solution</a></li>
    <li><a href="#r2">$R^2$</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      </aside>
    </div>

    

    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
