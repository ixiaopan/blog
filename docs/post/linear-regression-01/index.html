<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Linear Regression 01</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Linear Regression 01"/>
<meta name="twitter:description" content="There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and possibly the first algorithm that most of people learn in the world of machine learning."/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script>

    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Linear Regression 01</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 14, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;5 min  read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and possibly the first algorithm that most people learn in the world of machine learning. So let&rsquo;s go!</p>
<h2 id="problem-statement">Problem statement</h2>
<p>Suppose you are a teacher, and you record some data about the hours students spent on study and the grades they achieved. Then you want to predict the grade for given hours that someone spent. Here are some sample data you collected:</p>
<table>
<thead>
<tr>
<th>Hours</th>
<th>0.5</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grade</td>
<td>20</td>
<td>21</td>
<td>22</td>
<td>24</td>
<td>25</td>
</tr>
</tbody>
</table>
<p>Since there are only two variables, let&rsquo;s plot them.</p>
<p>
  <figure>
    <img src="/blog/post/images/simple-linear-grade.png" alt="scatter-plot">
    <figcaption>Figure 1: The scatter plot of hours and grade</figcaption>
  </figure>

</p>
<p>Well, we can see that the variable <code>grade</code> is positive related to the variable <code>hours</code>. For simplicity, we can use a simple line(the red line in this figure)  to approximate this relationship. And this is exactly our first simple linear model.</p>
<h2 id="simple-linear-regression">Simple Linear Regression</h2>
<p>Remember a line equation is written in this way:</p>
<p>$$
y = ax + b
$$</p>
<p>In this example, $x$ is the variable <code>hours </code>and $y$ is the variable <code>grade</code> , which we already know. So the problem is how to calculate the parameter $a, b$. Technically, this is called <strong>parameter estimation</strong>. Usually, there are two ways to do this: minimising the loss and maximising the likelihood. Now we focus on the former.</p>
<h2 id="loss">Loss</h2>
<p>What is the loss? Basically, it&rsquo;s the error between the esitmated value and our true value. Minimising the error is simply to make the estimated value as close to the true value as possible.</p>
<p>
  <figure>
    <img src="/blog/post/images/linear-regression-residual.png" alt="simple-linear-grade">
    <figcaption>Figure 2: The error of a single data (Bradthiessen.com 2021)</figcaption>
  </figure>

</p>
<p>For a single data point, the loss function is defined below, where $y$ is the true value and $y'$ is our estimated value for a given $a, b$.</p>
<p>$$
error = y_i - y'_i = y_i - ax_i - b
$$</p>
<p>Since we have many data points, we need to sum up them all to evaluate the overall errors.</p>
<h3 id="error">error</h3>
<p>Unfortunately, some error terms will cancel out when you do this calculation directly.</p>
<p>$$
L = \sum_i^n (y_i - y'_i) = \sum_i^n (y_i - ax_i - b)
$$</p>
<h3 id="the-absolute-value-of-error">the absolute value of error</h3>
<p>One way to tackle this is taking the absolute value of the error terms.</p>
<p>$$
L =  \sum_i^n |y_i - y_i'|
$$</p>
<p>However, the absoulte value of $x$ is not differentiable at $0$.</p>
<h3 id="the-squared-value-of-error">the squared value of error</h3>
<p>Instead of taking absolute value, we will square all the errors. One reason is that the errors will become larger and can be distinguished easily when squaring them. It looks like the errors are zoomed in and we can find them and minimize them quickly. It is also known as <strong>Residual Sum of Squares(RSS)</strong>  or <strong>Sum of Squared Error (SSE)</strong>.</p>
<p>$$
L =  \sum_i^n (y_i - y_i')^2
$$</p>
<p>PS: We will revisit the squared error later from the perspective of MLE.</p>
<h2 id="closed-form-solution">Closed-form solution</h2>
<p>Okay, finally we find a function to measure the loss. Next we need to find the parameters that minimize the squared error. Good news is that our loss function is differentiable and convex! It means that we have a global minimial value and can be calculated directly by taking derivatives.</p>
<p>Let&rsquo;s take the first derivatve of $b$</p>
<p>$$
\frac{\partial L}{\partial b} = \sum_i^n -2(y_i - ax_i-b)
$$</p>
<p>and then set this equation to $0$,</p>
<p>$$
-2(\sum_i^ny_i -a\sum_i^nx_i - \sum_i^nb) = -2(n\overline y-an\overline x - nb) = 0
$$</p>
<p>$$
b = \overline y - a\overline x
$$</p>
<p>Let&rsquo;s take the first derivatve of $a$</p>
<p>$$
\frac{\partial L}{\partial a} = \sum_i^n -2x_i(y_i - ax_i-b)
$$</p>
<p>and then plug $b$ into this equaiton and set this equation to 0 again,</p>
<p>$$
\sum_i^n -2x_i(y_i - ax_i-\overline y+a\overline x) = \sum_i^n -2x_i[(y_i-\overline y)- a(x_i -\overline x)]
$$</p>
<p>$$
a = \frac{\sum_i^nx_i(y_i-\overline y)}{\sum_i^nx_i(x_i -\overline x)}
$$</p>
<p>Here, we use a slight algebra trick,</p>
<p>$$
a\sum_i^n(x_i - \overline x_i) = 0
$$</p>
<p>Then we plug this into the previous equation</p>
<p>$$
a = \frac{\sum_i^nx_i(y_i-\overline y)}{\sum_i^nx_i(x_i -\overline x)} = \frac{\sum_i^nx_i(y_i-\overline y) - \sum_i^n\overline x(y_i - \overline y)}{\sum_i^nx_i(x_i -\overline x) - \sum_i^n\overline x(x_i - \overline x)}
$$</p>
<p>$$
= \frac{\sum_i^n(x_i-\overline x)(y_i-\overline y)}{\sum_i^n(x_i -\overline x)^2}
$$</p>
<p>$$
= \frac{Cov(x, y)}{Var(x)}
$$</p>
<p>Finally, we find the best estimators for simple linear regression.</p>
<h2 id="r2">$R^2$</h2>
<p>So how to evaluate our model? In other words, how good is it? We can use $R^2$ to measure our model. Let&rsquo;s rewrite the previous equation by multiplying both the denominator and numerator by $\sqrt {\sum_i^n(y_i-\overline y)^2}$</p>
<p>$$
a = \frac{\sum_i^n (x_i - \overline x)(y_i - \overline y) \sqrt {\sum_i^n(y_i-\overline y)^2}}{\sqrt {\sum_i^n(x_i-\overline x)^2} \sqrt {\sum_i^n(x_i-\overline x)^2} \sqrt {\sum_i^n(y_i-\overline y)^2}}
$$</p>
<p>$$
a = R\frac{s_y}{s_x}
$$</p>
<p>where</p>
<p>$$
R = \frac{Cov(x, y)}{\sqrt{var(x)} \sqrt{var(y)}}
$$</p>
<p>$$
s_y =  \sqrt{Var(y)}
$$</p>
<p>$$
s_x = \sqrt{Var(x)}
$$</p>
<p>Remember that the error is defined as $e_i = y_i' - y_i$, so the mean of $e$ is</p>
<p>$$
E(e) = \frac{1}{N} \sum_i^n e_i =  \frac{1}{N} \sum_i^n b + ax_i - y_i =b + a\overline x - \overline y = 0
$$</p>
<p>and the variance is</p>
<p>$$
var(e) = \sum_i^n (e_i - \overline e)^2 = \sum_i^n  (y_i - b - ax_i)^2 = \sum_i^n (y_i - \overline y + a\overline x - ax_i)^2
$$</p>
<p>Let&rsquo;s plug $a$ into this equation</p>
<p>$$
var(e) = \sum_i^n [(y_i - \overline y) - R\frac{s_y}{s_x}( x_i - \overline x)]^2 = var(y) (1-R^2)
$$</p>
<p>Or you might be more familiar with this equation</p>
<p>$$
R^2 = 1 - \frac{var(e)}{var(y)} = 1 - \frac{RSS}{TSS}
$$</p>
<p>Therefore, $R^2$ tells us how much variance of $y$ has been explained by our models. The higher the $R^2$ is, the better our model is.</p>
<h2 id="references">References</h2>
<p>[1] <em>Bradthiessen.com</em>, 2021. [Online]. Available: <a href="https://www.bradthiessen.com/html5/docs/ols.pdf">https://www.bradthiessen.com/html5/docs/ols.pdf</a>. [Accessed: 14- Apr- 2021].</p>
<p>[2] “Linear Regression - ML Wiki,” Mlwiki.org. [Online]. Available: <a href="http://mlwiki.org/index.php/Linear_Regression">http://mlwiki.org/index.php/Linear_Regression</a>. [Accessed: 14-Apr-2021].</p>



        
      </article>

      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#problem-statement">Problem statement</a></li>
    <li><a href="#simple-linear-regression">Simple Linear Regression</a></li>
    <li><a href="#loss">Loss</a>
      <ul>
        <li><a href="#error">error</a></li>
        <li><a href="#the-absolute-value-of-error">the absolute value of error</a></li>
        <li><a href="#the-squared-value-of-error">the squared value of error</a></li>
      </ul>
    </li>
    <li><a href="#closed-form-solution">Closed-form solution</a></li>
    <li><a href="#r2">$R^2$</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      </aside>
    </div>

    

    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
