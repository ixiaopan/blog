<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Linear Regression 01</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Linear Regression 01"/>
<meta name="twitter:description" content="There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and maybe it&#39;s the first algorithm that most of people learn in the world of machine learning."/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Linear Regression 01</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 14, 2021
  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
  <p>There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and maybe it's the first algorithm that most of people learn in the world of machine learning. So let's go!</p>

<h2 id="problem-statement">Problem statement</h2>

<p>Suppose you are a teacher, and you record some data about the hours students spent on study and the grades they achieved. Then you want to predict the grade for given hours that someone spent. Here are some sample data you collected:</p>

<table>
<thead>
<tr>
<th>Hours</th>
<th>0.5</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>

<tbody>
<tr>
<td>Grade</td>
<td>20</td>
<td>21</td>
<td>22</td>
<td>24</td>
<td>25</td>
</tr>
</tbody>
</table>

<p>Since there are only two variables, let's plot them.</p>

<p><figure><img src="/blog/post/images/simple-linear-grade.png# half" alt="scatter-plot" title="Figure 1: The scatter plot of hours and grade"><figcaption>Figure 1: The scatter plot of hours and grade</figcaption></figure></p>

<p>Well, we can see that the variable <span  class="math">\(grade\)</span> is positive related to the variable <span  class="math">\(hours\)</span>. For simplicity, we can use a simple line(the red line in this figure)  to approximate this relationship. And this is exactly our first simple linear model.</p>

<h2 id="simple-linear-regression">Simple Linear Regression</h2>

<p>Remember a line equation is written in this way:</p>

<p><span  class="math">\[
y = ax + b
\]</span></p>

<p>In this example, <span  class="math">\(x\)</span> is the variable <span  class="math">\(hours\)</span> and <span  class="math">\(y\)</span> is the variable <span  class="math">\(grade\)</span> , which we already know. So the problem is how to calculate the parameter <span  class="math">\(a, b\)</span>. Technically, this is called <strong>parameter estimation</strong>. Usually, there are two ways to do this: minimising the loss and maximising likelihood. Now we focus on minimising the loss.</p>

<h2 id="loss">Loss</h2>

<p>What is the loss? Basically, it's the error between the esitmated value and our true value. Minimising the error is simply to make the estimated value as close as possible to the true value.</p>

<p><figure><img src="/blog/post/images/linear-regression-residual.png# half" alt="simple-linear-grade" title="Figure 2: The error for a single data (Bradthiessen.com 2021)"><figcaption>Figure 2: The error for a single data (Bradthiessen.com 2021)</figcaption></figure></p>

<p>For a single data point, the loss function is defined below, where <span  class="math">\(y\)</span> is the true value and <span  class="math">\(y'\)</span> is our estimated value for a given <span  class="math">\(a, b\)</span>.</p>

<p><span  class="math">\[
error = y_i - y'_i = y_i - ax_i - b
\]</span></p>

<p>Since we have many data points, we need to sum up them all to evaluate the overall errors.</p>

<h3 id="1-error">1. error</h3>

<p>Unfortunately, some error terms will cancel out when you do this calculation directly.</p>

<p><span  class="math">\[
L = \sum_i^n (y_i - y'_i) = \sum_i^n (y_i - ax_i - b)
\]</span></p>

<h3 id="2-the-absolute-value-of-error">2. the absolute value of error</h3>

<p>One way to tackle this is taking the absolute value of the error terms.</p>

<p><span  class="math">\[
L =  \sum_i^n |y_i - y_i'|
\]</span></p>

<p>However, the absoulte value of <span  class="math">\(x\)</span> is not differentiable at <span  class="math">\(0\)</span>.</p>

<h3 id="3-the-squared-value-of-error">3. the squared value of error</h3>

<p>Instead of taking absolute value, we will square all the errors. One reason is that the errors will become larger and can be distinguished easily when squaring them. It looks like the errors are zoomed in and we can find them and minimize them quickly. It is also known as <code>Residual Sum of Squares(RSS)</code> or <code>Sum of Squared Error (SSE)</code></p>

<p><span  class="math">\[
L =  \sum_i^n (y_i - y_i')^2
\]</span></p>

<p>PS: We will revisit the squared error later from the perspective of MLE.</p>

<h2 id="closedform-solution">Closed-form solution</h2>

<p>Okay, finally we find a function to measure the loss. Next we need to find the parameters that minimize the squared error. Good news is that our loss function is differentiable and convex! It means that we have a global minimial value and can be calculated directly by taking derivatives.</p>

<p>Let's take the first derivatve of <span  class="math">\(b\)</span></p>

<p><span  class="math">\[
\frac{\partial L}{\partial b} = \sum_i^n -2(y_i - ax_i-b)
\]</span></p>

<p>and then set this equation to <span  class="math">\(0\)</span>,</p>

<p><span  class="math">\[
-2(\sum_i^ny_i -a\sum_i^nx_i - \sum_i^nb) = -2(n\overline y-an\overline x - nb) = 0 \\
b = \overline y-a\overline x
\]</span></p>

<p>Let's take the first derivatve of <span  class="math">\(a\)</span></p>

<p><span  class="math">\[
\frac{\partial L}{\partial a} = \sum_i^n -2x_i(y_i - ax_i-b)
\]</span></p>

<p>and then plug <span  class="math">\(b\)</span> into this equaiton and set this equation to <span  class="math">\(0\)</span> again,</p>

<p><span  class="math">\[
\sum_i^n -2x_i(y_i - ax_i-\overline y+a\overline x) = \sum_i^n -2x_i[(y_i-\overline y)- a(x_i -\overline x)]\\
a = \frac{\sum_i^nx_i(y_i-\overline y)}{\sum_i^nx_i(x_i -\overline x)}
\]</span></p>

<p>Here, we use a slight algebra trick,</p>

<p><span  class="math">\[
a\sum_i^n(x_i - \overline x_i) = 0
\]</span></p>

<p>Then we plug this into the previous equation</p>

<p><span  class="math">\[
a = \frac{\sum_i^nx_i(y_i-\overline y)}{\sum_i^nx_i(x_i -\overline x)} = \frac{\sum_i^nx_i(y_i-\overline y) - \sum_i^n\overline x(y_i - \overline y)}{\sum_i^nx_i(x_i -\overline x) - \sum_i^n\overline x(x_i - \overline x)}\\
= \frac{\sum_i^n(x_i-\overline x)(y_i-\overline y)}{\sum_i^n(x_i -\overline x)^2}\\
= \frac{Cov(x, y)}{Var(x)}
\]</span></p>

<p>Finally, we find the best estimators for simple linear regression.</p>

<h2 id="reference">Reference</h2>

<p>[1] <em>Bradthiessen.com</em>, 2021. [Online]. Available: <a href="https://www.bradthiessen.com/html5/docs/ols.pdf">https://www.bradthiessen.com/html5/docs/ols.pdf</a>. [Accessed: 14- Apr- 2021].</p>




      
    </article>
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
