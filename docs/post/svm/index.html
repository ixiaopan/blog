<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Support Vector Machine</title>



  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Support Vector Machine"/>
<meta name="twitter:description" content="Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,
$$ y_i d_i \ge \Delta $$
where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin."/>




<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />




<link
  crossorigin="anonymous"
  href="/blog/css/styles.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>


  <link rel="stylesheet" href="https://ixiaopan.github.io/blog/css/main.css" />



<link
  id="dark-mode-theme"
  crossorigin="anonymous"
  href="/blog/css/dark.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>



<script defer crossorigin="anonymous" src="/blog/js/theme.js" integrity=""></script>

<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

 



<meta name="generator" content="Hugo 0.91.0" />
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/me.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/about" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/categories" id="Category"
              ><em class="fas fa-folder-open fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  <div class="intro-header">
    <h1 class="post-heading">
      Support Vector Machine
    </h1>
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Jun 15, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;9 min read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
</span>

  </div>
  
</header>

    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <h2 id="maximise-the-margin">Maximise the margin</h2>
<p>Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,</p>
<p>$$
y_i d_i \ge \Delta
$$</p>
<p>where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin. Our goal is to find a separating plane that maximises $\Delta$. This is known as Support Vector Machine.</p>
<h3 id="distance-to-hyperplanes">Distance to hyperplanes</h3>
<p>How to calculate the distance from a point to a hyperplane ?</p>
<p>
  <figure>
    <img src="/blog/post/images/distance2plane.png" alt="">
    <figcaption>Figure 1: Distance from a point to a hyperplane.</figcaption>
  </figure>
</p>
<p>The black line in Figure 1 represents a hyperplane defined by an orthogonal vector $w$ and a bias $b$</p>
<p>$$
w^Tx - b||w|| = 0
$$</p>
<p>The distance from the origin to the hyperplane is given by</p>
<p>$$
\frac{w^Tx_0}{||w||} = \frac{ b||w||}{||w||} = b
$$</p>
<p>Thus, the distance from a point $x_i$ ($x_i \neq 0$) to the hyperplane is given by</p>
<p>$$
d_i = \frac{w^Tx_i}{||w||} - b
$$</p>
<h3 id="primal-problem">Primal Problem</h3>
<p>Our goal is to find $w$ and $b$ to maximise $\Delta$ subject to the following constraints</p>
<p>$$
y_i ( \frac{w^Tx_i}{||w||} - b) \ge \Delta \text{ for all i = 1, 2, 3, &hellip;, n}
$$</p>
<p>Divide both sides by $\Delta$,</p>
<p>$$
y_i(\frac{w^Tx_i}{||w||\Delta} - \frac{b}{\Delta}) \ge 1
$$</p>
<p>Then define $w' = w/(||w||\Delta)$  and $b'= b/\Delta$,</p>
<p>$$
y_i(w'^Tx_i - b') \ge 1
$$</p>
<p>Note</p>
<p>$$
||w'|| = ||\frac{w}{||w||\Delta}|| = \frac{1}{\Delta}
$$</p>
<p>Therefore, minimising $||w'||^2$ is equivalent to maximising the margin $\Delta$. Now the problem becomes a quadratic programming problem defined below,</p>
<p>$$
\text{min}_{(w',b')} \frac{||w'||^2}{2} \text{ }\text{ }\text{ subject to  $y_i(w'^Tx_i - b') \ge 1$ for all data points}
$$</p>
<h3 id="extended-feature-space">Extended Feature Space</h3>
<p>However, data are not always linearly separated, such as data in Figure 2.</p>
<p>
  <figure>
    <img src="/blog/post/images/non-linearly-separable.png" alt="">
    <figcaption>Figure 2: Non-linearly separable data</figcaption>
  </figure>
</p>
<p>Maybe we could do some data transformation and work in a new feature space where data are linearly speparable. Yea, in fact, SVM maps all feature vectors to an extended feature space by defining a special $\phi(x)$, which is a function of $x$</p>
<p>$$
x \rarr \phi(x)
$$</p>
<p>Below are some examples of $\phi(x)$. You can choose any mapping you like.</p>
<p>$$
\phi(x) = x_1^2, \phi(x) = x_2^2, \phi(x) = \sqrt {x_1x_2}
$$</p>
<h3 id="dual-form">Dual Form</h3>
<p>In the extended feature space, we substitute $x$ for $\phi(x)$. So the question is defined as follows,</p>
<p>$$
\text{min}_{(w,b)} \frac{||w||^2}{2} \text{ }\text{ }\text{ subject to  $y_i(w^T \phi(x_i) - b) \ge 1$ for all data points}
$$</p>
<p>As mentioned earlier, we can use Lagrange multiplier to solve constrained optimisation. The Lagrangian function or the primal problem is given by</p>
<p>$$
\text{min}_{w, b}\text{max}_{\alpha} \mathcal{L} (w, b, \alpha)
$$
where</p>
<p>$$
\mathcal{L} (w, b, \alpha) =  \frac{||w||^2}{2}  - \sum_{i=1}^N \alpha_i (y_i(w^T \phi(x_i) - b) - 1)
$$</p>
<p>subject to  $\alpha_i \ge 0$ (because we have inequality constraints).</p>
<p>Then we tranform the primal problem into the dual problem. We first minimise Lagrange function w.r.t $w, b$ then maximise with respect to $\alpha$</p>
<p>$$
\text{max}_{\alpha} \text{min}_{w, b} \mathcal{L} (w, b, \alpha)
$$</p>
<p>Taking the derivative of $\mathcal{L} (w, b, \alpha)$ w.r.t. $w, b$ respectively,</p>
<p>$$
\nabla_w \mathcal{L} = w - \sum_{i=1}^N \alpha_i y_i\phi(x_i)  = 0
$$</p>
<p>$$
\nabla_b \mathcal{L} = \sum_{i=1}^N \alpha_i y_i  = 0
$$</p>
<p>Substituing back to the Lagrangian function</p>
<p>$$
\text{max}_{\alpha} \frac{||w||^2}{2}  - \sum_{i=1}^N \alpha_i (y_i(w^T \phi(x_i) - b) - 1)
$$</p>
<p>$$
= \text{max}_{\alpha} \frac{1}{2} \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{i=1}^N \alpha_i y_i\phi(x_i)  - \sum_{i=1}^N (\alpha_i y_iw^T \phi(x_i) - \alpha_i y_ib - \alpha_i)
$$</p>
<p>$$
= \text{max}_{\alpha} \frac{1}{2} \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{i=1}^N \alpha_i y_i\phi(x_i)  - \sum_{i=1}^N (\alpha_i y_i (\sum_{j=1}^N \alpha_j y_j\phi(x_j)^T) \phi(x_i) - \alpha_i)
$$</p>
<p>$$
= \text{max}_{\alpha} \frac{1}{2} \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{i=1}^N \alpha_i y_i\phi(x_i)  -  \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{j=1}^N \alpha_j y_j  \phi(x_j) + \sum_{i=1}^N \alpha_i
$$</p>
<p>$$
= \text{max}_{\alpha}  \sum_{i=1}^N \alpha_i - \frac{1}{2}  \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{j=1}^N \alpha_j y_j \phi(x_j)
$$</p>
<p>$$
= \text{max}_{\alpha}  \sum_{i=1}^N \alpha_i - \frac{1}{2}  \sum_{i=1}^N \sum_{j=1}^N  \alpha_i \alpha_j y_i y_j  \phi(x_i)^T  \phi(x_j)
$$</p>
<p>The dual problem is to find $\alpha$ that maximise $\mathcal{L}$, subject to constraints</p>
<p>$$
\sum_{i=1}^N \alpha_i y_i  = 0
$$</p>
<p>The Hessian of $\mathcal{L}$ is $-\frac{1}{2} X^TX$ where $X _{ik}$= $y_k \phi_i(x_k)$, so it&rsquo;s negative seim-definite, and thus there is a unique maximum.</p>
<h2 id="soft-margin">Soft Margin</h2>
<p>Okay, let&rsquo;s just leave the dual problem alone for a while(we will go back in the next section). Now we look at another situation where we want to relax the margin constraints. In other words, we allow some samples to appear in the margin area. This is called soft margin. We do it by introducing a slack variable $s_i$ shown in Figure 3.</p>
<p>
  <figure>
    <img src="/blog/post/images/svm-slack.png" alt="">
    <figcaption>Figure 3: Soft margin </figcaption>
  </figure>
</p>
<p>Now the constraint is</p>
<p>$$
y_i(w'^Tx_i - b') \ge 1 - s_i
$$</p>
<p>where $s_k \ge 0$. Obviously, the value of $s_i$ includes three situations</p>
<ul>
<li>For samples that lies far away from the hyperplane, $s_i = 0$, because they are far enough</li>
<li>For $0 \lt s_i \le 1$, there exist some samples that lie between margin and on the correct side of hyperplane</li>
<li>For $s_i \gt 1$, samples are misclassified</li>
</ul>
<h3 id="objective">Objective</h3>
<p>The objective function becomes</p>
<p>$$
\text{min}_{(w',b')} \frac{||w'||^2}{2}  + C\sum_{i=1}^N s_i
$$</p>
<p>where $C$ controls the degree of misclassification we desire</p>
<ul>
<li>a small C allows more freedom to relax the margin, which means it&rsquo;s acceptable to tolerate some misclassifications</li>
<li>a large C means we want to correctly classify as many samples as possbile</li>
</ul>
<p>The Lagragian function with slack variables is</p>
<p>$$
\mathcal{L} =  \frac{||w||^2}{2} +  C\sum_{i=1}^N s_i  - \sum_{i=1}^N \alpha_i (y_i(w^T \phi(x_i) - b) - 1 + s_i) - \sum_{i=1}^N \beta_is_i
$$</p>
<p>where $\beta_i$ are Lagrange multipliers that satisfy $\beta_i \ge 0$ (KKT condition)</p>
<p>Again, we minimise $\mathcal{L}$ w.r.t $s_i$</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial s_i} = C - \alpha_i - \beta_i = 0
$$
So we have</p>
<p>$$
\alpha_i = C - \beta_i
$$</p>
<p>Since $\beta_i \ge 0$,</p>
<p>$$
0 \le \alpha_i \le C
$$</p>
<h3 id="hinge-loss">Hinge Loss</h3>
<p>By the way, the objective function can also be written in the following form</p>
<p>$$
\text{min}_{(w',b')} \frac{||w'||^2}{2}  + C\sum_{i=1}^N \text{max} (0, 1-y_i f(x_i))
$$</p>
<p>where the second term is hinge loss function. So,</p>
<ul>
<li>$y_i f(x_i) \gt 1 $,
<ul>
<li>$x_i$ is outside margin and on the right side of the hyperplane, so there is no contribution to loss</li>
</ul>
</li>
<li>$y_i f(x_i) = 1 $,
<ul>
<li>$x_i$ is on the margin. Again, there is no contribution to loss</li>
</ul>
</li>
<li>$y_i f(x_i) \lt 1 $
<ul>
<li>$x_i$ lies between the margin or it&rsquo;s misclassified, so it increases the loss</li>
</ul>
</li>
</ul>
<h2 id="kernel-trick">Kernel Trick</h2>
<h3 id="kernel">Kernel</h3>
<p>Kernel is simply the inner product in feature space,</p>
<p>$$
K(x, y) = \phi(x)^T \phi(y) = K(y, x)
$$</p>
<p>where</p>
<ul>
<li>$\phi(x) = [\phi_1(x), \phi_2(x), &hellip;, \phi_k(x)]$</li>
<li>$\phi_i(x)$ are real valued functions of $x$ ( $\phi_i(x)$ is just a real number )</li>
<li>$k$ is the number of $\phi_i(x)$; it could be a specific number or INFINITE( that&rsquo;s crazy! )</li>
</ul>
<p>so kernel tells us the closeness or similarity between $x$ and $y$. The space spanned by $\phi(x)$ is called <strong>feature space</strong> or <strong>kernel space</strong></p>
<p>$$
\mathcal{K} = \text{span } [ \phi(x) | x \in \mathcal{X} ] \in R^k
$$</p>
<h3 id="symmetric">Symmetric</h3>
<p>Given a kernel $K:  \mathcal{X} \times  \mathcal{X} \rarr R$ and a training data set $\mathcal{X} = [x_1, x_2, &hellip;, x_n]$, we can construct a kernel matrix or Gram matrix $G \in R^{n \times n}$</p>
<p>$$
G_{ij} = K(x_i, x_j)
$$</p>
<p>Obviously, $G$ is a <strong>symmetric</strong> matrix and can be decomposed into $G = X^TX$, where</p>
<ul>
<li>$X$ is a $k \times n$ matrix</li>
<li>each column of $X$ is $\phi(x_i)$</li>
</ul>
<h3 id="positive-semi-definite">Positive Semi-Definite</h3>
<p>Consider a new vector $w = X v \in R^k$, we have</p>
<p>$$
||w||^2 = v^TX^T Xv = v^TGv \ge 0
$$</p>
<p>Hence, $G$ is <strong>positive semi-definite</strong> and all eigenvalues of $G$ are equal or greater than zero, i.e. $\lambda_i \ge 0$.</p>
<h3 id="eigenfunction">Eigenfunction</h3>
<p>Remember the eigenvector of $G$ is defined as follows</p>
<p>$$
Gv = \lambda v
$$</p>
<p>This means,</p>
<p>$$
\sum_{j=1}^n G_{ij} v_j = \lambda v_i
$$</p>
<p>If we extend the dimention of $G$ to INFINITE, as shown in Figure 4, we have</p>
<p>$$
\sum_{j=1}^{\infin} G_{ij} v_j = \sum_{j=1}^{\infin} K(x_i, x_j) v_j = \lambda v_i
$$</p>
<p>
  <figure>
    <img src="/blog/post/images/kernel-infinite.png" alt="">
    <figcaption>Figure 4: G could have infinite dimension (Ref[3])</figcaption>
  </figure>
</p>
<p>Then we define an <strong>eigenfunciton</strong> $\psi(x)$, which is a real funtion of $x$</p>
<p>$$
\sum_{j=1}^{\infin} K(x_i, x_j) \psi(x_j) = \lambda \psi(x_i)
$$</p>
<p>With the help of the integral operator, we can rewrite it as follows,</p>
<p>$$
\int_{y \in \mathcal{X}} K(x, y) \psi(y) dy = \lambda \psi(x)
$$</p>
<p>Thus, $Gv = \lambda v$ is just a special case of this, when $\mathcal{X}$ is a finite set.</p>
<h3 id="mercers-theorem">Mercer’s theorem</h3>
<p>In general there will be a denumerable set of eigenfunctions $[ \psi_1(x),\psi_2(x), &hellip; ] $ and the corresponding $[\lambda_1, \lambda_2, &hellip;]$ where</p>
<p>$$
K(x, y) = \sum_i^{\infin} \lambda_i \psi_i(x) \psi_i(y)
$$</p>
<p>This is known as Mercer’s theorem. And we find that $G_{ij} = \sum_{k=1}^n \lambda v_i^k v_j^k$ is just a special case of this, when $\mathcal{X}$ is a finite set.</p>
<p>If we define  $\phi_i(x) = \sqrt \lambda_i \psi_i(x) $ then</p>
<p>$$
K(x, y) = \sum_i^{\infin} \sqrt \lambda_i \psi_i(x)  \sqrt \lambda_i\psi_i(y) = \sum_i \phi_i(x) \phi_i(y) = \phi(x)^T \phi(y)
$$</p>
<p>An immediate consequence is that for any real $\psi(x)$,</p>
<p>$$
\int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} K(x, y) \psi(y) \psi(x) dy dx
$$</p>
<p>$$
= \int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} (\sum_i^{\infin} \lambda_i \psi_i(x) \psi_i(y)) \psi(y) \psi(x) dy dx
$$</p>
<p>$$
= \sum_i^{\infin}  \int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} [\sqrt\lambda_i \psi_i(y) \psi(y)]  [\sqrt\lambda_i \psi_i(x) \psi(x) ] dydx
$$</p>
<p>$$
= \sum_i^{\infin}  \int_{x \in \mathcal{X}}[\sqrt\lambda_i \psi_i(x) \psi(x) ] (\int_{y \in \mathcal{X}} [\sqrt\lambda_i \psi_i(y) \psi(y)] dy) dx
$$</p>
<p>$$
= \sum_i^{\infin}  (\int_{x \in \mathcal{X}}[\sqrt\lambda_i \psi_i(x) \psi(x) ] dx)^2 \ge 0
$$</p>
<p>Thus, $v^TGv \ge 0$ is just a special case of this, when $\mathcal{X}$ is a finite set.</p>
<p>Putting it together, the following statements are equivalent,</p>
<ul>
<li>
<p>$K(x, y)$ is positive semi-definite</p>
</li>
<li>
<p>The eigenvalue of $K(x, y)$ are non-negative</p>
</li>
<li>
<p>The kernel can be written
$$
K(x, y) = \sum_i^{\infin} \lambda_i \psi_i(x) \psi_i(y) = \sum_i \phi_i(x) \phi_i(y)
$$
where $\phi(x)$ are real functions</p>
</li>
<li>
<p>For any real function $\psi(x)$
$$
\int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} K(x, y) \psi(y) \psi(x) dy dx \ge 0
$$</p>
</li>
</ul>
<h2 id="properties-of-kernels">Properties of Kernels</h2>
<h3 id="adding-kernels">Adding Kernels</h3>
<p>If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) + K_2(x, y)$ is also a kernel</p>
<p>$$
\int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} K_3(x, y) \psi(y) \psi(x) dy dx
$$</p>
<p>$$
= \int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} (K_1(x, y) + K_2(x, y)) \psi(y) \psi(x) dy dx \ge 0
$$</p>
<p>Similarly, If $K(x, y)$ is a valid kernel so is $cK(x, y)$ for $c \gt 0$.</p>
<h3 id="product-of-kernels">Product of Kernels</h3>
<p>If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) K_2(x, y)$ is also a kernel</p>
<p>$$
K_3(x, y) = K_1(x, y) K_2(x, y) = \sum_i \phi_i^1(x) \phi_i^1(y) \sum_j \phi_j^2(x) \phi_j^2(y)
$$</p>
<p>$$
= \sum_i \sum_j \phi_i^1(x) \phi_j^2(x) \phi_i^1(y)   \phi_j^2(y)
$$</p>
<p>Define $\phi_k(z) = \phi_i^1(z)\phi_j^2(z)$, and the number of $\phi_k(z)$ is $i * j$</p>
<p>$$
K_3(x, y) = \sum_k \phi_k(x) \phi_k(y)
$$</p>
<h3 id="exponentiating-kernels">Exponentiating Kernels</h3>
<p>$\text{exp} (K(x, y))$ is a valid kernel since</p>
<p>$$
exp(K) = 1 +  K + \frac{1}{2} K^2 + &hellip;
$$</p>
<p>since the addition and multiplication of kernels yield valid kernels, each term is also a kernel and therefore the exponential of a kernel is a kernel.</p>
<h2 id="common-kernels">Common Kernels</h2>
<p>TODO</p>
<h3 id="linear">Linear</h3>
<h3 id="gaussian">Gaussian</h3>
<h3 id="quadratic">Quadratic</h3>
<h2 id="comments">Comments</h2>
<ul>
<li>SVM relys on distances between data points, so it would be better to normalise data if we don&rsquo;t know what features are important.</li>
<li>Different C can make a great difference in performance. We can use cross-validation to choose the optimal C.</li>
<li>A linear decision boundary doesn&rsquo;t always exist, especially we obtain a poor performance. If so, we might try a non-linear boundary with Kernel. There are many kernel functions designed for particular data types. Often, Kernel comes with its parameters, so fine-tunining them is also important to improve model&rsquo;s performance.</li>
<li>We don&rsquo;t need to explicitly know what $\phi(x)$ are. All we need to know is that there exist a hyperplane that can separate the data linearly in a higher dimensional space. Though we are in the extended feature space, we do computation of the inner product in the original feature space.</li>
</ul>
<h2 id="references">References</h2>
<p>[1] <a href="https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel">https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel</a></p>
<p>[2] <a href="http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/">http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/</a></p>
<p>[3] <a href="https://www.cs.cmu.edu/~bapoczos/other_presentations/kernel_methods_01_10_2009.pdf">https://www.cs.cmu.edu/~bapoczos/other_presentations/kernel_methods_01_10_2009.pdf</a></p>




        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#maximise-the-margin">Maximise the margin</a>
      <ul>
        <li><a href="#distance-to-hyperplanes">Distance to hyperplanes</a></li>
        <li><a href="#primal-problem">Primal Problem</a></li>
        <li><a href="#extended-feature-space">Extended Feature Space</a></li>
        <li><a href="#dual-form">Dual Form</a></li>
      </ul>
    </li>
    <li><a href="#soft-margin">Soft Margin</a>
      <ul>
        <li><a href="#objective">Objective</a></li>
        <li><a href="#hinge-loss">Hinge Loss</a></li>
      </ul>
    </li>
    <li><a href="#kernel-trick">Kernel Trick</a>
      <ul>
        <li><a href="#kernel">Kernel</a></li>
        <li><a href="#symmetric">Symmetric</a></li>
        <li><a href="#positive-semi-definite">Positive Semi-Definite</a></li>
        <li><a href="#eigenfunction">Eigenfunction</a></li>
        <li><a href="#mercers-theorem">Mercer’s theorem</a></li>
      </ul>
    </li>
    <li><a href="#properties-of-kernels">Properties of Kernels</a>
      <ul>
        <li><a href="#adding-kernels">Adding Kernels</a></li>
        <li><a href="#product-of-kernels">Product of Kernels</a></li>
        <li><a href="#exponentiating-kernels">Exponentiating Kernels</a></li>
      </ul>
    </li>
    <li><a href="#common-kernels">Common Kernels</a>
      <ul>
        <li><a href="#linear">Linear</a></li>
        <li><a href="#gaussian">Gaussian</a></li>
        <li><a href="#quadratic">Quadratic</a></li>
      </ul>
    </li>
    <li><a href="#comments">Comments</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      
      </aside>
    </div>
    
    
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=xiaopan.wpp@outlook.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/ixiaopan" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/ixiaopan" name="Linkedin">
        <em class="fab fa-linkedin"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2023
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
