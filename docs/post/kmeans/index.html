<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>K-means</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="K-means"/>
<meta name="twitter:description" content="So far, we&rsquo;ve talked much about supervised learning. Aside from it, there exist many other learning types such as unspervised learning, semi-supervised learning and so on. This post will introduce one of the most widely used unsupervised clustering algorithms — K-means. We will cover the implementation of K-means algorithm, the limitation of this algorithm as well as its applications."/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script>

    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>K-means</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Jun 14, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;4 min  read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>So far, we&rsquo;ve talked much about supervised learning. Aside from it, there exist many other learning types such as unspervised learning, semi-supervised learning and so on. This post will introduce one of the most widely used unsupervised clustering algorithms — K-means. We will cover the implementation of K-means algorithm, the limitation of this algorithm as well as its applications.</p>
<h2 id="problem-formulation">Problem Formulation</h2>
<p>Given an unlabelled dataset $D =[x_1, x_2, &hellip;, x_N] $, we want to explore whether there is a pattern in the data and what pattern is. In the case of clustering, we want to know which samples are close to each other so as to form a cluster. To achieve this, we assign to each sample a membership indicator denoted by  $r_{nk}=1$ if the sample is in cluster $k$, otherwise $r_{nj}=0$ for $j \ne k$. Thus,</p>
<ul>
<li>there could be $N$ clusters, i.e. each sample forms a cluster.</li>
<li>there could be only one cluster, i.e. the whole dataset forms a cluster.</li>
</ul>
<p>So what&rsquo;s the optimal value for $K$?</p>
<h2 id="objective-function">Objective Function</h2>
<p>The idea behind K-means is intutive to understand. Similar people or people with similar interests tends to form a group. Such a group also has a small variation. Based on this idea, the aim of K-means is to find $r_{nk}$ to minimise the within cluster variation,</p>
<p>$$
J = \sum^N_n\sum^K_k r_{nk} || x_n - \mu_k ||^2
$$</p>
<p>where $\mu_k$ is the center of the cluster that a sample $x_n$ belongs to.</p>
<p>We should notice that this objective also limits the ability of K-means somehow. You can imageine $\mu_k$ as the center of a circle, and the average within cluster variation is the maximum radius of that circle. Any point outside that circle doesn&rsquo;t belong to this cluster. So K-means works better for circular clusters.</p>
<p>
  <figure>
    <img src="/blog/post/images/k-means-shape.png#full" alt="">
    <figcaption>Figure 1: K-means works better for data with circular clusters (Ref[2])</figcaption>
  </figure>

</p>
<h2 id="approach">Approach</h2>
<p>Since  $\mu_k$ and $r_{nk}$ are both unknown, how to solve it? Well, we solve this by repeating the following two steps,</p>
<ul>
<li>Step 1: Randomly choose $K$ cluster centers, and then minimise $J$ w.r.t $r_{nk}$. In this step, we assign to each sample a membership indicator given the fixed cluster centers.</li>
</ul>
<p>$$
r_{nk} =\begin{cases} 1 &amp; \text{if $ k =\text{argmin}_j ||x_n - \mu_j||^2$} \\ 0 &amp; \text{otherwise}\end{cases}
$$</p>
<ul>
<li>
<p>Step 2: Keep $r_{nk}$ fixed and then calculate the new cluster centers — $\mu_k$ is equal to the mean of all samples assigned to that cluster $k$. This is because we minimise $J$ w.r.t $\mu_k$ by taking the derivative of $J$ and setting it to zero</p>
<p>$$
\frac{\partial J}{\partial \mu_k} = 2 \sum^N_{x_n \in C_k}  r_{nk} || x_n - \mu_k || =0
$$</p>
<p>$$
=&gt; \mu_k = \frac{\sum_{x_n \in C_k} r_{nk} x_n}{\sum_{x_n \in C_k} r_{nk}}
$$</p>
</li>
</ul>
<p>So when will this process stop? There are some stopping criterions</p>
<ul>
<li>The centroids remain the same</li>
<li>Within cluster variance reaches below the threshold you&rsquo;ve set</li>
<li>Fixed number of iterations have been reached</li>
</ul>
<h2 id="choosing-the-optimal-k">Choosing the optimal K</h2>
<p>Like the method used in K-NN, we plot the metric as a function of $k$ shown below.</p>
<p>
  <figure>
    <img src="/blog/post/images/kmeans-k.png#full" alt="">
    <figcaption>Figure 2: The optimal K lies at the elbow point (Hands-on machine learning)</figcaption>
  </figure>

</p>
<p>We can see that inertia decreases greatly as k increases up to 4, after that, it decreases slowly. On the other hand, increasing k indeed decrease the sum of squared distance from each sample to its closest centroid. This is because the more clusters k there are, the more cohesive each cluster is, and therefore the lower the within cluster variation is.</p>
<h2 id="limits-of-k-means">Limits of K-means</h2>
<h3 id="specify-k-explicitly">Specify K explicitly</h3>
<p>Obvisously, we have to explicitly specify $K$. But more often, we don&rsquo;t what the right $K$ is.</p>
<h3 id="sensitive-to-shape">Sensitive to shape</h3>
<p>As said earlier, K-means performs poorly when the clusters vary in sizes or densities, or have nonspherical shape. Figure below shows how K-means deals with a data set containing three ellipsoida clusters with different densities and orientation. Though the solution on the right has a lower inertia, it obviously breaks the inner structure of real clusters. Mabybe we could try Gaussian mixture models for this case.</p>
<p>
  <img src="/blog/post/images/kmeans-failure.png#full" alt="">

</p>
<h3 id="centroids-initialization">Centroids initialization</h3>
<p>Although K-means algorithm is guaranteed to converge, it might not coverge to the right solution. And this depends on the initialization of the centroids, which can also be seen in above Figure. To avoid this, we&rsquo;d better run the algorithm several times and select the solution that has the minimum within cluster variation.</p>
<h3 id="hard-classification">Hard classification</h3>
<p>Clustering assignment used in K-means is called hard classification since each sample is assigned to a cluster indicator. But sometimes we would expect probability to estimate uncertainty in clustering assignment. For example, if two clusters overlap, it&rsquo;s hard to have 100% confidence to say that some sample belongs to one of the two clusters.</p>
<h2 id="references">References</h2>
<p>[1] A. Géron, <em>Hands-on machine learning with Scikit-Learn and TensorFlow</em>. Sebastopol (CA): O&rsquo;Reilly Media, 2019.</p>
<p>[2]	J. VanderPlas, Python Data Science Handbook. Sebastopol, CA: O’Reilly Media, 2016.</p>



        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#problem-formulation">Problem Formulation</a></li>
    <li><a href="#objective-function">Objective Function</a></li>
    <li><a href="#approach">Approach</a></li>
    <li><a href="#choosing-the-optimal-k">Choosing the optimal K</a></li>
    <li><a href="#limits-of-k-means">Limits of K-means</a>
      <ul>
        <li><a href="#specify-k-explicitly">Specify K explicitly</a></li>
        <li><a href="#sensitive-to-shape">Sensitive to shape</a></li>
        <li><a href="#centroids-initialization">Centroids initialization</a></li>
        <li><a href="#hard-classification">Hard classification</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      </aside>
      
    </div>

    

    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
