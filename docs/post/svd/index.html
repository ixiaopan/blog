<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Singular Value Decomposition</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Singular Value Decomposition"/>
<meta name="twitter:description" content="Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post."/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script>

    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Singular Value Decomposition</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;May 4, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;7 min  read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into the multiplication of three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post.</p>
<h2 id="change-of-basis">Change of Basis</h2>
<p>Suppose there is a point in the 2D space, how do you describe it? The common way is to use the Cartesian coordinate system, which is composed of two fixed perpendicular oriented axes, measured in the same unit of length. The two perpendicular axes are just a special set of vectors served as the basis of the 2D space. Actually, there are many other sets of vectors that can be the basis for the 2D space. For example, in Figure 1, the position of the red point is <code>(-4, 1)</code> when using the standard basis <code>(1, 0), (0, 1)</code> (colored in grey). If we change the basis to <code>(2, 1), (-1, 1)</code> (colored in blue), the position is <code>(-1, 2)</code>.</p>
<p>
  <figure>
    <img src="/blog/post/images/change-basis-example.png" alt="">
    <figcaption>Figure 1: The same point with different coordinates in two different coordinate spaces</figcaption>
  </figure>

</p>
<p>From Figure 1, we can see that the absolute position of the red point always stay the same. However, the relative positions to the different bases are different. Mathematically, the red point can be described from the perspective of basis as follows,</p>
<p>$$
x = P_b[x]_b = c_1 \bold b_1 + c_2 \bold b_2 + &hellip; + c_n \bold b_n
$$</p>
<p>$$
P_b = [\bold b_1, \bold b_2,  &hellip; , \bold b_n ]
$$</p>
<p>$$
[x]_b = [c_1, c_2, &hellip; c_n]
$$</p>
<p>where</p>
<ul>
<li>$[x]_b$ is a set of scalars, which represent the length of projection onto each axis of the current coordinate system</li>
<li>$P_b$ is the corresponding basis of the current coordinate system</li>
</ul>
<p>Let&rsquo;s plug the above point and the basis <code>(1, 0), (0, 1)</code>  (colored in grey) into the equation,</p>
<p>$$
P_b = [ (1, 0), (0, 1)]
$$</p>
<p>$$
[x]_b = (-4, 1)
$$</p>
<p>$$
x_b = -4 \begin{bmatrix}1\\ 0 \end{bmatrix} + 1 \begin{bmatrix}0\\ 1 \end{bmatrix} = \begin{bmatrix}-4\\ 1 \end{bmatrix}
$$</p>
<p>Let&rsquo;s do the same calculation with another basis (colored in blue),</p>
<p>$$
P_b = [ (2, 1), (-1, 1)]
$$</p>
<p>$$
[x]_b = (-1, 2)
$$</p>
<p>$$
x_b = -1 \begin{bmatrix}2\\ 1 \end{bmatrix} + 2 \begin{bmatrix}-1\\ 1 \end{bmatrix} = \begin{bmatrix}-4\\ 1 \end{bmatrix}
$$</p>
<p>As expected, they yield the same result. And the second one essentially changes the basis of $R^2$ from <code>(2,1),(-1,1)</code> to<code>(1,0),(0,1)</code>, which is the standard basis of $R^2$.</p>
<p>Actually, this example is a special case of the change of basis, where the new basis is the standard basis. More generally, $P_{c \larr b}$ is known as **the change of coordinate matrix from the old basis $b$ to the new basis $c$, which we are going to switch to** in $R^n$.</p>
<p>Say we are in the basis $b$ and $[x]_b$ is known, the corresponding coodinates of $x$ under the new basis $c$ can be computed as follows,</p>
<p>$$
x_c = P_{c \larr b} x_b
$$</p>
<p>Since $P_{c \larr b}$ is invertible, we have</p>
<p>$$
(P_{c \larr b})^{-1}x_c = x_b
$$</p>
<p>which is the inverse operation of change of basis from $b$ to $c$. We can generalize this to any number of points and dimensions.</p>
<p>$$
A=US\\(D,N)= (D, M) \times (M, N)
$$</p>
<p>$$
U^{-1} A  = S\\(M, D) \times (D, N) = (M, N)
$$</p>
<p>where</p>
<ul>
<li>$A$ is a $D\times N$ matrix with $D$ dimensions and $N$ points</li>
<li>$S$ is a  $M\times N$ matrix with $M$ dimensions and $N$ points described in a new vector space decided by another basis</li>
<li>$U$ is the the change of coordinate matrix from $S$ to $A$</li>
<li>$U^{-1}$ is the the change of coordinate matrix from $A$ to $S$</li>
</ul>
<p>If we do some transformation for a point in the standard coordinate system, what&rsquo;re the new coordinates of the same point in another system? This problem can be solved by the following equation,</p>
<p>$$
x_s' = U^{-1}TUx_s
$$</p>
<p>where $T$ represents the transformation matrix. If $T=I$,  $x_s'$ is exactly $x_s$.</p>
<h2 id="svd">SVD</h2>
<h3 id="linear-regression">Linear Regression</h3>
<h3 id="principal-component">Principal Component</h3>
<h2 id="pca">PCA</h2>
<p>Principal component analysis(PCA) is  often used to reduce dimentionality. The idea of PCA is to find directions along which data has the largest variation. The variation can be computed by projecting data onto that direction. Mathematically, we want to find a vector $v$ with $||v||=1$ to maximise</p>
<p>$$
\sigma^2 = \frac{1}{n-1}\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2
$$</p>
<p>There are two things to notice here:</p>
<ul>
<li>$v$ is an unit vector since we are care about the direction only</li>
<li>data are centralized first for simple computation; centralizing data doesn&rsquo;t change the distribution of data</li>
</ul>
<p>We can solve the above equation by introducing Lagrange multiplier</p>
<p>$$
L = \frac{1}{n-1}\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2 - \lambda (||v||^2 - 1)
$$</p>
<p>$$
= \frac{1}{n-1}\sum_i^n \bold v^T (\bold x_i - \bold \mu)(\bold x_i - \bold \mu)^T\bold v - \lambda (||v||^2 - 1)
$$</p>
<p>$$
= \bold v^T \bold C \bold v - \lambda (\bold  v^T\bold v - 1)
$$</p>
<p>where $\bold C$ is the covariance matrix of $X$. Then we take the derivative of $L$ w.r.t $\bold v$ , and then set it to $0$</p>
<p>$$
\frac{\partial L}{\partial \bold v} = 2(C \bold v - \lambda \bold v) = 0
$$</p>
<p>so $\bold v$ is the eigenvector of $\bold C$, and the variance along this direction is,</p>
<p>$$
\sigma^2 = \frac{1}{n-1}\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2 = \bold v^T \bold C \bold v = \lambda \bold v^T  \bold v = \lambda
$$</p>
<h3 id="properties-of-covariance-matrix">Properties of Covariance Matrix</h3>
<p>we know that the quadratic form of a vector and a matrix is defined as</p>
<p>$$
x^T M x
$$</p>
<p>Thus, the quadratic form of $C$ is</p>
<p>$$
x^TCx = x^T XX^Tx=u^Tu \ge 0
$$</p>
<p>which is also known as positive semi-definite. What does this mean? Well, it tells us that all eigenvalues of $C$ are greater than or equal to zero. Suppose $\mu$ is an eigenvector of $C$, since $\mu^TC\mu\ge0$ and $||\mu||&gt;0$, then we have</p>
<p>$$
\mu^TC\mu = \mu^T \lambda \mu =&gt; \lambda = \frac{\mu^TC\mu }{||\mu||^2 } \ge 0
$$</p>
<h3 id="geometry-of-pca">Geometry of PCA</h3>
<p>Since $C$ is a $p \times p$ symmetric matrix, it can be orthogonally diagonalized as $C = P^TDP$, where $P$ is an orthogonal matrix and $D$ is a diagonal matrix. Thus, the above formula can also be written as follows,</p>
<p>$$
\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2 = \sum_i^n \bold v^T (\bold x_i - \bold \mu)(\bold x_i - \bold \mu)^T\bold v = \bold v^T C \bold v
$$</p>
<p>$$
= \bold v^T P^TDP \bold v = y^TDy
$$</p>
<p>where $y = Pv$. Mathematically, $\bold v^T C \bold v$ and $y^TDy$ yield the same result.  They both compute the length of projection of each data point onto some vector, which is also the deviation of each data point from the original point along the direction determined by this vector, as shown in Figure 2.</p>
<p>
  <figure>
    <img src="/blog/post/images/change-variable.png" alt="">
    <figcaption>Figure 2: Change of variable in $x^T A x$ (Introduction to Linear Algebra[1])</figcaption>
  </figure>

</p>
<p>Geometrically,  $P_{y \larr v}$ is the change coordinate matrix from $v$ to $y$, which finally transform the shape of the quadratic form $\bold v^TC\bold v$ into the standard position because $D$ is a diagonal matrix, such as in the figure below.</p>
<p>
  <figure>
    <img src="/blog/post/images/ellipse-standard.png" alt="">
    <figcaption>Figure 3: An ellipse and a hyperbola in standard position (Introduction to Linear Algebra[1])</figcaption>
  </figure>

</p>
<h3 id="projection">Projection</h3>
<p>For a data set with $p$ features, we want to reduce its dimension to $k$, there are a few steps to follow</p>
<ul>
<li>Construct a covariance matrix $C$ using centralized data</li>
<li>Find  all the eigenvectors $v_i$ and eigenvalues $\lambda_i$ of $C$</li>
<li>Keep $k$ eigenvectors with the $k$ largest eigenvalues (principal components)</li>
<li>Project the original data into the space spanned by the principal components</li>
</ul>
<p>$$
\bold  z = P(\bold  x - \bold \mu)
$$</p>
<p>where $z$ is our new inputs.</p>
<h3 id="reconstruction">Reconstruction</h3>
<p>From the perspective of projection, the projection onto a vector $\bold v_j$ can be seen as approximating the inputs by</p>
<p>$$
\hat {\bold x_i} = \bold \mu + \sum_j^k z_j^i \bold v_j
$$</p>
<p>$$
z_j^i = \bold v_j^T(\bold x_i - \bold \mu)
$$</p>
<p>So our goal is to minimize the error</p>
<p>$$
E[ ||\hat {\bold x_i} - \bold x_i ||^2] = \frac{1}{n} \sum_{n=1}^n ||\sum_{i=1}^k \bold u_i^T\bold x_n \bold u_i + \sum_{i=k+1}^p \bold u_i^T\bold x_n \bold u_i - \sum_{j=1}^k \bold v_j^T\bold x_n \bold v_j = \frac{1}{n} \sum_{n=1}^n \sum_{i=k+1}^p ||\bold u_i^T\bold x_n \bold u_i||^2
$$</p>
<p>$$
= \frac{1}{n}\sum_{n=1}^n\sum_{i=k+1}^p (\bold u_i^T\bold x_n) \bold u_i^T \cdot (\bold u_i^T\bold x_n) \bold u_i =\frac{1}{n} \sum_{n=1}^n\sum_{i=k+1}^p (\bold u_i^T\bold x_n)^2
$$</p>
<p>$$
= \frac{1}{n} \sum_{n=1}^n\sum_{i=k+1}^p \bold u_i^T\bold x_n \bold x_n^T \bold u_i= \sum_{i=k+1}^p \bold u_i^T (\frac{1}{n} \sum_{n=1}^n\bold x_n \bold x_n^T )\bold u_i
$$</p>
<p>$$
\sum_{i=k+1}^p \bold u_i^T S \bold u_i = \sum_{i=k+1}^p \lambda_i \bold u_i^T  \bold u_i = \sum_{i=k+1}^p \lambda_i
$$</p>
<p>We can see that the error is exactly the sum of the eigenvalues in the directions that are discarded.</p>
<h3 id="pca-for-images">PCA for images</h3>
<h2 id="references">References</h2>
<p>[1]	G. Strang, Introduction to Linear Algebra, 5th ed. Wellesley, MA: Wellesley-Cambridge Press, 2016.</p>



        
      </article>

      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#change-of-basis">Change of Basis</a></li>
    <li><a href="#svd">SVD</a>
      <ul>
        <li><a href="#linear-regression">Linear Regression</a></li>
        <li><a href="#principal-component">Principal Component</a></li>
      </ul>
    </li>
    <li><a href="#pca">PCA</a>
      <ul>
        <li><a href="#properties-of-covariance-matrix">Properties of Covariance Matrix</a></li>
        <li><a href="#geometry-of-pca">Geometry of PCA</a></li>
        <li><a href="#projection">Projection</a></li>
        <li><a href="#reconstruction">Reconstruction</a></li>
        <li><a href="#pca-for-images">PCA for images</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      </aside>
    </div>

    

    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
