<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Deep Learning - Neural Networks</title>



  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-203640627-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-203640627-1');
  </script>
  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning - Neural Networks"/>
<meta name="twitter:description" content="Neural Networks Neural networks are simply function compositions.
 Feedforward networks  $$ y = f(g(x, \theta_g), \theta_f) $$
 Recurrent networks  $$ y_t = f(y_{t-1}, x_t, \theta) $$
Generally, you can think of neural networks as function approximation machines, where we want to find appropriate $\theta$ to make $f^{*}$ as similar as possible to the true function $f$.
In practice, there are lots of things to consider when designing a neural network."/>




<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />
<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

 




  <link rel="stylesheet" href="https://ixiaopan.github.io/blog/css/main.css" />


<meta name="generator" content="Hugo 0.81.0" />
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/me.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


    
    <div class="intro-header">
      <div class="container">
        <div class="post-heading">
          
            <h1>
              Deep Learning - Neural Networks
            </h1>
          
          
            <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Sep 18, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;5 min read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/deep-learning/"
        >Deep Learning</a
      >&nbsp;
    
  
</span>

          
        </div>
      </div>
    </div>
    
  </header>

    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <h2 id="neural-networks">Neural Networks</h2>
<p>Neural networks are simply function compositions.</p>
<ul>
<li>Feedforward networks</li>
</ul>
<p>$$
y = f(g(x, \theta_g), \theta_f)
$$</p>
<ul>
<li>Recurrent networks</li>
</ul>
<p>$$
y_t = f(y_{t-1}, x_t, \theta)
$$</p>
<p>Generally, you can think of neural networks as function approximation machines, where we want to find appropriate $\theta$ to make $f^{*}$ as similar as possible to the true function $f$.</p>
<p>In practice, there are lots of things to consider when designing a neural network. Basically, we need to consider the following factors,</p>
<ul>
<li>Loss function</li>
<li>Architecture of the network
<ul>
<li>How many layers?</li>
<li>How many units in each layer?</li>
<li>How are these layers connected?</li>
</ul>
</li>
<li>Optimisation Algorithm</li>
</ul>
<h2 id="loss-function">Loss Function</h2>
<p>In classification tasks, target variables are categorical, so we need to find a method to transform them into numerical values. Typically, we use one-hot encoding to encode each category into a binary value. If an example belongs to class $j$, then the true target value is $C_j = 1$, otherwise $C_j = 0$. When we make predictions, we&rsquo;d like to predict the probability of an example belonging to a class. For example, we may have such a prediction $(0.1,0.7,0.2)$. Since class 2 has the highest probability, we say that this example is classified as class 2.</p>
<p>But how to quantify the difference between predictions and the ground truth so as to find the best parameters? In Machine Learning, we have learned mean squared error (MSE) and cross-entropy as the loss functions. So, which one to use?</p>
<h3 id="mse">MSE</h3>
<p>Let&rsquo;s take the derivative of MSE w.r.t $w$. The equation below shows that the gradient of MSE w.r.t $w$ is the product of the difference between the target value $y$ and predicted value $\sigma(z)$ and the gradient of the activation function.</p>
<p>$$
\frac {\partial }{\partial  w}\frac{1}{2}(y - \sigma(z))^2 = (y - \sigma(z))\sigma'(z)x
$$</p>
<p>A widely used activation function in binary classification is the sigmoid function. From Figure 1, we see that when the predicted value is close to 1/0, the curve gets very flat, indicating $\sigma'(z)$ becomes very small. If the ground truth is opposite to the predicted value, our machine will learn slowly as the  gradient of MSE w.r.t. $w$ is small.</p>
<p>
  <figure>
    <img src="/blog/post/images/sigmoid.png" alt="">
    <figcaption>Figure 1: Sigmoid function</figcaption>
  </figure>
</p>
<h3 id="cross-entropy">Cross-Entropy</h3>
<p>Entropy is an important concept in information theory. Basically, it tells us the average information a random variable conveys. Cross-entropy measures the difference between two distributions, where $f(x)$ is considered as the ground truth. Thus, a smaller cross-entropy indicates better learning.</p>
<p>$$
E = -\sum_{x \in X} f(x) \text{log} g(x)
$$</p>
<p>Does it have the problem of slow learning? Let&rsquo;s take the derivative of $E$ w.r.t $w_j$ (suppose it&rsquo;s binary classification)</p>
<p>$$
\frac{\partial L}{\partial w_j} = - \frac{1}{n} \sum_x (\frac{y}{\sigma(z)} - \frac{1 - y}{1 - \sigma(z)})\sigma'(z)x_j = - \frac{1}{n} \sum_x (\sigma(z) - y) x_j
$$</p>
<p>We see that the gradient of $w_j$ is determined by the error in the output. In other words, cross-entropy punishes the wrong classification heavily. Figure 2 also shows that misclassified point conveys much information.</p>
<p>
  <figure>
    <img src="/blog/post/images/negative-log.png" alt="">
    <figcaption>Figure 2: Negative logarithm between 0 and 1.</figcaption>
  </figure>
</p>
<h2 id="activation-function">Activation Function</h2>
<h3 id="why">Why</h3>
<p>Why do we need an activation function?</p>
<p>Deep learning works by mimicking how a human brain learns. Every second, our brains receive a lot of information, but not all of them are useful. Some information will be ignored, otherwise our brains will be overloaded. In deep learning, we need to find a method to do the same thing. Activation functions, as their name suggests, decide whether to fire neurons or not.</p>
<p>On the other hand, since neural networks are composed of multiple layers and each layer can be expressed as $Y = XW$, the final output is given as $ y = XW_1 W_2..W_n = XW$, where $W = W_1W_2&hellip;W_n$. If all functions are linear, there is no need to add the depth of the network as we can just use a single layer to achieve this. To capture the features, we must use a nonlinear function, also known as an activation function, which is applied to each hidden unit following an affine transformation controlled by parameters.</p>
<h3 id="what">What</h3>
<p>So, what should a valid activation function look like?</p>
<ul>
<li>differentiable
<ul>
<li>because deep learning is gradient-based learning, we need to know in which direction and how much to move in the parameter space</li>
</ul>
</li>
<li>monotonic</li>
</ul>
<h3 id="binary-step-function">Binary Step function</h3>
<p>A simple way to fire a neuron is to set a threshold. If the output of a neuron is greater than zero, the step function will return 1 and this neuron will be activated, otherwise it will be deactivated.</p>
<ul>
<li>Equation</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">step</span>(x):
    <span style="color:#66d9ef">if</span> x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0</span>

</code></pre></div><ul>
<li>Range: $\{0, 1\}$</li>
<li>Pros
<ul>
<li>simple</li>
</ul>
</li>
<li>Cons
<ul>
<li>only suitable for binary classificaition</li>
<li>the gradient is zero, so the parameters will not updated via backprop</li>
</ul>
</li>
</ul>
<p>
  <figure>
    <img src="/blog/post/images/step-activation.png" alt="">
    <figcaption>Figure 4: Binary step activation function.</figcaption>
  </figure>
</p>
<h3 id="identity">Identity</h3>
<p>Identity function is a linear function that returns the multiple of the input.</p>
<ul>
<li>Equation</li>
</ul>
<p>$$
f(x) = ax
$$</p>
<ul>
<li>
<p>Range: $(-\infin, \infin)$</p>
</li>
<li>
<p>Pros</p>
<ul>
<li>The gradient is  non-zero</li>
</ul>
</li>
<li>
<p>Cons</p>
<ul>
<li>However, the gradient is a constant value that does not depend on the inputs, so learning will not be improved from errors since the gradient is always the same.</li>
</ul>
</li>
</ul>
<p>
  <figure>
    <img src="/blog/post/images/identity-activation.png" alt="">
    <figcaption>Figure 3: Identity activation function.</figcaption>
  </figure>
</p>
<h3 id="sigmoid">Sigmoid</h3>
<ul>
<li>Equation</li>
</ul>
<p>$$
f(z) = \frac{1}{1 + e^{-z}}
$$</p>
<ul>
<li>Range: $[0, 1]$</li>
<li>Pros
<ul>
<li>Smooth S-shaped and continuously differentiable</li>
<li>It is suitable for predicting the probability as an output</li>
</ul>
</li>
<li>Cons
<ul>
<li>The gradient is small as $z$ is far away from 0, which will cause &lsquo;vanishing gradient&rsquo;</li>
</ul>
</li>
</ul>
<p>
  <figure>
    <img src="/blog/post/images/sigmoid-function.png#full" alt="">
    <figcaption>Figure 5: Sigmoid function and its derivative</figcaption>
  </figure>
</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">0.1</span>)
<span style="color:#75715e"># F.sigmoid is deprecated</span>
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sigmoid(x)

</code></pre></div><h3 id="tanh">Tanh</h3>
<ul>
<li>Equation</li>
</ul>
<p>$$
f(x) = \text{tanh} (x) = \frac{2}{1+e^{-2x}} - 1
$$</p>
<ul>
<li>
<p>Range: $[-1, 1]$</p>
</li>
<li>
<p>Pros</p>
<ul>
<li>Similar to sigmoid, but it&rsquo;s symmetric around 0</li>
<li>The gradient is steeper than sigmoid</li>
</ul>
</li>
<li>
<p>Cons</p>
<ul>
<li>vanishing gradient</li>
</ul>
</li>
</ul>
<p>
  <figure>
    <img src="/blog/post/images/tanh.png#full" alt="">
    <figcaption>Figure 6: Tanh function</figcaption>
  </figure>
</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">0.1</span>)
<span style="color:#75715e"># F.tanh is deprecated</span>
y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tanh(x)

</code></pre></div><h3 id="relu">ReLU</h3>
<ul>
<li>Equation</li>
</ul>
<p>$$
f(x) = \text{max} (0, x)
$$</p>
<ul>
<li>
<p>Range: $[0, \infin)$</p>
</li>
<li>
<p>Pros</p>
<ul>
<li>sparse network and efficient</li>
<li>simpler mathematical operation</li>
</ul>
</li>
<li>
<p>Cons</p>
<ul>
<li>dying reLU, since neurons with negative values will be deactivated, and thus the weights for them will not be updated due to the zero gradient</li>
</ul>
</li>
</ul>
<p>
  <figure>
    <img src="/blog/post/images/reLU.png" alt="">
    <figcaption>Figure 6: ReLU function</figcaption>
  </figure>
</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">0.1</span>)
y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(x)
</code></pre></div><h3 id="leaky-relu">Leaky ReLU</h3>
<p>A solution to solve dying ReLU problem is to make the horizontal line into non-horizaontal line. The leak coefficient that controls the slope, also known negative_slope in PyTorch, is a learned parameter, typically, it&rsquo;s $0.01$.</p>
<p>$$
f(x) = \text{max} (0, x) + \text{negative\_slope} * \text{min}(0, x)
$$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">0.1</span>)
y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>leaky_relu(x)
</code></pre></div><h3 id="parametric-relu">Parametric ReLU</h3>
<p>$$
f(x) = \text{max} (0, x) + \text{a} * \text{min}(0, x)
$$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">0.1</span>)
<span style="color:#75715e"># weight must be a tensor</span>
y <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>prelu(x, torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">0.25</span>))
</code></pre></div><h2 id="reference">Reference</h2>
<ul>
<li><a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">Understanding Activation Functions in Neural Networks</a></li>
<li><a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6">Activation Functions in Neural Networks</a></li>
</ul>




        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#neural-networks">Neural Networks</a></li>
    <li><a href="#loss-function">Loss Function</a>
      <ul>
        <li><a href="#mse">MSE</a></li>
        <li><a href="#cross-entropy">Cross-Entropy</a></li>
      </ul>
    </li>
    <li><a href="#activation-function">Activation Function</a>
      <ul>
        <li><a href="#why">Why</a></li>
        <li><a href="#what">What</a></li>
        <li><a href="#binary-step-function">Binary Step function</a></li>
        <li><a href="#identity">Identity</a></li>
        <li><a href="#sigmoid">Sigmoid</a></li>
        <li><a href="#tanh">Tanh</a></li>
        <li><a href="#relu">ReLU</a></li>
        <li><a href="#leaky-relu">Leaky ReLU</a></li>
        <li><a href="#parametric-relu">Parametric ReLU</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
      
      </aside>
    </div>
    
    
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=xiaopan.wpp@outlook.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/ixiaopan" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/ixiaopan" name="Linkedin">
        <em class="fab fa-linkedin"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
