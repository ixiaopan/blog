<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Metrics for classification</title>



  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Metrics for classification"/>
<meta name="twitter:description" content="In linear regression, we choose $R^2$ as our metrics to measure how good our algorithms are. But how about classification? In this post, we are going to talk about several commonly used metrics for classification."/>




<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />




<link
  crossorigin="anonymous"
  href="/blog/css/styles.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>


  <link rel="stylesheet" href="https://ixiaopan.github.io/blog/css/main.css" />



<link
  id="dark-mode-theme"
  crossorigin="anonymous"
  href="/blog/css/dark.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>



<script defer crossorigin="anonymous" src="/blog/js/theme.js" integrity=""></script>

<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

 



<meta name="generator" content="Hugo 0.91.0" />
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/me.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/about" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/categories" id="Category"
              ><em class="fas fa-folder-open fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  <div class="intro-header">
    <h1 class="post-heading">
      Metrics for classification
    </h1>
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Oct 26, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;5 min read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
</span>

  </div>
  
</header>

    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>In linear regression, we choose $R^2$ as our metrics to measure how good our algorithms are. But how about classification? In this post, we are going to talk about several commonly used metrics for classification.</p>
<h2 id="metrics">Metrics</h2>
<p>We first consider the binary classification, where an example must be classified as either A or B. Thus, there are four possible cases,</p>
<ul>
<li>the ground truth is A, and the predicted lable is A</li>
<li>the ground truth is A, but the predicted lable is B</li>
<li>the ground truth is B, and the predicted lable is B</li>
<li>the ground truth is B, but the predicted lable is A</li>
</ul>
<p>Often, we use a confusion matrix to contain all these conditions, as Table 1 shows.</p>
<h3 id="confusion-matrix">Confusion Matrix</h3>
<table>
<thead>
<tr>
<th>predicted\ground truth</th>
<th>P</th>
<th>N</th>
</tr>
</thead>
<tbody>
<tr>
<td>P</td>
<td>TP</td>
<td>FP</td>
</tr>
<tr>
<td>N</td>
<td>FN</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>Based on the confusion matrix, we can derive some useful metrics to measure model performance from various aspects.</p>
<h3 id="accuracy">Accuracy</h3>
<p>Accuracy tells us how well a classifier is in general by considering both positive and negative examples that are correctly classified.</p>
<p>$$
\text{Accuracy} = \frac{TP + TN}{ TP + FP + FN + TN}
$$</p>
<p>The drawbacks of accuracy are obvious. When we have imbalanced data where the number of negative examples is much more than positive examples, TN tends to be dominant, making no sense in most cases. For example, the number of O is more than the number of named entities in NER, resulting 99% accuracy. However, it does not indicate our classifier works well. By contrast, it may not be able to recognize any entity at all!</p>
<h3 id="precision">Precision</h3>
<p>Precision tells us how many true positive examples are in all predicted positive examples, which is calculated as follows,</p>
<p>$$
\text{ Precision } = \frac{TP}{ TP + FP}
$$</p>
<p>From the above equation, we can see that larger TP and smaller FP will lead to higher precision. That makes sense because more examples that belong to either the class of interest or the negative are identified correctly. (PS: FP is also known as Type-1 Error.)</p>
<p>Spam detection is a classical binary classification. It&rsquo;s fine if several spam emails are escaped from our classifier. However, if some non-spam (or ham) emails are identified as spam, that&rsquo;s really a stupid decision — we would miss some important emails! Similarly, precision are more important in recommender systems, where wrong recommendation would bring bad experience to customers.</p>
<h3 id="tprrecall">TPR/Recall</h3>
<p>Sometimes, we would prefer to identify all cases from the actual positive classes even at a cost of lower precision. That is, we seek a higher recall (sensitiviy, TPR), which is shown below.</p>
<p>$$
\text { Recall } = \frac{TP}{ TP +FN }
$$</p>
<p>For example, it does not cause much serious impacts if people who are not pregnant were diagnosed as pregnant. However, if women who are pregnant were not identified, well, you are in a big trouble. (PS: FN is also called Type-II Error.)</p>
<p>I&rsquo;m often confused with the two metrics. Which one should I focus more on ? Precision or recall?</p>
<ul>
<li>FP is a higher concern, then it&rsquo;s precision; otherwise, it&rsquo;s recall</li>
</ul>
<p>You can also consider in this way — whether the examples of interest are not identified (or <strong>recalled</strong>) result in little or no consequence,</p>
<ul>
<li>if yes, then it&rsquo;s precision; otherwise, it&rsquo;s recall.</li>
</ul>
<p>One or two spam emails in our inbox is acceptable but one neglected patient with illness can cause a big flu!</p>
<h3 id="specificity">Specificity</h3>
<p>Like Recall, we can also define specificity — out of all the negative classes, how much we predicted correctly.</p>
<p>$$
\text { Specificity } = \frac{TN}{ FP + TN }
$$</p>
<h3 id="f1-score">F1-Score</h3>
<p>F1-Score is the harmonic mean of precision and recall. F1-Score is preferable if we want to balance the precision and recall.</p>
<p>$$
\text {F1-score} = 2 * \frac{ \text {precision} * \text {recall} }{\text {precision} + \text {recall}}
$$
It is an effective metric in the following cases:</p>
<ul>
<li>FP and FN are equally cost</li>
<li>True Negative is high</li>
<li>Adding more data doesn&rsquo;t effectively change the outcome</li>
</ul>
<h3 id="roc-auc">ROC-AUC</h3>
<p>The ROC (Receiver Operator Characteristic) curve is plotted with TPR (True Positive Rate or Recall) against the FPR (False Positive Rate) at various threshold values, where TPR is on the y-axis and FPR is on the x-axis.</p>
<ul>
<li>A greater FPR means higher the number of FP than TN</li>
<li>A smaller TPR means higher the number of FN than TP</li>
</ul>
<p>Thus, the ROC curve depicts the trade-off between Type-I and Type-II Error.  Below are code snippets of the implementation of ROC, and the plot is shown in Figure 1.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ROC</span>(yp1, yp2):
    pmin <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>min(np<span style="color:#f92672">.</span>array( (np<span style="color:#f92672">.</span>min(yp1), np<span style="color:#f92672">.</span>min(yp2)) ))
    pmax <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(np<span style="color:#f92672">.</span>array( (np<span style="color:#f92672">.</span>max(yp1), np<span style="color:#f92672">.</span>max(yp2)) ))
    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;min: </span><span style="color:#e6db74">{</span>pmin<span style="color:#e6db74">}</span><span style="color:#e6db74">, max:</span><span style="color:#e6db74">{</span>pmax<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
    
    iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
    thRange <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(pmin, pmax, iters)
    ROC <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((iters, <span style="color:#ae81ff">2</span>)) 
    accuracy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(iters)
    
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(iters):
        threshold <span style="color:#f92672">=</span> thRange[i]
        
        <span style="color:#75715e"># C_2 is positive while C_1 is negative, and if y &gt; threshold, then it&#39;s positive</span>
        TP <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(yp2 <span style="color:#f92672">&gt;</span> threshold) <span style="color:#f92672">/</span> len(yp2)
        FP <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(yp1 <span style="color:#f92672">&gt;</span> threshold) <span style="color:#f92672">/</span> len(yp1)

        ROC[i, :] <span style="color:#f92672">=</span> [ TP, FP ]
        accuracy[i] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(yp2 <span style="color:#f92672">&gt;</span> threshold) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>sum(yp1 <span style="color:#f92672">&lt;</span> threshold)
        
<span style="color:#75715e">#     auc = -1 * np.trapz(ROC[:, 0], RPC[:, 1])</span>

    <span style="color:#66d9ef">return</span> ROC, accuracy

roc, accuracy <span style="color:#f92672">=</span> ROC(yp1, yp2)
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">6</span>)) 
ax<span style="color:#f92672">.</span>plot(roc[:,<span style="color:#ae81ff">1</span>], roc[:,<span style="color:#ae81ff">0</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;m&#39;</span>) <span style="color:#75715e"># FP vs TP</span>
ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;False Positive&#39;</span>) 
ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;True Positive&#39;</span>) 
<span style="color:#75715e"># ax.set_title(f&#39;ROC curve, AUC = {auc:.2f}&#39; )</span>
ax<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)

</code></pre></div><p>
  <figure>
    <img src="/blog/post/images/roc.png" alt="">
    <figcaption>Figure 1: The ROC Curve.</figcaption>
  </figure>
</p>
<p>AUC (Area Under Curve) is the area under the ROC curve. The value is between 0 and 1. The greater the AUC, the better is our classifier.</p>
<ul>
<li>
<p>If two classes are well separated, the auc is close to 1</p>
<p>
  <img src="/blog/post/images/roc=1.png" alt="">
</p>
</li>
<li>
<p>Usually, two classes overlap, so we get ROC like the one below</p>
<p>
  <img src="/blog/post/images/auc=0.6.png" alt="">
</p>
</li>
<li>
<p>If they overlap further so that the AUC is approximately 0.5, our model has no ability to distinguish them</p>
<p>
  <img src="/blog/post/images/auc=0.5.png" alt="">
</p>
</li>
<li>
<p>The worst case is that the model gives the contrary prediction as shown below</p>
<p>
  <img src="/blog/post/images/auc=0.png" alt="">
</p>
</li>
</ul>
<h2 id="multiclass">Multiclass</h2>
<p>In the case of the multiclass classification, we have the corresponding confusion matrix for each class based on the one-vs-all methodology, as Figure 2 shows. Then, there are two choices we&rsquo;re facing</p>
<ul>
<li>macro-averaging
<ul>
<li>combine all classes and average them</li>
<li>a more balanced overview</li>
</ul>
</li>
<li>micro-averaging
<ul>
<li>pool the corresponding group from each class</li>
<li>the result would be dominated by frequent classes</li>
</ul>
</li>
</ul>
<p>
  <figure>
    <img src="/blog/post/images/multiclass.png" alt="">
    <figcaption>Figure 2: Metrics for multiclass classification</figcaption>
  </figure>
</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://harshsinghal.dev/classification-metrics-every-data-scientist-must-know/">Classification Metrics Every Data Scientist Must Know</a></li>
<li><a href="https://lukeoakdenrayner.wordpress.com/2018/01/07/the-philosophical-argument-for-using-roc-curves/">The philosophical argument for using ROC curves</a></li>
</ul>



        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#metrics">Metrics</a>
      <ul>
        <li><a href="#confusion-matrix">Confusion Matrix</a></li>
        <li><a href="#accuracy">Accuracy</a></li>
        <li><a href="#precision">Precision</a></li>
        <li><a href="#tprrecall">TPR/Recall</a></li>
        <li><a href="#specificity">Specificity</a></li>
        <li><a href="#f1-score">F1-Score</a></li>
        <li><a href="#roc-auc">ROC-AUC</a></li>
      </ul>
    </li>
    <li><a href="#multiclass">Multiclass</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      
      </aside>
    </div>
    
    
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=xiaopan.wpp@outlook.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/ixiaopan" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/ixiaopan" name="Linkedin">
        <em class="fab fa-linkedin"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2023
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
