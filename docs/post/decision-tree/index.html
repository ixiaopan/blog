<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Decision Tree</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Decision Tree"/>
<meta name="twitter:description" content="The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as &#39;Is it a fiction movie? Is it directed by David Fincher?&#39;"/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script>

    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Decision Tree</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 19, 2021
  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
  <p>The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as &lsquo;Is it a fiction movie? Is it directed by David Fincher?&rsquo;</p>
<p>
  <figure>
    <img src="/blog/post/images/decision-tree-explain.png" alt="The process of movie selection">
    <figcaption>Figure 1: The process of movie selection</figcaption>
  </figure>

</p>
<p>From Figure 1, we can see that a decision tree builds a binary tree to partion the data. Each node is a decision rule based on a feature and the tree can grow endlessly. Two questions then arise:</p>
<ul>
<li>How is the decision rule made?</li>
<li>How deep is the decision tree?</li>
</ul>
<h2 id="decision-rule">Decision Rule</h2>
<p>Since there are so many features and each feature can have different values, we have to loop over all of them and find the best split. Well, how to measure the performance of a split? There are 3 measures widely used — Gini Impurity, Entropy, and Information Gain.</p>
<h3 id="gini-index">Gini Index</h3>
<p>Gini Index, also known as Gini Impurity, measures how often a randomly selected element from the set is classified incorrectly.</p>
<p>$$
Q_m^g(L) = \sum_{c \in C } p_c(L) (1 - p_c(L)) = 1 - \sum_{c \in C } p_c(L)^2
$$</p>
<p>where $L$ is short for the children node and $P_c(L)$ is the faction of class $c$ in the leaf node $L$,</p>
<p>$$
p_c(L) = \frac{1}{|L|} \sum_{x, y \in L} [y == c]
$$</p>
<p>where $[y == c] = 1$  if $y$ belongs to class $c$ and 0 otherwise.</p>
<p>
  <figure>
    <img src="/blog/post/images/gini-index.png" alt="Gini Index">
    <figcaption>Figure 2: The plot of Gini Index</figcaption>
  </figure>

</p>
<p>Figure 2 shows that the value of Gini Index is the lowest at the start and end of the x-axis and maximum at the middle of the x-axis. In other words,</p>
<ul>
<li>If the leaf node only has one class, then this node is a pure node and the gini impurity of this leaf node is 0.</li>
<li>On the other hand, if all elements in this leaf node belong to an individual class, then the Gini Index of this node has the maximum value of $ 1 - 1/len(L) $.</li>
</ul>
<h3 id="entropy">Entropy</h3>
<p>Entropy is a concept of Information Theory. Before introducing the entropy, we should have a little understanding of information.</p>
<p>Information is related to the surprise in some way. For example, if I told you that you will go to work tomorrow. Well, that&rsquo;s not surprising because you work every day. However, if I told you that tomorrow is the end of the world, you are likely to be shocked because it is an breaking news.</p>
<p>We can measure this surprise by the following equation</p>
<p>$$
I(x) = -log(p(x))
$$</p>
<p>If an event is unlikely to happen, then $p(x)$ is close to 0 and $I(x)$ tends to be infinity, which means it conveys much information since impossible things happened and it must have a significant implication behind it.</p>
<p>Then what&rsquo;s the entropy? The previous equation calculate the information contained in one outcome. However, it&rsquo;s quite often to have many outcomes for a random variable $X$. Therefore, the expected information over all outcomes is defined as the entropy.</p>
<p>$$
H(X) = E_{x \in X}[-log(p(x)]
$$</p>
<p>In this case, the classes in the leaf node is the random variable $X$ and the probability of each outcome is the fraction of each class, which is expressed as</p>
<p>$$
Q_m^e(L) = \sum_{c \in C}- p_c(L) log(p_c(L))
$$</p>
<p>In a word, entropy measures the randomness of a set. Lower entropy means a purer set while higher entropy means there are more other classes that is not the class $c$ in a set.</p>
<h3 id="information-gain">Information Gain</h3>
<p>Information Gain is simply the difference between the impurity of the parent node $A$ before splitting and the sum of impurity of all children nodes after splitting. It measures how much the impurity of a set $A$ were reduced after splitting. The larger the information gain is, the better the split is.</p>
<p>$$
IG(A) = H(A) - \sum_{L \in children \ nodes} p(L) H(L)
$$</p>
<p>In summary, when a decisiton tree makes a split on a feature, it tries to achieve,</p>
<ul>
<li>a lower impurity</li>
<li>a lower entropy</li>
<li>a higher information gain</li>
</ul>
<h2 id="cart">CART</h2>
<p>CART is short for classification and regression tree algorithm, which is used to train Decisioin Trees. CART tries to find the best split that produces purest subsets by calculating the weighted Gini Impurity on each split made by each feature $k$ from the parent node&rsquo;s training sample and corresponding threshold $t_k$.</p>
<p>Specifically, it tries to minimize the following loss function for a classification task,</p>
<p>$$
L(k, t_k) = \frac{|C_m^L|}{|N_m|}Q_m^G(C_m^L) + \frac{|C_m^R|}{|N_m|} Q_m^G(C_m^R)
$$</p>
<p>where $C_m^L$ and $C_m^R$ are children nodes splitted on node $N_m$.</p>
<p>For a regression task, it minimizes the sum of squred error</p>
<p>$$
L(k, t_k) = \frac{|C_m^L|}{|N_m|}SSE(C_m^L) + \frac{|C_m^R|}{|N_m|} SSE(C_m^R)
$$</p>
<p>where $SSE(subset)$ is defined as</p>
<p>$$
SSE(subset) = \sum_{n \in subset} (y_n - \overline y)^2
$$</p>
<p>$$
\overline y = \frac{1}{|subset|} \sum_{n \in subset} y_n
$$</p>
<h2 id="prunning">Prunning</h2>
<p>Now let&rsquo;s address the second problem, i.e. when to stop growing the tree. You can either stop growing the tree when you build the tree or trim the tree after building, which are known as pre-pruning and post-pruning respectively.</p>
<h3 id="pre-pruning">Pre-pruning</h3>
<p>Scikit-learn provides several hyperparameters to do a pre-pruning:</p>
<ul>
<li>the maximum depth</li>
<li>the minimum number of the samples a node must have to split</li>
<li>the minimum number of the samples in a leaf node</li>
<li>the maximum number of leaf nodes</li>
</ul>
<p>All of these parameters can be tuned with cross-validation.</p>
<h3 id="post-pruning">Post Pruning</h3>
<p>Though pre-pruning is straightforward, it is a bit short-sighted since it doesn&rsquo;t build a full tree and there might be some splits works better later on. Therefore, it would be better to have a large and full-size tree and then we trim some useless branches to get a better substree. Cost complexity pruning, also known as weakest link pruning, is one way to do this.</p>
<h2 id="pros-and-cons">Pros and Cons</h2>
<p>Advantages:</p>
<ul>
<li>Decision trees are simple and intutive to interpret, and we can easily visualize the process of decision making.</li>
<li>They can be used for both classification and regression.</li>
<li>There is no need to normalize or scale data.</li>
<li>They can help to understand what features are most important.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>They can be easy to overfit.</li>
<li>They are sensitive to variations of training data. If you rotate the same data, you will get a completely different tree because all splits are perpendicular to an axis.</li>
</ul>
<p>
  <figure>
    <img src="/blog/post/images/decision-tree-sensitve.png" alt="Decision Trees are sensitive">
    <figcaption>Figure 3: senstivity to variations of training set (Hands-on machine learning 2019)</figcaption>
  </figure>

</p>
<h2 id="references">References</h2>
<p>[1] A. Géron, <em>Hands-on machine learning with Scikit-Learn and TensorFlow</em>. Sebastopol (CA): O&rsquo;Reilly Media, 2019.</p>
<p>[2] <a href="https://www.kaggle.com/arunmohan003/pruning-decision-trees">pruning-decision-trees, Kaggle</a></p>
<p>[3] <a href="https://stats.stackexchange.com/questions/193538/how-to-choose-alpha-in-cost-complexity-pruning">how-to-choose-alpha-in-cost-complexity-pruning</a></p>
<p>[4] <a href="http://mlwiki.org/index.php/Cost-Complexity_Pruning">Cost-Complexity Pruning</a></p>



      
    </article>
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
