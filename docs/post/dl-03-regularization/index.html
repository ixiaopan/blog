<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Deep Learning - Regularization</title>



  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-203640627-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-203640627-1');
  </script>
  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning - Regularization"/>
<meta name="twitter:description" content="In the previous post, we introduced how to initialize weights. However, weights could be very large or small (great variance in weights). For larger weights, a tiny change in data will lead to large variance; For smaller weights, it has little influence on model, which can be totally discarded."/>




<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />
<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

 




  <link rel="stylesheet" href="https://ixiaopan.github.io/blog/css/main.css" />


<meta name="generator" content="Hugo 0.81.0" />
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/me.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


    
    <div class="intro-header">
      <div class="container">
        <div class="post-heading">
          
            <h1>
              Deep Learning - Regularization
            </h1>
          
          
            <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Nov 22, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;4 min read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/deep-learning/"
        >Deep Learning</a
      >&nbsp;
    
  
</span>

          
        </div>
      </div>
    </div>
    
  </header>

    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>In the previous post, we introduced how to initialize weights. However, weights could be very large or small (great variance in weights). For larger weights, a tiny change in data will lead to large variance; For smaller weights, it has little influence on model, which can be totally discarded.</p>
<p>Regularization is such an technique to set an upper threshold for weights, thereby producing a set of weights with smaller variance. Ridge (L2) and Lasso (L1) are two widely used regularization methods, which can help alleviate overfitting and produce stable computation. Apart from this, dropout is another technique widely used in Deep Learning. In this post, we will explain how regularization works in detail.</p>
<h2 id="regularization">Regularization</h2>
<h3 id="l2-ridge">L2 Ridge</h3>
<p>Take linear regression as an example, our goal is to minimize the following loss function with L2 regularization</p>
<p>$$
L_{min} = \sum_{i=0}^m (y_i - \bold x_i^t \bold w)^2 \\ \text { subject to } \sum_{j=1}^n \bold w_j^2 \le t
$$
PS: $w_0$ is exclude in L2 regularization. It depends (I am not sure, but pls be careful.)</p>
<p>Obviously, this is an inequality constraints optimization, and we can apply Lagrange to solve it,</p>
<p>$$
L = f(x) - \lambda g(x) \\ = \frac{1}{2m} \sum_{i=0}^m (y_i - \bold x_i^T \bold w)^2 + \lambda \sum_{j=1}^n \bold w_j^2 - \lambda t \\ = \frac{1}{2m}  (\bold X \bold w - \bold y)^T (\bold X \bold w - \bold y) + \lambda \bold w^T \bold w
$$</p>
<p>where $\lambda \ge 0$ according to KKT conditions. Take the derivative of $L$ w.r.t $\bold w$, we have</p>
<p>$$
\frac{\partial}{\partial \bold w} L = \frac{1}{m}  X^T(\bold X^T \bold w -\bold y) + \lambda \bold w
$$</p>
<p>The let $\frac{\partial}{\partial \bold w} L = 0$, we obtain an analytical solution to $\bold w$</p>
<p>$$
\bold w' = (\bold X^T \bold X + \lambda I)^{-1} \bold X^T \bold y
$$</p>
<ul>
<li>$\lambda = 0$, it&rsquo;s the OLS</li>
<li>$\lambda \rarr \infin$, $w' \rarr 0$</li>
</ul>
<h3 id="l1-lasso">L1 Lasso</h3>
<p>L1 is a bit different from L2. The regularization term is the sum of the absolute value of the weights, or L1 norm. Unfortunately, there is no closed-form solution, so we use gradient descent to find the estimate.</p>
<p>$$
L = (\bold X \bold w - \bold y)^T (\bold X \bold w - \bold y) + \lambda \sum_{d=1}^D |\theta_d|
$$</p>
<p>Take the derivative of $L$ w.r.t $w_k$</p>
<p>$$
\frac{\partial}{ w_k} L = - \sum_{i=0}^m (y_i - \sum_{j \ne k}^d x_{ij} w_j - w_kx_{ik}) x_{ik} \\ = - \sum_{i=0}^m (y_i - \sum_{j \ne k}^d x_{ij} w_j) x_{ik} + w_k \sum_{i=0}^m x_{ik}^2 \\ \ \\  \sim - \rho_k + w_k z_k
$$</p>
<p>where $z_k$ is the sum of the squared features of all the examples. If the data is normalized, $z_k = 1$</p>
<h3 id="intuition">Intuition</h3>
<p>Though L1 and L2 both shrink the weights, L1 can further reduce weights to $0$ to make feature selection automatically. Note that L2 is also known as weight decay because it encourages weight values to decay towards 0 in sequential learning.</p>
<p>
  <figure>
    <img src="/blog/post/images/lasso-ridge.png#full" alt="">
    <figcaption>Figure 1: Lasso Vs Ridge. The lasso gives a sparse solution.</figcaption>
  </figure>
</p>
<p>From the view of Lagrange, the solution is the intersection point between the loss function and the regularization term, as Figure 1 shows. We can see that L1 is spikier than L2 in high dimension. Thus, the ellipse of the loss function is more likely to touch the sharp points in L1.</p>
<h2 id="dropout">Dropout</h2>
<p>Dropout means that we randomly drop out hidden/visible units during training a neural network. The dropout rate is denoted by $p$. When a node is dropped out, all connections including incoming and ougoing connections related to it are removed from the network temporarily (in each epoch, so it could be kept during next epoch.)</p>
<p>For each layer $l$, there is a random variable $\bold r$ following Bernoulli distribution, i.e. $r \sim \text {Bern} (p)$. The value of $r_l$ is either 0 or 1, so with dropout, the forward process is shown below</p>
<p>$$
y_{l}' = r_{l} \odot y_{l} \\ \ \\ z_{l+1} = w_{l+1} y_l' + b_{l+1} \\ \ \\ y_{l+1} = \sigma(z_{l+1})
$$</p>
<p>
  <figure>
    <img src="/blog/post/images/dropout-network.png" alt="">
    <figcaption>Figure 2: Dropout network. Source from [3]</figcaption>
  </figure>
</p>
<p>When testing, we do not drop out nodes, but we need to scale the network by $1-p$ because we train a sub-network with its weights scaled with $1-p$. From the below equation, we see that the expected hidden value at training time is $(1-p)h_i$ for each input node. Similary, at test time, we hope the same expected value, so we multiply by $1-p$ . In practice, many deep learning libraries will rescale the network by multiplying $\frac{1}{1-p}$ during training phrase, so there is no need to scale the network again during testing.</p>
<p>$$
E(h_i) = (1-p) h_i + p * 0 = (1-p) h_i
$$</p>
<p>Intuitively, dropout is an ensemble method, we combine many different models to reduce variance and improve generalisation. Typically, $p$ is set to $0.5$ to achieve the best regularization.</p>
<h2 id="early-stopping">Early Stopping</h2>
<p>Early stopping is simple and efficient regularization technique, which stops training when the validation error stops consecutive decreasing, typically we observe 5 times.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">early_stop_num_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>():
  <span style="color:#f92672">...</span>
  <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">20</span>):
    <span style="color:#66d9ef">if</span> cur_loss <span style="color:#f92672">&lt;</span> prev_loss:
      epoch_no_improve <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
	<span style="color:#66d9ef">else</span>:
      epoch_no_improve <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
      
    <span style="color:#66d9ef">if</span> epoch_no_improve <span style="color:#f92672">&gt;=</span> early_stop_num_epoch:
      <span style="color:#66d9ef">break</span>
  <span style="color:#f92672">...</span>
</code></pre></div><h2 id="refer">Refer</h2>
<p>[1] <a href="https://xavierbourretsicotte.github.io/intro_ridge.html">Ridge regression and L2 regularization - Introduction</a></p>
<p>[2] <a href="https://stats.stackexchange.com/questions/322101/does-l2-normalization-of-ridge-regression-punish-intercept-if-not-how-to-solve">does-l2-normalization-of-ridge-regression-punish-intercept-if-not-how-to-solve</a></p>
<p>[3] <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">Dropout: a simple way to prevent neural networks from overfitting</a></p>
<p>[4] <a href="https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275">Simplified Math Behind Dropout</a></p>



        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#regularization">Regularization</a>
      <ul>
        <li><a href="#l2-ridge">L2 Ridge</a></li>
        <li><a href="#l1-lasso">L1 Lasso</a></li>
        <li><a href="#intuition">Intuition</a></li>
      </ul>
    </li>
    <li><a href="#dropout">Dropout</a></li>
    <li><a href="#early-stopping">Early Stopping</a></li>
    <li><a href="#refer">Refer</a></li>
  </ul>
</nav>
      
      </aside>
    </div>
    
    
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=xiaopan.wpp@outlook.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/ixiaopan" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/ixiaopan" name="Linkedin">
        <em class="fab fa-linkedin"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
