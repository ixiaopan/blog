<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Constrained Optimisation - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" />
  <meta name="description" content="When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&amp;rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key." />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/ml/constrained-optimisation/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Constrained Optimisation" />
<meta property="og:description" content="When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/ml/constrained-optimisation/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-06-03T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-06-03T00:00:00+00:00" />

<meta itemprop="name" content="Constrained Optimisation">
<meta itemprop="description" content="When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key."><meta itemprop="datePublished" content="2021-06-03T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-06-03T00:00:00+00:00" />
<meta itemprop="wordCount" content="3636">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Constrained Optimisation"/>
<meta name="twitter:description" content="When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Constrained Optimisation</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-06-03">
      2021-06-03
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/machine-learning/"> Machine Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I&rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key.</p>
<p>As we all know, the main effort in machine learning is to find a loss function and optimise it, i.e. find the minimum or maximum point, and this is the question of optimisation. However, we may only find local optimisation because of some constraints. Even without constraints, there is still a chance that we would reach local optimisation only. In short, there are two main situations we need to consider: unconstrained optimisation and constrained optimisation. And constrained optimisation further falls into two categories, equality constraints or inequality constraints.</p>
<p>On the other hand, the extreme value of a function typically relates to some property of that function. That means if we know a function has some particular property, then we know that it must have an extreme value or not. This property can be characterised by convexity. As you can see, this post will be very mathematical. Seems a bit scary, ummm&hellip;</p>
<h2 id="equality-constraints">Equality Constraints</h2>
<p>Suppose we want to minimise a function $f(x)$ subject to an equality constraint, $g(x) = 0$. This can be solved by introducing Lagrange multiplier $\alpha$</p>
<p>$$
\mathcal{L}(x, \alpha) = f(x) - \alpha g(x)
$$</p>
<p>Then set the gradient of $\mathcal{L}$ equal to the zero vector</p>
<p>$$
\nabla_x \mathcal{L} = \nabla_x f(x) - \alpha \nabla_xg(x) = 0
$$</p>
<p>$$
\frac{\partial \mathcal L}{\partial \alpha}  = -g(x) = 0
$$</p>
<p>Finally, solving the above equations will give us the minimum point we are seeking.</p>
<h3 id="multiple-constraints">Multiple Constraints</h3>
<p>If we have multiple constraints, then multiple Lagrange multipliers are introduced,</p>
<p>$$
\mathcal{L}(x, \alpha) = f(x) - \sum_i^m \alpha_i g_i(x)
$$
And the corresponding solutions are given by,</p>
<p>$$
\nabla_x f(x) = \sum_i^m \alpha_i \nabla_xg_i(x)
$$</p>
<p>$$
\frac{\partial \mathcal L}{\partial \alpha_i}  = -g_i(x) = 0
$$</p>
<h3 id="lagrange">Lagrange</h3>
<p>But what&rsquo;s the rationale behind these formulas? Though we are not required to know everything, basic understanding of Lagrange is still needed. Remember that the question is to find a point $x^*$ that satisfies both $g(x^*) = 0$ and $f(x^*) =m $, where $m$ is the minimum value of $f(x)$ given the constraint.</p>
<p>The line represented by $f(x) = c$ is known as a contour line. Since $f(x)$ can have many values, we can plot many contour lines with equal intervals between lines in ascending or descending order from inner to outer. Graphically, we want to find a point that lies on the line of $g(x) = 0$ and the line of $f(x)$ with the minimum value simultaneously as shown in Figure 1. The green point is the point we are looking for.</p>
<p><img src="/blog/post/images/lagrange.png" alt="" title="Figure 1: Illustration of equality constraints"></p>
<p>But we still need to figure out equations to calculate the position of the gree point. Let&rsquo;s start from $x_0$, the magenta point in Figure 1. Mathematically, the above process of finding $x^*$ can be described as follows,</p>
<p>$$
f(x_0 + \delta x) &lt; f(x_0)
$$</p>
<p>$$
g(x_0) = g(x_0 + \delta x) = 0
$$</p>
<p>So in which direction should we move at $x_0$? With the aid of Taylor expansion, we have
$$
g(x_0 + \delta x) = g(x_1) = g(x_0) + (x_1 - x_0)^T\nabla_xg(x_0) + \frac{1}{2} (x_1 - x_0)^T H (x_1 - x_0)
$$</p>
<p>$$
= g(x_0) + (x_1 - x_0)^T\nabla_xg(x_0) + O(||x_1 - x_0||^2)
$$</p>
<p>where $x_1 = x_0 + \delta x$ and $H$ is a matrix of second derivative of $g(x)$ known as Hessian. The third term tends to be zero if $\delta x$ is small enough, then we are left with</p>
<p>$$
g(x_0 + \delta x) = g(x_0) + (x_1 - x_0)^T\nabla_xg(x_0) = g(x_0)
$$</p>
<p>Thus,</p>
<p>$$
(x_1 - x_0)^T\nabla_xg(x_0) = 0
$$</p>
<p>which means that the moving direction from $x_0$ should be perpedicular to $\nabla_xg(x_0)$. But we are not done, because not all $(\delta x = x_1 - x_0)$ point to the right direction along which $f(x)$ decreases. In order to ensure this, we require that  $\delta x$ must satisfy</p>
<p>$$
(x_1 - x_0)^T ( - \nabla_x f(x)) &gt; 0
$$</p>
<p>where $- \nabla_x f(x)$ indicates the descent direction. Therefore, as long as the value of the dot product is greater than zero, the moving of point will continue unless the value becomes zero. If so, it means that the direction of $\nabla_x f(x)$ is parallel to $\nabla_x g(x)$, i.e.</p>
<p>$$
-\nabla_x f(x) = \lambda \nabla_x g(x)
$$</p>
<p>We can rewrite this by replacing $\lambda$ with $\alpha = -\lambda $</p>
<p>$$
\nabla_x f(x) = \alpha \nabla_x g(x)
$$</p>
<p>Finally, we come to the method of Lagrange multipliers.</p>
<h2 id="inequality-constraints">Inequality Constraints</h2>
<p>Now we consider another situation where we have inequality constraints, i.e. $g(x) \ge 0$. It looks a little complicated, but if we think about it for a while, we can find that only two things could happen, as shown in Figure 2,</p>
<ul>
<li>either the optimum point satisfies $g(x) \gt 0$ or</li>
<li>the (local) optimum point lies on the boundary, $g(x) = 0$</li>
</ul>
<p><img src="/blog/post/images/inequality.png#full" alt="" title="Figure 2: Two possible outcomes of inequality constraints"></p>
<p>Again, we use Lagrange to solve it,</p>
<p>$$
\mathcal{L}(x, \alpha) = f(x) - \alpha g(x)
$$</p>
<h3 id="case-1">Case 1</h3>
<p>In the first case where the optimum point lies in the interior of the constraint, i.e. $g(x) &gt; 0$, which is filled in red in Figure 2 a). We set $\alpha = 0$, which means the constrains has no influence on $f(x)$.</p>
<h3 id="case-2">Case 2</h3>
<p>As for the second case, it is exactly the same as the equality constraints, i.e.</p>
<p>$$
\nabla_x f(x) = \alpha \nabla_xg(x)
$$</p>
<p>but with an additional constraint $\alpha &gt; 0$. So why do we set $\alpha &gt; 0$ here?</p>
<h3 id="alpha-ge-0">$\alpha \ge 0$</h3>
<p>Visually, it can be seen From Figure 2 b) that both the magenta and blue points seem to be the right point we are seeking. But in fact, only the bule one is in a lower position. And we find that $\nabla_x f(x) $ and $\nabla_x g(x)$ point to the same direction at the blue point. Thus, $\alpha$ is positive.</p>
<p>In theory, if we are at a point where $-\nabla_x f(x)$ points to the feasible region, which is the area defined by $g(x) &gt; 0$, i.e. any value of $x$ inside this region is valid, it means that a point with a smaller value of $f(x)$ could be found in the feasible region. But it contradicts the assumption that we can only move along the boundary of the region. In other words, this is not the optimal point.</p>
<p>If $-\nabla_x f(x)$ at some point points to the exterior of the feasible region, then we are in the right position because the outer of the feasible region is invalid and we cannot move forward any further(we are already on the border of the region).</p>
<p>It is noticeable that $\nabla_x g(x)$ points in towards the feasible region. Therefore, we conclude that $\nabla_x f(x)$ and $\nabla_x g(x)$ have the same direction. Thus, $\alpha$ is positive.</p>
<p>From above, we can also draw another conclusion shown below</p>
<p>$$
\alpha g(x) = 0
$$</p>
<h3 id="kkt-conditions">KKT Conditions</h3>
<p>Putting it together, we want to minimise $f(x)$ subject $g(x) \ge 0$, and the Lagrangian function is defined as follows,</p>
<p>$$
\mathcal{L}(x, \alpha) = f(x) - \alpha g(x)
$$</p>
<p>Then we can find a local mininum $x^*$ s.t.</p>
<ul>
<li>$\nabla_x f(x) = \alpha \nabla_xg(x)$</li>
<li>$\alpha \ge 0$
<ul>
<li>$\alpha = 0$, the solution is in the interior or</li>
<li>$\alpha \gt 0$ and $g(x) = 0$, i.e. the solution is on the boundary</li>
</ul>
</li>
<li>$\alpha g(x) = 0$</li>
</ul>
<p>These are the Karush-Kuhn-Tucker (KKT) conditions.</p>
<h3 id="many-inequalities">Many Inequalities</h3>
<p>Once again, if we have many ineuqality constraints, then Lagrangian function is given by,</p>
<p>$$
\mathcal{L}(x, \alpha) = f(x) - \sum_i^m \alpha_i g_i(x)
$$</p>
<p>And the corresponding solutions are given by,</p>
<p>$$
\nabla_x f(x) = \sum_i^m \alpha_i \nabla_xg_i(x)
$$</p>
<p>plus the constraints that</p>
<ul>
<li>either $\alpha_i = 0$ or</li>
<li>$\alpha_i \gt 0$ and $g_i(x) = 0$</li>
<li>$\alpha_i \nabla_x g_i(x) = 0$</li>
</ul>
<h2 id="duality">Duality</h2>
<p>Let&rsquo;s revisit the above problem again. Consider minimising a function $f(x)$ subject to $g(x) \ge 0$, the lagrangian function is given by</p>
<p>$$
\mathcal{L}(x, \alpha) = f(x) - \alpha g(x)
$$</p>
<h3 id="primal-problem">Primal Problem</h3>
<p>Now consider maximising $\mathcal{L}(x, \alpha)$ w.r.t $\alpha$,</p>
<p>$$
\text{max}_\alpha  \mathcal{L}(x, \alpha) = \begin{cases} f(x) &amp; \text{if $g(x) \ge 0$} \\ \infin &amp; \text{otherwise}\end{cases}
$$</p>
<ul>
<li>
<p>For any $x'$ that satisfies the constraint $g(x) \ge 0$, we conclude that</p>
<p>$$
f(x') \ge \mathcal{L}(x', \alpha)
$$</p>
<p>i.e. the upper bound of  $\mathcal{L}(x, \alpha)$ is $f(x)$. Thus, the maximum value of $\mathcal{L}$ we can obtain is to set $\alpha = 0$</p>
</li>
<li>
<p>On the contratry, if the constraint is not satisfied, then there exists some $x'$ that satisfies $g(x) \lt 0$. If so, then we can make $\mathcal{L}$ infinite by taking $\alpha \rarr \infin$.</p>
</li>
</ul>
<p>Next we take the minimum of the maximum of $\mathcal{L}$, i.e.</p>
<p>$$
\text{min}_\text{x} \text{max}_\alpha \mathcal{L}(x, \alpha)
$$</p>
<p>which is equivalent to the problem of minimising $f(x)$ subject to $g(x) \ge 0$. We call the original problem as <strong>primal problem</strong>.</p>
<h3 id="dual-problem">Dual Problem</h3>
<p>Yet we still don&rsquo;t find a solution to $x$. How about reversing the order of max and min like the below formula?</p>
<p>$$
\text{max}_\alpha \text{min}_\text{x} \mathcal{L}(x, \alpha)
$$</p>
<p>We solve $\text{min}_\text{x} \mathcal{L}(x, \alpha)$ using the method of Lagrange, and find that $x$ is a function of $\alpha$. Then we plug $x$ into $ \mathcal{L}(x, \alpha)$ and obtain a new function of $\alpha$, say $h(\alpha)$. So the problem becomes to maximise $h(\alpha$), also known as <strong>dual problem</strong>,</p>
<p>$$
\text{max}_\alpha  h(\alpha)
$$</p>
<p>However, is the solution to dual problem the same as the primal problem? Why do we bother solving a dual problem rather than the original problem?</p>
<p>Since $\alpha \ge 0$  and $g(x) \ge 0$, we have,</p>
<p>$$
\text{min}_\text{x} \mathcal{L}(x, \alpha) \le \text{min}_x f(x) = p^*
$$</p>
<p>$$
d^* = \text{max}_\alpha \text{min}_\text{x} \mathcal{L}(x, \alpha) \le p^*
$$</p>
<p>where $p^*$ and $d^*$ are the optima of the primal and dual problem respectively.</p>
<p>It can be seen that the solution of dual problem gives us a lower bound on the primal  problem. If possible, we can also have the same solution.</p>
<p>The reason why we solve the dual problem is that the primal problem works in a feature space that may have high dimensions while the dual problems depends on the number of constraints, which is much smaller than the dimensitionality of $x$.</p>
<h3 id="linear-programming">Linear Programming</h3>
<p>In linear programming, we minimise a linear function $c^Tx$ subject to a series of linear constraints $g(x) = Mx - b \ge 0$. The primal problem is described as follows,</p>
<p>$$
\mathcal{L} (x, \alpha) =  c^Tx - \alpha^T(Mx - b)
$$</p>
<p>$$
\text{minimise } c^T x
$$</p>
<p>$$
\text{subject to } Mx \ge b
$$</p>
<p>and the dual problem is defined as,</p>
<p>$$
\mathcal{L} (x, \alpha) =  b^T\alpha - x^T(M^T \alpha - c)
$$</p>
<p>$$
\text{maximise } b^T \alpha
$$</p>
<p>$$
\text{subject to } M^T \alpha\le c
$$</p>
<p>Let&rsquo;s see and example.</p>
<p>Primal problem</p>
<p>$$
\text{minimise } z = 15x_1 + 12x_2\\ \text{subject to } x_1 + 2x_2 \ge 3, 2x_1 - 4 x_2 \ge 5
$$</p>
<p>Dual problem</p>
<p>$$
\text{maximise } w = 3y_1 + 5y_2\\ \text{subject to } y_1 + 2y_2 \le 15, 2y_1 - 4 y_2 \le 12
$$</p>
<h3 id="quadratic-programming">Quadratic Programming</h3>
<p>In quadratic programming, we minimise a quadratic function $x^TQx$ subject to a series of linear constraints $g(x) = Mx - b \ge 0$. The primal problem is described as follows,</p>
<p>$$
\text{max}_\alpha  \text{min}_\text{x} \mathcal{L} (x, \alpha) =  x^TQx - \alpha^T(Mx - b)
$$</p>
<p>Using the method of Lagrange, we have $x^* = \frac{1}{2}Q^{-1}M^T\alpha$, and we substitue it into $\mathcal{L}$</p>
<p>$$
\text{max}_\alpha  -\frac{1}{4} \alpha^TMQ^{-1}M^T\alpha + \alpha^Tb
$$</p>
<h2 id="convexity">Convexity</h2>
<h3 id="quadratic-form">Quadratic Form</h3>
<blockquote>
<p>Quadratic form is a polynominal function with terms all of degree of two. For example, $4x^2 + 2xy - 3y^2$.  — Wikipedia</p>
</blockquote>
<p>Here &ldquo;the degree of two&rdquo; means that the sum of exponents for each term is 2. A general quadratic form of $n$ variables is defined below, where $M$ could be chosen symmetric.</p>
<p>$$
Q(\bold  x) = \bold x^T \bold  M \bold x = \sum_{i,j}^d \bold  M_{ij} \bold x_i \bold x_j
$$</p>
<p>In this example $4x^2 + 2xy - 3y^2$, the quadratic form is given by,</p>
<p>$$
Q(\bold x) = \bold x^TM\bold x = \displaystyle{\begin{bmatrix}x&amp;y\end{bmatrix}
\begin{bmatrix}4&amp;1\\1&amp;-3 \end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}
}
$$</p>
<p>Basically, quadratic form is a mapping from $R^d$ to $R$. Without doubtness, for any quadratic form, we have $Q(\bold 0) = 0$. But is $x=\bold 0$ is the minimum or maximum point for $Q$ ? The answer is determined by the <strong>definiteness</strong> of $Q$ described below,</p>
<ul>
<li>$Q(\bold x) \gt0$,
<ul>
<li>positive definite</li>
</ul>
</li>
<li>$Q(\bold x) \ge 0$,
<ul>
<li>positive semi-definite</li>
</ul>
</li>
<li>$Q(\bold x) \lt 0$ ,
<ul>
<li>negative definite</li>
</ul>
</li>
<li>$Q(\bold x) \le 0$,
<ul>
<li>negative semi-definite</li>
</ul>
</li>
<li>$Q(\bold x)$ could be both positive and negative
<ul>
<li>indefinite</li>
</ul>
</li>
</ul>
<p>Clearly,</p>
<ul>
<li>if $Q$ is positive definite, then $x = 0$ is global minimum</li>
<li>if $Q$  is negative definite, then $x = 0$ is global maximum</li>
</ul>
<p>Furthermore, quadratic form can also be characterised in terms of eigenvalues:</p>
<ul>
<li>positive definite if and only if the eigenvalues of $M$ are positive,</li>
<li>negative definite if and only if the eigenvalues of $M$ are negative,</li>
<li>indefinite if and only if $M$ has bothe positive and negative eigenvalues</li>
</ul>
<p>Here are some proofs. First, we should know that any two eigenvectors from different eigenspaces of a symmetric matrix are orthogonal and any symmetric matrix can be orthogonally diagonalizable.</p>
<p>Let $\bold P = [\bold v1, \bold v2, &hellip;, \bold v_n]$ be eigenvectors that correspond to different eigenvalues $\Lambda= \lambda_1, \lambda_2, &hellip;, \lambda_n$ of a symmetric matrix $A$. To show that $v_1 \cdot v_2 = 0$, compute</p>
<p>$$
\lambda_1 v_1 \cdot v_2 = (A v_1)^T v_2 = v_1^T A v_2 = \lambda_2 v_1^Tv_2
$$</p>
<p>$$
(\lambda_1 - \lambda_2) v_1^T v_2 = 0
$$</p>
<p>But $ \lambda_1 \ne \lambda_2$, so $v1 \cdot v2=0$. Furthermore, $\bold P ^T \bold P = I$, so $P^{-1} = P^T$. Then we have</p>
<p>$$
AP = PD\\A = PDP^{-1} = PDP^T
$$</p>
<p>The quadratic form of $A$ can be written as follows,</p>
<p>$$
x^T A x= x^T PDP^Tx = (P^Tx)^T D (P^Tx) = y^TDy = \lambda_1y_1^2 + \lambda_2y_2^2 + &hellip; + \lambda_ny_n^2
$$</p>
<p>If the eigenvalues of $A$ are positive, then $x^TAx &gt; 0$ and $A$ is positive definite. On the other hand, if $A$ is positive definite, then $x^TAx &gt; 0$ for any $x \ne 0$. If we substitute $x$ with an eigenvector $v_1$, we have</p>
<p>$$
v_1^T A v_1 = v_1^T \lambda v_1 = \lambda v_1^Tv_1 = \lambda ||v_1||^2 \gt 0
$$</p>
<p>Thus, $\lambda  &gt; 0$.</p>
<h3 id="convex-region">Convex Region</h3>
<p>Before we talk about convex function, let&rsquo;s start with convex region and convex set.</p>
<p>A region $R$  is said to be a convex region if any two points $x$ and $y$ in that region plus any $a \in [0, 1]$ satisfy,</p>
<p>$$
z = a x + ( 1 - a )y \in R
$$</p>
<p><img src="/blog/post/images/convex-region.png" alt="" title="Figure 3: Convex region and non-convex region"></p>
<h3 id="convex-set">Convex Set</h3>
<p>Similarly, for any set of points $S$, if for any two points $x, y \in S$ and any $a \in [0, 1]$ satisfy
$$
z = a x + ( 1 - a )y \in S
$$
then $S$ is a convex set. We can prove that the set of positive semi-definite matrices form a convex set. Let $A_1, A_2 \in S$, compute</p>
<p>$$
x^T z x = x^T (a A_1 + ( 1 - a)A_2)x = x^TaA_1x + x^T ( 1-a) A_2 x \ge 0
$$</p>
<p>Thus, $z$ is positive semi-definite and $z \in S$.</p>
<h3 id="convex-function">Convex Function</h3>
<p>Any function is said to be <strong>convex</strong> if any two points $x$ and $y$ plus $a \in [0, 1]$ satisfy</p>
<p>$$
af(x) + (1 - a) f(y) \ge f(a x + (1 -a ) y)
$$</p>
<p><img src="/blog/post/images/convex-func.png" alt="" title="Figure 4: Convex function"></p>
<p>Conversely, if the condition doesn&rsquo;t meet, the function then is said to be a <strong>convex-down or concave</strong> function. These two functions are symmetric — everything true for convex functions is also true for concave functions.</p>
<p>At the beginning, I often mixed up them. I couldn&rsquo;t tell which figure is convex or concave. But later, I found a simple way to distinguish them correctly. First we find the lowest point or the highest point of a figure. Then we observe the direction along which the curve expands. If the direction is toward down, the function is concave. Otherwise, it&rsquo;s convex. By the way, the area enclosed by the curve (lies on or above the curve ) is defined as <strong>epigraph</strong>. The epigraph of a convex function forms a convex region, and if the epigraph of a function forms a convex region then the function is convex.</p>
<h4 id="linear-functions">Linear Functions</h4>
<p>Now let&rsquo;s take a look at a special case — equality. A function that we are quite familar with satisfies the equality, which is linear function.</p>
<p>$$
f(x) = mx + c
$$
It&rsquo;s easy to proove it.</p>
<p>$$
m(ax + (1 - a)y) + c = max + my - may + c = af(x) - ac  + my(1 -a) + c
\\= af(x) + my(1-a) + c(1-a) = af(x) + (1-a)f(y)
$$</p>
<p>So is it a convex or concave function? The answer is both.</p>
<h4 id="strictly-convex">Strictly Convex</h4>
<p>What about the condition without euqality? Well, such a condition is called strict inequality and functions that satisfy the strict inequality is said to be <strong>strictly convex/concave</strong>.</p>
<h4 id="sums-of-convex-functions">Sums of Convex functions</h4>
<p>If we have a set of convex functions, then it&rsquo;s easy to prove that the sum of the multiplication of positive factors and these functions is also a convex function using the property that the second derivative is equal or greater than zero. Below is the proof.</p>
<p>$$
g(x) = \sum_i \alpha_i f_i(x)
$$</p>
<p>$$
g''(x) = \sum_i \alpha_i f''_i(x) \ge 0
$$</p>
<h3 id="second-derivative">Second derivative</h3>
<p>One thing we should remember is that any tangent line of a convex funtion lies on or below the function. Let $t, z$ be two points on the graph of a convex function, then we can derive the following conditions,</p>
<ul>
<li>$ t  \lt z$</li>
</ul>
<p>$$
f(t) + f'(t) (z - t)\le f(z)
$$</p>
<p>$$
f'(t) \le \frac{f(z) - f(t)}{z - t}
$$</p>
<ul>
<li>$ t  \gt z$</li>
</ul>
<p>$$
f(t) - f'(t)(t - z) \le f(z)
$$</p>
<p>$$
\frac{f(t) - f(z)}{(t - z)}  \le f'(t)
$$</p>
<p>The two conditions can be combined as a single condition for any two points $a, b$ that satisfies $a &lt; b$ as follows,</p>
<p>$$
f'(a) \le \frac{f(a) - f(b)}{b-a}  \le f'(b)
$$</p>
<p>Hence, $f''(a) \ge 0$.</p>
<p>In high dimension, the second derivative of a function is known as Hessian. And a necessary and sufficient condition for that function to be convex is that its Hesssian must be positive semi-positive at all points.</p>
<h3 id="unique-minimum">Unique Minimum</h3>
<p>As said early, convexity can help to find the extreme value of a function. How does it work? Let $x^*$ be a local minimum of a function, suppose there exists another points $\hat x$ such that $f(\hat x) &lt; f(x^*)$. By the definition of convexity, we have</p>
<p>$$
f(a \hat x + (1-a)x^*) \le af(\hat x) + (1-a) f(x^*) \le af(x^*) + (1-a) f(x^*) = f(x^*)
$$</p>
<p>If we set $ a \rarr 0 $, it means that there exist points around $x^*$ with a smaller value than $f(x*)$, which is a contradiction to the definition of local minimum.</p>
<p>Thus, we can see that any local minimum of a convex funtion is a global minimum. Besides,</p>
<ul>
<li>there could be many local minimum for a convex function. In other words, the minimum of a convex function will form a convex set.</li>
<li>a strictly convex function has at most one global minimum</li>
</ul>
<p>Putting it together, the whole process of determining whether a function would have a minimum is shown in Figure 5</p>
<p><img src="/blog/post/images/find-minimum.png#full" alt="" title="Figure 5: Using Hessian to determine whether a function is a convex function "></p>
<h3 id="inverse-of-convex-funtions">Inverse of Convex Funtions</h3>
<p>Let $f(x)$ be a convex function, how about the convexity of the inverse of it, i.e. $g(x) = f^{-1}(x)$?</p>
<p>First, the second derivative of a composite function is given by</p>
<p>$$
\frac{d^2f(g(x))}{dx^2} = \frac{d f'(g(x)) g'(x)}{dx} = f''(g(x)) [g'(x)]^2 + f'(x) g''(x)
$$</p>
<p>Besides, if $f^{-1}(x)$ is the invese of $(x)$, we have</p>
<p>$$
f(f^{-1}(x)) = x
$$</p>
<p>$$
f''(f^{-1}(x)) = 0
$$</p>
<p>Thus, we conclude that</p>
<p>$$
g''(x) = - \frac{f''(g(x)) [g'(x)]^2}{f'(g(x))}
$$</p>
<p>Since $f''(x) \ge 0$ and $[g'(x)]^2 \ge 0$, the sign of $g''(x)$ is determined by $-f'(g(x))$.</p>
<p>Here is an example. Let $f(x) = x^2$ , so that $f''(x) = 2 \gt 0$ and $f'(x) = 2x$. Since  $g(y) = f^{-1}(y) = \sqrt y \ge 0$, $f'(g(x)) \ge 0$ and consequently $\sqrt x$ is concave.</p>
<h2 id="jensens-inequality">Jensen’s inequality</h2>
<p>Jensen’s inequality involves inequality of convex function, which states that for any convex function $f(x)$,</p>
<p>$$
E[f(x)] \ge f(E[x])
$$</p>
<p>and for any concave function,</p>
<p>$$
E[f(x)] \le f(E[x])
$$</p>
<p>It&rsquo;s easy to prove using the fact that a convex function must lie on or above its tangent line at any point $x'$</p>
<p>$$
f(\hat x) \ge f(x') + (\hat x - x')^T \nabla f(x)
$$</p>
<p>so this is true for $x' = E[x]$</p>
<p>$$
f(\hat x) \ge f(E[x]) + (\hat x - E[x])^T \nabla f(x)
$$</p>
<p>then taking expectations of both sides</p>
<p>$$
E[f(\hat x)] \ge f(E[x]) + (E[\hat x] - E[x])^T \nabla f(x) = f(E[x])
$$</p>
<p>A typical example is $f(x) = x^2$, Jensen&rsquo;s inequality shows that</p>
<p>$$
E[x^2] - E^2[x] \ge 0
$$</p>
<p>Well, it&rsquo;s the formula of variance, and variance are non-negative.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Finally, we&rsquo;re here. I spent several days writing up this article. To be honest, I am not a math person, and it was a struggle to explain these mathematical concepts and formulas clearly and accurately. But in doing so, I had a better understanding about optimisation. But knowing these equations only is not enough, the key point is to learn to apply them in machine learning to solve real-world problems. Anyway, we are done for now.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf">https://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf</a></li>
<li><a href="https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf">https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf</a></li>
</ul>

        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/ml/information-theory/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Information Theory</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/ml/probabilistic-model/">
                <span class="next-text nav-default">Probabilistic Model</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#equality-constraints">Equality Constraints</a>
      <ul>
        <li><a href="#multiple-constraints">Multiple Constraints</a></li>
        <li><a href="#lagrange">Lagrange</a></li>
      </ul>
    </li>
    <li><a href="#inequality-constraints">Inequality Constraints</a>
      <ul>
        <li><a href="#case-1">Case 1</a></li>
        <li><a href="#case-2">Case 2</a></li>
        <li><a href="#alpha-ge-0">$\alpha \ge 0$</a></li>
        <li><a href="#kkt-conditions">KKT Conditions</a></li>
        <li><a href="#many-inequalities">Many Inequalities</a></li>
      </ul>
    </li>
    <li><a href="#duality">Duality</a>
      <ul>
        <li><a href="#primal-problem">Primal Problem</a></li>
        <li><a href="#dual-problem">Dual Problem</a></li>
        <li><a href="#linear-programming">Linear Programming</a></li>
        <li><a href="#quadratic-programming">Quadratic Programming</a></li>
      </ul>
    </li>
    <li><a href="#convexity">Convexity</a>
      <ul>
        <li><a href="#quadratic-form">Quadratic Form</a></li>
        <li><a href="#convex-region">Convex Region</a></li>
        <li><a href="#convex-set">Convex Set</a></li>
        <li><a href="#convex-function">Convex Function</a></li>
        <li><a href="#second-derivative">Second derivative</a></li>
        <li><a href="#unique-minimum">Unique Minimum</a></li>
        <li><a href="#inverse-of-convex-funtions">Inverse of Convex Funtions</a></li>
      </ul>
    </li>
    <li><a href="#jensens-inequality">Jensen’s inequality</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.97e3349af25266740f15f60ba3a52fa1c247c87c6b73b9bca9290ec5c41c7cbe.js" integrity="sha256-l&#43;M0mvJSZnQPFfYLo6UvocJHyHxrc7m8qSkOxcQcfL4=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
