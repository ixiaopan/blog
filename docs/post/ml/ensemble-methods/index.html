<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Ensemble Methods - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" /><meta name="description" content="Ensemble means a group of people or a collection of things. Ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods usually defeat other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting." />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/ml/ensemble-methods/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Ensemble Methods" />
<meta property="og:description" content="Ensemble means a group of people or a collection of things. Ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods usually defeat other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/ml/ensemble-methods/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-04-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-04-20T00:00:00+00:00" />

<meta itemprop="name" content="Ensemble Methods">
<meta itemprop="description" content="Ensemble means a group of people or a collection of things. Ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods usually defeat other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting."><meta itemprop="datePublished" content="2021-04-20T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-04-20T00:00:00+00:00" />
<meta itemprop="wordCount" content="2415">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ensemble Methods"/>
<meta name="twitter:description" content="Ensemble means a group of people or a collection of things. Ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods usually defeat other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Ensemble Methods</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-04-20">
      2021-04-20
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/machine-learning/"> Machine Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.</p>
<h2 id="overview">Overview</h2>
<p>In real life, we often take advices from others. For example, suppose you want to know whether a movie is worthwhile to watch, you may ask your friends who&rsquo;ve watched to give the movie a score(out of 5), say <code>3, 3, 2, 2, 2, 4</code>. Since 5 people gave a score that is lower than 4, you may think about choosing another movie, let&rsquo;s say Avatar. Then you ask your friends again and have some scores like this <code>5, 5, 5, 5, 5</code>. Wow! All of your friends think that Avatar is an amazing movie that should certainly not be missed. You agree with their opinons and decide to watch Avatar finally. From this example, we can see that <strong>gathering plenty of opinions from different people</strong> are likely to make an informed decision.</p>
<p>Here, I highlight the words <strong>different people</strong>. It makes little sense if we only asks for people who have the same interests. Therefore, the more diverse the people are, the more sensible our decisions are. Basically, the idea behind it is the wisdom of collaborating.</p>
<h2 id="voting">Voting</h2>
<p>The method used to decide whether to watch a movie or not in this example is known as <strong>voting</strong>. For classification, there are 2 types of voting named <strong>hard voting</strong> and <strong>soft voting</strong>.</p>
<ul>
<li>Hard voting returns the most popular class shown in Figure 1.</li>
<li>Soft voting averages the probability of each class and then return the class that has the maximum probability.</li>
</ul>
<h3 id="hard-voting">Hard Voting</h3>
<p><img src="/blog/post/images/ensemble-voting.png" alt="Ensemble Voting" title="Figure 1: Hard voting classifier predicitons (Hands-on machine learning, 2019)"></p>
<p>Figure 1 can also be illustrated  in a mathematical way,</p>
<p>$$
y' = mode(C_1(x), C_2(x), &hellip;, C_n(x))
$$</p>
<p>For example, <code>{0, 1, 0, 1, 1}</code> are the class labels predicted by our 5 different classifiers for a data point $x$. By hard voting, the final class label is <code>class 1</code> .</p>
<pre tabindex="0"><code>C1 -&gt; 0
C2 -&gt; 1
C3 -&gt; 0
C4 -&gt; 1
C5 -&gt; 1
</code></pre><h3 id="weighted-hard-voting">Weighted Hard Voting</h3>
<p>Hard voting works nice, but in some cases, some people might be more professional than others. Hence, their opinions are much more significant. How to distinguish professionals and common people?</p>
<p>We assign weights to them. Specifically, we assign higher weights to professionals while common people have lower weights. Then we calculate weighted sum of occurrence of each class label and find the class label that has the maximum value.</p>
<p>$$
y' = \operatorname*{argmax}_i w_j\sum_j [C_j == i]
$$</p>
<p>where $[C_j == i] = 1$ if classifier $j$ predicts class label i and 0 otherwise.</p>
<p>For example, if we assign the following weights to the previous 5 classifiers, then we will have <code>0.7</code> for class 0 and<code>0.5</code> for class 1. Thus, <code>class 0</code> wins because 0.7 is greater than 0.5.</p>
<pre tabindex="0"><code>0.4, C1 -&gt; 0
0.1, C2 -&gt; 1
0.3, C3 -&gt; 0
0.2, C4 -&gt; 1
0.2, C5 -&gt; 1
</code></pre><h3 id="soft-voting">Soft Voting</h3>
<p>Instead of predicting the class label directly, some classifiers like logistic regression can predict the probability of each class label that $x$ belongs to. Then we simply average these probabilities for each class label. Certainly, you can assign weights to classifiers.</p>
<p>$$
y' = \operatorname*{argmax}<em>i \frac{1}{n} \sum_j^n w_j p</em>{ij}
$$</p>
<p>where $p_{ij}$ is the probability of class label $i$ that $x$ belongs to when using classifier $C_j$.</p>
<h3 id="average-for-regression">Average for Regression</h3>
<p>We simply <strong>average the predictions of different machines</strong> for a regression task.</p>
<p>$$
y' = \frac{1}{n} \sum_j^n w_j C_j
$$</p>
<h2 id="bagging">Bagging</h2>
<p>In order to make our models different from each other, we use various algorithms to train the same data, as discussed above. Another way to have a set of diverse models is to train the same model on different data sets. But usually we only have one training data set. Where do other data sets come from? Well, they are sampled with replacement from the original data set, which is known as <strong>bootstrapping</strong>.</p>
<p><img src="/blog/post/images/bootstrap.png" alt="The process of bagging" title="Figure 2: The process of bagging  (Hands-on machine learning, 2019)"></p>
<p>Specifically, given a training data set $D=(x_i, y_i)_i^n$ of the size $N$, we build an ensemble model of size $m$ according to the following steps:</p>
<ul>
<li>
<p>For $i=1, 2, 3, &hellip;, m$</p>
<ul>
<li>draw $N'(N' \le N)$ samples with replacement from $D$, which is denoted by $D^*_i$</li>
<li>build a model (e.g. decision tree) $T^*_i$   based on  $D_i^*$</li>
</ul>
</li>
<li>
<p>For an unseen data, aggregate the predictions of all $T^*$</p>
<ul>
<li>perform a majority vote for classification</li>
<li>average the predictions for regression</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> BaggingClassifier 
<span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier

ensemble_clf <span style="color:#f92672">=</span> BaggingClassifier(DecisionTreeClassifier(), n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, max_sample<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, bootstrap<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

</code></pre></div><h3 id="random-forest">Random Forest</h3>
<p>Bagging can be used for any models. Among them random forest is the special one. As its name suggests, it is exclusively designed for decision trees. Besides, it introduces extra randomness when growing trees.</p>
<p><img src="/blog/post/images/random-forest.jpeg" alt="" title="Figure 3: A random subset of features at each split for each tree (Reference [2])"></p>
<p>Specifically, it randomly choose a subset of $m'$ of the features at each split instead of using all features shown in Figure 3. By doing so, all trees can have much different training data set further, so they are <strong>less similar</strong> to each other, which results in more significant predictions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestClassifier, BaggingClassifier 
<span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier

random_forest_clf <span style="color:#f92672">=</span> BaggingClassifier(splitter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;random&#39;</span>, DecisionTreeClassifier(), max_leaf_nodes<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, max_sample<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, bootstrap<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e">## is equivalent to this</span>
random_forest_clf<span style="color:#f92672">=</span>RandomForestClassifier(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, max_leaf_nodes<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>)

</code></pre></div><h3 id="extra-trees">Extra-Trees</h3>
<p>TODO</p>
<h3 id="why-bagging-works">Why Bagging works</h3>
<p>Take random forests as an example, each decision tree is a machine learned from a data set. Based on the theory of bias and variance, we know that the mean meachine can be expressed as $f'_m=E_D[f'(x|D)]$. Thus, $f'(x|D)$ can be interpreted as a random variable $X$, and $f'_m$ can be described as $\mu = E(X)$.</p>
<p>Since we have $N$ decision trees in a random forest, there are $N$ random variables $X_i$, where $\mu = E(X_i)$. We can construct a new random variable $Z = \frac{1}{n}\sum_i^nX_i$, which represents the mean of $n$ random <strong>independent</strong> variables $X_i$.</p>
<p>The expected value of $Z$ is given by</p>
<p>$$
E[Z] = E[ \frac{1}{n}\sum_i^nX_i ] =\frac{1}{n} E[\sum_i^nX_i] = \frac{1}{n} nE[X_i] = \mu
$$</p>
<p>The variance is</p>
<p>$$
E[(Z - E[Z])^2] = E[(\frac{1}{n}\sum_i^nX_i - \mu)^2] = \frac{1}{n^2} E[(\sum_i^n (X_i - \mu))^2]
$$</p>
<p>$$
= \frac{1}{n^2}E[\sum_i^n(X_i - \mu)^2 + \sum_i^n\sum_{j=1,i \ne j}^n (X_i -\mu)(X_j -\mu)]
$$</p>
<p>Since $X_i$ is independent of $X_j$ ( $i\ne j$ ),</p>
<p>$$
E[\sum_i^n\sum_{j=1,i \ne j}^n (X_i -\mu)(X_j -\mu)] = 0
$$</p>
<p>we are left with</p>
<p>$$
E[(Z - E[Z])^2] = \frac{1}{n^2}E[\sum_i^n(X_i - \mu)^2] = \frac{1}{n} \sigma^2
$$</p>
<p>where $E[(X_i-\mu)^2]=\sigma^2$.</p>
<p>From the above euqation, we can see that ensemble methods reduce variances as $n$ increases when our models are <strong>uncorrelated</strong>.</p>
<p>On the contrary, if our models are correlated with the correlation coefficient $\rho$</p>
<p>$$
\rho = \frac{E[(X_i - \mu)(X_j - \mu)]} {\sqrt{\sigma^2(X_i)}\sqrt{\sigma^2(X_j)}}
$$</p>
<p>The variance is</p>
<p>$$
E[(Z - E[Z])^2] = \frac{1}{n} \sigma^2 + \frac{n-1}{n} \rho \sigma^2 = \rho \sigma^2 + \frac{(1 - \rho)}{n} \sigma^2
$$</p>
<p>As $n$ increases, the second term vanishes and we are left with the first term. Therefore, if we want the ensemble methods to do well, we need our models to be uncorrelated.</p>
<p><strong>Random forests do a good job because of both the randomness of training data sets sampled via bootstrapping and the randomness of features considered at each split.</strong> The algorithm results in much less correlated trees, which trades a higher bias for a lower variance, generally yielding better results.</p>
<h3 id="feature-importance">Feature Importance</h3>
<p>Like decision tree, random forests can tell us the relative importance of each feature. It is measured by calculating the sum of the reduction in impurity over all the nodes that are split on that feature.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
random_forest_clf<span style="color:#f92672">.</span>feature_importances_

</code></pre></div><h2 id="boosting">Boosting</h2>
<p>In boosting, we combine a group of weak learners into a strong learner.</p>
<p>What&rsquo;s the weak learners? They are the learning machines that do a little better than chance. Thus, they have high bias but low variance. The goal of boosting is to reduce the bias by combining them.</p>
<p>How to construct a weak learner? Well, one of the widely used types of weak learner are very shallow trees, for example, the stump with only one depth.</p>
<p>AdaBoost and GradientBoost are two popular algorithms for boosting. Let&rsquo;s lookt at AdaBoost first.</p>
<h3 id="adaboost">AdaBoost</h3>
<p>AdaBoost is a classic algorithm for binary classification. Suppose we have a data set $D=(x^i,y^i)_i^N$ with $y^i \in {-1, 1}$, and our weak learner $h(x)$ provides a prediction $h(x^i) \in {-1, 1 }$. The goal of AdaBoost is to construct a strong learner by combining all the weak learners, which can be written as a weighted sum of weak learners,</p>
<p>$$
C_n(x) = \sum_i \alpha_i h_i(x)
$$</p>
<p>How to find $\alpha_i$ and $h_i(x)$? Well, it&rsquo;s difficult to find all the coefficients for one time, so we will solve this equation greedily,</p>
<p>$$
C_n(x) = C_{n-1}(x) + \alpha_n h_n(x)
$$</p>
<p>where $C_{n-1}(x)$ is the current ensemble model that fit the training data best and $h_n(x)$ is the weak learner we are going to add.</p>
<h3 id="exponential-loss">Exponential Loss</h3>
<p>The second step is to find an appropriate loss function to optimize. How do we measure the performance of a classification model? One of the most widely used loss functions is <strong>0-1 loss</strong>,</p>
<p>$$
L = \sum_i^N [y_i \ne y'_i]
$$</p>
<p>where $[y_i \ne y'_i] = 1$ if $x_i$ is classified incorrectly and 0 otherwise. However, it&rsquo;s not-convex and difficult to optimize shown in Figure 4. In AdaBoost, we use <strong>exponential loss</strong>.</p>
<p>$$
L = \sum_i^n e^{-y^iC_n(x^i)}
$$</p>
<p>From Figure 4, it can be seen that data that are classified correctly have lower value while misclassfication observations have much larger values, which means exponential loss punishes examples classified incorrecly much more than correct classifications.</p>
<p><img src="/blog/post/images/exp-loss-adaboost.png" alt="Exponential loss" title="Figure 4: Exponential loss in AdaBoost"></p>
<h3 id="intuition">Intuition</h3>
<p>To minimize the loss, we plug the previous $C_n(x)$ into the loss function</p>
<p>$$
L = \sum_i^n e^{-y^i (C_{n-1}(x^i) + \alpha_n h_n(x^i))} = \sum_i^n e^{-y^i C_{n-1}(x^i)}  e^{-y^i \alpha_n h_n(x^i)} =\sum_{y^i\ne h_n(x^i)}^n w_n^i e^{\alpha_n} + \sum_{y^i= h_n(x^i) }^n w_n^i e^{-\alpha_n}
$$</p>
<p>$$
= \sum_{y^i= h_n(x^i) }^n w_n^i e^{-\alpha_n} + \sum_{y^i\ne h_n(x^i) }^n w_n^i e^{-\alpha_n} - \sum_{y^i\ne h_n(x^i) }^n w_n^i e^{-\alpha_n} + \sum_{y^i\ne h_n(x^i)}^n w_n^i e^{\alpha_n}
$$</p>
<p>$$
= e^{-\alpha_n} \sum_i^n w_n^i + (e^{\alpha_n} - e^{-\alpha_n}) \sum_{y^i\ne h_n(x^i) }^n w_n^i
$$</p>
<p>Then we find that minimizing the loss is equivalent to minimizing the sum of weights of each data that $h_n(x)$ misclassified, and that the value of weights depend on the current ensemble model $C_{n-1}(x)$.</p>
<p>$$
\sum_{y^i\ne h_n(x^i) }^n w_n^i = \sum_{y^i\ne h_n(x^i) }^n e^{-y^i C_{n-1}(x^i)}
$$</p>
<ul>
<li>If the data misclassified by $C_{n-1}(x)$ are still classified incorrectly by  $h_n(x)$, then $w_n^i$ is extremely large.</li>
<li>If the data classified correcly by $C_{n-1}(x)$ are misclassified by  $h_n(x)$, then $w_n^i$ is small.</li>
</ul>
<p>Simply put, misclassified data points will get high weights while correctly classified data points will get their weights decreased.</p>
<p>Therefore, we are finding some weak learner that tries to correct the errors the previous learners made. Furthermore, we also notice that we need to update $w_n^i$ for the next weak learner $h_{n+1}(x)$,</p>
<p>$$
w_{n+1}^i = e^{-y^i C_{n}(x^i)} = e^{-y^i (C_{n-1}(x^i) + \alpha_nh_n(x^i))} = w_n^i e^{-y^i\alpha_nh_n(x^i)}
$$</p>
<p>So the new weight of each data depends on the last weight of that data, the weight of the previous weak learner $h_n(x)$ and itself. But wait, what&rsquo;s the initial weight of each data? We simply initialize weights $w_1^i = \frac{1}{N}$ for every training sample.</p>
<p>Okay, now we are only left with $\alpha_n$. To find $\alpha_n$, we take the derivative of $L$ with respect to $\alpha_n$</p>
<p>$$
\frac{\partial L}{\partial \alpha_n} = e^{\alpha_n} \sum_{y^i\ne h_n(x^i)}^n w_n^i  - e^{-\alpha_n} \sum_{y^i= h_n(x^i) }^n w_n^i
$$</p>
<p>That is</p>
<p>$$
\alpha_n = \frac{1}{2} In\frac{\sum_{y^i= h_n(x^i) }^n w_n^i}{\sum_{y^i\ne h_n(x^i) }^n w_n^i}
$$</p>
<h3 id="algorithm">Algorithm</h3>
<p>Let&rsquo;s put it all together. The algorithm of AdaBoost can be summarised as below,</p>
<ul>
<li>
<p>Given a data set $D=(x^i,y^i)_i^N$ with $y^i \in \{-1, 1\}$ and a group of weak learners $h(x)$ of size $T$</p>
</li>
<li>
<p>Associate a weight $w_1^i = \frac{1}{N}$ with every data point $(x^i, y^i)$</p>
</li>
<li>
<p>For $t = 1$ to $T$</p>
<ul>
<li>Train a weak learner $h_t(x)$ that minimises $\sum_{y^i\ne h_t(x^i) }^n w_t^i $</li>
<li>Update the weight of this learner, $\alpha_t = \frac{1}{2} In\frac{\sum_{y^i= h_t(x^i) }^n w_t^i}{\sum_{y^i\ne h_t(x^i) }^n w_t^i}$</li>
<li>Update weights for each training point, $w_{t+1}^i = w_t^i e^{-y^i\alpha_th_t(x^i)}$</li>
</ul>
</li>
<li>
<p>Make a prediction</p>
<ul>
<li>$C_n(x) = sign[\sum_t^T \alpha_t h_t(x)]$</li>
</ul>
</li>
</ul>
<h3 id="example">Example</h3>
<p>Step 1: Initialisation</p>
<p>Here we have 8 rows with 3 predictors <code>chest_pain, blocked_arteries and weight</code> and 1 target variable <code>heart_disease</code>. Each data point is initialised with an equal weight <code>0.125</code>.</p>
<p><img src="/blog/post/images/ada-example-df.png" alt="Toy data for AdaBoost" title="Figure 5: Toy data from 'StatQuest with Josh Starmer'"></p>
<p>Step 2: Find the weak learner</p>
<p>Here, we use stump as our weak learner and Figure 6 shows the first optimal tree where we only misclassified one observation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> tree

X <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#39;heart_disease&#39;</span>, <span style="color:#e6db74">&#39;weights&#39;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
y <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;heart_disease&#39;</span>]

clf <span style="color:#f92672">=</span> tree<span style="color:#f92672">.</span>DecisionTreeClassifier(max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) 
clf <span style="color:#f92672">=</span> clf<span style="color:#f92672">.</span>fit(X, y)

tree<span style="color:#f92672">.</span>plot_tree(clf<span style="color:#f92672">.</span>fit(X, y))
plt<span style="color:#f92672">.</span>show()

</code></pre></div><p><img src="/blog/post/images/ada-stump.png" alt="" title="Figure 6: The first stump"></p>
<p>Step 3: Update weights</p>
<p>Since we have only one misclassification, the error rate is <code>1/8</code> and $\alpha_1$ is 0.97. Then we update weights for each data point using $\alpha_1$.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cal_alpha</span>(error):
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log((<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> error)<span style="color:#f92672">/</span>error)

alpha_1 <span style="color:#f92672">=</span> cal_alpha(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">8</span>)

correct_samples <span style="color:#f92672">=</span> df[clf<span style="color:#f92672">.</span>predict(X) <span style="color:#f92672">==</span> y]
df<span style="color:#f92672">.</span>loc[clf<span style="color:#f92672">.</span>predict(X) <span style="color:#f92672">==</span> y, <span style="color:#e6db74">&#39;weights&#39;</span>] <span style="color:#f92672">=</span> correct_samples[<span style="color:#e6db74">&#39;weights&#39;</span>] <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>alpha_1)

misclassified_samples <span style="color:#f92672">=</span> df[clf<span style="color:#f92672">.</span>predict(X) <span style="color:#f92672">!=</span> y]
df<span style="color:#f92672">.</span>loc[clf<span style="color:#f92672">.</span>predict(X) <span style="color:#f92672">!=</span> y, <span style="color:#e6db74">&#39;weights&#39;</span>] <span style="color:#f92672">=</span> misclassified_samples[<span style="color:#e6db74">&#39;weights&#39;</span>] <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>exp(alpha_1)

print(alpha_1, df)

</code></pre></div><p><img src="/blog/post/images/ada-example-stump1.png" alt="" title="Figure 7: Updated weigts after training the first stump"></p>
<p>Step 4: Go back to Step 2 until the desired number of learners is reached.</p>
<h3 id="adjusted-impurity">Adjusted impurity</h3>
<p>Recall that Gini Index is written as</p>
<p>$$
Q_m^g(L) = 1 - \sum_{c \in C } p_c(L)^2
$$</p>
<p>and entropy discussed in Decision Tree as</p>
<p>$$
Q_m^e(L) = -p_c(L)logp_c(L)
$$</p>
<p>where $p_c(L)$ is the fraction of the observations belong to class $c$. In order to use the weight of each data in AdaBoost, we need to change it slightly.</p>
<p>$$
p_c(L) = \frac{\sum_{x^j \in C} w_n^j I[y_j == C]}{\sum_{x^i \in L} w_n^i}
$$</p>
<p>Why this works? Remember that the lower the impurity is, the better the split is. And a higher fraction leads to a lower impurity or entropy.</p>
<ul>
<li>If we classify the misclassified example in the node $L$ correctly, then the denominator of $p_c(L)$ becomes smaller and then $p_c(L)$ becomes larger.</li>
<li>On the contratry, if this split works so bad, then we will have many observations that classified incorrectly, resulting in smaller $p_c(L)$ due to a small numerator and large denominator.</li>
</ul>
<p>Thus, we are finding the best split that can correctly classify the examples that previous learners failed as much as possible.</p>
<h2 id="references">References</h2>
<p>[1] A. Géron, <em>Hands-on machine learning with Scikit-Learn and TensorFlow</em>. Sebastopol (CA): O&rsquo;Reilly Media, 2019.</p>
<p>[2]	T. Yiu, “Understanding random forest - towards data science,” Towards Data Science, 12-Jun-2019. [Online]. Available: <a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2">https://towardsdatascience.com/understanding-random-forest-58381e0602d2</a>. [Accessed: 23-Apr-2021].</p>
<p>[3]	J. Rocca, “Ensemble methods: bagging, boosting and stacking - Towards Data Science,” Towards Data Science, 23-Apr-2019. [Online]. Available: <a href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205</a>. [Accessed: 23-Apr-2021].</p>
<p>[4] Gradient Boosting Tutorial <a href="http://talks.albertauyeung.com/pycon2017-gradient-boosting/">http://talks.albertauyeung.com/pycon2017-gradient-boosting/</a></p>
        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/ml/end2end-project-01/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">An E2E Project - EDA</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/ml/decision-tree/">
                <span class="next-text nav-default">Decision Tree</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#voting">Voting</a>
      <ul>
        <li><a href="#hard-voting">Hard Voting</a></li>
        <li><a href="#weighted-hard-voting">Weighted Hard Voting</a></li>
        <li><a href="#soft-voting">Soft Voting</a></li>
        <li><a href="#average-for-regression">Average for Regression</a></li>
      </ul>
    </li>
    <li><a href="#bagging">Bagging</a>
      <ul>
        <li><a href="#random-forest">Random Forest</a></li>
        <li><a href="#extra-trees">Extra-Trees</a></li>
        <li><a href="#why-bagging-works">Why Bagging works</a></li>
        <li><a href="#feature-importance">Feature Importance</a></li>
      </ul>
    </li>
    <li><a href="#boosting">Boosting</a>
      <ul>
        <li><a href="#adaboost">AdaBoost</a></li>
        <li><a href="#exponential-loss">Exponential Loss</a></li>
        <li><a href="#intuition">Intuition</a></li>
        <li><a href="#algorithm">Algorithm</a></li>
        <li><a href="#example">Example</a></li>
        <li><a href="#adjusted-impurity">Adjusted impurity</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.4484b3f29dd568c80320701800e1b69704b179f367b8223a43c728d819f39b97.js" integrity="sha256-RISz8p3VaMgDIHAYAOG2lwSxefNnuCI6Q8co2Bnzm5c=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
