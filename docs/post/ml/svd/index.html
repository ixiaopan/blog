<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Singular Value Decomposition - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" /><meta name="description" content="Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post." />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/ml/svd/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Singular Value Decomposition" />
<meta property="og:description" content="Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/ml/svd/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-05-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-05-04T00:00:00+00:00" />

<meta itemprop="name" content="Singular Value Decomposition">
<meta itemprop="description" content="Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post."><meta itemprop="datePublished" content="2021-05-04T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-05-04T00:00:00+00:00" />
<meta itemprop="wordCount" content="2818">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Singular Value Decomposition"/>
<meta name="twitter:description" content="Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Singular Value Decomposition</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-05-04">
      2021-05-04
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/machine-learning/"> Machine Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into the multiplication of three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post.</p>
<h2 id="change-of-basis">Change of Basis</h2>
<p>Suppose there is a point in the 2D space, how do you describe it? The common way is to use the Cartesian coordinate system, which is composed of two fixed perpendicular oriented axes, measured in the same unit of length. The two perpendicular axes are just a special set of vectors served as the basis of the 2D space. Actually, there are many other sets of vectors that can be the basis for the 2D space. For example, in Figure 1, the position of the red point is <code>(-4, 1)</code> when using the standard basis <code>(1, 0), (0, 1)</code> (colored in grey). If we change the basis to <code>(2, 1), (-1, 1)</code> (colored in blue), the position is <code>(-1, 2)</code>.</p>
<p><img src="/blog/post/images/change-basis-example.png" alt="" title="Figure 1: The same point with different coordinates in two different coordinate spaces"></p>
<p>From Figure 1, we can see that the absolute position of the red point always stay the same. However, the relative positions to the different bases are different. Mathematically, the red point can be described from the perspective of basis as follows,</p>
<p>$$
x = P_b[x]_b = c_1 \bold b_1 + c_2 \bold b_2 + &hellip; + c_n \bold b_n
$$</p>
<p>$$
P_b = [\bold b_1, \bold b_2,  &hellip; , \bold b_n ]
$$</p>
<p>$$
[x]_b = [c_1, c_2, &hellip; c_n]
$$</p>
<p>where</p>
<ul>
<li>$[x]_b$ is a set of scalars, which represent the length of projection onto each axis of the current coordinate system</li>
<li>$P_b$ is the corresponding basis of the current coordinate system</li>
</ul>
<p>Let&rsquo;s plug the above point and the basis <code>(1, 0), (0, 1)</code>  (colored in grey) into the equation,</p>
<p>$$
P_b = [ (1, 0), (0, 1)]
$$</p>
<p>$$
[x]_b = (-4, 1)
$$</p>
<p>$$
x_b = -4 \begin{bmatrix}1\\ 0 \end{bmatrix} + 1 \begin{bmatrix}0\\ 1 \end{bmatrix} = \begin{bmatrix}-4\\ 1 \end{bmatrix}
$$</p>
<p>Let&rsquo;s do the same calculation with another basis (colored in blue),</p>
<p>$$
P_b = [ (2, 1), (-1, 1)]
$$</p>
<p>$$
[x]_b = (-1, 2)
$$</p>
<p>$$
x_b = -1 \begin{bmatrix}2\\ 1 \end{bmatrix} + 2 \begin{bmatrix}-1\\ 1 \end{bmatrix} = \begin{bmatrix}-4\\ 1 \end{bmatrix}
$$</p>
<p>As expected, they yield the same result. And the second one essentially changes the basis of $R^2$ from <code>(2,1),(-1,1)</code> to<code>(1,0),(0,1)</code>, which is the standard basis of $R^2$.</p>
<p>Actually, this example is a special case of the change of basis, where the new basis is the standard basis. More generally, $P_{c \larr b}$ is known as <strong>the change of coordinate matrix from the old basis $b$ to the new basis $c$, which we are going to switch to</strong> in $R^n$.</p>
<p>Say we are in the basis $b$ and $[x]_b$ is known, the corresponding coodinates of $x$ under the new basis $c$ can be computed as follows,</p>
<p>$$
x_c = P_{c \larr b} x_b
$$</p>
<p>Since $P_{c \larr b}$ is invertible, we have</p>
<p>$$
(P_{c \larr b})^{-1}x_c = x_b
$$</p>
<p>which is the inverse operation of change of basis from $b$ to $c$. We can generalize this to any number of points and dimensions.</p>
<p>$$
A=US\\(D,N)= (D, M) \times (M, N)
$$</p>
<p>$$
U^{-1} A  = S\\(M, D) \times (D, N) = (M, N)
$$</p>
<p>where</p>
<ul>
<li>$A$ is a $D\times N$ matrix with $D$ dimensions and $N$ points</li>
<li>$S$ is a  $M\times N$ matrix with $M$ dimensions and $N$ points described in a new vector space decided by another basis</li>
<li>$U$ is the change of coordinate matrix from $S$ to $A$</li>
<li>$U^{-1}$ is the the change of coordinate matrix from $A$ to $S$</li>
</ul>
<p>If we do some transformation for a point in the standard coordinate system, what&rsquo;re the new coordinates of the same point in another system? This problem can be solved by the following equation,</p>
<p>$$
x_s' = U^{-1}TUx_s
$$</p>
<p>where $T$ represents the transformation matrix. If $T=I$,  $x_s'$ is exactly $x_s$.</p>
<h2 id="svd">SVD</h2>
<p>SVD is a technique in linear algebra that can be used to decompose <strong>any</strong> $N \times P$ matrix</p>
<p>$$
X = U S V^T
$$</p>
<ul>
<li>$U$ is an $N \times N$ orthogonal matrix, where the columns of $U$ are the eigenvectors of $XX^T$</li>
<li>$S$ is an $N \times P$ diagonal matrix whose diagonal entries are the sorted singluar values, which are square roots of eigenvalues of $XX^T$ or $X^TX$</li>
<li>$V$ is a $P \times P$ orthogonal matrix, where the columns of $V$ are the eigenvectors of $X^TX$</li>
</ul>
<p>$$
C = X^TX =  (USV^T )^T USV^T = VS^TU^T  USV^T = VSS^TV^T
$$</p>
<p>$$
D = XX^T =   USV^T  (USV^T )^T = USS^TU^T
$$</p>
<p>$$
=&gt;  C [\bold v_1, \bold v_2, &hellip;, \bold v_p] = [\lambda_1 \bold v_1, \lambda_2\bold v_2, &hellip;, \lambda_p \bold v_p]
$$</p>
<p>$$
=&gt;  D [\bold u_1, \bold u_2, &hellip;, \bold u_n] = [\lambda_1 \bold u_1, \lambda_2\bold u_2, &hellip;, \lambda_n \bold u_n]
$$</p>
<p>Therefore, $V$ and $U$ are matrices of eigenvectors for $X^TX$ and $XX^T$.</p>
<p>If we look at the formula of SVD from the view of &lsquo;the change of basis&rsquo; discussed above, we will find that</p>
<ul>
<li>$V^T$ is the change of matrix from, say basis A, to the standard basis</li>
<li>$S$ is a scaling matrix</li>
<li>$U$ is another change of matrix from the standard basis to basis A</li>
</ul>
<p>Geometrally, the multiplication between a matrix $A$ and a vector $x$ represents a linear transformation, where we using another basis of $R^n$ to represent the same point and $A$ is the change of matrix between bases. By decomposing $A$, we can clearly see that this transformation is composed of three transformations:</p>
<ul>
<li>$ V^T$: rotation</li>
<li>$S$: scaling</li>
<li>$U$:  rotation</li>
</ul>
<h3 id="economical-forms-of-svd">Economical Forms of SVD</h3>
<p>Since $S$ is an $r \times r$ diagonal matrix for some $r$ not exceeding the smaller of $N$ and $P$, we could simplify $S$ and the correspoding $V$ and $U$. Specifically, for $X$ with more samples than features $N &gt; P$ (tall and thin), we ignore the last $N - P$ columns of U</p>
<p><img src="/blog/post/images/economical-form-svd-2.png" alt=""></p>
<p>and for $X$ with more features than examples $N &lt; P$ (short and fat), we ignore the last $P - N$ rows of $V^T$</p>
<p><img src="/blog/post/images/economical-form-svd-1.png" alt=""></p>
<h3 id="linear-regression-revisited">Linear Regression Revisited</h3>
<p>In the previous post <a href="https://ixiaopan.github.io/blog/post/linear-regression-02/">Linear Regression 02</a>, we introduced pseudo-inverse $A^+$</p>
<p>$$
\bold {\hat w} = A^+ \bold y = (\bold X^T\bold X)^{-1} \bold X^T \bold y = V (S^TS)^{-1}S^TU^T \bold y = VS^+U^T \bold y
$$</p>
<p>where the elements of $S^+$ are the reciprocal of the singular values,</p>
<p>$$
S^+ = (S^TS)^{-1}S^T = \begin{bmatrix}s_1^{-1}&amp;0 &amp; 0 &amp; &hellip; &amp; 0 &amp; 0 &amp; 0 &amp; &hellip; &amp; 0\\ 0&amp;s_2^{-1} &amp; 0 &amp; &hellip; &amp; 0 &amp; 0 &amp; 0 &amp; &hellip; &amp; 0\\ &hellip; \\ 0&amp;0 &amp; 0 &amp; &hellip; &amp; s_p^{-1} &amp; 0 &amp; 0 &amp; &hellip; &amp; 0\end{bmatrix}
$$</p>
<p>This means if any of the singular values of $X$ are small, then $S^{-1}$ will magnify component in that direction. Thus, little change in $\bold y$  will lead to a greatly different model and eventually a poor generalization.</p>
<p>One way to tackle this is regularisation. Ridge Regression is one variant of linear regression by adding a regulariser $\lambda ||\bold  w||^2$ to the loss function,</p>
<p>$$
L = ||\bold X \bold w - \bold y||^2 + \lambda ||\bold  w||^2
$$</p>
<p>The estimate $\bold w$ is given by</p>
<p>$$
\bold {\hat w} = V(S^TS + \lambda I)^{-1} S^TU^T \bold y
$$</p>
<p>where</p>
<p>$$
(S^TS + \lambda I)^{-1} S^T = \begin{bmatrix}\frac{s_1}{s_1^2+\lambda}&amp;0 &amp; 0 &amp; &hellip; &amp; 0 &amp; 0 &amp; 0 &amp; &hellip; &amp; 0\\ 0&amp;\frac{s_2}{s_2^2+\lambda} &amp; 0 &amp; &hellip; &amp; 0 &amp; 0 &amp; 0 &amp; &hellip; &amp; 0\\ &hellip; \\ 0&amp;0 &amp; 0 &amp; &hellip; &amp; \frac{s_p}{s_p^2+\lambda} &amp; 0 &amp; 0 &amp; &hellip; &amp; 0\end{bmatrix}
$$</p>
<ul>
<li>if $s_i = 0$, then $\frac{s_i}{s_i^2 + \lambda} =0$ and the &lsquo;inverse&rsquo; is defined</li>
<li>if $s_i &laquo; \lambda$, then $\frac{s_i}{s_i^2 + \lambda} \simeq \frac{1}{\lambda }$</li>
<li>if $si &raquo; \lambda$, then $\frac{s_i}{s_i^2 + \lambda}\simeq s_i^{-1}$</li>
</ul>
<p>Therefore, adding a regulariser can make our model much more stable.</p>
<h2 id="pca">PCA</h2>
<p>Principal component analysis(PCA) is  often used to reduce dimentionality. The idea of PCA is to find directions along which data has the largest variation. The variation can be computed by projecting data onto that direction. Mathematically, we want to find a vector $v$ with $||v||=1$ to maximise</p>
<p>$$
\sigma^2 = \frac{1}{n-1}\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2
$$</p>
<p>There are two things to notice here:</p>
<ul>
<li>$v$ is an unit vector since we are care about the direction only</li>
<li>data are centralized first for simple computation; centralizing data doesn&rsquo;t change the distribution of data</li>
</ul>
<p>We can solve the above equation by introducing Lagrange multiplier</p>
<p>$$
L = \frac{1}{n-1}\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2 - \lambda (||v||^2 - 1)
$$</p>
<p>$$
= \frac{1}{n-1}\sum_i^n \bold v^T (\bold x_i - \bold \mu)(\bold x_i - \bold \mu)^T\bold v - \lambda (||v||^2 - 1)
$$</p>
<p>$$
= \bold v^T \bold C \bold v - \lambda (\bold  v^T\bold v - 1)
$$</p>
<p>where $\bold C$ is the covariance matrix of $X$. Then we take the derivative of $L$ w.r.t $\bold v$ , and then set it to $0$</p>
<p>$$
\frac{\partial L}{\partial \bold v} = 2(C \bold v - \lambda \bold v) = 0
$$</p>
<p>so $\bold v$ is the eigenvector of $\bold C$, and the variance along this direction is,</p>
<p>$$
\sigma^2 = \frac{1}{n-1}\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2 = \bold v^T \bold C \bold v = \lambda \bold v^T  \bold v = \lambda
$$</p>
<h3 id="properties-of-covariance-matrix">Properties of Covariance Matrix</h3>
<p>We know that the quadratic form of a vector and a matrix is defined as</p>
<p>$$
x^T M x
$$</p>
<p>Thus, the quadratic form of $C$ is</p>
<p>$$
x^TCx = x^T XX^Tx=u^Tu \ge 0
$$</p>
<p>so $C$ is <strong>positive semi-definite</strong>, which means <strong>all eigenvalues of $C$ are greater than or equal to zero</strong>. Why? Suppose $\mu$ is an eigenvector of $C$, since $\mu^TC\mu\ge0$ and $||\mu||&gt;0$, then we have</p>
<p>$$
\mu^TC\mu = \mu^T \lambda \mu =&gt; \lambda = \frac{\mu^TC\mu }{||\mu||^2 } \ge 0
$$</p>
<h4 id="zero-eigenvalue">zero eigenvalue</h4>
<p>What if $C$ has a zero eigenvalue? Well, a zero eigenvalue means that there is no variation in the direction of the corresponding eigenvector. So when will we have zero eigenvalues? Based on the definition of eigenvalue, we have</p>
<p>$$
Cx = 0x = 0
$$</p>
<p>Since $x$ is nonzero vector, $C$ is singular or non-invertible. And this will inevitably happen if the number of features is much more than the number of examples. Conversely, if $C$ is invertible, it has no zero eigenvalues and is said to be <strong>positive definite</strong> (since all eigenvalues are greater than 0).</p>
<h3 id="geometry-of-pca">Geometry of PCA</h3>
<p>Since $C$ is a $p \times p$ symmetric matrix, it can be orthogonally diagonalized as $C = PDP^T$, where $P$ is an orthogonal matrix and $D$ is a diagonal matrix. Thus, the above formula can also be written as follows,</p>
<p>$$
\sum_i^n(\bold v^T (\bold x_i - \bold \mu))^2 = \sum_i^n \bold v^T (\bold x_i - \bold \mu)(\bold x_i - \bold \mu)^T\bold v = \bold v^T C \bold v
$$</p>
<p>$$
= \bold v^T PDP^T \bold v = \bold y^T D \bold y
$$</p>
<p>where $\bold  y = P^T \bold v$. Mathematically, $\bold v^T C \bold v$ and $\bold y^T D \bold y$ yield the same result, which represents the sum of deviation of each sample from the original point along the direction determined by this vector as shown in Figure 2.</p>
<p><img src="/blog/post/images/change-variable.png" alt="" title="Figure 2: Change of variable in $x^T A x$ (Introduction to Linear Algebra[1])"></p>
<p>Geometrically,  $P^T_{y \larr v}$ is the change coordinate matrix from $v$ to $y$, which finally transform the shape of the quadratic form $\bold v^TC\bold v$ into the standard position, such as the shapes in the figure below.</p>
<p><img src="/blog/post/images/ellipse-standard.png" alt="" title="Figure 3: An ellipse and a hyperbola in standard position (Introduction to Linear Algebra[1])"></p>
<h3 id="projection">Projection</h3>
<p>For a data set with $p$ features, we want to reduce its dimension to $k$, there are a few steps to follow</p>
<ul>
<li>
<p>Construct a $p\times p$ covariance matrix $C$ using centralized data</p>
</li>
<li>
<p>Find all the eigenvectors $v_i$ and eigenvalues $\lambda_i$ of $C$</p>
</li>
<li>
<p>Construct a $p \times k$ projection matrix $P$ with $k$ eigenvectors determined by the $k$ largest eigenvalues (principal components)</p>
</li>
<li>
<p>Project the original data into the space spanned by the principal components</p>
</li>
</ul>
<p>$$
\bold  z = P^T(\bold  x - \bold \mu)
$$</p>
<p>where $\bold z$ is our new inputs.</p>
<p>Usually, there are two common ways to find the eigenvalues:</p>
<ul>
<li>eigendecomposition</li>
<li>SVD</li>
</ul>
<p>Eigen-decomposition follows the above steps, and we can see it from the following code,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pca</span>(X):
    <span style="color:#75715e"># Data matrix X, assumes 0-centered</span>
    P, N <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape

    <span style="color:#75715e"># Compute covariance matrix, where X is a P by N matrix</span>
    C <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X, X<span style="color:#f92672">.</span>T) <span style="color:#f92672">/</span> (N<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    
    <span style="color:#75715e"># Eigen decomposition</span>
    eigen_vals, eigen_vecs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>eig(C)
    
    <span style="color:#75715e"># Project X onto eigen space</span>
    X_pca <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(eigen_vecs<span style="color:#f92672">.</span>T, X)

    <span style="color:#66d9ef">return</span> X_pca, eigen_vals, eigen_vecs
</code></pre></div><p>Instead, SVD decomposes $X$ directly. Besides, we don&rsquo;t need to centralize data first in SVD, though most people will do.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">svd</span>(X):
  N, P <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
  
  <span style="color:#75715e"># In practice, we usually subtract the mean from data and then perform SVD</span>
  X_c <span style="color:#f92672">=</span> X <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>mean(X, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
	
	<span style="color:#75715e"># the columns of Vt are the eigenvalues of X^TX</span>
  U, Sigma, Vt <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svd(X_c)
  
	<span style="color:#75715e"># Project X onto eigen space</span>
  X_pca <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X, Vt<span style="color:#f92672">.</span>T)

  <span style="color:#66d9ef">return</span> U, Sigma, Vt
	 
</code></pre></div><p>So why do we use SVD? Simply put,  If the number of features are much more than the number of example, i.e. $P&raquo;N$, it&rsquo;s not easy to compute eigenvalues using eigen-decomposition. But in SVD, the algorithm will compute eigenvalues by choosing the smaller of two matrice $X^TX$ and $XX^T$.  In this case,  $X^TX$ has less elements( $N * N$ ) than $XX^T$($ P * P$). More details can be found in the following section &lsquo;PCA for image&rsquo;.</p>
<h3 id="reconstruction">Reconstruction</h3>
<p>From the perspective of projection, the projection onto a vector $\bold v_j$ can be seen as approximating the inputs by</p>
<p>$$
\hat {\bold x_i} = \bold \mu + \sum_j^k z_j^i \bold v_j
$$</p>
<p>$$
z_j^i = \bold v_j^T(\bold x_i - \bold \mu)
$$</p>
<p>So our goal is to minimize the error</p>
<p>$$
E[ ||\hat {\bold x_i} - \bold x_i ||^2] = \frac{1}{n} \sum_{n=1}^n ||\sum_{i=1}^k \bold u_i^T\bold x_n \bold u_i + \sum_{i=k+1}^p \bold u_i^T\bold x_n \bold u_i - \sum_{j=1}^k \bold v_j^T\bold x_n \bold v_j = \frac{1}{n} \sum_{n=1}^n \sum_{i=k+1}^p ||\bold u_i^T\bold x_n \bold u_i||^2
$$</p>
<p>$$
= \frac{1}{n}\sum_{n=1}^n\sum_{i=k+1}^p (\bold u_i^T\bold x_n) \bold u_i^T \cdot (\bold u_i^T\bold x_n) \bold u_i =\frac{1}{n} \sum_{n=1}^n\sum_{i=k+1}^p (\bold u_i^T\bold x_n)^2
$$</p>
<p>$$
= \frac{1}{n} \sum_{n=1}^n\sum_{i=k+1}^p \bold u_i^T\bold x_n \bold x_n^T \bold u_i= \sum_{i=k+1}^p \bold u_i^T (\frac{1}{n} \sum_{n=1}^n\bold x_n \bold x_n^T )\bold u_i
$$</p>
<p>$$
\sum_{i=k+1}^p \bold u_i^T S \bold u_i = \sum_{i=k+1}^p \lambda_i \bold u_i^T  \bold u_i = \sum_{i=k+1}^p \lambda_i
$$</p>
<p>We can see that the error is exactly the sum of the eigenvalues in the directions that are discarded.</p>
<h3 id="pca-for-images">PCA for images</h3>
<p>Suppose we have an image with the size of $256 \times 256$, so it has nearly $64K$ pixels or features. Then we could create a covariance matrix $C$ with more than $4\times10^9$ elements using PCA. However, this huge matrix is not easy to compute eigenvalues. To make this problem tractable, we usually work in a dual space instead of a feature space. The dual space we choose is spanned by $n$ vectors, which are exactly the sample images. Specifically, if we have $n$ images, the subpace of $R^n$ has at most $n-1$ dimensions, and usually $n$ is much smaller than $p$. But how do we find the eigenvalues of $C$ in this dual space?</p>
<p>We&rsquo;ve known that $C = XX^T$ is a $p\times p$ matrix, where $X$ is a $p \times n $ matrix. Now we construct another matrix $D=X^TX$ with $n \times n$ elements, which is also a symmetric matrix. Suppose $v$ is the eigenvalue of $D$, then we have</p>
<p>$$
Dv = \lambda v
$$</p>
<p>$$
XX^TXv = \lambda Xv
$$</p>
<p>$$
CXv = \lambda Xv
$$</p>
<p>$$
Cu = \lambda u
$$</p>
<p>where $u = Xv$. We find that $C$ and $D$ has the same eigenvalues. Thus, we can use the dual $n \times n$ matrix $D$ to find eigenvalues and eigenvectors of $C$.</p>
<h3 id="find-k-components">Find K Components</h3>
<p>The last question is how to decide the number of components. Instead of guessing the number of dimensions that we want to keep, we choose the right $k$ components along which the sum of the explained variance ratio is greater than a threshold. The explained variance ratio of each component indicates how much the variance explained along component. In Sklearn, it can be accessed via <code>explained_variance_ratio_</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA

pca <span style="color:#f92672">=</span> PCA(n_components<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
X2D <span style="color:#f92672">=</span> pca<span style="color:#f92672">.</span>fit_transform(X)
pca<span style="color:#f92672">.</span>explained_variance_ratio_

</code></pre></div><p>The following code shows how to find $k$ components with a variance ratio of $0.95$ using Sklearn.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA

pca <span style="color:#f92672">=</span> PCA()
pca<span style="color:#f92672">.</span>fit(X_train)
cumsum <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cumsum(pca<span style="color:#f92672">.</span>explained_variance_ratio_)
d <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(cumsum<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0.95</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>

<span style="color:#75715e"># or simply</span>
pca <span style="color:#f92672">=</span> PCA(n_components<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>)
X_reduced <span style="color:#f92672">=</span> pca<span style="color:#f92672">.</span>fit_transform(X_train)
pca<span style="color:#f92672">.</span>n_components_

</code></pre></div><p>Also, we can plot the explained variance ratio as a function of the number of dimensions, and the elbow in the curve is where the appropriate $k$ lies.</p>
<p><img src="/blog/post/images/explained-variance-ratio-pca.png" alt="" title="Figure 4: Explained variance as a function of k (Hands-on machine learning, 2019)"></p>
<h2 id="references">References</h2>
<p>[1]	G. Strang, Introduction to Linear Algebra, 5th ed. Wellesley, MA: Wellesley-Cambridge Press, 2016.</p>
<p>[2] A. GÃ©ron, <em>Hands-on machine learning with Scikit-Learn and TensorFlow</em>. Sebastopol (CA): O&rsquo;Reilly Media, 2019.</p>
        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/ml/semanticweb/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Semantic Web</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/ml/end2end-project-01/">
                <span class="next-text nav-default">An E2E Project - EDA</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#change-of-basis">Change of Basis</a></li>
    <li><a href="#svd">SVD</a>
      <ul>
        <li><a href="#economical-forms-of-svd">Economical Forms of SVD</a></li>
        <li><a href="#linear-regression-revisited">Linear Regression Revisited</a></li>
      </ul>
    </li>
    <li><a href="#pca">PCA</a>
      <ul>
        <li><a href="#properties-of-covariance-matrix">Properties of Covariance Matrix</a></li>
        <li><a href="#geometry-of-pca">Geometry of PCA</a></li>
        <li><a href="#projection">Projection</a></li>
        <li><a href="#reconstruction">Reconstruction</a></li>
        <li><a href="#pca-for-images">PCA for images</a></li>
        <li><a href="#find-k-components">Find K Components</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.4484b3f29dd568c80320701800e1b69704b179f367b8223a43c728d819f39b97.js" integrity="sha256-RISz8p3VaMgDIHAYAOG2lwSxefNnuCI6Q8co2Bnzm5c=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
