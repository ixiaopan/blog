<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Probabilistic Model - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" /><meta name="description" content="The goal of machine learning is to infer an unknown pattern from data. However, both parameters and predictions are estimated values, so how confident are we in these values? To measure uncertainty, we use probability. On the other hand, Bayes&#39; theorem provides a framework for us to invert the problem into a forward process, where we observe data from parameters instead of making inferences from data." />







<meta name="generator" content="Hugo 0.81.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/ml/probabilistic-model/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.74977c7446e205bad48a3e6fbb98e0a1566bd939e7c40ca1aecde689a0a1376e.css" integrity="sha256-dJd8dEbiBbrUij5vu5jgoVZr2TnnxAyhrs3miaChN24=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Probabilistic Model" />
<meta property="og:description" content="The goal of machine learning is to infer an unknown pattern from data. However, both parameters and predictions are estimated values, so how confident are we in these values? To measure uncertainty, we use probability. On the other hand, Bayes&#39; theorem provides a framework for us to invert the problem into a forward process, where we observe data from parameters instead of making inferences from data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/ml/probabilistic-model/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-05-26T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-26T00:00:00&#43;00:00" />

<meta itemprop="name" content="Probabilistic Model">
<meta itemprop="description" content="The goal of machine learning is to infer an unknown pattern from data. However, both parameters and predictions are estimated values, so how confident are we in these values? To measure uncertainty, we use probability. On the other hand, Bayes&#39; theorem provides a framework for us to invert the problem into a forward process, where we observe data from parameters instead of making inferences from data."><meta itemprop="datePublished" content="2021-05-26T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2021-05-26T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="3121">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Probabilistic Model"/>
<meta name="twitter:description" content="The goal of machine learning is to infer an unknown pattern from data. However, both parameters and predictions are estimated values, so how confident are we in these values? To measure uncertainty, we use probability. On the other hand, Bayes&#39; theorem provides a framework for us to invert the problem into a forward process, where we observe data from parameters instead of making inferences from data."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">Tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Probabilistic Model</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-05-26">
      2021-05-26
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/machine-learning/"> Machine Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>The goal of machine learning is to infer an unknown pattern from data. However, both parameters and predictions are estimated values, so how confident are we in these values? To measure uncertainty, we use probability. On the other hand, Bayes' theorem provides a framework for us to invert the problem into a forward process, where we observe data from parameters instead of making inferences from data.</p>
<h2 id="probability">Probability</h2>
<h3 id="random-variable">Random Variable</h3>
<p>At the beginning, let&rsquo;s have a quick refresh on probability.</p>
<blockquote>
<p>A random variable is variable that can take on different values randomly.</p>
<p>On its own, it is a description the states that are possible; it must be coupled with a probability distribution that specifies how likely each of these states are.</p>
<p>— Deep Learning, p54</p>
</blockquote>
<p>In short, a random variable covers two aspects: possible value and the likelihood of taking that value. Conventionly, we use a capital letter, such as $X$ or $Y$, to represent a random variable.</p>
<h3 id="conditional-probability">Conditional Probability</h3>
<p>Suppose we have two random variables of interest, $X$ and $Y$,</p>
<ul>
<li>
<p>The joint probability of $X$ that takes the value of $x$ and $Y$ that takes the value of $y$ is written as $P(X = x, Y=y)$, which means that the probability of $x$ and $y$ happening at the same time</p>
</li>
<li>
<p>Given $Y=y$, the conditional probability of $X$ given $Y=y$ is denoted by $P(X|Y=y)$</p>
</li>
</ul>
<p>There are two major rules of probability that we should remember,</p>
<ul>
<li>the sum rule, i.e. the marginal probability of $X$ that takes the value of $x$, irrespective of the value of $Y$</li>
</ul>
<p>$$
P(X=x) = \sum_{y \in Y} P(X=x, Y=y)
$$</p>
<ul>
<li>the product rule, i.e. the joint probability of $X$ and $Y$ can be written as the product of the conditional probability and the marginal probability</li>
</ul>
<p>$$
P(X, Y) = P(Y|X) P(X) = P(X|Y)P(Y)
$$</p>
<p>From the above formula, we can deduce the following equation, which is also known as Bayes' Rule,</p>
<p>$$
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
$$</p>
<h3 id="expectation">Expectation</h3>
<p>The expectation of the function $f(x)$ of a random variable $x$ is the mean value of $f(x)$</p>
<p>$$
E_{x \sim P(x) } f(x) = \sum_x P(x) f(x)
$$</p>
<p>The variance measures how much a new sample drawn from $P(x)$ deviate from the mean value</p>
<p>$$
\text{Var(x)} = E[  (f(x) - E[f(x)])^2 ]
$$</p>
<h2 id="bayes-inference">Bayes' Inference</h2>
<p>In machine learning, our goal is to learn a model $\Theta$ from a given data set $D$, but $\Theta$ is uncertain. One way to measure uncertainty is probability, so the question becomes how to compute the probablity of $\Theta$ given the data $D$, i.e. $P(\Theta|D)$.  But the only thing we know is the observed data, so how can we do it? The answer is the Bayes' Rule, which helps us convert this into a forward problem, where parameters are known and thus we can draw data from the distribution determined by that paramaters. The formula is defined as follows,</p>
<p>$$
P(\Theta|D) = \frac{P(D|\Theta)P(\Theta)}{P(D)}
$$</p>
<ul>
<li>$P(\Theta)$ is called the prior probability, i.e. the best guess about $\Theta$ before we see the data. You might have a good estimate on it or simply have no idea at all</li>
<li>$P(D|\Theta)$ is the likelihood of the data given the parameters $\Theta$</li>
<li>$P(D)$ is the evidence or marginal probability denoted by $P(D) = \sum_{\theta \in \Theta }P(D, \theta)$</li>
<li>$P(\Theta|D)$ is the posterior probability, i.e. the updated probability of $\theta_i$ after we see the data</li>
</ul>
<h3 id="pros-and-cons">Pros and cons</h3>
<p>Pros</p>
<ul>
<li>Bayes' rule provides us a full probabilistic description of the parameters</li>
<li>It doesn&rsquo;t overfit since we are not choosing the best parameters that fit the data perfectly</li>
</ul>
<p>Cons</p>
<ul>
<li>However, we need to compute $P(D)$, whichsometimes  is not reasonable, e.g. there are too many possible values of $\Theta$</li>
<li>Posterior might not be described as a nice probability function</li>
</ul>
<h3 id="map">MAP</h3>
<p>If we ignore the evidence $P(D)$ (after all, it&rsquo;s just a constant for any $\theta$), we are only left with the numerator. An easy way to compute $P(\Theta|D$) is to find the maximum value shown below, though it&rsquo;s not a strictly probability</p>
<p>$$
{\argmax}_{\Theta} log (P(D|\Theta)) + log (P(\Theta))
$$</p>
<p>This method is called <strong>Maximum A Posterior(MAP)</strong>. However, it can overfit because we are finding the parameters that maximise the likelihood of the observed data. Thus, we are likely to get a model that fit the data with no errors.</p>
<h3 id="mle">MLE</h3>
<p>Furthermore, if we ignore the prior $P(\Theta)$, the MAP is just <strong>maximising the likelihood(MLE)</strong>, which is widely used statistics in machine learning.</p>
<h2 id="conjugate-prior">Conjugate Prior</h2>
<p>In some cases, the likelihood of the observed data $D$ is simple to compute, and the posterior would have the same form as the prior if we could find a right prior. Such a likelihood and prior distribution are said to be &lsquo;conjugate&rsquo;. Here we consider two common distributions that a prior might follow: Bernoulli and Poisson.</p>
<h3 id="bernoulli-distribution">Bernoulli Distribution</h3>
<p>Suppose we have a binary random variable $X \in \{0, 1 \}$, and $X_i=1$ if the ith trial is a head and 0 otherwise. Then the likelihood of $X_i$ given the probability of a head $\mu$ is</p>
<p>$$
P(X_i = 1|\mu) = \mu
$$</p>
<p>$$
P(X_i = 0|\mu) = 1 - \mu
$$</p>
<p>Or we can write it in this form</p>
<p>$$
P(X_i|\mu) = \mu^{X_i} (1-\mu)^{1-X_i}
$$</p>
<p>Suppose we have a data set $ D = \{ x_1, x_2, &hellip;, x_n \}$, where $x_i \in \{0, 1 \}$, assuming these observations are drawn independently, then the likelihood of $D$ can be computed as follows,</p>
<p>$$
L(D;\mu) = \prod_{i=1}^N P(X_i|\mu) =  \prod_{i=1}^N \mu^{X_i} (1-\mu)^{1-X_i} = \mu^{N_h} (1-\mu)^{N-N_h}
$$
where $N_h = \sum_i X_i$, i.e number of heads.</p>
<h4 id="beta">Beta</h4>
<p>The next step is to choose the prior $P(\Theta)$. In the case of Bernoulli, it would be better if we can find a function that has a simliar exponential parts that appeared in the above likelihood function to model the probability of every single value of $\mu$. Luckily, Beta distribution shown below is the right function we are looking for.</p>
<p>$$
P(\mu) = Beta(\mu|a, b) = \frac{\mu^{a-1} (1-\mu)^{b-1}}{B(a, b)}
$$</p>
<p>where $B(a, b)$ is a normalisation constant</p>
<p>$$
B(a, b) = \int_0^1 \mu^{a-1} (1-\mu)^{b-1} d\mu
$$</p>
<p>So how to choose a, b? It depends. If we have no idea about $\mu$, it&rsquo;s natural to assume that there are equal chances to take all vaules of $\mu$. This corresponds to a beta distribution with $a = b = 1$.</p>
<p>The last step is to plug the prior and likelihood into the Bayes' rules,</p>
<p>$$
P(\mu|D) = \frac{P(D|\mu)P(\mu)}{P(D)} =  \frac{\mu^{N_h} (1-\mu)^{N-N_h} \mu^{a-1} (1-\mu)^{b-1}}{P(D) B(a, b)} = \frac{\mu^{N_h+a-1} (1-\mu)^{N+b-N_h-1}}{P(D) B(a, b)}
$$</p>
<p>and</p>
<p>$$
P(D) = \int_0^1 \frac{\mu^{N_h+a-1} (1-\mu)^{N+b-N_h-1}} {B(a, b)} d\mu = \frac{B(N_h +a, N+b-N_h)}{B(a, b)}
$$</p>
<p>So we have</p>
<p>$$
P(\mu|D) = Beta(\mu| N_h + a, N+b-N_h)
$$</p>
<p>In summary, before we see data, we have some beliefs about $\mu$ governed by $Beta(\mu|a, b)$. After seeing the data, the probability of $\mu$ now is updated via $Beta(\mu| N_h + a, N+b-N_h)$, which can be served as the prior for the next new observations.</p>
<h3 id="incremental-updating">Incremental Updating</h3>
<p>For independent data we can update the hyperparamters incrementally, we consider an individual data at a time so that,</p>
<p>$$
P(\mu|X_1) = \frac{P(X_1|\mu)P(\mu)}{P(X_1)}
$$</p>
<p>$$
P(\mu|X_2, X_1) = \frac{P(X_2, X_1|\mu)P(\mu)}{P(X_2, X_1)} = \frac{P(X_2|\mu)P(X_1|\mu)P(\mu)}{P(X_2)P(X_1)} = \frac{P(X_2|\mu)P(\mu|X_1)}{P(X_2)}
$$</p>
<p>It&rsquo;s clear that the previous posterior now becomes the prior for the next piece of data.</p>
<h3 id="poisson-distribution">Poisson Distribution</h3>
<p>Poisson distribution measures the probability of a given number of events occuring in a specific time range,  which is given by,</p>
<p>$$
Pois(N;\theta) = \frac{e^{-\theta}\theta^N}{N!}
$$</p>
<p>where $N$ is the number of occurences in a time slot and $\theta$ is the parameter of interest. For example, we want to know the rate of traffic along a road between 8am and 9am, so $N$ is the number of cars and $\mu$ is the rate of traffic per hour.</p>
<h4 id="gamma">Gamma</h4>
<p>Then we have Gamma distribution as our prior,</p>
<p>$$
P(\theta) = \Gamma(\theta|a, b) = \frac{b^a \theta^{a-1} e^{-b\theta}}{\Gamma(a)}
$$</p>
<p>so the posterior after seeing the first piece of data is</p>
<p>$$
P(\theta|N_1) = \frac{P(N_1|\theta)P(\theta)}{P(N_1)} = \frac{b^a}{\Gamma(a) N_1! P(N_1) }e^{-(b+1)\theta} \theta^{N+a-1} \propto e^{-(b+1)\theta} \theta^{N_1+a-1}
$$</p>
<p>we can see that the posterior is also a Gamma distribution with $a_1 = a + N_1$ and $b_1 = b + 1$.</p>
<h3 id="multinominal-distribution">Multinominal Distribution</h3>
<p>In the case of Bernoulli, we only consider two outcomes: 0 or 1. But what if we have 3 or more outcomes? Well, we use multinominal distribution, which is the generalization of the binominal distribution. Suppose we have a $k$-sided dice with the probability of $\mu_k$ for $x^k = 1$, and we roll the dice $N$ times, so there are $N$ independent observations $x_1, x_2, &hellip;, x_n$, the multinominal distribution are given by,</p>
<p>$$
M(m_1, m_2, &hellip;, m_k|\mu, N) = \frac{N!}{m_1!m_2!&hellip;m_k!} \prod_{k=1}^K \mu_k^{m_k}
$$</p>
<p>where  $m_k=\sum_n^Nx_n^k$ represents the number of $x_n^k = 1$.</p>
<h4 id="dirichlet">Dirichlet</h4>
<p>TODO</p>
<h3 id="gaussian-distribution">Gaussian Distribution</h3>
<p>In the case of a single variable $x$, the Gaussian distribution is defined as,</p>
<p>$$
N(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}
$$</p>
<p>For a vector-valued random variable $\bold x \in R^d$ , We can extend it to the Multivariate Gaussian distribution</p>
<p>$$
N(\bold x|\mu, \sigma^2) = \frac{1}{\sqrt{(2\pi)^d}\sigma_1\sigma_2&hellip;\sigma_d} e^{\frac{1}{-2{\sigma_1}^2}(x_1 - \mu_1)^2 + \frac{1}{-2{\sigma_2}^2}(x_2 - \mu_2)^2 + &hellip; + \frac{1}{-2{\sigma_d}^2}(x_d - \mu_d)^2}
$$</p>
<p>The exponential part of $e$ can be re-written in a matrix form, as shown below, where</p>
<p>$$
\begin{bmatrix} x_1 &amp;x_2 &amp; &hellip; &amp; x_d \end{bmatrix} \begin{bmatrix} \sigma_1^2 \\ &amp; \sigma_2^2 &amp; \\ &amp; &hellip; \\ &amp; &amp; \sigma_d^2 \end{bmatrix}^{-1} \begin{bmatrix} x_1 \\x_2 \\ &hellip; \\ x_d \end{bmatrix}
\\ = (x - \mu)^T {\sum}^{-1}(x-\mu)
$$</p>
<p>Thus,</p>
<p>$$
N(\bold x; \mu, \sum) = \frac{1}{\sqrt {(2 \pi)^d |\sum}|} e^{-\frac{1}{2} (x - \mu)^T\sum^{-1}(x-\mu)}
$$</p>
<h4 id="the-number-of-free-parameters">The number of free parameters</h4>
<p>A general symmetric covariance matrix has $ 1 + 2 + 3 + &hellip; + D = D(D+1)/2$ independent parameters, and there are another $D$ independent parameters in $\mu$ , giving $D(D+3)/2$ parameters in total.</p>
<p>If we consider diagonal covariance matrix $\sum = diag(\sigma_i^2)$, then we have a total of $D + D = 2D$ independent parameters.</p>
<p>If we restrict the covariance matrix to be proportional to the identity matrix, $\sum=\sigma^2I$, giving $D + 1 $ independent parameters, in this case, the $PDF$ is $\frac{1}{\sqrt{2\pi}^D\sigma^D} e^{\frac{1}{-2{\sigma}^2}\sum_{i=1}^D(x_i - \mu_i)^2}$, which means the density is only related to the distance to the mean from the $x$ (different $x$ with the same distance to the mean has equal density)</p>
<p><img src="/blog/post/images/multi-gaussion.png#full" alt=""></p>
<h4 id="maximum-likelihood-estimate">Maximum likelihood Estimate</h4>
<p>Given a data set $\bold  X = (\bold x_1 , . . . , \bold x_N )^T $ in which the observations ${ x_n }$ are assumed to be drawn independently from a multivariate Gaussian distribution,</p>
<h4 id="esimation-for-mu">Esimation for $\mu$</h4>
<p>Step 1: construct the likelihood function
$$
L(\mu,\sum|D) = \prod_{i=1}^N N(\mu, \sum)
$$</p>
<p>Step 2: The log likelihood function is given by
$$
\text{In} L =  \sum_{i=1}^N {\frac{-D}{2} \text{In}2\pi - \frac{1}{2} \text{In}\sum - \frac{1}{2}(x_i-\mu)^T{\sum}^{-1} (x_i-\mu)}
$$</p>
<p>Step 3: the derivative of the log likelihood with respect to $\mu$  is given by</p>
<p>$$
\frac{ \partial L }{\partial \mu} = \sum_{i=1}^N{\sum}^{-1}(x_i - \mu)
$$
Here, we use a bit trick</p>
<p>$$
x^TMx = 2Mx
$$
Since the element of $\sum^{-1}$ is positive，we have</p>
<p>$$
\mu_{ML} = \frac{1}{N} \sum^Nx_n
$$</p>
<h4 id="estimation-for-sigma-1">Estimation for $\Sigma^{-1}$</h4>
<p>$$
trace[ABC] = trace[BAC] = trace[CAB] \\ x^TAx = tr[x^TAx] = tr[xx^TA] \\ \frac{\partial}{\partial A} tr[AB] = B^T \\ \frac{\partial}{\partial A} log |A| = A^{-T} \\ \frac{\partial}{\partial A} x^TAx = xx^T
$$</p>
<p>The derivative of the log likelihood with respect to $\sum^{-1}$  is given by</p>
<p>$$
\frac{ \partial L }{\partial \sum^{-1}} =  \sum_{i=1}^N \frac{1}{2} \sum - \frac{1}{2} (x_i - \mu) (x_i - \mu)^T
$$
Thus,</p>
<p>$$
\sum_{ML} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu) (x_i - \mu)^T
$$</p>
<h2 id="discriminal-vs-generative-models">Discriminal vs Generative Models</h2>
<p>Take the problem mentioned in the introduction as an example, suppose we have 2 classes, &lsquo;cat&rsquo; and &lsquo;dog&rsquo;, and we want to know which class the new image belong to. The problem is to find the probability $P(C=cat|x)$ and $P(C=dog|x)$, and then we classify the image into the class with the largest probability.</p>
<p>Usually, we think of our observations as given and the predictions as random variables, and we model the target variable as a function of the predictors. This is known as discriminal model. In this example, we could use logistic regression to make a classification, and the corresponding probability can be computed using a sigmoid function</p>
<p>$$
P(C=cat|X)=\frac{1}{1 + e^{-(wx + b)}}
$$</p>
<p>However, we can also consider both features and target variables at the same time. Using the Bayes' rule below, we can calculate $P(Y|X)$ in another way,</p>
<p>$$
P(C=cat|X) \propto P(X, C=cat)= P(X|C=cat)P(C=cat)
$$</p>
<p>This is known as generative model, where we use Bayes' rule to turn $P(X|Y)$ into $P(Y|X)$.</p>
<p>In discriminal model, we don&rsquo;t need the prior of classes, so there are less parameters to be determined. Also, it might have a better performance if the estimate for that prior is far away from the true distribution.</p>
<h2 id="graphical-models">Graphical Models</h2>
<p>Given 2 random variables, $X$ and $Y$, they could be dependent directly or not. Even if they are not dependent directly, typically there is still correlation between them. If they are correlated, then</p>
<ul>
<li>X could affect Y</li>
<li>Y could affect X</li>
<li>X has no direct effect on Y, but they could be both affected by another random variable Z</li>
</ul>
<p>We can describe the above relationships using a graph, where each node represents a random variable and the links between nodes show that relationship. The above three relationships are captured by the following figure,</p>
<p><img src="/blog/post/images/bayesian-network.png" alt="" title="Figure 1: Directed graphical models representing the above three conditions "></p>
<p>Such a graph is known as Bayesian networks where we use a directed graph to show causal relationships between random variables. Specifically, we add directed links between $X$ and $Y$ if $X$ directly influences $Y$ shown in the left graph of Figure 1.</p>
<h3 id="conditional-independence">Conditional Independence</h3>
<p>The left and middle graphs of Figure 1 are easy to understand since they only have two variables, and they are denoted by $P(Y|X), P(X|Y)$ respectively. However, the last one with three variables, $X, Y, Z$, is a bit tricky. Let&rsquo;s first consider the conditional distribution of $X$ given Z and Y, we have</p>
<p>$$
P(X|Z, Y) = P(X|Z)
$$</p>
<p>If we further consider the joint distribution of X and Y conditioned on Z, then we have</p>
<p>$$
P(X, Y|Z) = \frac{P(X, Y, Z)}{P(Z)} = \frac{P(Z) P(Y|Z)P(X|Z, Y)}{P(Z)} = P(X|Z)P(Y|Z)
$$</p>
<p>This is called conditional independence, which means that X and Y are statistically independent, given Z. From the view of a graphical model, we can see that there is no direct link between X and Y shown in the right graph of Figure 1.</p>
<h2 id="latent-dirichlet-allocation">Latent Dirichlet Allocation</h2>
<p>Now let&rsquo;s learn a topic modeling method that utilises the knowledge we&rsquo;ve talked about so far, Latent Dirichlet Allocation(LDA). Note this is not linear discriminant analysis, which is also abbrivated to LDA. Here, LDA is an unsupervised learning method that is used to model topics within a set of documents. Speaking of &lsquo;topic&rsquo;, it means that when you find a group of words occuring many times in an article, such as &lsquo;banana, apple, fruit, vegetable&rsquo;, you will relate them to an area, like &lsquo;Food&rsquo; in this case.</p>
<h3 id="latent">Latent</h3>
<p>In LDA, documents are represented as a fixed group of topics, which are unknown as latent variables. And these topics are characterized by a small specific set of words. For example, words like &lsquo;teacher, student, school, exam, marks&rsquo; should occur more frequently in the area of Education than topics like Sports. This means <strong>different topics have different word distribution</strong>, as shown in Figure 2.</p>
<p><img src="/blog/post/images/topic-word.png" alt="" title="Figure 2: Word distribution varies in different topics"></p>
<p>If a document contains more words like &lsquo;teacher, school&rsquo;, it&rsquo;s likely to be identified as &lsquo;Education&rsquo;. But it could contain other topics. For example, if this is a document regarding a sports contest held in a school, then it&rsquo;s much likely to be associated with &lsquo;Sports&rsquo; rather than &lsquo;Education&rsquo;. Thus, we can see that a document can also contain different topics with different weights, i.e. each document has its own topic distribution.</p>
<p>From above, we know that a document consists of a group of topics, and each topic has its special words.   To generate a document, we can randomly choose N topics for N words in a document and then randomly choose a word from the corresponding topic. Of course, such an article contains little meaning since we ignore the order and semantics of words. But it&rsquo;s reasonable for LDA since LDA treats documents just as a bag of words(BOW).</p>
<h3 id="dirichlet-1">Dirichlet</h3>
<p>So the topic-word distribution and doc-topic distribution are the unknown parameters we want to find, but there are many possible values for them. Again, we need a right prior to describe this uncertainty of the values, but which prior should we use?</p>
<p>Remember that we draw a topic from $K$ topics for each word in a document with $N$ words, it means there are $K$ possbile outcomes available for each word, and we repeat this process $N$ times, so it&rsquo;s a multinominal distribution. And this is true for drawing a word from that topic with $V$ words. Furthermore, we&rsquo;ve known that the cojugate prior of the multinominal distribution is Dirichlet distribution, so Dirichlet distribution is chosen as our prior, where</p>
<ul>
<li>doc-topic distribution follows $\theta^d \sim Dir(\alpha)$</li>
<li>topic-word distribution follows $\phi^t \sim Dir(\beta)$</li>
</ul>
<h3 id="allocation">Allocation</h3>
<p>To sum up, the whole process of generating a document in LDA is illustrated in Figure 3,</p>
<p><img src="/blog/post/images/LDA.png" alt="" title="Figure 3: Graphical model for LDA"></p>
<p>The figure looks a bit scary, well, let&rsquo;s start with some notations first,</p>
<ul>
<li>$M=|D|$, the number of documents
<ul>
<li>$D={d_1, d_2, &hellip;, d_M}$,  where $d_i$ represents $i_{th}$ document</li>
</ul>
</li>
<li>$V=|W|$, the number of vocabulary appeared in all documents
<ul>
<li>$W={w_1, w_2, &hellip;, w_V}$,  where $w_i$ represents $i_{th}$ word</li>
</ul>
</li>
<li>$K = |T|$, the number of topics
<ul>
<li>$T={t_1, t_2, &hellip;, t_K}$, where $t_k$ represents $k_{th}$ topic</li>
</ul>
</li>
<li>$N_{d}$, the number of words in a document $d$</li>
<li>$\bold w^{d}={w_1^{d}, w_2^{d}, &hellip;, w_{N_{d}}^{d}}$, where $w_i^{d}$ represents $i_{th}$ word in a document $d$</li>
<li>$\theta^d$ is a probability vector, which represents the distribution of topics in a document $d$
<ul>
<li>$\Theta = (\theta^d|d \in D)$</li>
</ul>
</li>
<li>$\phi^t$ is a probability vector, which represents the distribution of words associated with a topic $t$
<ul>
<li>$\Phi = (\phi^t|t \in T)$</li>
</ul>
</li>
<li>$\bold t^{d} = t_1^{d}, t_2^{d}, &hellip;, t_{N_{d}}^{d}$, where $t_i^{d}$ represents a topic drawn from $\theta^d$</li>
</ul>
<p>From Figure 3, the joint distribution of the hidden and observed variables for a single document is given by,</p>
<p>$$
p(\bold t^d, \bold w^d, \theta^d, \Phi|\alpha, \beta) = P(\theta^d|\alpha) P(\Phi|\beta) P(\bold  t^d, \bold w^d|\theta^d, \Phi) = P(\theta^d|\alpha) P(\Phi|\beta) \prod_n^{N_d} P(  t_n^d, w_n^d|\theta^d, \Phi)
$$</p>
<p>$$
= P(\theta^d|\alpha) P(\Phi|\beta) \prod_n^{N_d} P(t_n^d |\theta^d, \Phi) P(w_n^d|t_n^d, \theta^d, \Phi)
$$</p>
<p>$$
= P(\theta^d|\alpha) P(\Phi|\beta) \prod_n^{N_d} P(t_n^d|\theta^d) P(w_n^d|\phi^{t_n^d})
$$</p>
<p>$$
= P(\theta^d|\alpha) \prod_t^TP(\phi^t|\beta) \prod_n^{N_d} P(t_n^d|\theta^d) P(w_n^d|\phi^{t_n^d})
$$</p>
<p>Our goal is to maximise this equation, but before that we need to eliminate the hidden variable $\bold t^d$,</p>
<p>$$
p(\bold w^d, \theta^d, \Phi|\alpha, \beta) = \int_{t^d} P(\theta^d|\alpha) P(\Phi|\beta) \prod_n^{N_d} P(t_n^d|\theta^d) P(w_n^d|\phi^{t_n^d})
$$</p>
<p>$$
= P(\theta^d|\alpha) P(\Phi|\beta) \prod_n^{N_d} \sum_{t_n^d=t_1}^{t_K} P(t_n^d|\theta^d) P(w_n^d|\phi^{t_n^d})
$$</p>
<p>There are two common ways to compute this: Gibbs sampling and variational inference.</p>
<h3 id="gibbs-sampling">Gibbs Sampling</h3>
<p>TODO</p>
<h3 id="variational-inference">Variational inference</h3>
<p>TODO</p>
<h2 id="references">References</h2>
<ul>
<li><a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/</a></li>
<li><a href="https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/">https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/</a></li>
</ul>

        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/ml/constrained-optimisation/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Constrained Optimisation</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/ml/semanticweb/">
                <span class="next-text nav-default">Semantic Web</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#probability">Probability</a>
      <ul>
        <li><a href="#random-variable">Random Variable</a></li>
        <li><a href="#conditional-probability">Conditional Probability</a></li>
        <li><a href="#expectation">Expectation</a></li>
      </ul>
    </li>
    <li><a href="#bayes-inference">Bayes' Inference</a>
      <ul>
        <li><a href="#pros-and-cons">Pros and cons</a></li>
        <li><a href="#map">MAP</a></li>
        <li><a href="#mle">MLE</a></li>
      </ul>
    </li>
    <li><a href="#conjugate-prior">Conjugate Prior</a>
      <ul>
        <li><a href="#bernoulli-distribution">Bernoulli Distribution</a></li>
        <li><a href="#incremental-updating">Incremental Updating</a></li>
        <li><a href="#poisson-distribution">Poisson Distribution</a></li>
        <li><a href="#multinominal-distribution">Multinominal Distribution</a></li>
        <li><a href="#gaussian-distribution">Gaussian Distribution</a></li>
      </ul>
    </li>
    <li><a href="#discriminal-vs-generative-models">Discriminal vs Generative Models</a></li>
    <li><a href="#graphical-models">Graphical Models</a>
      <ul>
        <li><a href="#conditional-independence">Conditional Independence</a></li>
      </ul>
    </li>
    <li><a href="#latent-dirichlet-allocation">Latent Dirichlet Allocation</a>
      <ul>
        <li><a href="#latent">Latent</a></li>
        <li><a href="#dirichlet-1">Dirichlet</a></li>
        <li><a href="#allocation">Allocation</a></li>
        <li><a href="#gibbs-sampling">Gibbs Sampling</a></li>
        <li><a href="#variational-inference">Variational inference</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.7.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.c1d1af3f9a0921a0b05ae7493629f72e7ca2ac9e76b08a207b14636632f3fdf7.js" integrity="sha256-wdGvP5oJIaCwWudJNin3LnyirJ52sIogexRjZjLz/fc=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
