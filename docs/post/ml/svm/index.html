<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Support Vector Machine - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" />
  <meta name="description" content="Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,
$$ y_i d_i \ge \Delta $$
where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin." />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/ml/svm/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Support Vector Machine" />
<meta property="og:description" content="Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,
$$ y_i d_i \ge \Delta $$
where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/ml/svm/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-06-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-06-15T00:00:00+00:00" />

<meta itemprop="name" content="Support Vector Machine">
<meta itemprop="description" content="Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,
$$ y_i d_i \ge \Delta $$
where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin."><meta itemprop="datePublished" content="2021-06-15T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-06-15T00:00:00+00:00" />
<meta itemprop="wordCount" content="2003">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Support Vector Machine"/>
<meta name="twitter:description" content="Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,
$$ y_i d_i \ge \Delta $$
where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Support Vector Machine</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-06-15">
      2021-06-15
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/machine-learning/"> Machine Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <h2 id="maximise-the-margin">Maximise the margin</h2>
<p>Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,</p>
<p>$$
y_i d_i \ge \Delta
$$</p>
<p>where $d_i$ is the distance from $x_i$ to the separating plane and $\Delta$ is the margin. Our goal is to find a separating plane that maximises $\Delta$. This is known as Support Vector Machine.</p>
<h3 id="distance-to-hyperplanes">Distance to hyperplanes</h3>
<p>How to calculate the distance from a point to a hyperplane ?</p>
<p><img src="/blog/post/images/distance2plane.png" alt="" title="Figure 1: Distance from a point to a hyperplane."></p>
<p>The black line in Figure 1 represents a hyperplane defined by an orthogonal vector $w$ and a bias $b$</p>
<p>$$
w^Tx - b||w|| = 0
$$</p>
<p>The distance from the origin to the hyperplane is given by</p>
<p>$$
\frac{w^Tx_0}{||w||} = \frac{ b||w||}{||w||} = b
$$</p>
<p>Thus, the distance from a point $x_i$ ($x_i \neq 0$) to the hyperplane is given by</p>
<p>$$
d_i = \frac{w^Tx_i}{||w||} - b
$$</p>
<h3 id="primal-problem">Primal Problem</h3>
<p>Our goal is to find $w$ and $b$ to maximise $\Delta$ subject to the following constraints</p>
<p>$$
y_i ( \frac{w^Tx_i}{||w||} - b) \ge \Delta \text{ for all i = 1, 2, 3, &hellip;, n}
$$</p>
<p>Divide both sides by $\Delta$,</p>
<p>$$
y_i(\frac{w^Tx_i}{||w||\Delta} - \frac{b}{\Delta}) \ge 1
$$</p>
<p>Then define $w' = w/(||w||\Delta)$  and $b'= b/\Delta$,</p>
<p>$$
y_i(w'^Tx_i - b') \ge 1
$$</p>
<p>Note</p>
<p>$$
||w'|| = ||\frac{w}{||w||\Delta}|| = \frac{1}{\Delta}
$$</p>
<p>Therefore, minimising $||w'||^2$ is equivalent to maximising the margin $\Delta$. Now the problem becomes a quadratic programming problem defined below,</p>
<p>$$
\text{min}_{(w',b')} \frac{||w'||^2}{2} \text{ }\text{ }\text{ subject to  $y_i(w'^Tx_i - b') \ge 1$ for all data points}
$$</p>
<h3 id="extended-feature-space">Extended Feature Space</h3>
<p>However, data are not always linearly separated, such as data in Figure 2.</p>
<p><img src="/blog/post/images/non-linearly-separable.png" alt="" title="Figure 2: Non-linearly separable data"></p>
<p>Maybe we could do some data transformation and work in a new feature space where data are linearly speparable. Yea, in fact, SVM maps all feature vectors to an extended feature space by defining a special $\phi(x)$, which is a function of $x$</p>
<p>$$
x \rarr \phi(x)
$$</p>
<p>Below are some examples of $\phi(x)$. You can choose any mapping you like.</p>
<p>$$
\phi(x) = x_1^2, \phi(x) = x_2^2, \phi(x) = \sqrt {x_1x_2}
$$</p>
<h3 id="dual-form">Dual Form</h3>
<p>In the extended feature space, we substitute $x$ for $\phi(x)$. So the question is defined as follows,</p>
<p>$$
\text{min}_{(w,b)} \frac{||w||^2}{2} \text{ }\text{ }\text{ subject to  $y_i(w^T \phi(x_i) - b) \ge 1$ for all data points}
$$</p>
<p>As mentioned earlier, we can use Lagrange multiplier to solve constrained optimisation. The Lagrangian function or the primal problem is given by</p>
<p>$$
\text{min}_{w, b}\text{max}_{\alpha} \mathcal{L} (w, b, \alpha)
$$
where</p>
<p>$$
\mathcal{L} (w, b, \alpha) =  \frac{||w||^2}{2}  - \sum_{i=1}^N \alpha_i (y_i(w^T \phi(x_i) - b) - 1)
$$</p>
<p>subject to  $\alpha_i \ge 0$ (because we have inequality constraints).</p>
<p>Then we tranform the primal problem into the dual problem. We first minimise Lagrange function w.r.t $w, b$ then maximise with respect to $\alpha$</p>
<p>$$
\text{max}_{\alpha} \text{min}_{w, b} \mathcal{L} (w, b, \alpha)
$$</p>
<p>Taking the derivative of $\mathcal{L} (w, b, \alpha)$ w.r.t. $w, b$ respectively,</p>
<p>$$
\nabla_w \mathcal{L} = w - \sum_{i=1}^N \alpha_i y_i\phi(x_i)  = 0
$$</p>
<p>$$
\nabla_b \mathcal{L} = \sum_{i=1}^N \alpha_i y_i  = 0
$$</p>
<p>Substituing back to the Lagrangian function</p>
<p>$$
\text{max}_{\alpha} \frac{||w||^2}{2}  - \sum_{i=1}^N \alpha_i (y_i(w^T \phi(x_i) - b) - 1)
$$</p>
<p>$$
= \text{max}_{\alpha} \frac{1}{2} \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{i=1}^N \alpha_i y_i\phi(x_i)  - \sum_{i=1}^N (\alpha_i y_iw^T \phi(x_i) - \alpha_i y_ib - \alpha_i)
$$</p>
<p>$$
= \text{max}_{\alpha} \frac{1}{2} \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{i=1}^N \alpha_i y_i\phi(x_i)  - \sum_{i=1}^N (\alpha_i y_i (\sum_{j=1}^N \alpha_j y_j\phi(x_j)^T) \phi(x_i) - \alpha_i)
$$</p>
<p>$$
= \text{max}_{\alpha} \frac{1}{2} \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{i=1}^N \alpha_i y_i\phi(x_i)  -  \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{j=1}^N \alpha_j y_j  \phi(x_j) + \sum_{i=1}^N \alpha_i
$$</p>
<p>$$
= \text{max}_{\alpha}  \sum_{i=1}^N \alpha_i - \frac{1}{2}  \sum_{i=1}^N \alpha_i y_i\phi(x_i)^T \sum_{j=1}^N \alpha_j y_j \phi(x_j)
$$</p>
<p>$$
= \text{max}_{\alpha}  \sum_{i=1}^N \alpha_i - \frac{1}{2}  \sum_{i=1}^N \sum_{j=1}^N  \alpha_i \alpha_j y_i y_j  \phi(x_i)^T  \phi(x_j)
$$</p>
<p>The dual problem is to find $\alpha$ that maximise $\mathcal{L}$, subject to constraints</p>
<p>$$
\sum_{i=1}^N \alpha_i y_i  = 0
$$</p>
<p>The Hessian of $\mathcal{L}$ is $-\frac{1}{2} X^TX$ where $X _{ik}$= $y_k \phi_i(x_k)$, so it&rsquo;s negative seim-definite, and thus there is a unique maximum.</p>
<h2 id="soft-margin">Soft Margin</h2>
<p>Okay, let&rsquo;s just leave the dual problem alone for a while(we will go back in the next section). Now we look at another situation where we want to relax the margin constraints. In other words, we allow some samples to appear in the margin area. This is called soft margin. We do it by introducing a slack variable $s_i$ shown in Figure 3.</p>
<p><img src="/blog/post/images/svm-slack.png" alt="" title="Figure 3: Soft margin "></p>
<p>Now the constraint is</p>
<p>$$
y_i(w'^Tx_i - b') \ge 1 - s_i
$$</p>
<p>where $s_k \ge 0$. Obviously, the value of $s_i$ includes three situations</p>
<ul>
<li>For samples that lies far away from the hyperplane, $s_i = 0$, because they are far enough</li>
<li>For $0 \lt s_i \le 1$, there exist some samples that lie between margin and on the correct side of hyperplane</li>
<li>For $s_i \gt 1$, samples are misclassified</li>
</ul>
<h3 id="objective">Objective</h3>
<p>The objective function becomes</p>
<p>$$
\text{min}_{(w',b')} \frac{||w'||^2}{2}  + C\sum_{i=1}^N s_i
$$</p>
<p>where $C$ controls the degree of misclassification we desire</p>
<ul>
<li>a small C allows more freedom to relax the margin, which means it&rsquo;s acceptable to tolerate some misclassifications</li>
<li>a large C means we want to correctly classify as many samples as possbile</li>
</ul>
<p>The Lagragian function with slack variables is</p>
<p>$$
\mathcal{L} =  \frac{||w||^2}{2} +  C\sum_{i=1}^N s_i  - \sum_{i=1}^N \alpha_i (y_i(w^T \phi(x_i) - b) - 1 + s_i) - \sum_{i=1}^N \beta_is_i
$$</p>
<p>where $\beta_i$ are Lagrange multipliers that satisfy $\beta_i \ge 0$ (KKT condition)</p>
<p>Again, we minimise $\mathcal{L}$ w.r.t $s_i$</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial s_i} = C - \alpha_i - \beta_i = 0
$$
So we have</p>
<p>$$
\alpha_i = C - \beta_i
$$</p>
<p>Since $\beta_i \ge 0$,</p>
<p>$$
0 \le \alpha_i \le C
$$</p>
<h3 id="hinge-loss">Hinge Loss</h3>
<p>By the way, the objective function can also be written in the following form</p>
<p>$$
\text{min}_{(w',b')} \frac{||w'||^2}{2}  + C\sum_{i=1}^N \text{max} (0, 1-y_i f(x_i))
$$</p>
<p>where the second term is hinge loss function. So,</p>
<ul>
<li>$y_i f(x_i) \gt 1 $,
<ul>
<li>$x_i$ is outside margin and on the right side of the hyperplane, so there is no contribution to loss</li>
</ul>
</li>
<li>$y_i f(x_i) = 1 $,
<ul>
<li>$x_i$ is on the margin. Again, there is no contribution to loss</li>
</ul>
</li>
<li>$y_i f(x_i) \lt 1 $
<ul>
<li>$x_i$ lies between the margin or it&rsquo;s misclassified, so it increases the loss</li>
</ul>
</li>
</ul>
<h2 id="kernel-trick">Kernel Trick</h2>
<h3 id="kernel">Kernel</h3>
<p>Kernel is simply the inner product in feature space,</p>
<p>$$
K(x, y) = \phi(x)^T \phi(y) = K(y, x)
$$</p>
<p>where</p>
<ul>
<li>$\phi(x) = [\phi_1(x), \phi_2(x), &hellip;, \phi_k(x)]$</li>
<li>$\phi_i(x)$ are real valued functions of $x$ ( $\phi_i(x)$ is just a real number )</li>
<li>$k$ is the number of $\phi_i(x)$; it could be a specific number or INFINITE( that&rsquo;s crazy! )</li>
</ul>
<p>so kernel tells us the closeness or similarity between $x$ and $y$. The space spanned by $\phi(x)$ is called <strong>feature space</strong> or <strong>kernel space</strong></p>
<p>$$
\mathcal{K} = \text{span } [ \phi(x) | x \in \mathcal{X} ] \in R^k
$$</p>
<h3 id="symmetric">Symmetric</h3>
<p>Given a kernel $K:  \mathcal{X} \times  \mathcal{X} \rarr R$ and a training data set $\mathcal{X} = [x_1, x_2, &hellip;, x_n]$, we can construct a kernel matrix or Gram matrix $G \in R^{n \times n}$</p>
<p>$$
G_{ij} = K(x_i, x_j)
$$</p>
<p>Obviously, $G$ is a <strong>symmetric</strong> matrix and can be decomposed into $G = X^TX$, where</p>
<ul>
<li>$X$ is a $k \times n$ matrix</li>
<li>each column of $X$ is $\phi(x_i)$</li>
</ul>
<h3 id="positive-semi-definite">Positive Semi-Definite</h3>
<p>Consider a new vector $w = X v \in R^k$, we have</p>
<p>$$
||w||^2 = v^TX^T Xv = v^TGv \ge 0
$$</p>
<p>Hence, $G$ is <strong>positive semi-definite</strong> and all eigenvalues of $G$ are equal or greater than zero, i.e. $\lambda_i \ge 0$.</p>
<h3 id="eigenfunction">Eigenfunction</h3>
<p>Remember the eigenvector of $G$ is defined as follows</p>
<p>$$
Gv = \lambda v
$$</p>
<p>This means,</p>
<p>$$
\sum_{j=1}^n G_{ij} v_j = \lambda v_i
$$</p>
<p>If we extend the dimention of $G$ to INFINITE, as shown in Figure 4, we have</p>
<p>$$
\sum_{j=1}^{\infin} G_{ij} v_j = \sum_{j=1}^{\infin} K(x_i, x_j) v_j = \lambda v_i
$$</p>
<p><img src="/blog/post/images/kernel-infinite.png" alt="" title="Figure 4: G could have infinite dimension (Ref[3])"></p>
<p>Then we define an <strong>eigenfunciton</strong> $\psi(x)$, which is a real funtion of $x$</p>
<p>$$
\sum_{j=1}^{\infin} K(x_i, x_j) \psi(x_j) = \lambda \psi(x_i)
$$</p>
<p>With the help of the integral operator, we can rewrite it as follows,</p>
<p>$$
\int_{y \in \mathcal{X}} K(x, y) \psi(y) dy = \lambda \psi(x)
$$</p>
<p>Thus, $Gv = \lambda v$ is just a special case of this, when $\mathcal{X}$ is a finite set.</p>
<h3 id="mercers-theorem">Mercer’s theorem</h3>
<p>In general there will be a denumerable set of eigenfunctions $[ \psi_1(x),\psi_2(x), &hellip; ] $ and the corresponding $[\lambda_1, \lambda_2, &hellip;]$ where</p>
<p>$$
K(x, y) = \sum_i^{\infin} \lambda_i \psi_i(x) \psi_i(y)
$$</p>
<p>This is known as Mercer’s theorem. And we find that $G_{ij} = \sum_{k=1}^n \lambda v_i^k v_j^k$ is just a special case of this, when $\mathcal{X}$ is a finite set.</p>
<p>If we define  $\phi_i(x) = \sqrt \lambda_i \psi_i(x) $ then</p>
<p>$$
K(x, y) = \sum_i^{\infin} \sqrt \lambda_i \psi_i(x)  \sqrt \lambda_i\psi_i(y) = \sum_i \phi_i(x) \phi_i(y) = \phi(x)^T \phi(y)
$$</p>
<p>An immediate consequence is that for any real $\psi(x)$,</p>
<p>$$
\int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} K(x, y) \psi(y) \psi(x) dy dx
$$</p>
<p>$$
= \int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} (\sum_i^{\infin} \lambda_i \psi_i(x) \psi_i(y)) \psi(y) \psi(x) dy dx
$$</p>
<p>$$
= \sum_i^{\infin}  \int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} [\sqrt\lambda_i \psi_i(y) \psi(y)]  [\sqrt\lambda_i \psi_i(x) \psi(x) ] dydx
$$</p>
<p>$$
= \sum_i^{\infin}  \int_{x \in \mathcal{X}}[\sqrt\lambda_i \psi_i(x) \psi(x) ] (\int_{y \in \mathcal{X}} [\sqrt\lambda_i \psi_i(y) \psi(y)] dy) dx
$$</p>
<p>$$
= \sum_i^{\infin}  (\int_{x \in \mathcal{X}}[\sqrt\lambda_i \psi_i(x) \psi(x) ] dx)^2 \ge 0
$$</p>
<p>Thus, $v^TGv \ge 0$ is just a special case of this, when $\mathcal{X}$ is a finite set.</p>
<p>Putting it together, the following statements are equivalent,</p>
<ul>
<li>
<p>$K(x, y)$ is positive semi-definite</p>
</li>
<li>
<p>The eigenvalue of $K(x, y)$ are non-negative</p>
</li>
<li>
<p>The kernel can be written
$$
K(x, y) = \sum_i^{\infin} \lambda_i \psi_i(x) \psi_i(y) = \sum_i \phi_i(x) \phi_i(y)
$$
where $\phi(x)$ are real functions</p>
</li>
<li>
<p>For any real function $\psi(x)$
$$
\int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} K(x, y) \psi(y) \psi(x) dy dx \ge 0
$$</p>
</li>
</ul>
<h2 id="properties-of-kernels">Properties of Kernels</h2>
<h3 id="adding-kernels">Adding Kernels</h3>
<p>If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) + K_2(x, y)$ is also a kernel</p>
<p>$$
\int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} K_3(x, y) \psi(y) \psi(x) dy dx
$$</p>
<p>$$
= \int_{x \in \mathcal{X}} \int_{y \in \mathcal{X}} (K_1(x, y) + K_2(x, y)) \psi(y) \psi(x) dy dx \ge 0
$$</p>
<p>Similarly, If $K(x, y)$ is a valid kernel so is $cK(x, y)$ for $c \gt 0$.</p>
<h3 id="product-of-kernels">Product of Kernels</h3>
<p>If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) K_2(x, y)$ is also a kernel</p>
<p>$$
K_3(x, y) = K_1(x, y) K_2(x, y) = \sum_i \phi_i^1(x) \phi_i^1(y) \sum_j \phi_j^2(x) \phi_j^2(y)
$$</p>
<p>$$
= \sum_i \sum_j \phi_i^1(x) \phi_j^2(x) \phi_i^1(y)   \phi_j^2(y)
$$</p>
<p>Define $\phi_k(z) = \phi_i^1(z)\phi_j^2(z)$, and the number of $\phi_k(z)$ is $i * j$</p>
<p>$$
K_3(x, y) = \sum_k \phi_k(x) \phi_k(y)
$$</p>
<h3 id="exponentiating-kernels">Exponentiating Kernels</h3>
<p>$\text{exp} (K(x, y))$ is a valid kernel since</p>
<p>$$
exp(K) = 1 +  K + \frac{1}{2} K^2 + &hellip;
$$</p>
<p>since the addition and multiplication of kernels yield valid kernels, each term is also a kernel and therefore the exponential of a kernel is a kernel.</p>
<h2 id="common-kernels">Common Kernels</h2>
<p>TODO</p>
<h3 id="linear">Linear</h3>
<h3 id="gaussian">Gaussian</h3>
<h3 id="quadratic">Quadratic</h3>
<h2 id="comments">Comments</h2>
<ul>
<li>SVM relys on distances between data points, so it would be better to normalise data if we don&rsquo;t know what features are important.</li>
<li>Different C can make a great difference in performance. We can use cross-validation to choose the optimal C.</li>
<li>A linear decision boundary doesn&rsquo;t always exist, especially we obtain a poor performance. If so, we might try a non-linear boundary with Kernel. There are many kernel functions designed for particular data types. Often, Kernel comes with its parameters, so fine-tunining them is also important to improve model&rsquo;s performance.</li>
<li>We don&rsquo;t need to explicitly know what $\phi(x)$ are. All we need to know is that there exist a hyperplane that can separate the data linearly in a higher dimensional space. Though we are in the extended feature space, we do computation of the inner product in the original feature space.</li>
</ul>
<h2 id="references">References</h2>
<p>[1] <a href="https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel">https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel</a></p>
<p>[2] <a href="http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/">http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/</a></p>
<p>[3] <a href="https://www.cs.cmu.edu/~bapoczos/other_presentations/kernel_methods_01_10_2009.pdf">https://www.cs.cmu.edu/~bapoczos/other_presentations/kernel_methods_01_10_2009.pdf</a></p>

        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/ml/naive-bayes/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Naive Bayes Classification</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/ml/kmeans/">
                <span class="next-text nav-default">K-means</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#maximise-the-margin">Maximise the margin</a>
      <ul>
        <li><a href="#distance-to-hyperplanes">Distance to hyperplanes</a></li>
        <li><a href="#primal-problem">Primal Problem</a></li>
        <li><a href="#extended-feature-space">Extended Feature Space</a></li>
        <li><a href="#dual-form">Dual Form</a></li>
      </ul>
    </li>
    <li><a href="#soft-margin">Soft Margin</a>
      <ul>
        <li><a href="#objective">Objective</a></li>
        <li><a href="#hinge-loss">Hinge Loss</a></li>
      </ul>
    </li>
    <li><a href="#kernel-trick">Kernel Trick</a>
      <ul>
        <li><a href="#kernel">Kernel</a></li>
        <li><a href="#symmetric">Symmetric</a></li>
        <li><a href="#positive-semi-definite">Positive Semi-Definite</a></li>
        <li><a href="#eigenfunction">Eigenfunction</a></li>
        <li><a href="#mercers-theorem">Mercer’s theorem</a></li>
      </ul>
    </li>
    <li><a href="#properties-of-kernels">Properties of Kernels</a>
      <ul>
        <li><a href="#adding-kernels">Adding Kernels</a></li>
        <li><a href="#product-of-kernels">Product of Kernels</a></li>
        <li><a href="#exponentiating-kernels">Exponentiating Kernels</a></li>
      </ul>
    </li>
    <li><a href="#common-kernels">Common Kernels</a>
      <ul>
        <li><a href="#linear">Linear</a></li>
        <li><a href="#gaussian">Gaussian</a></li>
        <li><a href="#quadratic">Quadratic</a></li>
      </ul>
    </li>
    <li><a href="#comments">Comments</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.4484b3f29dd568c80320701800e1b69704b179f367b8223a43c728d819f39b97.js" integrity="sha256-RISz8p3VaMgDIHAYAOG2lwSxefNnuCI6Q8co2Bnzm5c=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
