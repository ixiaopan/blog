<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Ensemble Methods</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ensemble Methods"/>
<meta name="twitter:description" content="Ensemble means a group of people or a collection of things. Ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods usually defeat other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting."/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script>

    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Ensemble Methods</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 20, 2021
  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
  <p>Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.</p>
<h2 id="example">Example</h2>
<p>In real life, we often take advices from others. For example, suppose you want to know whether a movie is worthwhile to watch, you may ask your friends who&rsquo;ve watched to give the movie a score(out of 5), say <code>3, 3, 2, 2, 2, 4</code>. Since 5 people gave a score that is lower than 4, you may think about choosing another movie, let&rsquo;s say Avatar. Then you ask your friends again and have some scores like this <code>5, 5, 5, 5, 5</code>. Wow! All of your friends think that Avatar is an amazing movie that should certainly not be missed. You agree with their opinons and decide to watch Avatar finally. From this example, we can see that <strong>gathering plenty of opinions from different people</strong> are likely to make an informed decision.</p>
<p>Here, I highlight the words <strong>different people</strong>. It makes little sense if we only asks for people who have the same interests. Therefore, the more diverse the people are, the more sensible our decisions are. Basically, the idea behind it is the wisdom of collaborating.</p>
<h2 id="voting">Voting</h2>
<p>The method used to decide whether to watch a movie or not in this example is known as <strong>voting</strong>. For classification, there are 2 types of voting named <strong>hard voting</strong> and <strong>soft voting</strong>.</p>
<ul>
<li>Hard voting returns the most popular class shown in Figure 1.</li>
<li>Soft voting averages the probability of each class and then return the class that has the maximum probability.</li>
</ul>
<h3 id="hard-voting">Hard Voting</h3>
<p>
  <figure>
    <img src="/blog/post/images/ensemble-voting.png" alt="Ensemble Voting">
    <figcaption>Figure 1: Hard voting classifier predicitons (Hands-on machine learning, 2019)</figcaption>
  </figure>

</p>
<p>Figure 1 can also be illustrated  in a mathematical way,</p>
<p>$$
y' = mode(C_1(x), C_2(x), &hellip;, C_n(x))
$$</p>
<p>For example, <code>{0, 1, 0, 1, 1}</code> are the class labels predicted by our 5 different classifiers for a data point $x$. By hard voting, the final class label is <code>class 1</code> .</p>
<pre><code>C1 -&gt; 0
C2 -&gt; 1
C3 -&gt; 0
C4 -&gt; 1
C5 -&gt; 1
</code></pre><h3 id="weighted-hard-voting">Weighted Hard Voting</h3>
<p>Hard voting works nice, but in some cases, some people might be more professional than others. Hence, their opinions are much more significant. How to distinguish professionals and common people?</p>
<p>We assign weights to them. Specifically, we assign higher weights to professionals while common people have lower weights. Then we calculate weighted sum of occurrence of each class label and find the class label that has the maximum value.</p>
<p>$$
y' = \operatorname*{argmax}_i w_j\sum_j [C_j == i]
$$</p>
<p>where $[C_j == i] = 1$ if classifier $j$ predicts class label i and 0 otherwise.</p>
<p>For example, if we assign the following weights to the previous 5 classifiers, then we will have <code>0.7</code> for class 0 and<code>0.5</code> for class 1. Thus, <code>class 0</code> wins because 0.7 is greater than 0.5.</p>
<pre><code>0.4, C1 -&gt; 0
0.1, C2 -&gt; 1
0.3, C3 -&gt; 0
0.2, C4 -&gt; 1
0.2, C5 -&gt; 1
</code></pre><h3 id="soft-voting">Soft Voting</h3>
<p>Instead of predicting the class label directly, some classifiers like logistic regression can predict the probability of each class label that $x$ belongs to. Then we simply average these probabilities for each class label. Certainly, you can assign weights to classifiers.</p>
<p>$$
y' = \operatorname*{argmax}_i \frac{1}{n} \sum_j^n w_j p_{ij}
$$</p>
<p>where $p_{ij}$ is the probability of class label $i$ that $x$ belongs to when using classifier $C_j$.</p>
<h3 id="average-for-regression">Average for Regression</h3>
<p>We simply <strong>average the predictions of different machines</strong> for a regression task.</p>
<p>$$
y' = \frac{1}{n} \sum_j^n w_j C_j
$$</p>
<h2 id="bagging">Bagging</h2>
<p>In order to make our models different from each other, we use various algorithms to train the same data, as discussed above. Another way to have a set of diverse models is to train the same model on different data sets. But usually we only have one training data set. Where do other data sets come from? Well, they are sampled with replacement from the original data set, which is known as <strong>bootstrapping</strong>.</p>
<p>
  <figure>
    <img src="/blog/post/images/bootstrap.png" alt="The process of bagging">
    <figcaption>Figure 2: The process of bagging  (Hands-on machine learning, 2019)</figcaption>
  </figure>

</p>
<p>Specifically, given a training data set $D=(x_i, y_i)_i^n$ of the size $N$, we build an ensemble model of size $m$ according to the following steps:</p>
<ul>
<li>
<p>For $i=1, 2, 3, &hellip;, m$</p>
<ul>
<li>draw $N'(N' \le N)$ samples with replacement from $D$, which is denoted by $D^*_i$</li>
<li>build a model (e.g. decision tree) $T^*_i$   based on  $D_i^*$</li>
</ul>
</li>
<li>
<p>For an unseen data, aggregate the predictions of all $T^*$</p>
<ul>
<li>perform a majority vote for classification</li>
<li>average the predictions for regression</li>
</ul>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> BaggingClassifier 
<span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier

ensemble_clf <span style="color:#f92672">=</span> BaggingClassifier(DecisionTreeClassifier(), n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, max_sample<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, bootstrap<span style="color:#f92672">=</span>True)

</code></pre></div><h3 id="random-forest">Random Forest</h3>
<p>Bagging can be used for any models. Among them random forest is the special one. As its name suggests, it is exclusively designed for decision trees. Besides, it introduces extra randomness when growing trees.</p>
<p>
  <figure>
    <img src="/blog/post/images/random-forest.jpeg" alt="">
    <figcaption>Figure 3: A random subset of features at each split for each tree (Reference [2])</figcaption>
  </figure>

</p>
<p>Specifically, it randomly choose a subset of $m'$ of the features at each split instead of using all features shown in Figure 3. By doing so, all trees can have much different training data set further, so they are <strong>less similar</strong> to each other, which results in more significant predictions.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestClassifier, BaggingClassifier 
<span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier

random_forest_clf <span style="color:#f92672">=</span> BaggingClassifier(splitter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;random&#39;</span>, DecisionTreeClassifier(), max_leaf_nodes<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, max_sample<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, bootstrap<span style="color:#f92672">=</span>True)

<span style="color:#75715e">## is equivalent to this</span>
random_forest_clf<span style="color:#f92672">=</span>RandomForestClassifier(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, max_leaf_nodes<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>)

</code></pre></div><h3 id="extra-trees">Extra-Trees</h3>
<p>TODO</p>
<h3 id="why-bagging-works">Why Bagging works</h3>
<p>Take random forests as an example, each decision tree is a machine learned from a data set. Based on the theory of bias and variance, we know that the mean meachine can be expressed as $f'_m=E_D[f'(x|D)]$. Thus, $f'(x|D)$ can be interpreted as a random variable $X$, and $f'_m$ can be described as $\mu = E(X)$.</p>
<p>Since we have $N$ decision trees in a random forest, there are $N$ random variables $X_i$, where $\mu = E(X_i)$. We can construct a new random variable $Z = \frac{1}{n}\sum_i^nX_i$, which represents the mean of $n$ random <strong>independent</strong> variables $X_i$.</p>
<p>The expected value of $Z$ is given by</p>
<p>$$
E[Z] = E[ \frac{1}{n}\sum_i^nX_i ] =\frac{1}{n} E[\sum_i^nX_i] = \frac{1}{n} nE[X_i] = \mu
$$</p>
<p>The variance is</p>
<p>$$
E[(Z - E[Z])^2] = E[(\frac{1}{n}\sum_i^nX_i - \mu)^2] = \frac{1}{n^2} E[(\sum_i^n (X_i - \mu))^2]
$$</p>
<p>$$
= \frac{1}{n^2}E[\sum_i^n(X_i - \mu)^2 + \sum_i^n\sum_{j=1,i \ne j}^n (X_i -\mu)(X_j -\mu)]
$$</p>
<p>Since $X_i$ is independent of $X_j$ ( $i\ne j$ ),</p>
<p>$$
E[\sum_i^n\sum_{j=1,i \ne j}^n (X_i -\mu)(X_j -\mu)] = 0
$$</p>
<p>we are left with</p>
<p>$$
E[(Z - E[Z])^2] = \frac{1}{n^2}E[\sum_i^n(X_i - \mu)^2] = \frac{1}{n} \sigma^2
$$</p>
<p>where $E[(X_i-\mu)^2]=\sigma^2$.</p>
<p>From the above euqation, we can see that ensemble methods reduce variances as $n$ increases when our models are <strong>uncorrelated</strong>.</p>
<p>On the contrary, if our models are correlated with the correlation coefficient $\rho$</p>
<p>$$
\rho = \frac{E[(X_i - \mu)(X_j - \mu)]} {\sqrt{\sigma^2(X_i)}\sqrt{\sigma^2(X_j)}}
$$</p>
<p>The variance is</p>
<p>$$
E[(Z - E[Z])^2] = \frac{1}{n} \sigma^2 + \frac{n-1}{n} \rho \sigma^2 = \rho \sigma^2 + \frac{(1 - \rho)}{n} \sigma^2
$$</p>
<p>As $n$ increases, the second term vanishes and we are left with the first term. Therefore, if we want the ensemble methods to do well, we need our models to be uncorrelated.</p>
<p><strong>Random forests do a good job because of both the randomness of training data sets sampled via bootstrapping and the randomness of features considered at each split.</strong> The algorithm results in much less correlated trees, which trades a higher bias for a lower variance, generally yielding better results.</p>
<h3 id="feature-importance">Feature Importance</h3>
<p>Like decision tree, random forests can tell us the relative importance of each feature. It is measured by calculating the sum of the reduction in impurity over all the nodes that are split on that feature.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
random_forest_clf<span style="color:#f92672">.</span>feature_importances_

</code></pre></div><h2 id="boosting">Boosting</h2>
<h2 id="references">References</h2>
<p>[1] A. Géron, <em>Hands-on machine learning with Scikit-Learn and TensorFlow</em>. Sebastopol (CA): O&rsquo;Reilly Media, 2019.</p>
<p>[2] T. Yiu, “Understanding random forest - towards data science,” Towards Data Science, 12-Jun-2019. [Online]. Available: <a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2">https://towardsdatascience.com/understanding-random-forest-58381e0602d2</a>. [Accessed: 23-Apr-2021].</p>
<p><a href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205</a></p>
<p><a href="https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76">https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76</a></p>



      
    </article>
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
