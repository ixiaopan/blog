<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Bias-Variance dilemma</title>


  



<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Bias-Variance dilemma"/>
<meta name="twitter:description" content="When you learn more about machine learning, you must  hear people talking about high bias or high variance something like that. What does they mean by &#39;high bias&#39; or &#39;high variance&#39;?"/>



<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />

<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>

<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>

<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
<meta name="generator" content="Hugo 0.81.0" />
   
    
     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false}
          ]
      });
  });
</script>

    
  </head>
  <body>
    
  




<header>
  <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/icon.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


  
  
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Bias-Variance dilemma</h1>
        
        
        
          <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 15, 2021
  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
</span>

        
      </div>
    </div>
  </div>
  
</header>




    
  <div class="container" role="main">
    <article class="article" class="blog-post">
      
  <p>When you learn more about machine learning, you must  hear people talking about high bias or high variance something like that. What does they mean by &lsquo;high bias&rsquo; or &lsquo;high variance&rsquo;?</p>
<p>Actually, when I first heard these terms, I was completely confused. Even though I tried to find the answer on Google, I still had no idea until I took the Advanced Machine Learning module in semester 2. Therefore, I&rsquo;m writing this post to try to explain this. I hope this post can help people who are still struggling with them understand the two most important concepts clearly.</p>
<h2 id="generalization-error">Generalization Error</h2>
<p>The underlying assumption of machine learning is that there are some relationships between data. However, we are not able to know this true function, otherwise there is no need to learn it.</p>
<p>Suppose we have a true realtionship denoted by $f(x)$ (the red dot in Figure 1), and we want to construct a machine denoted by $f'(x)$ to approximate the true function based on the data $D$ sampled from the population $\chi$.</p>
<p>The <strong>training loss</strong> is defined by the following equation, where $f'(x|D)$ is the machine we learn from this particular data set $D$</p>
<p>$$
L_T(D) = \sum_{x\in D}(f'(x|D) - f(x))^2
$$</p>
<p>However, our goal is to know how well this machine works on unseen data, which is known as <strong>generalization</strong>. The generalization loss is expressed as</p>
<p>$$
L_G(D) = \sum_{x\in \chi} p(x) (f'(x|D) - f(x))^2
$$</p>
<p>If we have another data set $D_1$, then we will get another machine $f'(x|D_1)$ and another generalization loss $L_G(D_1)$ shown in Figure 1.</p>
<p>
  <figure>
    <img src="/blog/post/images/generalization-error.png" alt="generalisation error">
    <figcaption>Figure 1: Generalisation error</figcaption>
  </figure>

</p>
<p>We can see that the generalization loss is depend on our training data. Thus, the generalization loss for a particular data set doesn&rsquo;t make much sense. Instead, the average generalization loss over all the data set with the same size of $n$ is what we expect.</p>
<p>$$
E_G = E_D[L_G(D)] = E_D[\sum_{x\in \chi} p(x)(f'(x|D) - f(x))^2]
$$</p>
<h2 id="mean-machine">Mean Machine</h2>
<p>We have already known that there is a different machine $f'(x|D)$ for a given data set $D$. Thus, for an unseen data $x$, we will have many predictions of many different machines, which are represented in blut dots shown in Figure 2.</p>
<p>
  <figure>
    <img src="/blog/post/images/bias-variance.png" alt="Bias-Variance">
    <figcaption>Figure 2: Bias and variance</figcaption>
  </figure>

</p>
<p>The average prediction for an unseen data is the mean prediction(the yellow dot in Figure 2).</p>
<p>$$
f'_m(x) = E_D[f'(x|D)]
$$</p>
<h2 id="bias">Bias</h2>
<p>Bias is the distance between the mean prediction(the yellow dot) and the true value(the red dot) shown in Figure 2. High bias implies that our model is too simple and the prediction value is much far away from the true value.</p>
<p>$$
B = \sum_{x \in \chi} p(x) (f&rsquo;m - f(x))^2
$$</p>
<h2 id="variance">Variance</h2>
<p>Variance measures the variation in the prediction of the machine when we change different data set we train on. If we have a complex machine, as mentioned earlier, the machine will try its best to match every data in training data set. In other words, the machine memorized the trainining data and a little change in data set will cause significant variation in prediction.</p>
<p>$$
V = \sum_{x \in \chi}p(x) E_D[ (f'(x|D) - f&rsquo;m)^2 ]
$$</p>
<h2 id="bias-variance-dilemma">Bias-Variance dilemma</h2>
<p>Now it&rsquo;s time to decompose the average generalisation error. Let&rsquo;s plug the $f'_m(x)$ into the previous equation</p>
<p>$$
E_G = E_D[L_G(D)] = E_D[\sum_{x\in \chi} p(x)(f'(x|D) - f(x))^2]
$$</p>
<p>$$
= E_D[\sum_{x\in \chi}p(x) (f'(x|D) - f_m' + f_m' - f(x))^2]
$$</p>
<p>$$
= E_D[\sum_{x\in \chi}p(x){(f'(x|D) - f_m')^2 + (f_m' - f(x))^2 + 2(f'(x|D) - f_m')(f_m' - f(x)) }]
$$</p>
<p>It&rsquo;s noticeable that the cross-term will cancel out because $f&rsquo;m$ and $f(x)$ are constants no matter what data set $D$ is.</p>
<p>$$
E_D[\sum_{x\in \chi}p(x)2(f'(x|D) - f_m')(f_m' - f(x))]
$$</p>
<p>$$
= \sum_{x\in \chi}p(x) (2E_D[f'(x|D)]-f&rsquo;m)(f&rsquo;m-f(x)) = 0
$$</p>
<p>Therefore, we are left with</p>
<p>$$
E_G = E_D[L_G(D)] = E_D[\sum_{x\in \chi}p(x)(f'(x|D) - f_m')^2 + \sum_{x\in \chi}p(x)(f_m' - f(x))^2]
$$</p>
<p>$$
= \sum_{x\in \chi}p(x) E_D[(f'(x|D) - f_m')^2] + \sum_{x\in \chi}p(x)(f_m' - f(x))^2
$$</p>
<p>$$
= V + B
$$</p>
<p>In summary,</p>
<ul>
<li>If our machine is too simple, then we might not be able to fit the training data. Since the machine knows little about the data, it&rsquo;s unlikely to work well on unseen data. This means our model has a <strong>high bias</strong>.</li>
<li>If our machine is too complex, then we might be able to fit the training data perfectly. It means that the machine knows too much about the data, even the noise that it should not learn. Thus, it&rsquo;s too sensitive to training data so that a little change in data will cause a great variance. This means our model has a <strong>high variance</strong>.</li>
</ul>
<p>Throughout the world of machine learning, we are always trying to find a balance between bias and variance.</p>



      
    </article>
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=wxp201013@163.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://github.com/ixiaopan/" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp; &nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>




  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
