<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>Naive Bayes Classification</title>



  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-203640627-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-203640627-1');
  </script>
  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Naive Bayes Classification"/>
<meta name="twitter:description" content="In the previous articles, we introduced several classification algorithms like logistic regression. These models are often called discriminative models since they make prediction by calculating $P(Y|X)$ directly. Sometimes it might be hard to compute. Another way to think of this is that samples are generated from the existed distributions. And one of the most popular models is Naive Bayes classification."/>




<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />




<link
  crossorigin="anonymous"
  href="/blog/css/styles.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>


  <link rel="stylesheet" href="https://ixiaopan.github.io/blog/css/main.css" />



<link
  id="dark-mode-theme"
  crossorigin="anonymous"
  href="/blog/css/dark.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>



<script defer crossorigin="anonymous" src="/blog/js/theme.js" integrity=""></script>

<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

 



<meta name="generator" content="Hugo 0.91.0" />
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/me.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search/" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


    
    <div class="intro-header">
      <div class="container">
        <div class="post-heading">
          
            <h1>
              Naive Bayes Classification
            </h1>
          
          
            <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Jun 16, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;5 min read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/machine-learning/"
        >Machine Learning</a
      >&nbsp;
    
  
</span>

          
        </div>
      </div>
    </div>
    
  </header>

    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>In the previous articles, we introduced several classification algorithms like logistic regression. These models are often called discriminative models since they make prediction by calculating $P(Y|X)$ directly. Sometimes it might be hard to compute. Another way to think of this is that samples are generated from the existed distributions. And one of the most popular models is Naive Bayes classification.</p>
<h2 id="model">Model</h2>
<p>Given a sample $\bold x = [x_1, x_2, &hellip;, x_d]$, where $x_i$ represent each individual feature, we want to know the probability of $\bold x$ belonging to each class $C_k$. With the aid of Bayes' inference, we can inverse this problem into the forward problem</p>
<p>$$
P(Y=C_k|X= \bold x) = \frac{P(X= \bold x|Y=C_k)P(Y=C_k)}{P(X= \bold x)}
$$</p>
<p>Then, Naive Bayes makes the following assumptions</p>
<ul>
<li>all features are independent â€” one paticular feature doesn&rsquo;t affect another, though it&rsquo;s not real in practice. That&rsquo;s why it&rsquo;s called naive (lack of experience)</li>
<li>all features have equal importance</li>
</ul>
<p>Based on the above assumptions, we can rewrite Bayes' theorem as follows,</p>
<p>$$
P(Y=C_k|X=\bold x) = \frac{P(x_1|Y=C_k)P(x_2|Y=C_k)&hellip;P(x_d|Y=C_k)P(Y=C_k)}{P(X=\bold x)}
$$</p>
<p>Since $P(X=\bold x)$ is the evidence,</p>
<p>$$
P(Y=C_k|X=\bold x) \propto P(Y=C_k) \prod_{i=1}^d P(x_i|Y=C_k)
$$</p>
<p>Now the question is that where $P(x_i|Y=C_k)$ comes from. What distribution does the class $C_k$ follow? In Naive Bayes, there are three typical distributions</p>
<ul>
<li>Bernoulli</li>
<li>Multinonimal</li>
<li>Gaussian</li>
</ul>
<h2 id="bernoulli-naive-bayes">Bernoulli Naive Bayes</h2>
<p>In Bernoulli distribution, the outcome of an event, denoted by $y_i$, is either 1 or 0. Let $p$ be the probability of being 1, so we have</p>
<p>$$
P(Y=y_i) = y_ip +  (1- y_i )(1 - p)
$$</p>
<p>Since 0 or 1 can encode the occurrence of an event, we can use this to filter spam email. We don&rsquo;t care how many suspect words occur in an email. Instead, once they occur, this email should gain more attention. Hence, each email can be represented by a word vector with binary element taking value 1 if the corresponding word appears in the email and 0 otherwise,</p>
<p>$$
\bold  x = (x_1, x_2, &hellip;, x_{|v|}), \text{ where } x_i \in {1, 0}
$$</p>
<p>where  $V$ is the vocabulary of the training emails. In other words, we do $|V|$ Bernoullis trials for each email. For each word in $V$, if it&rsquo;s present in that email, the outcome is 1, otherwise 0. This repeats $|V|$ times until all words in $V$ have been visited.</p>
<p>Now suppose there are $D$ emails, including $D_s$ spam emails and $D_h$ ham emails, given a new email $\bold  x $, we want to know whether it&rsquo;s a spam email or not. Based on the model defined above, we need to calculate two things</p>
<ul>
<li>the probability of being each class, $P(Y = C_k)$</li>
<li>given that class $C_k$, the probability of sampling the new data $x$, $P(X=\bold x|Y=C_k) = \prod_{i=1}^V P(x_i|Y=C_k)$</li>
</ul>
<p>The first question can be solved by the following equation,</p>
<p>$$
P(Y=S) = \frac{|D_s|}{|D|}, P(Y=H) = 1 - P(Y=S)
$$</p>
<p>Based on the assumption of Naive Bayes, we consider each word in the vocabulary as a single independent random variable that follows Bernoulli distribution with probability $p$, where $p$ is the probability of a word $x_i$ occurring in an email of class $C_k$ and can be computed as follows,</p>
<p>$$
p(x_i | Y=C_k) = \frac{|D_s(w_i)|}{|D_s|}
$$</p>
<p>where $|D_s(w_i)|$ represent the number of emails that belong to class $C_i$ and contain the word $x_i$. Therefore, the probability of $\bold x$ being classified into the spam category is</p>
<p>$$
P(Y = S| X = \bold x ) \propto P(Y=S) \prod_{i=1}^V P(x_i|Y = S)
$$</p>
<p>$$
= \frac{|D_s|}{|D|} \prod_{i=1}^V \text{ } y_i  P(x_i|Y=S)  +  (1- y_i )(1 - P(x_i|Y=S))
$$</p>
<h2 id="multinomial-naive-bayes">Multinomial Naive Bayes</h2>
<p>In multinomnial distribution, there are $K$ possible outcomes, so the probability is</p>
<p>$$
p(x_k) = \prod_{k}^K \mu_k^{I_{k}}, \text{ where } _{k} = 1 \text{ if } x_k = 1, \text{otherwise } 0
$$</p>
<p>If we do $N$ trials, the likelihood of the observations is</p>
<p>$$
P(m_1, m_2, &hellip;, m_k) = \prod_{n=1}^N \prod_{k}^K \mu_k^{I_{nk}} = \frac{N!}{m_1!m_2!&hellip;m_k!}\prod_{k}^K \mu_k^{m_k}
$$</p>
<p>where  $m_k=\sum_n^N I_{nk}$ represents the number of $I_{nk} = 1$ and $\sum_{i=1}^k m_i = N$. Through MLE, $\mu_k$ is esimated as</p>
<p>$$
\mu_k = \frac{m_k}{N}
$$</p>
<p>Sometimes we are more interested in the frequency of a word, for example, sentiment classification. If words about happy occur more frequently, then we&rsquo;re more likely to classify the text as the positive category. In this case, each text can be represented by a word vector with element whose value is the frequency of that word.</p>
<p>$$
\bold  x = (x_1, x_2, &hellip;, x_{|V|}), \text{ where } x_i \in Z
$$</p>
<p>where $V$ is the vocabulary of the training texts. Thus, we can see that a text can be generated by doing $|N_x|$ multinominal trials.</p>
<p>Suppose there are $D$ texts, including $D_p$ positive texts and $D_n$ negative texts, given a new text $\bold  x $, we want to know whether the sentiment that that text shows is positve or negative.</p>
<p>Again, the probability of being a class $C_k$ is given by,</p>
<p>$$
P(Y=\text{Positive}) = \frac{|D_p|}{|D|}, P(Y=\text{Negative}) = 1 - P(Y=\text{Positive}) = \frac{|D_n|}{|D|}
$$</p>
<p>The likelihood of generating the sample $\bold x$ is given by,</p>
<p>$$
P(X=\bold x|Y=C_k) = P(x_1, x_2, &hellip;, x_{|V|}|Y=C_k) \propto \prod_{k}^V \mu_k^{x_k}
$$</p>
<p>where $x_k$ represents the frequency of that word that occurs in the text $\bold x$ and  $\sum_{i=1}^V x_i = N_x$.</p>
<p>For words that not occur in the text, $\mu_k^0 = 1$. Thus, the posterior probability can be rewritten as follows,</p>
<p>$$
P(Y = \text{Positive} | X = \bold x ) \propto P(Y= \text{Positive} ) \prod_{k=1}^{N_x} \mu_k
$$</p>
<h2 id="gaussian-naive-bayes">Gaussian Naive Bayes</h2>
<p>Similarly, Gaussian Naive Bayes assumes that data from each class follows a Gaussian distribution.</p>
<h2 id="zero-probability">Zero Probability</h2>
<p>If a new sample contains a word that is unseen in the training data, the probability is zero.</p>
<p>$$
\mu = \frac{m_k}{N} = 0
$$</p>
<p>If one of the term $\mu_k$ is zero, the whole probability will be zero too. One way to avoid this is to use add-one smoothing, adds a count of one to each unique word.</p>
<p>$$
\mu = \frac{m_k + 1} {N + |V|}
$$</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf">https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf</a></li>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html">https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html</a></li>
<li><a href="https://medium.com/atoti/how-to-solve-the-zero-frequency-problem-in-naive-bayes-cd001cabe211">https://medium.com/atoti/how-to-solve-the-zero-frequency-problem-in-naive-bayes-cd001cabe211</a></li>
</ul>



        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#model">Model</a></li>
    <li><a href="#bernoulli-naive-bayes">Bernoulli Naive Bayes</a></li>
    <li><a href="#multinomial-naive-bayes">Multinomial Naive Bayes</a></li>
    <li><a href="#gaussian-naive-bayes">Gaussian Naive Bayes</a></li>
    <li><a href="#zero-probability">Zero Probability</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      
      </aside>
    </div>
    
    
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=xiaopan.wpp@outlook.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/ixiaopan" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/ixiaopan" name="Linkedin">
        <em class="fab fa-linkedin"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2022
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
