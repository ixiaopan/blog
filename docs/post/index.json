[
    
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression/",
                "title": "Linear Regression 01",
                "section": "post",
                "date" : "2021.04.14",
                "body": "There are two main tasks in machine learning: regression and classfication. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and maybe it's the first algorithm that most of people learn in the world of machine learning. So let's go!\nProblem statement Suppose you are a teacher, and you record some data about the hours students spent on study and the grades they achieved. Then you want to predict the grade for given hours that someone spent. Here are some sample data you collected:\n   Hours 0.5 1 2 3 4     Grade 20 21 22 24 25    Since there are only two variables, let's plot them.\nFigure 1: The scatter plot of hours and grade\nWell, we can see that the variable \\(grade\\) is positive related to the variable \\(hours\\). For simplicity, we can use a simple line(the red line in this figure) to approximate this relationship. And this is exactly our first simple linear model.\nSimple Linear Regression Remember a line equation is written in this way:\n\\[ y = ax + b \\]\nIn this example, \\(x\\) is the variable \\(hours\\) and \\(y\\) is the variable \\(grade\\) , which we already know. So the problem is how to calculate the parameter \\(a, b\\). Technically, this is called parameter estimation. Usually, there are two ways to do this: minimising the loss and maximising likelihood. Now we focus on minimising the loss.\nLoss What is the loss? Basically, it's the error between the esitmated value and our true value. Minimising the error is simply to make the estimated value as close to the true value as possible.\nFigure 2: The error for a single data (Bradthiessen.com 2021)\nFor a single data point, the loss function is defined below, where \\(y\\) is the true value and \\(y'\\) is our estimated value for a given \\(a, b\\).\n\\[ error = y_i - y'_i = y_i - ax_i - b \\]\nSince we have many data points, we need to sum up them all to evaluate the overall errors.\n1. error Unfortunately, some error terms will cancel out when you do this calculation directly.\n\\[ total \\ error = \\sum_i^n (y_i - y'_i) = \\sum_i^n (y_i - ax_i - b) \\]\n2. the absolute value of error One way to tackle this is taking the absolute value of the error terms.\n\\[ total \\ error = \\sum_i^n |y_i - y_i'| \\]\nHowever, the absoulte value of \\(x\\) is not differentiable at \\(0\\).\n3. the squared value of error Instead of taking absolute value, we will square all the errors. One reason is that the errors will become larger and can be distinguished easily when squaring them. It looks like the errors are zoomed in and we can find them and minimize them quickly.\n\\[ L = \\sum_i^n (y_i - y_i')^2 \\]\nPS: We will revisit the squared error later from the perspective of MLE.\nClosed-form solution Okay, finally we find a function to measure the loss. Next we need to find the parameters that minimize the squared error. Good news is that our loss function is differentiable and convex! It means that we have a global minimial value and can be calculated directly by taking derivatives.\nLet's take the first derivatve of \\(b\\)\n\\[ \\frac{\\partial L}{\\partial b} = \\sum_i^n -2(y_i - ax_i-b) \\]\nand then set this equation to \\(0\\),\n\\[ -2(\\sum_i^ny_i -a\\sum_i^nx_i - \\sum_i^nb) = -2(n\\overline y-an\\overline x - nb) = 0 \\\\ b = \\overline y-a\\overline x \\]\nLet's take the first derivatve of \\(a\\)\n\\[ \\frac{\\partial L}{\\partial a} = \\sum_i^n -2x_i(y_i - ax_i-b) \\]\nand then plug \\(b\\) into this equaiton and set this equation to \\(0\\) again,\n\\[ \\sum_i^n -2x_i(y_i - ax_i-\\overline y+a\\overline x) = \\sum_i^n -2x_i[(y_i-\\overline y)- a(x_i -\\overline x)]\\\\ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} \\]\nHere, we use a slight algebra trick,\n\\[ a\\sum_i^n(x_i - \\overline x_i) = 0 \\]\nThen we plug this into the previous equation\n\\[ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} = \\frac{\\sum_i^nx_i(y_i-\\overline y) - \\sum_i^n\\overline x(y_i - \\overline y)}{\\sum_i^nx_i(x_i -\\overline x) - \\sum_i^n\\overline x(x_i - \\overline x)}\\\\ = \\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2}\\\\ = \\frac{Cov(x, y)}{Var(x)} \\]\nFinally, we find the best estimators for simple linear regression.\nReference [1] Bradthiessen.com, 2021. [Online]. Available: https://www.bradthiessen.com/html5/docs/ols.pdf. [Accessed: 14- Apr- 2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/descriptive-statistics/",
                "title": "Descriptive Statistics",
                "section": "post",
                "date" : "2021.04.13",
                "body": "In this post , I'm going to go through some basic concepts of statistics required in Data Science.\nThere are two main branches of statistics :\n descriptive statistics tells us the statistics about data like mean, mode and standard deviation, which you've learned in high school.\n inferential statistics, on the other hand, uses a random dataset sampled from population to make inferences about population.\n  We will firstly focus on descriptive statistics, specifically, the central tendency and dispersion. Central tendency measures the center of the data while dispersion measures how spread out a given data is.\nCentral tendency Mean, mode and median are three mainly used measures of central tendency.\nMean Mean is the average of the data and calculated by summing up all data values and then dividing them by the number of data.\n\\[ \\overline x = \\frac{\\sum_i^nx_i}{n} \\]\nFor instance, say we have a group of data 1,2,3,4,5,6, the mean is 3.5.\nMedian Though mean is widely used, it's sensitive to outliers. Suppose you have a set of data 1,2,3,4,5,6,100, after some calculations, you find that the mean is 17.28. However, it seems a bit strange since most of the data values is less than 10 except one extreme value 100, which stretches the distribution of the whole data set to the right. This is why median comes.\nTo get the median, firstly we need to sort the data in ascending order and then find the middle number that separate the data into two groups with the same size. In this example, the median is 4.\nMode Mode is the most frequent value. There could be one, two or more data values that have the same frequency and that freqency is the highest.\nDispersion Range Range is the distance between the maximum value and the minimum value. Again, range is sensitive to outliers.\n\\[ r=max - min \\]\nQuantile Quantiles are used to divide data into several equal-sized groups. The most widely used cut points are 0, 25, 50, 75, 100, denoted by min, Q1, Q2, Q3, max respectively.\nIQR or interquartile range measures where the central 50% of the data is.\n\\[ IQR = Q3 - Q1 \\]\nIQR can be used to detect outliers. Data that is greater than upper boundary or less than lower boundary can be considered as an outlier.\n\\[ upper \\ boundary = Q3 + 1.5*IQR \\\\ \\]\n\\[ lower \\ boundary = Q1 - 1.5*IQR \\]\nVariance Deviation is the distance between a given data point and the mean. Since there are many data points, the variance calculates the average deviation from the mean.\nBut when you do this, you will always get a zero due to the definition of the mean.\n\\[ \\sum_i^n d_i = \\sum_i^n (x_i - \\overline x) = \\sum_i^nx_i - n\\overline x = n\\overline x - n\\overline x = 0 \\]\nFor simplicity, I omit the denominator n.\nOkay, let's ignore the sign and use the absolute value of the deviation.\n\\[ d_i = |x - \\overline x| \\]\nThough it works, the most popularly used method is calculate the square of the deviation.\n\\[ s^2 = \\frac{\\sum_i^n (x_i - \\overline x)^2}{n-1} \\]\nThough variance works fine, the value is a bit hard to interpret. For instance, we have 15 records of fish size measured in kilogram:\n[ 2.1, 2.4, 2.4, 2.4, 2.4, 2.6, 2.9, 3.2, 3.2, 3.9, 4.5, 6.3, 8.2, 12.8, 23.5 ].\nThe variance is 30.97. It means that if we randomly catch a fish, its weight would be 30.97 squared kilogram far away from the average weight. Emm...squared kilogram is an odd unit.\nStandard Deviation It's simple to solve this problem by taking the square root of the variance, which is standard deviation.\n\\[ s = \\sqrt{\\frac{\\sum (x - \\overline x)^2}{n-1}} \\]\nSo the standard deviation in the previous example is 5.56kg, which makes much sense.\nDistribution Skewness Skewness measures the symmetry of a distribution. It's quite common to have non-symmetric distributions.\n\nA left-/negative-skewed distribution\n it has a long left tail the mean is on the left of the median  A right-/positive-skewed distribution\n it has a long right tail the mean is on the right of the median  A skewed distribution implies that there are some special values that are larger/smaller than the common values. Let's see an example.\n\nFigure 3.2 shows the histogram of the fish sizes gathered from a fisherman. We can see that this distribution is right-skewed. The majority of fish sizes are between 0 and 3kg and there are a few special fishes that weigh over 3.5kg. It also can be seen that the average weight(1.67kg) is a bit higher than the median(1.62kg) shown in Table 3.2.\nReference [1] B. al., \u0026quot;Introduction to Statistics | Simple Book Production\u0026quot;, Courses.lumenlearning.com, 2021. [Online]. Available: https://courses.lumenlearning.com/introstats1. [Accessed: 14- Apr- 2021].\n"
            }
        
    
]