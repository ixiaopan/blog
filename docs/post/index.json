[
    
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/dl-00-pytorch/",
                "title": "Deep Learning - PyTorch",
                "section": "post",
                "date" : "2021.07.19",
                "body": "The most popular deep learning frameworks so far are Tensorflow and PyTorch. Well, during my study, I use PyTorch more often. Recently, I am building the classic BiLSTM-CRF model using PyTorch. It\u0026rsquo;s a bit hard for me when operating matrices since it uses various advanced techniques about indexing and slicing. I think it\u0026rsquo;s necessary to explain these amazing functions. Therefore, I am going to write this post to revisit the most important aspects of PyTorch for future references.\nTensor Tensor is the basic data structure in PyTorch. Simply put, a tensor means a multi-dimensional array where you can access an individual element by indexing. The dimension could be zero, one, two, three and so on. The most common tensors are scalars, vectors, and matrices shown below.\ntorch.tensor(1.) # scalar torch.tensor([1, 2, 3]) # vector torch.arange(12).reshape(3, 4) # matrix torch.arange(12).reshape(2, 3, 2) # matrix storage In fact, the tensor declared above is just a view of the underlying data. View means a kind of way to look at data. For example, you can look at an image from the top direction or the right direction. In the case of data, you could look at data in the original order or you could skip a fixed number of elements.\nNo matter how you view it, the data in memory stay the same. In fact, the real values are stored in a contiguous block of memory. In PyTorch, we can access it by invoking tensor.storage(). For example,\nx = torch.tensor([[0, 1, 2], [3, 4, 5]]) x.storage() The result is shown below. You might find that the elements are sorted along the rows of the tensor $x$. Tthe difference is that they are stored in a one-dimensional array.\n  Figure 1: The underlying data beneath the tensor x  Here comes a question: How does the indexing operation $x[0, 1]$ work?\nstride In oder to index an element, PyTorch needs to know meta data about a tensor, such as stride, which indicates the number of elements to be skipped along each dimension. In the above example, the stride is\nx.stride() # (3, 1) $3$ means we need to skip 3 elements to get to the next row, 1 means we just move one step so as to reach the next column. Mathematically, the indexing operation in 2D tensor are described as follows, where offset is usually zero\n$$ x[i, j] = i * \\text{stride}[0] + j * \\text{stride}[1] + \\text{offset} $$\nWhy is it designed like this? It\u0026rsquo;s less expensive for some operations like transpose since there is no need to reallocate memory space. Instead, we just modify the stride.\nFor instance, $x^T$ is shown below\n0 3 1 4 2 5 If we look at the underlying data of $x^T$, we will find that $x^T$ and $x$ have the same storage shown in Figure 1.\nx.data_ptr() == xt.data_ptr() # True Let\u0026rsquo;s have a look at the stride of $x^T$\nxt.stride() #(1, 3) In this case, we simply move one step forward if we want to reach the next row while we need to move 3 steps to get to the next column.\nApart from this, knowing how storage and stride work will also help understand other PyTorch functions' behaviour. This article discusses the difference between torch.expand() and torch.repeat(). Accoding to the PyTorch document\n torch.expand()\n​\tExpanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.\ntorch.repeat()\n​\tUnlike expand(), this function copies the tensor’s data.\n x = torch.arange(3) x.stride() # 1 y = x.expand(2, 3) y.stride() # (0, 1) z=x.repeat(2, 1) z.stride() # (3, 1) From the results of strides, we can see that torch.tensor.expand() does not require extra memory space. This is essential when working with large data set.\ncontiguous In Pytorch, there is a concept called contiguous, indicating whether a tensor has the same values as the storage when counting from the innermost dimension. In other words, when moving along the rows in 2D tensors, the values sorted in this way are exactly the order of the underlying data. Visually, such an order is more comfortable and consistent.\nFor example, if we flatten $x$ along the innermost dimension, we will get $$ 0, 1, 2, 3, 4, 5 $$\nIf we flatten $x^T$ along the innermost dimension, we will get\n$$ 0, 3, 1, 4, 2, 5 $$\nwhich is different from the storage shown in Figure 1. Therefore, we say $x$ is contiguous while $x^T$ is not. We can also check it in Pytorch shown below.\nx.is_contiguous() # True xt.is_contiguous() # False Why But wait, why do we need contiguous tensor?\nOne reason is that we can exploit the benefit of cache to improve the speed of fetching data. In short, when fetching an element of a matrix, we can also get the neighboring elements at the same time, requiring less memory accesses.\nAnother reason is that some functions only work in the contiguous tensor, such as view()\nxt.view(1, 6) # RuntimeError: view size is not compatible with input tensor\u0026#39;s size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. There are two approaches to fix it: call contiguous() to make the tensor contiguous or call reshape()\nxt.contiguous().view(1, 6) xt.reshape(1, 6) However, reshape() could return a view (if the tensor is contiguous) or a new tensor (if not).\nHow How do we make $x^T$ contiguous too?\nxtc = xt.contiguous()   Figure 2: The underlying data beneath the tensor $x^T$  Figure 2 shows the new storage. Thus, contiguous() reallocates a new memory and copy the original tensor into that memory space. We can check it using the code below.\nxt.data_ptr() == xtc.data_ptr() # False Meanwhile, the stride has also been changed to adapt to this new storage.\nxT.stride() # (2, 1) torch.nn nn.module torch.nn is the submodule that contains everything needed to build neural networks in PyTorch. We know that a neural network is composed of a stack of layers. In PyTorch, we call these layers modules and they are all subclasses of nn.Module.\nI think we call them modules because a single layer is a layer while multiple layers can also be considered as a big layer. Thus, we refer to both of them as modules regardless how many layers there are. Moreover, a module can also have other submodules (subclasses of nn.Module) as their attributes and thus track their parameters automatically.\nWhen I built the BiLSTM-CRF for NER, I split LSTM and CRF apart. However, I noticed that the BiLSTM model can still retrieve the parameters of the CRF layer. Now I found the reason.\nFirst, both the nn.Linear() and CRF() are moules in PyTorch. Second, when the instance of such a module is assigned to an attribute of another nn.Module ( in this case, it\u0026rsquo;s BiLSTM ), this module will be automatically registered as the submodule of BiLSTM, and thus BiLSTM have access to the parameters of its submodules.\nclass CRF(nn.Module): def __init__(self): super(CRF, self).__init__() class BiLSTM(nn.Module): def __init__(self): super(BiLSTM, self).__init__() self.fc = nn.Linear(10, 2) self.crf = CRF(self.num_of_tag) forward When I first used PyTorch, I often confused why this model doesn\u0026rsquo;t invoke forward(). For example, I have a simple forward network, I have seen two options shown below to move forward.\nself.fc = nn.Linear(5, 4) self.fc(inputs) # option 1, the right way self.fc.forward(inputs) # option 2, wrong Well, the reason is simple. In fact, the built-in PyTorch subclasses of nn.Module allows themselves to be called as a simple function call. In the __call__, it calls forward(). So what\u0026rsquo;s the difference? The difference is that PyTorch will do other operations by provided hooks before calling forward(). Thus, it\u0026rsquo;s possible to call forward() directly, but it shouldn\u0026rsquo;t do this unless we don\u0026rsquo;t provide any hooks. ( More details can be seen here )\nparameter We have built our models, so how to access to the parameters? PyTorch provides parameters() function, which allows us to collect all parameters from the first layer to the last layer.\nfor param in model.parameters(): print(param) However, it\u0026rsquo;s a bit hard to distinguish them when you have many layers. Thus, PyTorch provides another function named_parameters()\nfor name, param in model.named_parameters(): print(name, param) With the name of each layer, it\u0026rsquo;s easy to retrieve individual parameter by accessing the corresponding name directly, such as\nmodel.out_linear.weight model.out_linear.bias So far so good. But where are the names from?\nsequential PyTorch provide two ways to create neural networks. The first one is to use nn.Sequential(), as shown below\nmodel = nn.Sequential( nn.Linear(5, 4), nn.Tanh(), nn.Linear(4, 2) ) \u0026#39;\u0026#39;\u0026#39; Sequential( (0): Linear(in_features=5, out_features=4, bias=True) (1): Tanh() (2): Linear(in_features=4, out_features=2, bias=True) ) \u0026#39;\u0026#39;\u0026#39; Well, the names in this example are just numbers. It would be fine if there are few layers. To build a more semantic model structure, we can pass name for each layer,\nfrom collections import OrderedDict model = torch.nn.Sequential(OrderedDict([ (\u0026#39;hidden_layer\u0026#39;, torch.nn.Linear(5, 4)), (\u0026#39;active_func\u0026#39;, torch.nn.Tanh()), (\u0026#39;out_layer\u0026#39;, torch.nn.Linear(4, 2)) ])) model \u0026#39;\u0026#39;\u0026#39; Sequential( (hidden_layer): Linear(in_features=5, out_features=4, bias=True) (active_func): Tanh() (out_layer): Linear(in_features=4, out_features=2, bias=True) ) \u0026#39;\u0026#39;\u0026#39; As mentioned earlier, we can retrieve the parameters for an individual layer by accessing its name, such as\nmodel.hidden_layer.bias \u0026#39;\u0026#39;\u0026#39; Parameter containing: tensor([ 0.1177, -0.2876, -0.3861, -0.3367], requires_grad=True) \u0026#39;\u0026#39;\u0026#39; Though nn.Sequential() is handy, the order of layers is fixed. That\u0026rsquo;s where nn.Module comes into play. We rewrite the above code using nn.Module\nclass SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.hidden_layer = nn.Linear(5, 4) self.out_layer = nn.Linear(4, 2) def forward(self, inputs): outs = self.hidden_layer(inputs) outs = torch.tanh(outs) outs = self.out_layer(outs) return outs net = SimpleNet() \u0026#39;\u0026#39;\u0026#39; SimpleNet( (hidden_layer): Linear(in_features=5, out_features=4, bias=True) (out_layer): Linear(in_features=4, out_features=2, bias=True) ) \u0026#39;\u0026#39;\u0026#39; Advanced Techniques torch.scatter_ This function is often used to implement one-hot encoding. As its name suggests, scatter means to distribute a list of values over another tensor.\nIn the case of one-hot encoding, the matrix is a parse matrix whose element value is either zero or one. So one is the scalar to be scattered at the corresponding column index for each observation.\nlabel = torch.tensor([2, 3, 4, 0]).view(4, 1) one_hot = torch.zeros(len(label), 5) one_hot.scatter_(1, label, 1) \u0026#39;\u0026#39;\u0026#39; tensor([[0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [1., 0., 0., 0., 0.]]) \u0026#39;\u0026#39;\u0026#39; Further, the scatterd item could be any multi-dimensional array rather than a simple scalar, for example,\nlabels = torch.tensor([[2, 3], [4, 0], [1, 5]]) # 3 observations one_hot = torch.zeros(len(labels), 6).long() # 6 labels in total source = torch.arange(1, 7).long().view(3, 2) one_hot.scatter_(1, labels, source) \u0026#39;\u0026#39;\u0026#39; tensor([[0, 0, 1, 2, 0, 0], [4, 0, 0, 0, 3, 0], [0, 5, 0, 0, 0, 6]]) \u0026#39;\u0026#39;\u0026#39; From the above examples, we conclude that\n labels and source should have the same size along the specified dimension (the first argument) All tensors (one_hot, lables, source) should keep the same shape except the specified dimension  torch.squeeze() squeeze() and unsqueeze() often appear in pairs to expand dimension of a tensor for broadcasting.\n  squeeze() remove all dimensions with the value of 1\nx = torch.arange(8).view(1,2,4) \u0026#39;\u0026#39;\u0026#39; tensor([[[0, 1, 2, 3], [4, 5, 6, 7]]]) \u0026#39;\u0026#39;\u0026#39; x.squeeze(0) \u0026#39;\u0026#39;\u0026#39; tensor([[0, 1, 2, 3], [4, 5, 6, 7]]) \u0026#39;\u0026#39;\u0026#39;   unsqueeze() add one dimension along the specified dimension\n​\nx = torch.tensor([1, 2, 3]) x.unsqueeze(dim=0) # tensor([[1, 2, 3]]) x.unsqueeze(dim=1) \u0026#39;\u0026#39;\u0026#39; tensor([[1], [2], [3]]) \u0026#39;\u0026#39;\u0026#39;   torch.gather() I happened to meet this function when building BiLSTM+CRF NER models. It was hard to implement batch training for CRF layers until I found torch.gather(). Well, it\u0026rsquo;s a bit similar to torch.scatter_() except that torch.gather() aims to fetch data while toch.scatter_() is used to write values.\nFor BilSTM+CRF models, we need to calculate the sentence score as follows,\n$$ \\text{Score} (D_j) = \\sum_{i=0}^{|D_j|} T(y^{w_i} \\rarr y^{w_{(i+1)}}) + E(w_{i+i}|y^{w_{(i+1)}}) $$\nwhere emission scores are the outputs of BiLSTM and the transition scores are the parameters of the CRF layer. For each batch training, we need to calculate each sentence score in this batch.\nemission score: (batch_size, max_seq_len, num_of_tag) [ [ O B I [ w00 0.1 0.2 0.7 ] [ w01 0.2 0.4 0.4 ] ... ], # sentence 1 [ O B I [ w10 0.1 0.2 0.7 ] [ w11 0.2 0.4 0.4 ] ... ] # sentence 2 ] tags: (batch_size, max_seq_len) [ [ w00=\u0026gt;B, w01=\u0026gt;I, ...], # sentence 1 [ w10=\u0026gt;B, w11=\u0026gt;O, ...], # sentence 2 ... ] It\u0026rsquo;s easy to fetch data from the specified index along the desired dimension using torch.gather(). In this case, we want to fetch data from the index that the true tag of each word belongs to along the innermost dimension of emission_score. For example, sentence 1 has two words with the labels B and I, so the corresponding scores in the emission score are emission_score[0][0][1] and emission_score[0][1][2].\nemission_score = torch.tensor([ [ [ 0.1, 0.2, 0.7 ], [ 0.2, 0.3, 0.5 ] ], [ [ 0.35, 0.25, 0.4 ], [ 0.6, 0.25, 0.15 ] ] ]) true_labels = torch.tensor([ [ 1, 2], [ 1, 0], ]) torch.gather(emission_score, 2, true_labels.unsqueeze(-1)).squeeze(-1).sum(-1) \u0026#39;\u0026#39;\u0026#39; tensor([[0.2000, 0.5000], [0.2500, 0.6000]]) =\u0026gt; tensor([0.7000, 0.8500]) \u0026#39;\u0026#39;\u0026#39; which is exactly the second term of $\\text{Score} (D_j) $.\nReferences  https://zhang-yang.medium.com/explain-pytorch-tensor-stride-and-tensor-storage-with-code-examples-50e637f1076d "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/sort/",
                "title": "Sorting",
                "section": "post",
                "date" : "2021.07.11",
                "body": "Data structure and algorithms are two important courses in Computer Science. Knowing how to utilise appropriate data structures and algorithms to fit our problems can greatly decrease memory space and improve performance. Besides, the ideas behind the classical algorithms are also worth learning to strengthen our logical thinking skills. Well, this series of articles are simply quick notes to help refresh my knowledge for future reference. So let\u0026rsquo;s start from sorting.\nBubble Sort Idea Bubble sort is the first sorting algorithm I learned in the module \u0026lsquo;Data Structure\u0026rsquo;. The idea is simple,\n each time, we compare numbers in pairs and make an exchange until we put the largest number at the rightmost position\n Say we have $n = 6$ numbers shown in Figure 1,\n  Figure 1: Toy data used throughout this post  For the first iteration, there are n = 6  items left to sort, so we need to compare n - 1 = 5 pairs of elements consecutively.\n(9, 4), 2, 16, 10, 12 4, (9, 2), 16, 10, 12 4, 2, (9, 16), 10, 12 4, 2, 9, (16, 10), 12 4, 2, 9, 10, (16, 12) Similarly, for the second loop, there are n - 1 = 5 items left to sort, and we need to compare n - 2 = 4 pairs of elements.\n(4, 2), 9, 10, 12, 16 2, (4, 9), 10, 12, 16 2, 4, (9, 10), 12, 16 2, 4, 9, (10, 12), 16 From this, we can conclude that, for the $i\\text{th}$ loop,\n there are $n - i$ items left to sort consequently, we need to compare $n - i - 1$ times  Since we place the largest number at the rightmost position for each loop, so there are $n-1$ loops in total. The following table shows the above procedure.\n   iterations times we need to compare     0 n - 1   1 n - 2   2 n - 3   \u0026hellip;    n - 2 1    Code def bubbleSort(nums): n = len(nums) for i in range(n - 1): # each loop for j in range(n - i - 1): if nums[j] \u0026gt; nums[j+1]: nums[j], nums[j+1] = nums[j+1], nums[j] return nums bubbleSort([9, 4, 2, 16, 10, 12]) # [2, 4, 9, 10, 12, 16] Analysis Stable - Yes\nSince we only swap values when the previous item is greater than the latter item, the items with the same values will keep their order.\nComplexity - O($n^2$)\nFrom the table above, we can see that,\n in the worst case, we need to run comparison $1 + 2 + \u0026hellip; + (n - 1) = \\frac{(n-1)n}{2}$ times in the best case, we still need to compare $\\frac{(n-1)n}{2}$ times  In this example, we notice that 2, (4, 9), 10, 12, 16 have been in order, but the comparison continues. One way is to set a flag to notice it. We can do this because Bubble Sort is an in-place sorting, which means the list has sorted in ascending order while comparing.\ndef bubbleSort(nums): n = len(nums) has_sorted = False for i in range(n - 1): # loop passes if has_sorted: break for j in range(n - i - 1): if nums[j] \u0026gt; nums[j+1]: nums[j], nums[j+1] = nums[j+1], nums[j] has_sorted = False else: has_sorted = True return nums Now the complexity of the enhanced Bubble Sort for the bset case is $O(n)$ while the worset case still stays the same.\nSelection Sort Idea Selection sort is similar to Bubble sort. It first finds the index of the largest value in the remaining list, then makes an exchange between the largest item and the rightmost item for each loop. Therefore, the exchange happens only once in each iteration.\nLike Bubble sort, there are $n - 1$ iterations. For each iteration, we need to find the largest value from the remaining $n -i$ items.\nCode def selectionSort(nums): n = len(nums) for i in range(n - 1): max_index = 0 for j in range(1, n - i): if nums[j] \u0026gt; nums[max_index]: max_index = j nums[n-i-1], nums[max_index] = nums[max_index], nums[n-i-1] return nums Analysis Stable - No\nAfter each iteration, we swap two items, so the order of the items are not ensured.\nComplexity\nObviously, no matter the best case or the worst case, we need $O(n^2)$ comparisons.\nInsertion Sort Idea As the name suggests, we insert an item into the proper position of the well-sorted sublist each time. In other words, we maintain the sublist sorted in each iteration.\nBut how? For each $x_i$ in the sorted list, we compare $x_i$ with the being sorted item $x'$,\n if $x' \u0026gt;= x_i$,  simply put $x'$ behind $x_i$ end   if $x' \u0026lt; x_i$,  move $x_i$ a step forward (in the right direction) then move on to the next $x_i$ in the sorted list repeat the above comparison   or we reach the start point, then we know that $x'$ is the smallest number so far. For example, $2$ is placed at the beginning of the list during the third iteration shown in Figure 2.    Figure 2: Insertion sort  Code def insertionSort(nums): n = len(nums) for i in range(1, n): cur_elem = nums[i] cur_index = i while cur_index \u0026gt; 0 and cur_elem \u0026lt; nums[cur_index - 1]: nums[cur_index] = nums[cur_index - 1] cur_index -= 1 nums[cur_index] = cur_elem return nums Analysis Stable - Yes\nComplexity\nFor the best case where all items are sorted well and the corresponding sublist is sorted, there is no need to do a move operation. Therefore, the complexity is $O(n)$.\nFor the worst case, we still need $O(n^2$) comparisons.\nQuick Sort Idea Quick sort adopts the idea of divide and conquer. The main strategy is that we do a partition at the pivot value, dividing the list into two parts\n the left part contains items that are smaller than the pivot value the right part contains items that are greater than the pivot value  How? Well, we use two pointers: the left pointer and the right pointer that point to the first and the last of the remaining list respectively.\n Move the left pointer forward(towards the right direction) until the pointed element is greater than the pivot value Move the right pointer backward(towards the left direction) until the pointed element is smaller than the pivot value  But how to choose the pivot? Well, you could use the first element of a list.\nLet\u0026rsquo;s take an example. In Figure 3, we choose 9 as our pivot, after some steps we stop at the second row. We do a swap between pointers and then continue moving on until the two pointers pass by each other (the forth row). Well, since the two pointers have met, we cannot move further towards either the right or the left direction, which means that we are done. Obviously, the pivot value should be between the right pointer and the left pointer, so we do an exchange again.\n  Figure 3: Quick Sort  Analysis Stable - No\nWe do a swap between pointers, so the order of elements are not maintained.\nReferences  Problem Solving with Algorithms and Data Structures using Python "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/nlp-02-text-representation/",
                "title": "NLP - Text representation",
                "section": "post",
                "date" : "2021.07.10",
                "body": "In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven\u0026rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation.\nThere are two aspects to consider: the level of representation and the meaning of numbers. We know that a sentence is composed of words and each word consists of a group of characters. This means we can represent text at sentence level, character-level, or both. As for numbers, the simplest way is to count the number of occurrences of each word. The most common methods based on this idea includes one-hot encoding, bag-of-word and TF-IDF.\nFrequency-Based One-hot One-hot representation indicates whether a word/character is present in a sentence/word. If true, we assign the value of 1 to that word, otherwise 0.\ncharacter-level Figure 1 shows a simple word representation at character level. The whole vocabulary contains 26 English letters. For each word, for example, the word impossible\n each row represents a character in impossible, so there are 10 rows The corresponding value of each row is a vector whose element value is either 0 or 1, indicating whether the corresponding letter is present in the given word  Thus, we will get a (10, 26) matrix for the word impossible.\n  Figure 1: character-vocabulary occurrence matrix with the shape of (|word|, |vocabulary|)  Below are simple codes to construct such a matrix.\nline = \u0026#39;Impossible Mr Bennet impossible when I am not acquainted with him\u0026#39; line = \u0026#39;\u0026#39;.join(line.lower().split()) line, len(line) # (\u0026#39;impossiblemrbennetimpossiblewheniamnotacquaintedwithhim\u0026#39;, 55) # each row represent a character in the sentence alphabet = \u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39; letter_tensor = torch.zeros(len(line), len(alphabet) for i, letter in enumerate(line): idx = alphabet.index(letter) letter_tensor[i][idx] = 1 word-level From the view of sentences, the vocabulary is all the unique words in the corpus and each row represents each word. The implementation of one-hot encoding at word-level is similar to the above codes.\nline = \u0026#39;Impossible Mr Bennet impossible when I am not acquainted with him\u0026#39; # build vocabulary vocabulary = set(line.lower().split()) word2idx = {w: idx for idx, w in enumerate(vocabulary)} len(vocabulary), word2idx[\u0026#39;impossible\u0026#39;] # 10, {\u0026#39;impossible\u0026#39;: 0, \u0026#39;not\u0026#39;: 1, \u0026#39;i\u0026#39;: 2, \u0026#39;with\u0026#39;: 3, \u0026#39;acquainted\u0026#39;: 4, \u0026#39;bennet\u0026#39;: 5, \u0026#39;mr\u0026#39;: 6, \u0026#39;him\u0026#39;: 7, \u0026#39;am\u0026#39;: 8, \u0026#39;when\u0026#39;: 9} # each row represent a word in the sentence word_vector = np.zeros((len(line.split()), len(vocabulary))) for idx, w in enumerate(line.lower().split()): j = word2idx[w] word_vector[idx][j] = 1 word_vector.shape # (11, 10) The results are shown below, we can see that this sentence can be represented as a 11 x 10 matrix.\n# (|sentence|, |vocabulary|) Impossible [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] Mr [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] Bennet [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] Pros  simple and intuitive to understand and implement  Cons  The size of a ont-hot vector is proportional to the size of vocabulary, resulting in a sparse representation when we have a large corpus The representation matrix doesn\u0026rsquo;t have fixed size. The dimension varies in sentences or words with different lengths. Too naive to capture the similarity between words It cannot deal with out-of-vocabulary(OOV) problem  Bag-of-word The idea of Bag-of-word(BOW) is that all the words in the corpus are in a bag without considering the orders and context. The intuition is similar to the concepts introduced in LDA — a topic is characterized by a small specific set of words. Therefore, BOW is commonly used to classify documents. If two documents are similar (have the same words), they are likely to be classified into the same group. Each document is represented as a vector of |V| dimensions, where the element value of this vector is the frequency of the word in the corresponding doc. Say we have the following corpus,\ncorpus = [ \u0026#39;cat eats meat and dog eats meat\u0026#39;, \u0026#39;cat eats fish\u0026#39;, \u0026#39;dog eats bones\u0026#39; ] The vocabulary of this small corpus and the doc-term matrix are\n[\u0026#39;and\u0026#39; \u0026#39;bones\u0026#39; \u0026#39;cat\u0026#39; \u0026#39;dog\u0026#39; \u0026#39;eats\u0026#39; \u0026#39;fish\u0026#39; \u0026#39;meat\u0026#39;] cat eats meat and dog eats meat [1, 0, 1, 1, 2, 0, 2] cat eats fish [0, 0, 1, 0, 1, 1, 0] dog eats bones [0, 1, 0, 1, 1, 0, 0] one-hot Sometimes, we don\u0026rsquo;t care about the number of occurrence of words. Just like one-hot encoding, we only want to know whether a word is present in the sentence or not, which would be useful for sentiment analysis. Well, that\u0026rsquo;s easy to implement using Sklearn\n# occurrence vectorizer = CountVectorizer(binary=True) X_train_dtm = vectorizer.fit_transform(X_train) X_test_dtm = vectorizer.transform(X_test) Pros  Simple and intutive to understand and implement Captures the semantics of documents, if two docs have similar words, they will be close to each other in the word space Fixed matrix representation no matter how long a sentence is  cons  It ignores the word order(context), so Cat bites man and Man bites cat have the same representation The size of the matrix is proportional to the size of vocabulary It doesn\u0026rsquo;t deal with out-of-vocabulary(OOV) problem It doesn\u0026rsquo;t capture the similarity between different words that have the same meaning, e.g cat eats, cat ate, BOW will treat them as different vectors though they convey the same semantics. As mentioned earlier, the most frequent words are often function words like pronouns, determiners and conjuctions. However, they are of no help for classification.  TF-IDF So far, we have learned that one-hot encoding focuses more on the occurrence of words in text while BOW pays more attention to word frequency. In both cases, they consider each word in the corpus euqally(with the same weight).\nIn contrast, TF-IDF or term frequency-inverse document frequency allows us to measure the importance of each word relative to other words in the doc and the corpus. This is useful for information retrieval systems, where we expect that the most relevant documents should appear first.\nSo how does TF-IDF work? Well, as the name suggests, it calculates two things\n  term frequency(TF), the normalized frequency of each token $w_i$ in a given doc $d_j$\n$$ TF(w_i, d_j) = \\frac{|w_i^{d_j}|}{|d_j|} $$\n The intution is that the more frequent a word appears in a doc, the more important it is. Thus, we need to increase its importance    inverse document frequency(IDF), the logarithm of the inverse normalized frequency of each token across all documents\n​ $$ IDF(w_i) = \\text {log} \\frac{|D|}{|w_i^D|} $$\n  The intuiton is fair straightforward — if a word appears across all the docs, for instance, stop words like a, is, and, it\u0026rsquo;s unlikely to capture the characteristics of the doc it belong to, indicating that it\u0026rsquo;s a more common word relative to other words in the same doc. In other words, we need to reduce its importance, that\u0026rsquo;s why we invert the calculation.\n  we use logarithm to further punish terms that appear more frequently across all the docs\n    Putting it together, the TF-IDF is defined as\n$$ TF\\_IDF = TF(w_i, d_j) * IDF(w_i) $$\nDistributed Representation CBOW Skip-gram pre-trained"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/dl-01-intro/",
                "title": "Deep Learning - Introduction",
                "section": "post",
                "date" : "2021.06.24",
                "body": "So far, we\u0026rsquo;ve covered most of the things that we should know about machine learning, including concepts, optimization, and popular models under the hood. Yet, some advanced techniques, such as the Gaussian Process and MCMC, are not mentioned. I am afraid that I don\u0026rsquo;t have time to do this because I have to do my summer project, which requires me to dive into deep learning.\nTo this end, I am going to write something about deep learning, which I hope could improve my understanding of it, particularly RNN. In the first article of this series, we will learn how a neural network works by implementing two machine learning models, linear regression and logistic regression, and revisit the loss function cross-entropy.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/nlp-01-text-preprocessing/",
                "title": "NLP - Text preprocessing",
                "section": "post",
                "date" : "2021.06.22",
                "body": "From now on, we will focus on a specific domain — Natural Language Processing(NLP), in part because my summer project is about Named Entities Recognition(NER). Therefore, I need to know some text preprocessing techniques and have a good understanding of the state-of-art NLP models, particularly BiLSTM + CRF. The very first step in NLP is text preprocessing, so I am going to start from here.\nConverting text to lowercase For some letter-based languages such as English, letters could be either lowercase or uppercase, e.g. book , Book or BOOK. Since they represent the same word book with the same semantics, there is no need to encode them three times. The common way is to convert all words into a consistent written style — lowercase because of its readable and concise style.\nHowever, this method sometimes could affect semantics of some specific words, for instance, May and may are two different words. Well, for tasks that involve parts-of-speech analysis, it\u0026rsquo;s a matter.\nIn python, it\u0026rsquo;s easy to convert a string into lowercase,\ntext = text.lower() Removing Punctuation For some tasks, punctuations such as , . \u0026quot; @ # are meaningless to us, so we should remove them. In Python, we can do this using the sub() method of the re module to replace any matched punctuation with an empty character\nre.sub(\u0026#39;[,\\\u0026#39;.!\u0026#34;]\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;\u0026#34;20.2 dollars! That\\\u0026#39;s impossible\u0026#34;, he said.\u0026#39;) #\u0026gt;\u0026gt; 202 dollars Thats impossible he said From the above code, we can see some issues,\n Abbreviation, That's -\u0026gt; Thats Prices, 20.2 dollars -\u0026gt; 202 dollars  Maybe we can write some specific rules for a corpus to remove particular punctuations, but it will be time-consuming and inflexible.\nTokenization Tokenization is a technique to split a piece of text into a smaller unit. The unit could be\n a single word a character a combination of several words  Why do we tokenize text? The answer is that the ultimate goal of tokenization is to build vocabulary for a corpus. After all, words in each sentence are ultimately from the vocabulary.\nOkay, so how do we perform tokenization to get a sequence of words? A naive way is to split a string by whitespace shown below.\ntext = text.split(\u0026#39; \u0026#39;) However, there are some issues. For example, United Kindom will be split into two words, United and Kindom. A common practice is to use the popular natural language toolkit (NLTK) to do this\nfrom nltk.tokenize import word_tokenize tokens = word_tokenize(text) With these tokens, how to build vocabulary? Usually, there are two ways to do this,\n include each uniqe token in the vocabulary include only the top K frequently occurring tokens; the idea is that repetition usually conveys the most important information  Stemming \u0026amp; Lemmatization In grammatical aspect, a word could have several variants to express tense, mood or something. For example,\n go, goes, went, gone are different tenses of go higher, highest are comparative and superlative form of high  In other words, these variants are originated from their root words. Generally, most words follow a general rule of to generate the corresponding forms, though there are some rare cases, as shown below,\n eat, ate, eaten good, better, best  Stemming and Lemmatization are two common methods to convert each word back to its original form or the root.\nStemming Stemming is a simple and crude method that simply chopps off letters from the end of a word until a common root is found, which is the idea of Potter Stemming algorithm.\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer words = [\u0026#39;run\u0026#39;, \u0026#39;runner\u0026#39;, \u0026#39;running\u0026#39;, \u0026#39;ran\u0026#39;, \u0026#39;runs\u0026#39;, \u0026#39;easily\u0026#39;, \u0026#39;saw\u0026#39;] p_stemmer = PorterStemmer() for word in words: print(word + \u0026#39;--\u0026gt;\u0026#39; + p_stemmer.stem(word)) # run--\u0026gt;run # runner--\u0026gt;runner # running--\u0026gt;run # ran--\u0026gt;ran # runs--\u0026gt;run # easily--\u0026gt;easili # saw--\u0026gt;saw  We can see that the result of stemming may not be a word, e.g. easili, and that the tense forms of some special words like ran, saw are not processed correctly.\nLemmatization On the contrary, lemmatization returns the true root or lemma of a word by considering the whole vocabulary of a language and the context of that word in the sentence. In the case of ran and saw, the true root words or lemmas are run and see respectively.\nNLTK provides a popular lemmatizer for us — WordNet Lemmatizer, which is an large lexical database of English. But before using it, we need to know the parts of speech of each word, which can also be done using pos_tag() method of NLTK.\n# we mannually specify the parts of speech for each word words = [ (\u0026#34;grows\u0026#34;, \u0026#39;v\u0026#39;), (\u0026#39;running\u0026#39;, \u0026#39;v\u0026#39;), (\u0026#34;better\u0026#34;, \u0026#39;a\u0026#39;), (\u0026#34;cats\u0026#34;, \u0026#39;n\u0026#39;), (\u0026#39;quickly\u0026#39;, \u0026#39;r\u0026#39;) ] lemmatizer = WordNetLemmatizer() [ lemmatizer.lemmatize(w, p) for w, p in words ] # [\u0026#39;grow\u0026#39;, \u0026#39;run\u0026#39;, \u0026#39;good\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;quickly\u0026#39;] Removing stopping words It seems that we\u0026rsquo;ve obtained the tokens as desired. But wait, let\u0026rsquo;s plot the number of occurrence of each word.\n  Figure 1: The top 10 frequently tokens  Figure 1 shows the frequency of each token in the built-in NLTK corpus named 1789-Washington.txt. It can be seen that most tokens are determiners, prepositions, conjuctions or other function words that have little lexical meaning. Obvisously, we need to remove them, which can be done easily using NLTK,\nfrom nltk.corpus import stopwords stop_words_en = stopwords.words(\u0026#39;english\u0026#39;) tokens = [ w for w in tokens if w not in stop_words_en ] Putting it together So far, we\u0026rsquo;ve introduced some basic and necessary text preprocessing techniques. Now let\u0026rsquo;s put it together to build the whole text preprocessing pipeline.\nimport string, re import nltk from nltk.corpus import wordnet from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from nltk.tag import pos_tag from nltk.stem.wordnet import WordNetLemmatizer def get_wordnet_pos(tag): if tag.startswith(\u0026#34;J\u0026#34;): return wordnet.ADJ elif tag.startswith(\u0026#34;R\u0026#34;): return wordnet.ADV elif tag.startswith(\u0026#34;V\u0026#34;): return wordnet.VERB else: return wordnet.NOUN lem = WordNetLemmatizer() stop_words_en = stopwords.words(\u0026#39;english\u0026#39;) def clean_sentence(text): # lower text = text.lower() # remove punctuation text = re.sub(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;,text) text = text.translate(str.maketrans(\u0026#39;\u0026#39;,\u0026#39;\u0026#39;,string.punctuation)) # tokenization # text.split(\u0026#39; \u0026#39;) # naive methods, e.g. New York will be splitted into [\u0026#39;New\u0026#39;, \u0026#39;York\u0026#39;] tokens = word_tokenize(text) # lemmatize, [a, go, c, ...] tokens = pos_tag(tokens) # [(a, \u0026#39;NN\u0026#39;), (went, \u0026#39;VB\u0026#39;), (c, \u0026#39;NN\u0026#39;)] tokens = [ lem.lemmatize(w, get_wordnet_pos(tag)) for w, tag in tokens ] # remove stopping words tokens = [ w for w in tokens if w not in stop_words_en ] tokens = [ w for w in tokens if len(w) \u0026gt; 1 ] return \u0026#39; \u0026#39;.join(tokens) "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/naive-bayes/",
                "title": "Naive Bayes Classification",
                "section": "post",
                "date" : "2021.06.16",
                "body": "In the previous articles, we introduced several classification algorithms like logistic regression. These models are often called discriminative models since they make prediction by calculating $P(Y|X)$ directly. Sometimes it might be hard to compute. Another way to think of this is that samples are generated from the existed distributions. And one of the most popular models is Naive Bayes classification.\nModel Given a sample $\\bold x = [x_1, x_2, \u0026hellip;, x_d]$, where $x_i$ represent each individual feature, we want to know the probability of $\\bold x$ belonging to each class $C_k$. With the aid of Bayes' inference, we can inverse this problem into the forward problem\n$$ P(Y=C_k|X= \\bold x) = \\frac{P(X= \\bold x|Y=C_k)P(Y=C_k)}{P(X= \\bold x)} $$\nThen, Naive Bayes makes the following assumptions\n all features are independent — one paticular feature doesn\u0026rsquo;t affect another, though it\u0026rsquo;s not real in practice. That\u0026rsquo;s why it\u0026rsquo;s called naive (lack of experience) all features have equal importance  Based on the above assumptions, we can rewrite Bayes' theorem as follows,\n$$ P(Y=C_k|X=\\bold x) = \\frac{P(x_1|Y=C_k)P(x_2|Y=C_k)\u0026hellip;P(x_d|Y=C_k)P(Y=C_k)}{P(X=\\bold x)} $$\nSince $P(X=\\bold x)$ is the evidence,\n$$ P(Y=C_k|X=\\bold x) \\propto P(Y=C_k) \\prod_{i=1}^d P(x_i|Y=C_k) $$\nNow the question is that where $P(x_i|Y=C_k)$ comes from. What distribution does the class $C_k$ follow? In Naive Bayes, there are three typical distributions\n Bernoulli Multinonimal Gaussian  Bernoulli Naive Bayes In Bernoulli distribution, the outcome of an event, denoted by $y_i$, is either 1 or 0. Let $p$ be the probability of being 1, so we have\n$$ P(Y=y_i) = y_ip + (1- y_i )(1 - p) $$\nSince 0 or 1 can encode the occurrence of an event, we can use this to filter spam email. We don\u0026rsquo;t care how many suspect words occur in an email. Instead, once they occur, this email should gain more attention. Hence, each email can be represented by a word vector with binary element taking value 1 if the corresponding word appears in the email and 0 otherwise,\n$$ \\bold x = (x_1, x_2, \u0026hellip;, x_{|v|}), \\text{ where } x_i \\in {1, 0} $$\nwhere $V$ is the vocabulary of the training emails. In other words, we do $|V|$ Bernoullis trials for each email. For each word in $V$, if it\u0026rsquo;s present in that email, the outcome is 1, otherwise 0. This repeats $|V|$ times until all words in $V$ have been visited.\nNow suppose there are $D$ emails, including $D_s$ spam emails and $D_h$ ham emails, given a new email $\\bold x $, we want to know whether it\u0026rsquo;s a spam email or not. Based on the model defined above, we need to calculate two things\n the probability of being each class, $P(Y = C_k)$ given that class $C_k$, the probability of sampling the new data $x$, $P(X=\\bold x|Y=C_k) = \\prod_{i=1}^V P(x_i|Y=C_k)$  The first question can be solved by the following equation,\n$$ P(Y=S) = \\frac{|D_s|}{|D|}, P(Y=H) = 1 - P(Y=S) $$\nBased on the assumption of Naive Bayes, we consider each word in the vocabulary as a single independent random variable that follows Bernoulli distribution with probability $p$, where $p$ is the probability of a word $x_i$ occurring in an email of class $C_k$ and can be computed as follows,\n$$ p(x_i | Y=C_k) = \\frac{|D_s(w_i)|}{|D_s|} $$\nwhere $|D_s(w_i)|$ represent the number of emails that belong to class $C_i$ and contain the word $x_i$. Therefore, the probability of $\\bold x$ being classified into the spam category is\n$$ P(Y = S| X = \\bold x ) \\propto P(Y=S) \\prod_{i=1}^V P(x_i|Y = S) $$\n$$ = \\frac{|D_s|}{|D|} \\prod_{i=1}^V \\text{ } y_i P(x_i|Y=S) + (1- y_i )(1 - P(x_i|Y=S)) $$\nMultinomial Naive Bayes In multinomnial distribution, there are $K$ possible outcomes, so the probability is\n$$ p(x_k) = \\prod_{k}^K \\mu_k^{I_{k}}, \\text{ where } _{k} = 1 \\text{ if } x_k = 1, \\text{otherwise } 0 $$\nIf we do $N$ trials, the likelihood of the observations is\n$$ P(m_1, m_2, \u0026hellip;, m_k) = \\prod_{n=1}^N \\prod_{k}^K \\mu_k^{I_{nk}} = \\frac{N!}{m_1!m_2!\u0026hellip;m_k!}\\prod_{k}^K \\mu_k^{m_k} $$\nwhere $m_k=\\sum_n^N I_{nk}$ represents the number of $I_{nk} = 1$ and $\\sum_{i=1}^k m_i = N$. Through MLE, $\\mu_k$ is esimated as\n$$ \\mu_k = \\frac{m_k}{N} $$\nSometimes we are more interested in the frequency of a word, for example, sentiment classification. If words about happy occur more frequently, then we\u0026rsquo;re more likely to classify the text as the positive category. In this case, each text can be represented by a word vector with element whose value is the frequency of that word.\n$$ \\bold x = (x_1, x_2, \u0026hellip;, x_{|V|}), \\text{ where } x_i \\in Z $$\nwhere $V$ is the vocabulary of the training texts. Thus, we can see that a text can be generated by doing $|N_x|$ multinominal trials.\nSuppose there are $D$ texts, including $D_p$ positive texts and $D_n$ negative texts, given a new text $\\bold x $, we want to know whether the sentiment that that text shows is positve or negative.\nAgain, the probability of being a class $C_k$ is given by,\n$$ P(Y=\\text{Positive}) = \\frac{|D_p|}{|D|}, P(Y=\\text{Negative}) = 1 - P(Y=\\text{Positive}) = \\frac{|D_n|}{|D|} $$\nThe likelihood of generating the sample $\\bold x$ is given by,\n$$ P(X=\\bold x|Y=C_k) = P(x_1, x_2, \u0026hellip;, x_{|V|}|Y=C_k) \\propto \\prod_{k}^V \\mu_k^{x_k} $$\nwhere $x_k$ represents the frequency of that word that occurs in the text $\\bold x$ and $\\sum_{i=1}^V x_i = N_x$.\nFor words that not occur in the text, $\\mu_k^0 = 1$. Thus, the posterior probability can be rewritten as follows,\n$$ P(Y = \\text{Positive} | X = \\bold x ) \\propto P(Y= \\text{Positive} ) \\prod_{k=1}^{N_x} \\mu_k $$\nGaussian Naive Bayes Similarly, Gaussian Naive Bayes assumes that data from each class follows a Gaussian distribution.\nZero Probability If a new sample contains a word that is unseen in the training data, the probability is zero.\n$$ \\mu = \\frac{m_k}{N} = 0 $$\nIf one of the term $\\mu_k$ is zero, the whole probability will be zero too. One way to avoid this is to use add-one smoothing, adds a count of one to each unique word.\n$$ \\mu = \\frac{m_k + 1} {N + |V|} $$\nReferences  https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html https://medium.com/atoti/how-to-solve-the-zero-frequency-problem-in-naive-bayes-cd001cabe211 "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/svm/",
                "title": "Support Vector Machine",
                "section": "post",
                "date" : "2021.06.15",
                "body": "Maximise the margin Given a linearly separable set of data $D = (x_i, y_i)_{i=1}^n$ where $y_i \\in -1, 1$, there are many lines that separates the data. Which one is the best? Intuitively, the line with the largest distance to all samples generates more space to avoid misclassification. Mathematically, this can be described as follows,\n$$ y_i d_i \\ge \\Delta $$\nwhere $d_i$ is the distance from $x_i$ to the separating plane and $\\Delta$ is the margin. Our goal is to find a separating plane that maximises $\\Delta$. This is known as Support Vector Machine.\nDistance to hyperplanes How to calculate the distance from a point to a hyperplane ?\n  Figure 1: Distance from a point to a hyperplane.  The black line in Figure 1 represents a hyperplane defined by an orthogonal vector $w$ and a bias $b$\n$$ w^Tx - b||w|| = 0 $$\nThe distance from the origin to the hyperplane is given by\n$$ \\frac{w^Tx_0}{||w||} = \\frac{ b||w||}{||w||} = b $$\nThus, the distance from a point $x_i$ ($x_i \\neq 0$) to the hyperplane is given by\n$$ d_i = \\frac{w^Tx_i}{||w||} - b $$\nPrimal Problem Our goal is to find $w$ and $b$ to maximise $\\Delta$ subject to the following constraints\n$$ y_i ( \\frac{w^Tx_i}{||w||} - b) \\ge \\Delta \\text{ for all i = 1, 2, 3, \u0026hellip;, n} $$\nDivide both sides by $\\Delta$,\n$$ y_i(\\frac{w^Tx_i}{||w||\\Delta} - \\frac{b}{\\Delta}) \\ge 1 $$\nThen define $w' = w/(||w||\\Delta)$ and $b'= b/\\Delta$,\n$$ y_i(w'^Tx_i - b') \\ge 1 $$\nNote\n$$ ||w'|| = ||\\frac{w}{||w||\\Delta}|| = \\frac{1}{\\Delta} $$\nTherefore, minimising $||w'||^2$ is equivalent to maximising the margin $\\Delta$. Now the problem becomes a quadratic programming problem defined below,\n$$ \\text{min}_{(w',b')} \\frac{||w'||^2}{2} \\text{ }\\text{ }\\text{ subject to $y_i(w'^Tx_i - b') \\ge 1$ for all data points} $$\nExtended Feature Space However, data are not always linearly separated, such as data in Figure 2.\n  Figure 2: Non-linearly separable data  Maybe we could do some data transformation and work in a new feature space where data are linearly speparable. Yea, in fact, SVM maps all feature vectors to an extended feature space by defining a special $\\phi(x)$, which is a function of $x$\n$$ x \\rarr \\phi(x) $$\nBelow are some examples of $\\phi(x)$. You can choose any mapping you like.\n$$ \\phi(x) = x_1^2, \\phi(x) = x_2^2, \\phi(x) = \\sqrt {x_1x_2} $$\nDual Form In the extended feature space, we substitute $x$ for $\\phi(x)$. So the question is defined as follows,\n$$ \\text{min}_{(w,b)} \\frac{||w||^2}{2} \\text{ }\\text{ }\\text{ subject to $y_i(w^T \\phi(x_i) - b) \\ge 1$ for all data points} $$\nAs mentioned earlier, we can use Lagrange multiplier to solve constrained optimisation. The Lagrangian function or the primal problem is given by\n$$ \\text{min}_{w, b}\\text{max}_{\\alpha} \\mathcal{L} (w, b, \\alpha) $$ where\n$$ \\mathcal{L} (w, b, \\alpha) = \\frac{||w||^2}{2} - \\sum_{i=1}^N \\alpha_i (y_i(w^T \\phi(x_i) - b) - 1) $$\nsubject to $\\alpha_i \\ge 0$ (because we have inequality constraints).\nThen we tranform the primal problem into the dual problem. We first minimise Lagrange function w.r.t $w, b$ then maximise with respect to $\\alpha$\n$$ \\text{max}_{\\alpha} \\text{min}_{w, b} \\mathcal{L} (w, b, \\alpha) $$\nTaking the derivative of $\\mathcal{L} (w, b, \\alpha)$ w.r.t. $w, b$ respectively,\n$$ \\nabla_w \\mathcal{L} = w - \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) = 0 $$\n$$ \\nabla_b \\mathcal{L} = \\sum_{i=1}^N \\alpha_i y_i = 0 $$\nSubstituing back to the Lagrangian function\n$$ \\text{max}_{\\alpha} \\frac{||w||^2}{2} - \\sum_{i=1}^N \\alpha_i (y_i(w^T \\phi(x_i) - b) - 1) $$\n$$ = \\text{max}_{\\alpha} \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) - \\sum_{i=1}^N (\\alpha_i y_iw^T \\phi(x_i) - \\alpha_i y_ib - \\alpha_i) $$\n$$ = \\text{max}_{\\alpha} \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) - \\sum_{i=1}^N (\\alpha_i y_i (\\sum_{j=1}^N \\alpha_j y_j\\phi(x_j)^T) \\phi(x_i) - \\alpha_i) $$\n$$ = \\text{max}_{\\alpha} \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i) - \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{j=1}^N \\alpha_j y_j \\phi(x_j) + \\sum_{i=1}^N \\alpha_i $$\n$$ = \\text{max}_{\\alpha} \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\alpha_i y_i\\phi(x_i)^T \\sum_{j=1}^N \\alpha_j y_j \\phi(x_j) $$\n$$ = \\text{max}_{\\alpha} \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j) $$\nThe dual problem is to find $\\alpha$ that maximise $\\mathcal{L}$, subject to constraints\n$$ \\sum_{i=1}^N \\alpha_i y_i = 0 $$\nThe Hessian of $\\mathcal{L}$ is $-\\frac{1}{2} X^TX$ where $X _{ik}$= $y_k \\phi_i(x_k)$, so it\u0026rsquo;s negative seim-definite, and thus there is a unique maximum.\nSoft Margin Okay, let\u0026rsquo;s just leave the dual problem alone for a while(we will go back in the next section). Now we look at another situation where we want to relax the margin constraints. In other words, we allow some samples to appear in the margin area. This is called soft margin. We do it by introducing a slack variable $s_i$ shown in Figure 3.\n  Figure 3: Soft margin   Now the constraint is\n$$ y_i(w'^Tx_i - b') \\ge 1 - s_i $$\nwhere $s_k \\ge 0$. Obviously, the value of $s_i$ includes three situations\n For samples that lies far away from the hyperplane, $s_i = 0$, because they are far enough For $0 \\lt s_i \\le 1$, there exist some samples that lie between margin and on the correct side of hyperplane For $s_i \\gt 1$, samples are misclassified  Objective The objective function becomes\n$$ \\text{min}_{(w',b')} \\frac{||w'||^2}{2} + C\\sum_{i=1}^N s_i $$\nwhere $C$ controls the degree of misclassification we desire\n a small C allows more freedom to relax the margin, which means it\u0026rsquo;s acceptable to tolerate some misclassifications a large C means we want to correctly classify as many samples as possbile  The Lagragian function with slack variables is\n$$ \\mathcal{L} = \\frac{||w||^2}{2} + C\\sum_{i=1}^N s_i - \\sum_{i=1}^N \\alpha_i (y_i(w^T \\phi(x_i) - b) - 1 + s_i) - \\sum_{i=1}^N \\beta_is_i $$\nwhere $\\beta_i$ are Lagrange multipliers that satisfy $\\beta_i \\ge 0$ (KKT condition)\nAgain, we minimise $\\mathcal{L}$ w.r.t $s_i$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial s_i} = C - \\alpha_i - \\beta_i = 0 $$ So we have\n$$ \\alpha_i = C - \\beta_i $$\nSince $\\beta_i \\ge 0$,\n$$ 0 \\le \\alpha_i \\le C $$\nHinge Loss By the way, the objective function can also be written in the following form\n$$ \\text{min}_{(w',b')} \\frac{||w'||^2}{2} + C\\sum_{i=1}^N \\text{max} (0, 1-y_i f(x_i)) $$\nwhere the second term is hinge loss function. So,\n $y_i f(x_i) \\gt 1 $,  $x_i$ is outside margin and on the right side of the hyperplane, so there is no contribution to loss   $y_i f(x_i) = 1 $,  $x_i$ is on the margin. Again, there is no contribution to loss   $y_i f(x_i) \\lt 1 $  $x_i$ lies between the margin or it\u0026rsquo;s misclassified, so it increases the loss    Kernel Trick Kernel Kernel is simply the inner product in feature space,\n$$ K(x, y) = \\phi(x)^T \\phi(y) = K(y, x) $$\nwhere\n $\\phi(x) = [\\phi_1(x), \\phi_2(x), \u0026hellip;, \\phi_k(x)]$ $\\phi_i(x)$ are real valued functions of $x$ ( $\\phi_i(x)$ is just a real number ) $k$ is the number of $\\phi_i(x)$; it could be a specific number or INFINITE( that\u0026rsquo;s crazy! )  so kernel tells us the closeness or similarity between $x$ and $y$. The space spanned by $\\phi(x)$ is called feature space or kernel space\n$$ \\mathcal{K} = \\text{span } [ \\phi(x) | x \\in \\mathcal{X} ] \\in R^k $$\nSymmetric Given a kernel $K: \\mathcal{X} \\times \\mathcal{X} \\rarr R$ and a training data set $\\mathcal{X} = [x_1, x_2, \u0026hellip;, x_n]$, we can construct a kernel matrix or Gram matrix $G \\in R^{n \\times n}$\n$$ G_{ij} = K(x_i, x_j) $$\nObviously, $G$ is a symmetric matrix and can be decomposed into $G = X^TX$, where\n $X$ is a $k \\times n$ matrix each column of $X$ is $\\phi(x_i)$  Positive Semi-Definite Consider a new vector $w = X v \\in R^k$, we have\n$$ ||w||^2 = v^TX^T Xv = v^TGv \\ge 0 $$\nHence, $G$ is positive semi-definite and all eigenvalues of $G$ are equal or greater than zero, i.e. $\\lambda_i \\ge 0$.\nEigenfunction Remember the eigenvector of $G$ is defined as follows\n$$ Gv = \\lambda v $$\nThis means,\n$$ \\sum_{j=1}^n G_{ij} v_j = \\lambda v_i $$\nIf we extend the dimention of $G$ to INFINITE, as shown in Figure 4, we have\n$$ \\sum_{j=1}^{\\infin} G_{ij} v_j = \\sum_{j=1}^{\\infin} K(x_i, x_j) v_j = \\lambda v_i $$\n  Figure 4: G could have infinite dimension (Ref[3])  Then we define an eigenfunciton $\\psi(x)$, which is a real funtion of $x$\n$$ \\sum_{j=1}^{\\infin} K(x_i, x_j) \\psi(x_j) = \\lambda \\psi(x_i) $$\nWith the help of the integral operator, we can rewrite it as follows,\n$$ \\int_{y \\in \\mathcal{X}} K(x, y) \\psi(y) dy = \\lambda \\psi(x) $$\nThus, $Gv = \\lambda v$ is just a special case of this, when $\\mathcal{X}$ is a finite set.\nMercer’s theorem In general there will be a denumerable set of eigenfunctions $[ \\psi_1(x),\\psi_2(x), \u0026hellip; ] $ and the corresponding $[\\lambda_1, \\lambda_2, \u0026hellip;]$ where\n$$ K(x, y) = \\sum_i^{\\infin} \\lambda_i \\psi_i(x) \\psi_i(y) $$\nThis is known as Mercer’s theorem. And we find that $G_{ij} = \\sum_{k=1}^n \\lambda v_i^k v_j^k$ is just a special case of this, when $\\mathcal{X}$ is a finite set.\nIf we define $\\phi_i(x) = \\sqrt \\lambda_i \\psi_i(x) $ then\n$$ K(x, y) = \\sum_i^{\\infin} \\sqrt \\lambda_i \\psi_i(x) \\sqrt \\lambda_i\\psi_i(y) = \\sum_i \\phi_i(x) \\phi_i(y) = \\phi(x)^T \\phi(y) $$\nAn immediate consequence is that for any real $\\psi(x)$,\n$$ \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} K(x, y) \\psi(y) \\psi(x) dy dx $$\n$$ = \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} (\\sum_i^{\\infin} \\lambda_i \\psi_i(x) \\psi_i(y)) \\psi(y) \\psi(x) dy dx $$\n$$ = \\sum_i^{\\infin} \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} [\\sqrt\\lambda_i \\psi_i(y) \\psi(y)] [\\sqrt\\lambda_i \\psi_i(x) \\psi(x) ] dydx $$\n$$ = \\sum_i^{\\infin} \\int_{x \\in \\mathcal{X}}[\\sqrt\\lambda_i \\psi_i(x) \\psi(x) ] (\\int_{y \\in \\mathcal{X}} [\\sqrt\\lambda_i \\psi_i(y) \\psi(y)] dy) dx $$\n$$ = \\sum_i^{\\infin} (\\int_{x \\in \\mathcal{X}}[\\sqrt\\lambda_i \\psi_i(x) \\psi(x) ] dx)^2 \\ge 0 $$\nThus, $v^TGv \\ge 0$ is just a special case of this, when $\\mathcal{X}$ is a finite set.\nPutting it together, the following statements are equivalent,\n  $K(x, y)$ is positive semi-definite\n  The eigenvalue of $K(x, y)$ are non-negative\n  The kernel can be written $$ K(x, y) = \\sum_i^{\\infin} \\lambda_i \\psi_i(x) \\psi_i(y) = \\sum_i \\phi_i(x) \\phi_i(y) $$ where $\\phi(x)$ are real functions\n  For any real function $\\psi(x)$ $$ \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} K(x, y) \\psi(y) \\psi(x) dy dx \\ge 0 $$\n  Properties of Kernels Adding Kernels If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) + K_2(x, y)$ is also a kernel\n$$ \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} K_3(x, y) \\psi(y) \\psi(x) dy dx $$\n$$ = \\int_{x \\in \\mathcal{X}} \\int_{y \\in \\mathcal{X}} (K_1(x, y) + K_2(x, y)) \\psi(y) \\psi(x) dy dx \\ge 0 $$\nSimilarly, If $K(x, y)$ is a valid kernel so is $cK(x, y)$ for $c \\gt 0$.\nProduct of Kernels If $K_1(x, y)$ and $K_2(x, y)$ are valid kernels then $K_3(x, y) = K_1(x, y) K_2(x, y)$ is also a kernel\n$$ K_3(x, y) = K_1(x, y) K_2(x, y) = \\sum_i \\phi_i^1(x) \\phi_i^1(y) \\sum_j \\phi_j^2(x) \\phi_j^2(y) $$\n$$ = \\sum_i \\sum_j \\phi_i^1(x) \\phi_j^2(x) \\phi_i^1(y) \\phi_j^2(y) $$\nDefine $\\phi_k(z) = \\phi_i^1(z)\\phi_j^2(z)$, and the number of $\\phi_k(z)$ is $i * j$\n$$ K_3(x, y) = \\sum_k \\phi_k(x) \\phi_k(y) $$\nExponentiating Kernels $\\text{exp} (K(x, y))$ is a valid kernel since\n$$ exp(K) = 1 + K + \\frac{1}{2} K^2 + \u0026hellip; $$\nsince the addition and multiplication of kernels yield valid kernels, each term is also a kernel and therefore the exponential of a kernel is a kernel.\nCommon Kernels TODO\nLinear Gaussian Quadratic Comments  SVM relys on distances between data points, so it would be better to normalise data if we don\u0026rsquo;t know what features are important. Different C can make a great difference in performance. We can use cross-validation to choose the optimal C. A linear decision boundary doesn\u0026rsquo;t always exist, especially we obtain a poor performance. If so, we might try a non-linear boundary with Kernel. There are many kernel functions designed for particular data types. Often, Kernel comes with its parameters, so fine-tunining them is also important to improve model\u0026rsquo;s performance. We don\u0026rsquo;t need to explicitly know what $\\phi(x)$ are. All we need to know is that there exist a hyperplane that can separate the data linearly in a higher dimensional space. Though we are in the extended feature space, we do computation of the inner product in the original feature space.  References [1] https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n[2] http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/\n[3] https://www.cs.cmu.edu/~bapoczos/other_presentations/kernel_methods_01_10_2009.pdf\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/kmeans/",
                "title": "K-means",
                "section": "post",
                "date" : "2021.06.14",
                "body": "So far, we\u0026rsquo;ve talked much about supervised learning. Aside from it, there exist many other learning types such as unspervised learning, semi-supervised learning and so on. This post will introduce one of the most widely used unsupervised clustering algorithms — K-means. We will cover the implementation of K-means algorithm, the limitation of this algorithm as well as its applications.\nProblem Formulation Given an unlabelled dataset $D =[x_1, x_2, \u0026hellip;, x_N] $, we want to explore whether there is a pattern in the data and what pattern is. In the case of clustering, we want to know which samples are close to each other so as to form a cluster. To achieve this, we assign to each sample a membership indicator denoted by $r_{nk}=1$ if the sample is in cluster $k$, otherwise $r_{nj}=0$ for $j \\ne k$. Thus,\n there could be $N$ clusters, i.e. each sample forms a cluster. there could be only one cluster, i.e. the whole dataset forms a cluster.  So what\u0026rsquo;s the optimal value for $K$?\nObjective Function The idea behind K-means is intutive to understand. Similar people or people with similar interests tends to form a group. Such a group also has a small variation. Based on this idea, the aim of K-means is to find $r_{nk}$ to minimise the within cluster variation,\n$$ J = \\sum^N_n\\sum^K_k r_{nk} || x_n - \\mu_k ||^2 $$\nwhere $\\mu_k$ is the center of the cluster that a sample $x_n$ belongs to.\nWe should notice that this objective also limits the ability of K-means somehow. You can imageine $\\mu_k$ as the center of a circle, and the average within cluster variation is the maximum radius of that circle. Any point outside that circle doesn\u0026rsquo;t belong to this cluster. So K-means works better for circular clusters.\n  Figure 1: K-means works better for data with circular clusters (Ref[2])  Approach Since $\\mu_k$ and $r_{nk}$ are both unknown, how to solve it? Well, we solve this by repeating the following two steps,\n Step 1: Randomly choose $K$ cluster centers, and then minimise $J$ w.r.t $r_{nk}$. In this step, we assign to each sample a membership indicator given the fixed cluster centers.  $$ r_{nk} =\\begin{cases} 1 \u0026amp; \\text{if $ k =\\text{argmin}_j ||x_n - \\mu_j||^2$} \\\\ 0 \u0026amp; \\text{otherwise}\\end{cases} $$\n  Step 2: Keep $r_{nk}$ fixed and then calculate the new cluster centers — $\\mu_k$ is equal to the mean of all samples assigned to that cluster $k$. This is because we minimise $J$ w.r.t $\\mu_k$ by taking the derivative of $J$ and setting it to zero\n$$ \\frac{\\partial J}{\\partial \\mu_k} = 2 \\sum^N_{x_n \\in C_k} r_{nk} || x_n - \\mu_k || =0 $$\n$$ =\u0026gt; \\mu_k = \\frac{\\sum_{x_n \\in C_k} r_{nk} x_n}{\\sum_{x_n \\in C_k} r_{nk}} $$\n  So when will this process stop? There are some stopping criterions\n The centroids remain the same Within cluster variance reaches below the threshold you\u0026rsquo;ve set Fixed number of iterations have been reached  Choosing the optimal K Like the method used in K-NN, we plot the metric as a function of $k$ shown below.\n  Figure 2: The optimal K lies at the elbow point (Hands-on machine learning)  We can see that inertia decreases greatly as k increases up to 4, after that, it decreases slowly. On the other hand, increasing k indeed decrease the sum of squared distance from each sample to its closest centroid. This is because the more clusters k there are, the more cohesive each cluster is, and therefore the lower the within cluster variation is.\nLimits of K-means Specify K explicitly Obvisously, we have to explicitly specify $K$. But more often, we don\u0026rsquo;t what the right $K$ is.\nSensitive to shape As said earlier, K-means performs poorly when the clusters vary in sizes or densities, or have nonspherical shape. Figure below shows how K-means deals with a data set containing three ellipsoida clusters with different densities and orientation. Though the solution on the right has a lower inertia, it obviously breaks the inner structure of real clusters. Mabybe we could try Gaussian mixture models for this case.\n Centroids initialization Although K-means algorithm is guaranteed to converge, it might not coverge to the right solution. And this depends on the initialization of the centroids, which can also be seen in above Figure. To avoid this, we\u0026rsquo;d better run the algorithm several times and select the solution that has the minimum within cluster variation.\nHard classification Clustering assignment used in K-means is called hard classification since each sample is assigned to a cluster indicator. But sometimes we would expect probability to estimate uncertainty in clustering assignment. For example, if two clusters overlap, it\u0026rsquo;s hard to have 100% confidence to say that some sample belongs to one of the two clusters.\nReferences [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2]\tJ. VanderPlas, Python Data Science Handbook. Sebastopol, CA: O’Reilly Media, 2016.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/logistic-regression/",
                "title": "Logistic Regression",
                "section": "post",
                "date" : "2021.06.14",
                "body": "We\u0026rsquo;ve known that linear regression can be used to predict a continuous value, but sometimes the target variable might be categorical, i.e. whether tomorrow is sunny or someone has a cancer. Can we still use linear regression to solve this classification problem? The answer is Yes and the algorithm that we will introduce in this article is known as logistic regression. Well, we can also try Perceptron since it\u0026rsquo;s also a linear classifier. PS: Don\u0026rsquo;t mix it up with linear regression. Logistic regression is a classification algorithm.\nLogistic Function Given an input dataset $D = [x_1, x_2, \u0026hellip;, x_n]$ with the corresponding lable $Y = [y_1, y_2, \u0026hellip;, y_n]$ where $x_i \\in R^d$ and $y_i \\in {0, 1}$, we want to construct a linear classifer that gives us a value $z_i$ as a weighted sum of all features\n$$ z_i = w^T x_i + b $$\nRange issue Now the question is how $z_i$ relate to $y_i$ since the range of $z_i$ is from $-\\infin$ to $\\infin$ while $y_i$ is either 0 or 1. Any value that is larger then 1 or lower than 0 has no meaning because we interpret $y_i$ as probability.\nLog-odds Obviously, we need to come up with a way to transform $z_i$ so as to obtain a value that lies between 0 and 1. But how? What about $f(x) = \\text{log} x$ ? No, it fails since the domain of the log function can only be positive. Maybe we could make a modification about the log function like this,\n$$ z_i = \\text{log } \\frac{p_i}{1 - p_i} $$\nwhere $p/(1-p)$ is called odds and $z$ is often called the logit. The inverse of the log-odds function, also known as sigmoid function, is defined as follows,\n$$ \\hat y_i = p_i = \\frac{1}{1 + e^{-z_i}} $$\nFigure 1 shows what sigmoid function looks like. We can see that log-odds helps us to solve the range problem.\n  Figure 1: sigmoid activation function  Parameter Estimation We\u0026rsquo;ve defined the logistic model, the next step is to estimate the parameters, i.e. $w$. Minimising the loss function and MLE are two common ways to solve this. In fact, minimising the loss is equivalent to MLE in logistic regression.\nBinominal In the case of binary classification, $y_i$ follows Bernoulli Distribution, i.e. $y_i \\sim Bern(n, p_i)$\n$$ p(y_i = 1 |x_i) = p_i $$\n$$ p(y_i = 0 |x_i) = 1 - p_i $$\nThe likelihood of a single sample is,\n$$ p(y_i|x_i) = p_i^{y_i} (1-p_i)^{1-y_i} $$\nSo the likelihood function of the whole training data is,\n$$ L = \\prod_{i=1}^N p(y_i|x_i) $$\nThe log likelihood function is given by\n$$ \\text{log} L = \\sum_{i=1}^N \\text{ log } p(y_i|x_i) = \\sum_{i=1}^N y_i\\text{log} p_i + (1-y_i) \\text{log} (1-p_i) $$\n$$ = \\sum_{i=1}^N \\text{log} (1-p_i) + \\sum_{i=1}^N y_i \\text{log} \\frac{p_i}{1-p_i} $$\n$$ = \\sum_{i=1}^N \\text{log} (1-p_i) + \\sum_{i=1}^N y_i (w^Tx_i + b) $$\n$$ = \\sum_{i=1}^N \\text{log} (1- \\frac{1}{1 + e^{-z_i}}) + \\sum_{i=1}^N y_i (w^Tx_i + b) $$\n$$ = \\sum_{i=1}^N -\\text{log} (1 + e^{(w^Tx_i + b)}) + \\sum_{i=1}^N y_i (w^Tx_i + b) $$\nOur goal is to maximise the log likelihood function, so we take the derivative of $L$ w.r.t $w$\n$$ \\frac{\\partial L}{\\partial w_j} = -\\sum_{i=1}^N \\frac{e^{(w \\cdot x_i)}}{1+e^{(w \\cdot x_i)}} x_{ij} + \\sum_{i=1}^N y_i x_{ij} $$\n$$ = \\sum_{i=1}^N (y_i - z_i) x_{ij} $$\nHowever, this is a transcendental equation(an equation containing the variables being solved for), which means we cannot solve analytically, and thus there is no closed-form solution. But we can still find the estimation of $w$ through gradient descent using the above derivative.\nThe loss function used in logistic function is defined as follows,\n$$ L = - \\sum_{i=1}^N y_i \\text{log} p_i - (1-y_i) \\text{log} (1-p_i) $$\nwhich is often called the \u0026lsquo;Binary Cross Entropy\u0026rsquo;. Obviously, the loss function is exactly the negative log likelihood function. Thus, maximising likelihood is equal to minimising the loss.\ndef crossEntropyLoss(X, y, theta): epsilon = 1e-12 p = logistic(X @ theta) cost = -np.mean(y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon)) return cost Multinominal What if we have more than one target category? It\u0026rsquo;s not a simple yes or no classification since now we have many possible classes. Suppose there are $k$ classes, and each sample must belong to one class, so we have\n$$ \\sum_{j=1}^k p_{i, j} = 1 $$\nJust like the binominal case, we choose one class as the reference model and estimate the \u0026lsquo;odds\u0026rsquo;\n$$ z_{i,j} = \\text{log } \\frac{p_{i, j}}{p_{i, J}} = \\bold w_j ^Txx_i + b_j $$\n$$ p_{i, j} = p_{i, J} e^{z_{i, j}} = p_{i, J} e^{\\bold w_j ^Tx + b_j} $$\nTo obtain $p_{i, j}$, we need to eliminate $p_{i, J}$. Combing the two equations, we have\n$$ \\sum_{j=1}^k p_{i, j} = p_{i, J} \\sum_{j=1}^k e^{z_{i, j}} = 1 $$\n$$ \\text{=\u0026gt; } p_{i, J} = \\frac{1}{\\sum_{j=1}^k e^{z_{i, j}} } $$\nplug $p_{i, J}$ into $p_{i,j}$, we have\n$$ p_{i, j} = \\frac{ e^{\\bold w_j ^Tx + b_j} }{\\sum_{j=1}^k e^{z_{i, j}} } $$\nwhich is known as softmax function.\nAgain, we use MLE to estimate $\\bold w$. The difference from binominal case is that now there are $k$ estimates we need to calculate. The likelihood of a single sample is,\n$$ p(y_i|x_i) = \\prod_{j=1}^{k} p_{ij}^{I_{ij}} $$\nwhere $I_{ij} = 1$ if $x_i$ belong to class $j$, otherwise $I_{ij}=0$. The likelihood function of the whole training data is,\n$$ L = \\prod_{i=1}^N \\prod^k_{j=1} p_{ij}^{I_{ij}} $$\nThe log likelihood function is given by\n$$ \\text{log } L = \\sum_{i=1}^N \\sum_{j=1}^k I_{ij} \\text{log }(p_{ij})= \\sum_{i=1}^N \\sum_{j=1}^k I_{ij} \\text{log } \\frac{ e^{\\bold w_j ^Tx_i + b_j} }{\\sum_{m=1}^k e^{z_{i, m}} } $$\n$$ = \\sum_{i=1}^N \\sum_{j=1}^k I_{ij} (z_{ij} - \\text{log } \\sum_{m=1}^k e^{z_{i,m}}) $$\nNow let\u0026rsquo;s take the derivative of $L$ w.r.t the coefficients from the $j\\text{th}$ class $\\bold w_j$\n$$ \\frac{\\partial L}{\\partial \\bold w_j} = \\sum_{i=1}^N (I_{ij} x_i -\\sum_{j=1}^k I_{ij}\\frac{e^{z_{ij}} x_i}{\\sum_{m=1}^k e^{z_{i,m}}}) = \\sum_{i=1}^N (I_{ij} - p_{i, j}) x_i $$\nThe matrix form can be written as\n$$ \\frac{\\partial L}{\\partial \\bold w_j} = \\bold X^T (I_j - p_j) $$\nwhere $\\bold X = [x_1, x_2, \u0026hellip; x_N]^T $ is a $N \\times D$ matrix and\n$$ I_j = \\begin{bmatrix}I_{1j}\\\\ I_{2j}\\\\ \u0026hellip;\\\\ I_{nj} \\end{bmatrix}, p_j = \\begin{bmatrix}p_{1j}\\\\ p_{2j}\\\\ \u0026hellip;\\\\ p_{nj} \\end{bmatrix} $$\nTherefore,\n$$ \\frac{\\partial L}{\\partial \\bold W} = \\bold X^T (\\bold I - \\bold P) \\in R^{D \\times K} $$\nwhere $\\bold I = [I_1, I_2, \u0026hellip;, I_k] \\in R^{N \\times K}, \\bold P = [p_1, p_2, \u0026hellip;, p_k] \\in R^{N \\times K}$\nComments Logistic regression is a traditional classification model that often works effectively. However, it is problematic if data are linearly separable. If $w, b$ perfectly separates data linearly, so does cb, cw with $c \\gt 0$, so there is no parameter vector that maximises likelihood.\nReferences  https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/perceptron/",
                "title": "Perceptron",
                "section": "post",
                "date" : "2021.06.13",
                "body": "Perceptron is a traditional classification algorithm and it is the basis of the neural network. Though it\u0026rsquo;s out of date, it plays a historical role in the development of neural network. Knowing how it works will help us lay the foundations for the future study of the neural network.\nModel Perceptron is a linear binary classifier, where linear means that it separates the input space linearly and binary means that it classify a sample into one of the two categories. Let $\\bold x_1, \\bold x_2, \\bold x_3, \u0026hellip;, \\bold x_n$ be observations and $y_n \\in -1, 1$ be target classes, Perceptron model is defined as follows,\n$$ \\hat y_n = f( \\bold w^T \\bold x_n) $$\nwhere $f(x)$ is an active function\n$$ f(x) = \\begin{cases} 1 \u0026amp; \\text{if $x \\ge 0$} \\\\ -1 \u0026amp; \\text{otherwise}\\end{cases} $$\nLoss Function If we classifiy correctly, $\\hat y_n $ will have the same sign as $y_n$. Otherwise, it will have the opposite sign to $y_n$.\n$$ g(x) = \\begin{cases} \\hat y_n y_n \\ge 0\u0026amp; \\text{if classified correctly} \\\\ \\hat y_n y_n \\lt 0 \u0026amp; \\text{otherwise}\\end{cases} $$\nAt the same time, we find that the sign of $\\hat y_n$ is determined by $w^Tx$, so we have\n$$ g(x) = \\begin{cases} y_n (\\bold w^T \\bold x_n) \\ge 0\u0026amp; \\text{if classified correctly} \\\\ y_n (\\bold w^T \\bold x_n) \\lt 0 \u0026amp; \\text{otherwise}\\end{cases} $$\nWe don\u0026rsquo;t care about what value of $w^tx_n$ is, instead, we are more interested in its sign. Naturally, the loss function is defined as follows,\n$$ L = -\\sum_{n \\in M} y_n (\\bold w^T \\bold x_n) $$\nwhere $M$ is the set of misclassified points. It\u0026rsquo;s clear that the loss function will reach the minimum point if we classify all samples correctly.\nParameter Estimation Though we could take the derivative of $L$ w.r.t. $\\bold w$\n$$ \\frac{\\partial L}{\\partial \\bold w} = -\\sum_{n \\in M} \\bold x_n y_n $$\nObviously, we cannot set this equal to 0 and get a solution for $\\bold w$. A common way to find the optimal parameter is to use gradient descent, as shown below\n$$ \\bold w^{t+1} = \\bold w^t + \\eta \\sum_{n \\in M} \\bold x_n y_n $$\nwhere $\\eta$ is the learning rate. The updating of $\\bold w$ could be iteratively — update $w$ when a sample is misclassified each time. Since $y_n \\in -1, 1$, we can decompose the above equation into the below two equations\n$$ \\bold w^{t+1} = \\begin{cases} \\bold w + x_n \u0026amp; \\text{if $y_n = 1$} \\\\ \\bold w - x_n \u0026amp; \\text{if $y_n = -1$}\\end{cases} $$\nIf a point belongs to positive category, i.e. $y_n = 1$, but our model misclassifies it as a negative category, then we would move $\\bold w$ along the direction towards this point because of $w + x_n$ and vice versa.\nAlgorithm Putting it together, the whole algorithm of Perceptron is\n Randomly initialise $w$ Until convergence or some stopping rules reached,  for $x_1, x_2, \u0026hellip;, x_n$  compute $\\hat y_n = f(w^T x_n)$ if $\\hat y_n y_n \u0026lt; 0$,  if $y_n = 1$, update $w' = w + \\eta x_n$ if $y_n = -1$, update $w' = w - \\eta x_n$       "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/knn/",
                "title": "K-nearest neighbours",
                "section": "post",
                "date" : "2021.06.12",
                "body": "K-Nearest Neighbors(KNN) is a distance-based algorithm in machine learning used for both classification and regression. It\u0026rsquo;s simple and intutive to understand because it doesn\u0026rsquo;t require extra training step and the idea behind it is simple enough — similar points are tends to be close to each other. In this article, we will learn how KNN works.\nHow KNN works Suppose we have a dataset and a new data point, we want to know which class this new example belong to or what the predicted value is if it\u0026rsquo;s a regression task. KNN works in this way,\n Choose a value for $K$ Calculate distances between the new point and each sample of the training data Sort the distances from the nearest to farthest and get the first $K$ nearest samples For classification  Majority vote among the $K$ observations. The new sample belongs to class that has the highest votes.   For regression  Average the value of the $K$ observations    def majority_vote(labels): votes = Counter(labels) label, _ = votes.most_common(1)[0] return label def knn_classify(inputs, labels, x, k, distance): distances = sorted([ (distance(x, point), label) for point, label in zip(inputs, labels)], key=lambda: lp:lp[0]) k_nearest_labels = [ lp[1] for lp in distances[:k] ] return majority_vote(k_nearest_labels) What if we have the same votes for two different categories?\n Pick one of them randomly Calculate weighted voting Reduce k until we find a unique answer  Distance From above, we can see that the main problems are how to find an appropriate $K$ and how to measure distance. There are several common distance measures in machine learning, such as Euclidean and Manhattan. They are applied in different situations, so knowing how and when they can be best used is important to improve our model performance.\nEuclidean Distance Euclidean distance is defined as follows,\n$$ D = \\sqrt{\\sum_i^d (x_i - y_i)^2} $$\nIt\u0026rsquo;s simply the length of the straight line connecting two points. We should be careful with it because it\u0026rsquo;s sensitve to the scale of features. So we need to normalize data first. Besides, it is not suitable for high-dimensional space. Anyway, it\u0026rsquo;s still the most common and straightforward distance measure.\nCosine Distance Cosine distance measures the closeness between two points by calculating the angle between them.\n$$ D = \\text{cos}\\theta = \\frac{x \\cdot y}{||x|| \\text{ } ||y||} $$\nSo this method doesn\u0026rsquo;t care about the length of a vector. If we are more interested in the direction of a vector rather than the length, it would be a better choice. For example, two documents represented by word-document vector could be similar even they vary in the number of the same words. This might be because one document is just much longer than another one, but they are all related to the same topic.\nManhattan Distance Manhattan distance is defined as the sum of steps moving from one point to another point. You can only move along the horizontal direction or vertical direction. Imagine you stand on a chess board, how many cells do you need to move?\n$$ D = \\sum_i^d |x_i - y_i| $$\nThis method originated from how to calculate the distance between source and destination in a city. You cannot always walk through a building or something, so Euclidean distance is not helpful. Instead, a distance based on streets would be optimal.\nThrough Manhattan distance could work well in a high-dimensional space, it\u0026rsquo;s less intutive than Euclidean distance. Also, the distance measured by Manhattan is a bit larger than Euclidean because of the Triangle Inequality Theorem.\nFinding the best K How do we find the optimal value for $K$? If $K$ is too small, the model would have a high variance and a poor generalization. If $K$ is too large, the model would have a high bias.\n If we have some domain knowledge, for example, we want to classify whether a new flower belong to a specific species among given all species. $K$ could be the number of that specific species. Typically, we may have little domain knowledge, one way to find the best $K$ is cross-validation. We choose the \u0026lsquo;elbow\u0026rsquo; point as the optimal $K$ from all these scores returned by cross-validation, as shown below. As a rule of thumb, we could choose $k = \\sqrt N$, where $N$ is the number of samples    Figure 1: The red point in the figure represents the \u0026#39;elbow\u0026#39; point. In this example, the best k is 13.   Pros and Cons Pros  Easy, simple and straightforward to understand no need to train, it\u0026rsquo;s a non-parametric method Suitable for both classification and regression  Cons  More suitable for a small number of samples because distance is computed throughout nearly every data in the training set. Thus computation might be a matter if the dataset is large. Sensitive to outliers. If one category has some outliers, a new sample might be classified into this category since they might be more close to the new sample even if the new sample should belong to another category. Tends to be biased if samples are unbalanced. One category would dominate the majority voting of the new sample because it has a great number of samples.  References  https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa https://medium.com/machine-learning-101/k-nearest-neighbors-classifier-1c1ff404d265 "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/information-theory/",
                "title": "Information Theory",
                "section": "post",
                "date" : "2021.06.07",
                "body": "I remembered that I learned a little stuff about information when I was in my undergraduate course. Well, maybe I was wrong. It’s been almost ten years. But when it comes to information theory, I know that we will talk about a person — Shannon. So what’s the information theory? What problems does it help to solve?\nInformation First, let\u0026rsquo;s understand what information is. Consider a guessing game where Alice randomly chooses a number between 1 and 100, and then we need to guess what that number is. An intuitive method is to use a dichotomy search. We may ask, \u0026lsquo;is it larger or smaller than 50?\u0026rsquo;. If it\u0026rsquo;s less than 50, we continue to ask, \u0026lsquo;is it larger or smaller than 25?\u0026rsquo; We do this iteratively until we find the right number. Mathematically, the number of searches is $log_2(100)$. If the number Alice chooses is small, say 3, then it would take many steps to get the correct answer.\nIf we know that Alice has some preferences for some particular numbers, say she likes 88 most. Then the first question we ask at this time would be \u0026lsquo;is it 88?\u0026rsquo;. If we are lucky enough, we will get the number once. On the other hand, since 88 is the number that Alice likes most, there is no doubt that she is likely to choose 88 as the riddle. Conversely, if Alice is going to surprise us, she would choose a number that she doesn\u0026rsquo;t like very much. If so, it increases the uncertainty of our guess, i.e. we would need to ask more questions.\nThe number of questions we ask or the degree of surprise is information, which is measured by\n$$ I(X=x) = -log(p(X=x)) $$\nwhere $p(x)$ is a probability of occurence of x. So information quantifies how uncertain an event is. A rare event will cause much surprise and consequently contain much information.\nEntropy Suppose we have a probability distribution $p(x)$ over a random variable $X$, and the value of each outcome represents information associated with it. The entropy of $X$ is then defined as the expectation of information,\n$$ H(X) = -\\sum_{x \\in X} P(X=x) \\text{log} P(X=x) $$\nIf a random variable $X$ follows binominal distribution with probability $p$, then its entropy is computed as follows,\n$$ H(X) = -p\\text{In}p - (1-p)\\text{In}(1-p) $$\n  Figure 1: Plot of binary entropy (Wikipedia)  Figure 1 shows how $H(X)$ changes as $p$ changes. We can see that $H(X)$ tends to be zero if $p$ lies on the end of x-axis, which means that it tells us rare information since we know what to expect. On the contrary, $H(X)$ reaches the highest point if $p=0.5$, this is because we don\u0026rsquo;t know what to expect and anything could happen. From this view, entropy is also a measure of how uncertain $X$ is. The higher the entropy is, the higher the uncertainty is.\nKL Divergence Given two probability distribution $f(x)$ and $g(x)$ for a random variable $x$, the difference between them is defined as KL divergence, which is sometimes called relative entropy\n$$ D(f||g) = \\sum_{x \\in X} f(x) \\text{log} \\frac{f(x)}{g(x)} = -\\sum_{x \\in X} f(x) \\text{log} g(x) + f(x) \\text{log} f(x) $$\nCross-entropy Combing the definition of entropy, we have\n$$ H(f)= -\\sum_{x \\in X} f(x) \\text{log} f(x) $$\n$$ H(f, g) = -\\sum_{x \\in X} f(x) \\text{log} g(x) $$\n$$ D(f||g) = H(f, g) - H(f) $$\nwhere $H(f, g)$ is cross-entropy, which is the expected information of $X$ sampled from a probability distribution $f(x)$ with outcomes encoded using another distribution $g(x)$. We can find that KL divergence can be decomposed into two parts: cross-entropy and the entropy of $X$ sampled from $f(x)$. Usually $f(x)$ is considered as \u0026lsquo;true\u0026rsquo; probability distribution, so miniming KL divergence against a fixed reference distribution $f$ is equivalent to minimising cross-entropy.\nCross-entropy is a common loss function used in classification tasks. Suppose there are 3 classes, the class distribution for a given $x$ is $0, 1, 0$ and our model might give us a predicted class distribution, say $0.2, 0.5, 0.3$. Then, we calculate entropy for this observation, $H(f, g) = -(0 + 1 *\\text{log}0.5 + 0) = 1$. After a period of learning, the model could give us a better predicted label distribution say $0.05,0.9,0.05$, and this time, we find that the entropy decreases to $0.15$. It shows that the difference between our model and true distribution is decreasing.\nConditional Entropy Like conditional probability, we define conditional entropy as the uncertainty about a random variable $C$ after we observe one feature $x$ in $X$\n$$ H(C|X=x) = -\\sum_{c \\in C} p(c|X=x) \\text{log} p(c|X=x) $$\nThe difference between the entropy $H(C)$ and the conditional entropy $H(C|X=x)$ is realized information\n$$ I[C; X=x] = H(C) - H(C|X=x) $$\nHence, $I(C;X=x)$ measures how much uncertainty of $C$ changes due to observering $x$. Here, we use \u0026lsquo;change\u0026rsquo; because realized information is not always reduced.\nMutual Information Mutual information defined below tells us the expected reduction in uncertainty of $C$ that a feature gives us\n$$ I(C; X) = H(C) - \\sum_x p(X=x) H(C|X=x) $$ Here are some important notes about mutual information:\n it is always positive it is zero if $C$ and $X$ are statistically independent it is symmetric in X and C  In fact, we have met mutual information before — information gain in decision trees.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/constrained-optimisation/",
                "title": "Constrained Optimisation",
                "section": "post",
                "date" : "2021.06.03",
                "body": "When I first learned machine learning, I was scared by the complicated formulas. I spent much time going over subjects like Linear Algebra and Calculus since I\u0026rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-world problems is the key.\nAs we all know, the main effort in machine learning is to find a loss function and optimise it, i.e. find the minimum or maximum point, and this is the question of optimisation. However, we may only find local optimisation because of some constraints. Even without constraints, there is still a chance that we would reach local optimisation only. In short, there are two main situations we need to consider: unconstrained optimisation and constrained optimisation. And constrained optimisation further falls into two categories, equality constraints or inequality constraints.\nOn the other hand, the extreme value of a function typically relates to some property of that function. That means if we know a function has some particular property, then we know that it must have an extreme value or not. This property can be characterised by convexity. As you can see, this post will be very mathematical. Seems a bit scary, ummm\u0026hellip;\nEquality Constraints Suppose we want to minimise a function $f(x)$ subject to an equality constraint, $g(x) = 0$. This can be solved by introducing Lagrange multiplier $\\alpha$\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nThen set the gradient of $\\mathcal{L}$ equal to the zero vector\n$$ \\nabla_x \\mathcal{L} = \\nabla_x f(x) - \\alpha \\nabla_xg(x) = 0 $$\n$$ \\frac{\\partial \\mathcal L}{\\partial \\alpha} = -g(x) = 0 $$\nFinally, solving the above equations will give us the minimum point we are seeking.\nMultiple Constraints If we have multiple constraints, then multiple Lagrange multipliers are introduced,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\sum_i^m \\alpha_i g_i(x) $$ And the corresponding solutions are given by,\n$$ \\nabla_x f(x) = \\sum_i^m \\alpha_i \\nabla_xg_i(x) $$\n$$ \\frac{\\partial \\mathcal L}{\\partial \\alpha_i} = -g_i(x) = 0 $$\nLagrange But what\u0026rsquo;s the rationale behind these formulas? Though we are not required to know everything, basic understanding of Lagrange is still needed. Remember that the question is to find a point $x^*$ that satisfies both $g(x^*) = 0$ and $f(x^*) =m $, where $m$ is the minimum value of $f(x)$ given the constraint.\nThe line represented by $f(x) = c$ is known as a contour line. Since $f(x)$ can have many values, we can plot many contour lines with equal intervals between lines in ascending or descending order from inner to outer. Graphically, we want to find a point that lies on the line of $g(x) = 0$ and the line of $f(x)$ with the minimum value simultaneously as shown in Figure 1. The green point is the point we are looking for.\n  Figure 1: Illustration of equality constraints  But we still need to figure out equations to calculate the position of the gree point. Let\u0026rsquo;s start from $x_0$, the magenta point in Figure 1. Mathematically, the above process of finding $x^*$ can be described as follows,\n$$ f(x_0 + \\delta x) \u0026lt; f(x_0) $$\n$$ g(x_0) = g(x_0 + \\delta x) = 0 $$\nSo in which direction should we move at $x_0$? With the aid of Taylor expansion, we have $$ g(x_0 + \\delta x) = g(x_1) = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) + \\frac{1}{2} (x_1 - x_0)^T H (x_1 - x_0) $$\n$$ = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) + O(||x_1 - x_0||^2) $$\nwhere $x_1 = x_0 + \\delta x$ and $H$ is a matrix of second derivative of $g(x)$ known as Hessian. The third term tends to be zero if $\\delta x$ is small enough, then we are left with\n$$ g(x_0 + \\delta x) = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) = g(x_0) $$\nThus,\n$$ (x_1 - x_0)^T\\nabla_xg(x_0) = 0 $$\nwhich means that the moving direction from $x_0$ should be perpedicular to $\\nabla_xg(x_0)$. But we are not done, because not all $(\\delta x = x_1 - x_0)$ point to the right direction along which $f(x)$ decreases. In order to ensure this, we require that $\\delta x$ must satisfy\n$$ (x_1 - x_0)^T ( - \\nabla_x f(x)) \u0026gt; 0 $$\nwhere $- \\nabla_x f(x)$ indicates the descent direction. Therefore, as long as the value of the dot product is greater than zero, the moving of point will continue unless the value becomes zero. If so, it means that the direction of $\\nabla_x f(x)$ is parallel to $\\nabla_x g(x)$, i.e.\n$$ -\\nabla_x f(x) = \\lambda \\nabla_x g(x) $$\nWe can rewrite this by replacing $\\lambda$ with $\\alpha = -\\lambda $\n$$ \\nabla_x f(x) = \\alpha \\nabla_x g(x) $$\nFinally, we come to the method of Lagrange multipliers.\nInequality Constraints Now we consider another situation where we have inequality constraints, i.e. $g(x) \\ge 0$. It looks a little complicated, but if we think about it for a while, we can find that only two things could happen, as shown in Figure 2,\n either the optimum point satisfies $g(x) \\gt 0$ or the (local) optimum point lies on the boundary, $g(x) = 0$    Figure 2: Two possible outcomes of inequality constraints  Again, we use Lagrange to solve it,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nIn the first case where the optimum point lies in the interior of the constraint, i.e. $g(x) \u0026gt; 0$, which is filled in red in Figure 2 a). We set $\\alpha = 0$, which means the constrains has no influence on $f(x)$.\nAs for the second case, it is exactly the same as the equality constraints, i.e.\n$$ \\nabla_x f(x) = \\alpha \\nabla_xg(x) $$\nbut with an additional constraint $\\alpha \u0026gt; 0$. So why do we set $\\alpha \u0026gt; 0$ here?\n$\\alpha \\ge 0$ Visually, it can be seen From Figure 2 b) that both the magenta and blue points seem to be the right point we are seeking. But in fact, only the bule one is in a lower position. And we find that $\\nabla_x f(x) $ and $\\nabla_x g(x)$ point to the same direction at the blue point. Thus, $\\alpha$ is positive.\nIn theory, if we are at a point where $-\\nabla_x f(x)$ points to the feasible region, which is the area defined by $g(x) \u0026gt; 0$, i.e. any value of $x$ inside this region is valid, it means that a point with a smaller value of $f(x)$ could be found in the feasible region. But it contradicts the assumption that we can only move along the boundary of the region. In other words, this is not the optimal point.\nIf $-\\nabla_x f(x)$ at some point points to the exterior of the feasible region, then we are in the right position because the outer of the feasible region is invalid and we cannot move forward any further(we are already on the border of the region).\nIt is noticeable that $\\nabla_x g(x)$ points in towards the feasible region. Therefore, we conclude that $\\nabla_x f(x)$ and $\\nabla_x g(x)$ have the same direction. Thus, $\\alpha$ is positive.\nFrom above, we can also draw another conclusion shown below\n$$ \\alpha \\nabla_x g(x) = 0 $$\nKKT Conditions Putting it together, we want to minimise $f(x)$ subject $g(x) \\ge 0$, and the Lagrangian function is defined as follows,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nThen we can find a local mininum $x^*$ s.t.\n $\\nabla_x f(x) = \\alpha \\nabla_xg(x)$ $\\alpha \\ge 0$  $\\alpha = 0$, the solution is in the interior or $\\alpha \\gt 0$ and $g(x) = 0$, i.e. the solution is on the boundary   $\\alpha \\nabla_x g(x) = 0$  These are the Karush-Kuhn-Tucker (KKT) conditions.\nMany Inequalities Once again, if we have many ineuqality constraints, then Lagrangian function is given by,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\sum_i^m \\alpha_i g_i(x) $$\nAnd the corresponding solutions are given by,\n$$ \\nabla_x f(x) = \\sum_i^m \\alpha_i \\nabla_xg_i(x) $$\nplus the constraints that\n either $\\alpha_i = 0$ or $\\alpha_i \\gt 0$ and $g_i(x) = 0$ $\\alpha_i \\nabla_x g_i(x) = 0$  Duality Let\u0026rsquo;s revisit the above problem again. Consider minimising a function $f(x)$ subject to $g(x) \\ge 0$, the lagrangian function is given by\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nPrimal Problem Now consider maximising $\\mathcal{L}(x, \\alpha)$ w.r.t $\\alpha$,\n$$ \\text{max}_\\alpha \\mathcal{L}(x, \\alpha) = \\begin{cases} f(x) \u0026amp; \\text{if $g(x) \\ge 0$} \\\\ \\infin \u0026amp; \\text{otherwise}\\end{cases} $$\n  For any $x'$ that satisfies the constraint $g(x) \\ge 0$, we conclude that\n$$ f(x') \\ge \\mathcal{L}(x', \\alpha) $$\ni.e. the upper bound of $\\mathcal{L}(x, \\alpha)$ is $f(x)$. Thus, the maximum value of $\\mathcal{L}$ we can obtain is to set $\\alpha = 0$\n  On the contratry, if the constraint is not satisfied, then there exists some $x'$ that satisfies $g(x) \\lt 0$. If so, then we can make $\\mathcal{L}$ infinite by taking $\\alpha \\rarr \\infin$.\n  Next we take the minimum of the maximum of $\\mathcal{L}$, i.e.\n$$ \\text{min}_\\text{x} \\text{max}_\\alpha \\mathcal{L}(x, \\alpha) $$\nwhich is equivalent to the problem of minimising $f(x)$ subject to $g(x) \\ge 0$. We call the original problem as primal problem.\nDual Problem Yet we still don\u0026rsquo;t find a solution to $x$. How about reversing the order of max and min like the below formula?\n$$ \\text{max}_\\alpha \\text{min}_\\text{x} \\mathcal{L}(x, \\alpha) $$\nWe solve $\\text{min}_\\text{x} \\mathcal{L}(x, \\alpha)$ using the method of Lagrange, and find that $x$ is a function of $\\alpha$. Then we plug $x$ into $ \\mathcal{L}(x, \\alpha)$ and obtain a new function of $\\alpha$, say $h(\\alpha)$. So the problem becomes to maximise $h(\\alpha$), also known as dual problem,\n$$ \\text{max}_\\alpha h(\\alpha) $$\nHowever, is the solution to dual problem the same as the primal problem? Why do we bother solving a dual problem rather than the original problem?\nSince $\\alpha \\ge 0$ and $g(x) \\ge 0$, we have,\n$$ \\text{min}_\\text{x} \\mathcal{L}(x, \\alpha) \\le \\text{min}_x f(x) = p^* $$\n$$ d^* = \\text{max}_\\alpha \\text{min}_\\text{x} \\mathcal{L}(x, \\alpha) \\le p^* $$\nwhere $p^*$ and $d^*$ are the optima of the primal and dual problem respectively.\nIt can be seen that the solution of dual problem gives us a lower bound on the primal problem. If possible, we can also have the same solution.\nThe reason why we solve the dual problem is that the primal problem works in a feature space that may have high dimensions while the dual problems depends on the number of constraints, which is much smaller than the dimensitionality of $x$.\nLinear Programming In linear programming, we minimise a linear function $c^Tx$ subject to a series of linear constraints $g(x) = Mx - b \\ge 0$. The primal problem is described as follows,\n$$ \\mathcal{L} (x, \\alpha) = c^Tx - \\alpha^T(Mx - b) $$\n$$ \\text{minimise } c^T x $$\n$$ \\text{subject to } Mx \\ge b $$\nand the dual problem is defined as,\n$$ \\mathcal{L} (x, \\alpha) = b^T\\alpha - x^T(M^T \\alpha - c) $$\n$$ \\text{maximise } b^T \\alpha $$\n$$ \\text{subject to } M^T \\alpha\\le c $$\nLet\u0026rsquo;s see and example.\nPrimal problem\n$$ \\text{minimise } z = 15x_1 + 12x_2\\\\ \\text{subject to } x_1 + 2x_2 \\ge 3, 2x_1 - 4 x_2 \\ge 5 $$\nDual problem\n$$ \\text{maximise } w = 3y_1 + 5y_2\\\\ \\text{subject to } y_1 + 2y_2 \\le 15, 2y_1 - 4 y_2 \\le 12 $$\nQuadratic Programming In quadratic programming, we minimise a quadratic function $x^TQx$ subject to a series of linear constraints $g(x) = Mx - b \\ge 0$. The primal problem is described as follows,\n$$ \\text{max}_\\alpha \\text{min}_\\text{x} \\mathcal{L} (x, \\alpha) = x^TQx - \\alpha^T(Mx - b) $$\nUsing the method of Lagrange, we have $x^* = \\frac{1}{2}Q^{-1}M^T\\alpha$, and we substitue it into $\\mathcal{L}$\n$$ \\text{max}_\\alpha -\\frac{1}{4} \\alpha^TMQ^{-1}M^T\\alpha + \\alpha^Tb $$\nConvexity Quadratic Form  Quadratic form is a polynominal function with terms all of degree of two. For example, $4x^2 + 2xy - 3y^2$. — Wikipedia\n Here \u0026ldquo;the degree of two\u0026rdquo; means that the sum of exponents for each term is 2. A general quadratic form of $n$ variables is defined below, where $M$ could be chosen symmetric.\n$$ Q(\\bold x) = \\bold x^T \\bold M \\bold x = \\sum_{i,j}^d \\bold M_{ij} \\bold x_i \\bold x_j $$\nIn this example $4x^2 + 2xy - 3y^2$, the quadratic form is given by,\n$$ Q(\\bold x) = \\bold x^TM\\bold x = \\displaystyle{\\begin{bmatrix}x\u0026amp;y\\end{bmatrix} \\begin{bmatrix}4\u0026amp;1\\\\1\u0026amp;-3 \\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix} } $$\nBasically, quadratic form is a mapping from $R^d$ to $R$. Without doubtness, for any quadratic form, we have $Q(\\bold 0) = 0$. But is $x=\\bold 0$ is the minimum or maximum point for $Q$ ? The answer is determined by the definiteness of $Q$ described below,\n $Q(\\bold x) \\gt0$,  positive definite   $Q(\\bold x) \\ge 0$,  positive semi-definite   $Q(\\bold x) \\lt 0$ ,  negative definite   $Q(\\bold x) \\le 0$,  negative semi-definite   $Q(\\bold x)$ could be both positive and negative  indefinite    Clearly,\n if $Q$ is positive definite, then $x = 0$ is global minimum if $Q$ is negative definite, then $x = 0$ is global maximum  Furthermore, quadratic form can also be characterised in terms of eigenvalues:\n positive definite if and only if the eigenvalues of $M$ are positive, negative definite if and only if the eigenvalues of $M$ are negative, indefinite if and only if $M$ has bothe positive and negative eigenvalues  Here are some proofs. First, we should know that any two eigenvectors from different eigenspaces of a symmetric matrix are orthogonal and any symmetric matrix can be orthogonally diagonalizable.\nLet $\\bold P = [\\bold v1, \\bold v2, \u0026hellip;, \\bold v_n]$ be eigenvectors that correspond to different eigenvalues $\\Lambda= \\lambda_1, \\lambda_2, \u0026hellip;, \\lambda_n$ of a symmetric matrix $A$. To show that $v_1 \\cdot v_2 = 0$, compute\n$$ \\lambda_1 v_1 \\cdot v_2 = (A v_1)^T v_2 = v_1^T A v_2 = \\lambda_2 v_1^Tv_2 $$\n$$ (\\lambda_1 - \\lambda_2) v_1^T v_2 = 0 $$\nBut $ \\lambda_1 \\ne \\lambda_2$, so $v1 \\cdot v2=0$. Furthermore, $\\bold P ^T \\bold P = I$, so $P^{-1} = P^T$. Then we have\n$$ AP = PD\\\\A = PDP^{-1} = PDP^T $$\nThe quadratic form of $A$ can be written as follows,\n$$ x^T A x= x^T PDP^Tx = (P^Tx)^T D (P^Tx) = y^TDy = \\lambda_1y_1^2 + \\lambda_2y_2^2 + \u0026hellip; + \\lambda_ny_n^2 $$\nIf the eigenvalues of $A$ are positive, then $x^TAx \u0026gt; 0$ and $A$ is positive definite. On the other hand, if $A$ is positive definite, then $x^TAx \u0026gt; 0$ for any $x \\ne 0$. If we substitute $x$ with an eigenvector $v_1$, we have\n$$ v_1^T A v_1 = v_1^T \\lambda v_1 = \\lambda v_1^Tv_1 = \\lambda ||v_1||^2 \\gt 0 $$\nThus, $\\lambda \u0026gt; 0$.\nConvex Region Before we talk about convex function, let\u0026rsquo;s start with convex region and convex set.\nA region $R$ is said to be a convex region if any two points $x$ and $y$ in that region plus any $a \\in [0, 1]$ satisfy,\n$$ z = a x + ( 1 - a )y \\in R $$\n  Figure 3: Convex region and non-convex region  Convex Set Similarly, for any set of points $S$, if for any two points $x, y \\in S$ and any $a \\in [0, 1]$ satisfy $$ z = a x + ( 1 - a )y \\in S $$ then $S$ is a convex set. We can prove that the set of positive semi-definite matrices form a convex set. Let $A_1, A_2 \\in S$, compute\n$$ x^T z x = x^T (a A_1 + ( 1 - a)A_2)x = x^TaA_1x + x^T ( 1-a) A_2 x \\ge 0 $$\nThus, $z$ is positive semi-definite and $z \\in S$.\nConvex Function Any function is said to be convex if any two points $x$ and $y$ plus $a \\in [0, 1]$ satisfy\n$$ af(x) + (1 - a) f(y) \\ge f(a x + (1 -a ) y) $$\n  Figure 4: Convex function  Conversely, if the condition doesn\u0026rsquo;t meet, the function then is said to be a convex-down or concave function. These two functions are symmetric — everything true for convex functions is also true for concave functions.\nAt the beginning, I often mixed up them. I couldn\u0026rsquo;t tell which figure is convex or concave. But later, I found a simple way to distinguish them correctly. First we find the lowest point or the highest point of a figure. Then we observe the direction along which the curve expands. If the direction is toward down, the function is concave. Otherwise, it\u0026rsquo;s convex. By the way, the area enclosed by the curve (lies on or above the curve ) is defined as epigraph. The epigraph of a convex function forms a convex region, and if the epigraph of a function forms a convex region then the function is convex.\nLinear Functions Now let\u0026rsquo;s take a look at a special case — equality. A function that we are quite familar with satisfies the equality, which is linear function.\n$$ f(x) = mx + c $$ It\u0026rsquo;s easy to proove it.\n$$ m(ax + (1 - a)y) + c = max + my - may + c = af(x) - ac + my(1 -a) + c \\\\= af(x) + my(1-a) + c(1-a) = af(x) + (1-a)f(y) $$\nSo is it a convex or concave function? The answer is both.\nStrictly Convex What about the condition without euqality? Well, such a condition is called strict inequality and functions that satisfy the strict inequality is said to be strictly convex/concave.\nSecond derivative One thing we should remember is that any tangent line of a convex funtion lies on or below the function. Let $t, z$ be two points on the graph of a convex function, then we can derive the following conditions,\n $ t \\lt z$  $$ f(t) + f'(x) (z - t)\\le f(z) $$\n$$ f'(t) \\le \\frac{f(z) - f(t)}{z - t} $$\n $ t \\gt z$  $$ f(t) - f'(t)(t - z) \\le f(z) $$\n$$ \\frac{f(t) - f(z)}{(t - z)} \\le f'(t) $$\nThe two conditions can be combined as a single condition for any two points $a, b$ that satisfies $a \u0026lt; b$ as follows,\n$$ f'(a) \\le \\frac{f(a) - f(b)}{b-a} \\le f'(b) $$\nHence, $f''(a) \\ge 0$.\nIn high dimension, the second derivative of a function is known as Hessian. And a necessary and sufficient condition for that function to be convex is that its Hesssian must be positive semi-positive at all points.\nSums of Convex functions If we have a set of convex functions, then it\u0026rsquo;s easy to prove that the sum of the multiplication of positive factors and these functions is also a convex function using the property that the second derivative is equal or greater than zero. Below is the proof.\n$$ g(x) = \\sum_i \\alpha_i f_i(x) $$\n$$ g''(x) = \\sum_i \\alpha_i f''_i(x) \\ge 0 $$\nUnique Minimum As said early, convexity can help to find the extreme value of a function. How does it work? Let $x^*$ be a local minimum of a function, suppose there exists another points $\\hat x$ such that $f(\\hat x) \u0026lt; f(x^*)$. By the definition of convexity, we have\n$$ f(a \\hat x + (1-a)x^*) \\le af(\\hat x) + (1-a) f(x^*) \\le af(x^*) + (1-a) f(x^*) = f(x^*) $$\nIf we set $ a \\rarr 0 $, it means that there exist points around $x^*$ with a smaller value than $f(x*)$, which is a contradiction to the definition of local minimum.\nThus, we can see that any local minimum of a convex funtion is a global minimum. Besides,\n there could be many local minimum for a convex function. In other words, the minimum of a convex function will form a convex set. a strictly convex function has at most one global minimum  Putting it together, the whole process of determining whether a function would have a minimum is shown in Figure 5\n  Figure 5: Using Hessian to determine whether a function is a convex function   Inverse of Convex Funtions Let $f(x)$ be a convex function, how about the convexity of the inverse of it, i.e. $g(x) = f^{-1}(x)$?\nFirst, the second derivative of a composite function is given by\n$$ \\frac{d^2f(g(x))}{dx^2} = \\frac{d f'(g(x)) g'(x)}{dx} = f''(g(x)) [g'(x)]^2 + f'(x) g''(x) $$\nBesides, if $f^{-1}(x)$ is the invese of $(x)$, we have\n$$ f(f^{-1}(x)) = x $$\n$$ f''(f^{-1}(x)) = 0 $$\nThus, we conclude that\n$$ g''(x) = - \\frac{f''(g(x)) [g'(x)]^2}{f'(g(x))} $$\nSince $f''(x) \\ge 0$ and $[g'(x)]^2 \\ge 0$, the sign of $g''(x)$ is determined by $-f'(g(x))$.\nHere is an example. Let $f(x) = x^2$ , so that $f''(x) = 2 \\gt 0$ and $f'(x) = 2x$. Since $g(y) = f^{-1}(y) = \\sqrt y \\ge 0$, $f'(g(x)) \\ge 0$ and consequently $\\sqrt x$ is concave.\nJensen’s inequality Jensen’s inequality involves inequality of convex function, which states that for any convex function $f(x)$,\n$$ E[f(x)] \\ge f(E[x]) $$\nand for any concave function,\n$$ E[f(x)] \\le f(E[x]) $$\nIt\u0026rsquo;s easy to prove using the fact that a convex function must lie on or above its tangent line at any point $x'$\n$$ f(\\hat x) \\ge f(x') + (\\hat x - x')^T \\nabla f(x) $$\nso this is true for $x' = E[x]$\n$$ f(\\hat x) \\ge f(E[x]) + (\\hat x - E[x])^T \\nabla f(x) $$\nthen taking expectations of both sides\n$$ E[f(\\hat x)] \\ge f(E[x]) + (E[\\hat x] - E[x])^T \\nabla f(x) = f(E[x]) $$\nA typical example is $f(x) = x^2$, Jensen\u0026rsquo;s inequality shows that\n$$ E[x^2] - E^2[x] \\ge 0 $$\nWell, it\u0026rsquo;s the formula of variance, and variance are non-negative.\nConclusion Finally, we\u0026rsquo;re here. I spent several days writing up this article. To be honest, I am not a math person, and it was a struggle to explain these mathematical concepts and formulas clearly and accurately. But in doing so, I had a better understanding about optimisation. But knowing these equations only is not enough, the key point is to learn to apply them in machine learning to solve real-world problems. Anyway, we are done for now.\nReferences  https://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/probabilistic-model/",
                "title": "Probabilistic Model",
                "section": "post",
                "date" : "2021.05.26",
                "body": "In the previous post Descriptive Statistics, we focused on summary statistics on a particular data set. However, in machine learning, we usually attempt to make inference, i.e. infer the unknown parameters from the given data. For unknown things, we use probability to describe its uncertainty. For inference, we use Bayes' rule to invert it into a forward process.\nProbability At the beginning, let\u0026rsquo;s have a quick refresh on probability. Conventionly, we use a capital letter, such as $X$ or $Y$, to represent a random variable. Suppose we have two random variables of interest, $X$ and $Y$,\n  The joint probability of $X$ that takes the value of $x$ and $Y$ that takes the value of $y$ is written as $P(X = x, Y=y)$, which means that the probability of $x$ and $y$ happening at the same time\n  Given $Y=y$, the conditional probability of $X$ given $Y=y$ is denoted by $P(X|Y=y)$\n  There are two major rules of probability that we should remember,\n the sum rule, i.e. the marginal probability of $X$ that takes the value of $x$, irrespective of the value of $Y$  $$ P(X=x) = \\sum_{y \\in Y} P(X=x, Y=y) $$\n the product rule, i.e. the joint probability of $X$ and $Y$ can be written as the product of the conditional probability and the marginal probability  $$ P(X, Y) = P(Y|X) P(X) = P(X|Y)P(Y) $$\nFrom the above formula, we can deduce the following equation, which is also known as Bayes' Rule,\n$$ P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)} $$\nBayes' Inference In machine learning, our goal is to learn a model $\\Theta$ from a given data set $D$, but $\\Theta$ is uncertain. One way to measure uncertainty is probability, so the question becomes how to compute the probablity of $\\Theta$ given the data $D$, i.e. $P(\\Theta|D)$. But the only thing we know is the observed data, so how can we do it? The answer is the Bayes' Rule, which helps us convert this into a forward problem, where parameters are known and thus we can draw data from the distribution determined by that paramaters. The formula is defined as follows,\n$$ P(\\Theta|D) = \\frac{P(D|\\Theta)P(\\Theta)}{P(D)} $$\n $P(\\Theta)$ is called the prior probability, i.e. the best guess about $\\Theta$ before we see the data. You might have a good estimate on it or simply have no idea at all $P(D|\\Theta)$ is the likelihood of the data given the parameters $\\Theta$ $P(D)$ is the evidence or marginal probability denoted by $P(D) = \\sum_{\\theta \\in \\Theta }P(D, \\theta)$ $P(\\Theta|D)$ is the posterior probability, i.e. the updated probability of $\\theta_i$ after we see the data  Pros and cons Pros\n Bayes' rule provides us a full probabilistic description of the parameters It doesn\u0026rsquo;t overfit since we are not choosing the best parameters that fit the data perfectly  Cons\n However, we need to compute $P(D)$, whichsometimes is not reasonable, e.g. there are too many possible values of $\\Theta$ Posterior might not be described as a nice probability function  MAP If we ignore the evidence $P(D)$ (after all, it\u0026rsquo;s just a constant for any $\\theta$), we are only left with the numerator. An easy way to compute $P(\\Theta|D$) is to find the maximum value shown below, though it\u0026rsquo;s not a strictly probability\n$$ {\\argmax}_{\\Theta} log (P(D|\\Theta)) + log (P(\\Theta)) $$\nThis method is called Maximum A Posterior(MAP). However, it can overfit because we are finding the parameters that maximise the likelihood of the observed data. Thus, we are likely to get a model that fit the data with no errors.\nMLE Furthermore, if we ignore the prior $P(\\Theta)$, the MAP is just maximising the likelihood(MLE), which is widely used statistics in machine learning.\nConjugate Prior In some cases, the likelihood of the observed data $D$ is simple to compute, and the posterior would have the same form as the prior if we could find a right prior. Such a likelihood and prior distribution are said to be \u0026lsquo;conjugate\u0026rsquo;. Here we consider two common distributions that a prior might follow: Bernoulli and Poisson.\nBernoulli Distribution Suppose we have a binary random variable $X \\in \\{0, 1 \\}$, and $X_i=1$ if the ith trial is a head and 0 otherwise. Then the likelihood of $X_i$ given the probability of a head $\\mu$ is\n$$ P(X_i = 1|\\mu) = \\mu $$\n$$ P(X_i = 0|\\mu) = 1 - \\mu $$\nOr we can write it in this form\n$$ P(X_i|\\mu) = \\mu^{X_i} (1-\\mu)^{1-X_i} $$\nSuppose we have a data set $ D = \\{ x_1, x_2, \u0026hellip;, x_n \\}$, where $x_i \\in \\{0, 1 \\}$, assuming these observations are drawn independently, then the likelihood of $D$ can be computed as follows,\n$$ L(D;\\mu) = \\prod_{i=1}^N P(X_i|\\mu) = \\prod_{i=1}^N \\mu^{X_i} (1-\\mu)^{1-X_i} = \\mu^{N_h} (1-\\mu)^{N-N_h} $$ where $N_h = \\sum_i X_i$, i.e number of heads.\nBeta The next step is to choose the prior $P(\\Theta)$. In the case of Bernoulli, it would be better if we can find a function that has a simliar exponential parts that appeared in the above likelihood function to model the probability of every single value of $\\mu$. Luckily, Beta distribution shown below is the right function we are looking for.\n$$ P(\\mu) = Beta(\\mu|a, b) = \\frac{\\mu^{a-1} (1-\\mu)^{b-1}}{B(a, b)} $$\nwhere $B(a, b)$ is a normalisation constant\n$$ B(a, b) = \\int_0^1 \\mu^{a-1} (1-\\mu)^{b-1} d\\mu $$\nSo how to choose a, b? It depends. If we have no idea about $\\mu$, it\u0026rsquo;s natural to assume that there are equal chances to take all vaules of $\\mu$. This corresponds to a beta distribution with $a = b = 1$.\nThe last step is to plug the prior and likelihood into the Bayes' rules,\n$$ P(\\mu|D) = \\frac{P(D|\\mu)P(\\mu)}{P(D)} = \\frac{\\mu^{N_h} (1-\\mu)^{N-N_h} \\mu^{a-1} (1-\\mu)^{b-1}}{P(D) B(a, b)} = \\frac{\\mu^{N_h+a-1} (1-\\mu)^{N+b-N_h-1}}{P(D) B(a, b)} $$\nand\n$$ P(D) = \\int_0^1 \\frac{\\mu^{N_h+a-1} (1-\\mu)^{N+b-N_h-1}} {B(a, b)} d\\mu = \\frac{B(N_h +a, N+b-N_h)}{B(a, b)} $$\nSo we have\n$$ P(\\mu|D) = Beta(\\mu| N_h + a, N+b-N_h) $$\nIn summary, before we see data, we have some beliefs about $\\mu$ governed by $Beta(\\mu|a, b)$. After seeing the data, the probability of $\\mu$ now is updated via $Beta(\\mu| N_h + a, N+b-N_h)$, which can be served as the prior for the next new observations.\nIncremental Updating For independent data we can update the hyperparamters incrementally, we consider an individual data at a time so that,\n$$ P(\\mu|X_1) = \\frac{P(X_1|\\mu)P(\\mu)}{P(X_1)} $$\n$$ P(\\mu|X_2, X_1) = \\frac{P(X_2, X_1|\\mu)P(\\mu)}{P(X_2, X_1)} = \\frac{P(X_2|\\mu)P(X_1|\\mu)P(\\mu)}{P(X_2)P(X_1)} = \\frac{P(X_2|\\mu)P(\\mu|X_1)}{P(X_2)} $$\nIt\u0026rsquo;s clear that the previous posterior now becomes the prior for the next piece of data.\nPoisson Distribution Poisson distribution measures the probability of a given number of events occuring in a specific time range, which is given by,\n$$ Pois(N;\\theta) = \\frac{e^{-\\theta}\\theta^N}{N!} $$\nwhere $N$ is the number of occurences in a time slot and $\\theta$ is the parameter of interest. For example, we want to know the rate of traffic along a road between 8am and 9am, so $N$ is the number of cars and $\\mu$ is the rate of traffic per hour.\nGamma Then we have Gamma distribution as our prior,\n$$ P(\\theta) = \\Gamma(\\theta|a, b) = \\frac{b^a \\theta^{a-1} e^{-b\\theta}}{\\Gamma(a)} $$\nso the posterior after seeing the first piece of data is\n$$ P(\\theta|N_1) = \\frac{P(N_1|\\theta)P(\\theta)}{P(N_1)} = \\frac{b^a}{\\Gamma(a) N_1! P(N_1) }e^{-(b+1)\\theta} \\theta^{N+a-1} \\propto e^{-(b+1)\\theta} \\theta^{N_1+a-1} $$\nwe can see that the posterior is also a Gamma distribution with $a_1 = a + N_1$ and $b_1 = b + 1$.\nMultinominal Distribution In the case of Bernoulli, we only consider two outcomes: 0 or 1. But what if we have 3 or more outcomes? Well, we use multinominal distribution, which is the generalization of the binominal distribution. Suppose we have a $k$-sided dice with the probability of $\\mu_k$ for $x^k = 1$, and we roll the dice $N$ times, so there are $N$ independent observations $x_1, x_2, \u0026hellip;, x_n$, the multinominal distribution are given by,\n$$ M(m_1, m_2, \u0026hellip;, m_k|\\mu, N) = \\frac{N!}{m_1!m_2!\u0026hellip;m_k!} \\prod_{k=1}^K \\mu_k^{m_k} $$\nwhere $m_k=\\sum_n^Nx_n^k$ represents the number of $x_n^k = 1$.\nDirichlet TODO\nDiscriminal vs Generative Models Take the problem mentioned in the introduction as an example, suppose we have 2 classes, \u0026lsquo;cat\u0026rsquo; and \u0026lsquo;dog\u0026rsquo;, and we want to know which class the new image belong to. The problem is to find the probability $P(C=cat|x)$ and $P(C=dog|x)$, and then we classify the image into the class with the largest probability.\nUsually, we think of our observations as given and the predictions as random variables, and we model the target variable as a function of the predictors. This is known as discriminal model. In this example, we could use logistic regression to make a classification, and the corresponding probability can be computed using a sigmoid function\n$$ P(C=cat|X)=\\frac{1}{1 + e^{-(wx + b)}} $$\nHowever, we can also consider both features and target variables at the same time. Using the Bayes' rule below, we can calculate $P(Y|X)$ in another way,\n$$ P(C=cat|X) \\propto P(X, C=cat)= P(X|C=cat)P(C=cat) $$\nThis is known as generative model, where we use Bayes' rule to turn $P(X|Y)$ into $P(Y|X)$.\nIn discriminal model, we don\u0026rsquo;t need the prior of classes, so there are less parameters to be determined. Also, it might have a better performance if the estimate for that prior is far away from the true distribution.\nGraphical Models Given 2 random variables, $X$ and $Y$, they could be dependent directly or not. Even if they are not dependent directly, typically there is still correlation between them. If they are correlated, then\n X could affect Y Y could affect X X has no direct effect on Y, but they could be both affected by another random variable Z  We can describe the above relationships using a graph, where each node represents a random variable and the links between nodes show that relationship. The above three relationships are captured by the following figure,\n  Figure 1: Directed graphical models representing the above three conditions   Such a graph is known as Bayesian networks where we use a directed graph to show causal relationships between random variables. Specifically, we add directed links between $X$ and $Y$ if $X$ directly influences $Y$ shown in the left graph of Figure 1.\nConditional Independence The left and middle graphs of Figure 1 are easy to understand since they only have two variables, and they are denoted by $P(Y|X), P(X|Y)$ respectively. However, the last one with three variables, $X, Y, Z$, is a bit tricky. Let\u0026rsquo;s first consider the conditional distribution of $X$ given Z and Y, we have\n$$ P(X|Z, Y) = P(X|Z) $$\nIf we further consider the joint distribution of X and Y conditioned on Z, then we have\n$$ P(X, Y|Z) = \\frac{P(X, Y, Z)}{P(Z)} = \\frac{P(Z) P(Y|Z)P(X|Z, Y)}{P(Z)} = P(X|Z)P(Y|Z) $$\nThis is called conditional independence, which means that X and Y are statistically independent, given Z. From the view of a graphical model, we can see that there is no direct link between X and Y shown in the right graph of Figure 1.\nLatent Dirichlet Allocation Now let\u0026rsquo;s learn a topic modeling method that utilises the knowledge we\u0026rsquo;ve talked about so far, Latent Dirichlet Allocation(LDA). Note this is not linear discriminant analysis, which is also abbrivated to LDA. Here, LDA is an unsupervised learning method that is used to model topics within a set of documents. Speaking of \u0026lsquo;topic\u0026rsquo;, it means that when you find a group of words occuring many times in an article, such as \u0026lsquo;banana, apple, fruit, vegetable\u0026rsquo;, you will relate them to an area, like \u0026lsquo;Food\u0026rsquo; in this case.\nLatent In LDA, documents are represented as a fixed group of topics, which are unknown as latent variables. And these topics are characterized by a small specific set of words. For example, words like \u0026lsquo;teacher, student, school, exam, marks\u0026rsquo; should occur more frequently in the area of Education than topics like Sports. This means different topics have different word distribution, as shown in Figure 2.\n  Figure 2: Word distribution varies in different topics  If a document contains more words like \u0026lsquo;teacher, school\u0026rsquo;, it\u0026rsquo;s likely to be identified as \u0026lsquo;Education\u0026rsquo;. But it could contain other topics. For example, if this is a document regarding a sports contest held in a school, then it\u0026rsquo;s much likely to be associated with \u0026lsquo;Sports\u0026rsquo; rather than \u0026lsquo;Education\u0026rsquo;. Thus, we can see that a document can also contain different topics with different weights, i.e. each document has its own topic distribution.\nFrom above, we know that a document consists of a group of topics, and each topic has its special words. To generate a document, we can randomly choose N topics for N words in a document and then randomly choose a word from the corresponding topic. Of course, such an article contains little meaning since we ignore the order and semantics of words. But it\u0026rsquo;s reasonable for LDA since LDA treats documents just as a bag of words(BOW).\nDirichlet So the topic-word distribution and doc-topic distribution are the unknown parameters we want to find, but there are many possible values for them. Again, we need a right prior to describe this uncertainty of the values, but which prior should we use?\nRemember that we draw a topic from $K$ topics for each word in a document with $N$ words, it means there are $K$ possbile outcomes available for each word, and we repeat this process $N$ times, so it\u0026rsquo;s a multinominal distribution. And this is true for drawing a word from that topic with $V$ words. Furthermore, we\u0026rsquo;ve known that the cojugate prior of the multinominal distribution is Dirichlet distribution, so Dirichlet distribution is chosen as our prior, where\n doc-topic distribution follows $\\theta^d \\sim Dir(\\alpha)$ topic-word distribution follows $\\phi^t \\sim Dir(\\beta)$  Allocation To sum up, the whole process of generating a document in LDA is illustrated in Figure 3,\n  Figure 3: Graphical model for LDA  The figure looks a bit scary, well, let\u0026rsquo;s start with some notations first,\n $M=|D|$, the number of documents  $D={d_1, d_2, \u0026hellip;, d_M}$, where $d_i$ represents $i_{th}$ document   $V=|W|$, the number of vocabulary appeared in all documents  $W={w_1, w_2, \u0026hellip;, w_V}$, where $w_i$ represents $i_{th}$ word   $K = |T|$, the number of topics  $T={t_1, t_2, \u0026hellip;, t_K}$, where $t_k$ represents $k_{th}$ topic   $N_{d}$, the number of words in a document $d$ $\\bold w^{d}={w_1^{d}, w_2^{d}, \u0026hellip;, w_{N_{d}}^{d}}$, where $w_i^{d}$ represents $i_{th}$ word in a document $d$ $\\theta^d$ is a probability vector, which represents the distribution of topics in a document $d$  $\\Theta = (\\theta^d|d \\in D)$   $\\phi^t$ is a probability vector, which represents the distribution of words associated with a topic $t$  $\\Phi = (\\phi^t|t \\in T)$   $\\bold t^{d} = t_1^{d}, t_2^{d}, \u0026hellip;, t_{N_{d}}^{d}$, where $t_i^{d}$ represents a topic drawn from $\\theta^d$  From Figure 3, the joint distribution of the hidden and observed variables for a single document is given by,\n$$ p(\\bold t^d, \\bold w^d, \\theta^d, \\Phi|\\alpha, \\beta) = P(\\theta^d|\\alpha) P(\\Phi|\\beta) P(\\bold t^d, \\bold w^d|\\theta^d, \\Phi) = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P( t_n^d, w_n^d|\\theta^d, \\Phi) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d |\\theta^d, \\Phi) P(w_n^d|t_n^d, \\theta^d, \\Phi) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\n$$ = P(\\theta^d|\\alpha) \\prod_t^TP(\\phi^t|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\nOur goal is to maximise this equation, but before that we need to eliminate the hidden variable $\\bold t^d$,\n$$ p(\\bold w^d, \\theta^d, \\Phi|\\alpha, \\beta) = \\int_{t^d} P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} \\sum_{t_n^d=t_1}^{t_K} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\nThere are two common ways to compute this: Gibbs sampling and variational inference.\nGibbs Sampling TODO\nVariational inference TODO\nReferences  http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/  "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/semanticweb/",
                "title": "Semantic Web",
                "section": "post",
                "date" : "2021.05.08",
                "body": "In semester 2, I took a module called Semantic Web Technologies. There are some reasons why I choose this module. At first glance, the name itself sounds not appealing. Actually, it does. But it\u0026rsquo;s still on my shortlist because I\u0026rsquo;ve been working on the web for many years and I wondered what the semantic web was.\nOn the other hand, I was not interested in modules like Advanced Databases since I\u0026rsquo;ve learnt database during my undergraduate degree in Computer Science. Besides, I hope I could have more free time to do other things, and I don\u0026rsquo;t think I can deal with a more difficult module like Reinforcement Learning though it\u0026rsquo;s indeed useful and interesting. Maybe I could learn it later when needed.\nOkay, let\u0026rsquo;s go back to this post. The goal of this post is to go through the most important parts of this module for the final exam preparation.\nWeb for machine We all know that the current web pages are written in HTML. You can use \u0026lt;p\u0026gt; to define a paragraph like this.\n\u0026lt;p\u0026gt;Hello world\u0026lt;/p\u0026gt; HTML looks more like a natural language as it is designed to be readable and understandable for human, not machines. However, we hope machines can understand information as well. But why? Because we want machines to do reasoning about data rather than save data only.\nWeb pages can be linked using the markup \u0026lt;a\u0026gt;, but not all relevant documents are connected. Anyone can publish a new web page anytime. Moreover, not everyone uses the same language to describe the same thing. Thus, the interconnected web pages are simply a limited bundle of documents. More importantly, the data are not connected, and we don\u0026rsquo;t have explicit schemas that enable machines to understand documents.\nOn the contrary, the goal of Semantic Web is to make data readable and understandable for machines. To achieve this, the first step is to encourage people to share their data. Then things are denoted by their unique identifiers. Thus, data from various contexts can be connected through the identifiers, making a huge net finally (also known as Linked Data). Therefore, in Semantic Web, machines learn from data directly instead of documents that are written for people.\nWhat are the applications of Semantic web? Well, we can utilise the vocabulary of a domain to build a knowledge graph, which encodes the semantics of domain knowledge in this network. Search engines can augment their search results from such knowledge graphs since they provide domain-specific knowledge.\nRDF RDF is short for the Resource Description Framework. It\u0026rsquo;s a triple-based data model for knowledge representation. Figure 1 shows a triple composed of 3 components, which means Bob has a friend named Alice.\n  Bob is the subject\n  Alice is the object\n  hasFriend is the predicate that connects them\n    Figure 1: A triple-based data model  The above model is an abstract framework. Subjects and predicates can be anything. To describe the specific subjects and the relationship between them, we need to find a way to identify them explicitly. We also need a data format that defines and parses this model.\nURI In the World Wide Web, URI is used to define a unique object. Here are some examples,\nhttps://www.example.com mailto:aaa@xxx.com urn:isbn:123456 Well, URI can also be followed by an optional fragment identifier like this,\nhttps://www.example.com/index.html#introduction Namespace The above method has one problem. A term may have different meanings in different domains. If we include them all in a file, it will cause name conflicts. We can solve it using namespace. Specifically, a resource can be represented by a namespace, a colon and a local name.\n\u0026lt;http://example.org/ontology#hasFriend\u0026gt; // is equivalent to @prefix d: \u0026lt;http://example.org/ontology#\u0026gt; . d:hasFriend Turtle The last thing is to define a data structure so that machines can parse and serialize. One of the popular data formats is Turtle. The following are some syntaxes defined by Turtle,\n Resource URI are written in angle brackets Literal values are written in double quotes Triples end with a full stop  \u0026lt;http://example.org/data#Bob\u0026gt; \u0026lt;http://example.org/ontology#hasFriend\u0026gt; \u0026lt;http://example.org/data#Alice\u0026gt; . RDF/XML Apart from Turtle, we can also express RDF using RDF/XML, which is similar to XML. But it\u0026rsquo;s not easy for humans to read compared to Turtle.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;rdf:RDF xmlns:rdf=\u0026#34;http://www.w3.org/1999/02/22-rdf-syntax-ns#\u0026#34; xmlns:ns0=\u0026#34;http://example.org/ontology#\u0026#34;\u0026gt; \u0026lt;rdf:Description rdf:about=\u0026#34;http://example.org/data#Bob\u0026#34;\u0026gt; \u0026lt;ns0:hasFriend rdf:resource=\u0026#34;http://example.org/data#Alice\u0026#34;/\u0026gt; \u0026lt;/rdf:Description\u0026gt; \u0026lt;/rdf:RDF\u0026gt; Tools The following are some useful tools for understanding RDF.\n EasyRDF Converter  Tool for converting between different RDF syntaxes   W3C RDF Validator  Validator for RDF/XML, which can also visualise RDF as a graph    Generally, we are required to know how to read them and write some simple statements in this module.\nRDFS RDF only tells us some specific instances. For example, Bob knows Alice, Bob works for Google, etc. However, they don\u0026rsquo;t define the vocabulary used in those triples. From the perspective of object-oriented programming, we want to define the classes that generate these instances. Strictly speaking, we want to define the ontology of a domain. Moreover, if we define some rules in this domain, say Person worksFor Company, then we can infer that Bob is a person and Google is a company under the defined rule.\nRDFS and OWL are two common ontology languages to do this. We will talk RDFS first in this section. RDFS allows us to define classes and properties as well as the characteristics of classes and properties.\nclass definition In OOP, we use keyword class to declare a class, for example,\nclass Person {} In RDFS, we use predefined classes, such as rdfs:Class, rdfs:Property, to declare the corresponding object.\nex:Person rdf:type rdfs:Class .   Figure 2: Declare a class  Furthermore, we use rdfs:subClassOf to declare that a class is a subclass of another class. It\u0026rsquo;s clear that rdfs:subClassOf is transitive. For example, if Teacher is a subclass of UniStaff and UniStaff is a subclass of Person , then Teacher is a subclass of Person, as shown in Figure 3.\nex:Teacher rdf:type rdfs:Class ; rdf:subClassOf ex:UniStaff . ex:UniStaff rdf:type rdfs:Class ; rdf:subClassOf ex:Person .   Figure 3: RDFS class semantics   On the other hand, rdf:type distributes over rdf:subClassOf. This means that if C rdf:type A and A rdfs:subClassOf B , then C rdf:type B. For instance, Figure 4 illustrates that if Bob is an instance of Teacher, then Bob is also an instance of Unistaff and Person.\n  Figure 4: rdf:type distributes over rdfs:subClassOf  property definition Property declaration is similar to class declaration. The following code declares a property named ex:hasStreet.\nex:hasStreet rdf:type rdfs:Property . But not every object has an addresss. Besides, we sometimes want to limit the domain and range of a property. In this case, we would say, only people can have an address.\nex:hasStreet rdf:type rdfs:Property ; rdf:domain ex:Person . We also notice that ex:hasStreet is a more specific attribute of ex:hasAddress since addresses are usually composed of several components like towns and streets. So we define ex:hasStreet as a subproperty of ex:hasAddress.\nex:hasStreet rdf:type rdfs:Property ; rdf:domain ex:Person ; rdf:subPropertyOf ex:hasAddress . Figure 5 illustrates that if Bob lives in a street named West Road, it implies that West Road is Bob\u0026rsquo;s address. Furthermore, we have already stated that only Person can have street names, which means Bob must be a person under this semantics.\n  Figure 5: RDF Schema property semantics   SPARQL So far, we are able to represent data. But how do we retrieve it from databases? Well, SPARQL is a SQL-like query language that allows us to make a query.\nbasic syntax SELECT \u0026lt;variable\u0026gt; FROM \u0026lt;graph\u0026gt; WHERE { \u0026lt;triple patterns\u0026gt; }  variables are prefixed with ? triple patterns are expressed in Turtle  For example, the following codes return the names of the people who have street addresses.\nSELECT ?name WHERE { ?name ex:hasStreet ?street . } FILTER We can also query the names of the people who live on the streets only containing \u0026lsquo;Baker\u0026rsquo;.\nSELECT ?name WHERE { ?person ex:hasStreet ?street . FILTER regex(?street, \u0026#34;Baker\u0026#34;) ?person ex:hasName ?name . } Another example is to filter all the items whose prices are lower than 5 pounds.\nSELECT ?name WHERE { ?item ex:hasPrice ?price . FILTER (?price \u0026gt;= 5) ?item ex:hasName ?name . } UNION UNION allows us to either match this condition or that one, which is equivalent to the operation of OR\nSELECT ?name WHERE { { ?person ex:hasStreet \u0026#34;Baker Street\u0026#34; . } UNION { ?person ex:hasStreet \u0026#34;West Road\u0026#34; . } ?person ex:hasName ?name . } We won\u0026rsquo;t dive into SPARQL deeper because SPARQL has quite similar syntaxes to SQL. If you are familiar with SQL, you\u0026rsquo;ll know how to use SPARQL instantly.\nDescription Logic (DL) TODO\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/svd/",
                "title": "Singular Value Decomposition",
                "section": "post",
                "date" : "2021.05.04",
                "body": "Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into the multiplication of three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post.\nChange of Basis Suppose there is a point in the 2D space, how do you describe it? The common way is to use the Cartesian coordinate system, which is composed of two fixed perpendicular oriented axes, measured in the same unit of length. The two perpendicular axes are just a special set of vectors served as the basis of the 2D space. Actually, there are many other sets of vectors that can be the basis for the 2D space. For example, in Figure 1, the position of the red point is (-4, 1) when using the standard basis (1, 0), (0, 1) (colored in grey). If we change the basis to (2, 1), (-1, 1) (colored in blue), the position is (-1, 2).\n  Figure 1: The same point with different coordinates in two different coordinate spaces  From Figure 1, we can see that the absolute position of the red point always stay the same. However, the relative positions to the different bases are different. Mathematically, the red point can be described from the perspective of basis as follows,\n$$ x = P_b[x]_b = c_1 \\bold b_1 + c_2 \\bold b_2 + \u0026hellip; + c_n \\bold b_n $$\n$$ P_b = [\\bold b_1, \\bold b_2, \u0026hellip; , \\bold b_n ] $$\n$$ [x]_b = [c_1, c_2, \u0026hellip; c_n] $$\nwhere\n $[x]_b$ is a set of scalars, which represent the length of projection onto each axis of the current coordinate system $P_b$ is the corresponding basis of the current coordinate system  Let\u0026rsquo;s plug the above point and the basis (1, 0), (0, 1) (colored in grey) into the equation,\n$$ P_b = [ (1, 0), (0, 1)] $$\n$$ [x]_b = (-4, 1) $$\n$$ x_b = -4 \\begin{bmatrix}1\\\\ 0 \\end{bmatrix} + 1 \\begin{bmatrix}0\\\\ 1 \\end{bmatrix} = \\begin{bmatrix}-4\\\\ 1 \\end{bmatrix} $$\nLet\u0026rsquo;s do the same calculation with another basis (colored in blue),\n$$ P_b = [ (2, 1), (-1, 1)] $$\n$$ [x]_b = (-1, 2) $$\n$$ x_b = -1 \\begin{bmatrix}2\\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix}-1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix}-4\\\\ 1 \\end{bmatrix} $$\nAs expected, they yield the same result. And the second one essentially changes the basis of $R^2$ from (2,1),(-1,1) to(1,0),(0,1), which is the standard basis of $R^2$.\nActually, this example is a special case of the change of basis, where the new basis is the standard basis. More generally, $P_{c \\larr b}$ is known as **the change of coordinate matrix from the old basis $b$ to the new basis $c$, which we are going to switch to** in $R^n$.\nSay we are in the basis $b$ and $[x]_b$ is known, the corresponding coodinates of $x$ under the new basis $c$ can be computed as follows,\n$$ x_c = P_{c \\larr b} x_b $$\nSince $P_{c \\larr b}$ is invertible, we have\n$$ (P_{c \\larr b})^{-1}x_c = x_b $$\nwhich is the inverse operation of change of basis from $b$ to $c$. We can generalize this to any number of points and dimensions.\n$$ A=US\\\\(D,N)= (D, M) \\times (M, N) $$\n$$ U^{-1} A = S\\\\(M, D) \\times (D, N) = (M, N) $$\nwhere\n $A$ is a $D\\times N$ matrix with $D$ dimensions and $N$ points $S$ is a $M\\times N$ matrix with $M$ dimensions and $N$ points described in a new vector space decided by another basis $U$ is the change of coordinate matrix from $S$ to $A$ $U^{-1}$ is the the change of coordinate matrix from $A$ to $S$  If we do some transformation for a point in the standard coordinate system, what\u0026rsquo;re the new coordinates of the same point in another system? This problem can be solved by the following equation,\n$$ x_s' = U^{-1}TUx_s $$\nwhere $T$ represents the transformation matrix. If $T=I$, $x_s'$ is exactly $x_s$.\nSVD SVD is a technique in linear algebra that can be used to decompose any $N \\times P$ matrix\n$$ X = U S V^T $$\n $U$ is an $N \\times N$ orthogonal matrix, where the columns of $U$ are the eigenvectors of $XX^T$ $S$ is an $N \\times P$ diagonal matrix whose diagonal entries are the sorted singluar values, which are square roots of eigenvalues of $XX^T$ or $X^TX$ $V$ is a $P \\times P$ orthogonal matrix, where the columns of $V$ are the eigenvectors of $X^TX$  $$ C = X^TX = (USV^T )^T USV^T = VS^TU^T USV^T = VSS^TV^T $$\n$$ D = XX^T = USV^T (USV^T )^T = USS^TU^T $$\n$$ =\u0026gt; C [\\bold v_1, \\bold v_2, \u0026hellip;, \\bold v_p] = [\\lambda_1 \\bold v_1, \\lambda_2\\bold v_2, \u0026hellip;, \\lambda_p \\bold v_p] $$\n$$ =\u0026gt; D [\\bold u_1, \\bold u_2, \u0026hellip;, \\bold u_n] = [\\lambda_1 \\bold u_1, \\lambda_2\\bold u_2, \u0026hellip;, \\lambda_n \\bold u_n] $$\nTherefore, $V$ and $U$ are matrices of eigenvectors for $X^TX$ and $XX^T$.\nIf we look at the formula of SVD from the view of \u0026lsquo;the change of basis\u0026rsquo; discussed above, we will find that\n $V^T$ is the change of matrix from, say basis A, to the standard basis $S$ is a scaling matrix $U$ is another change of matrix from the standard basis to basis A  Geometrally, the multiplication between a matrix $A$ and a vector $x$ represents a linear transformation, where we using another basis of $R^n$ to represent the same point and $A$ is the change of matrix between bases. By decomposing $A$, we can clearly see that this transformation is composed of three transformations:\n $ V^T$: rotation $S$: scaling $U$: rotation  Economical Forms of SVD Since $S$ is an $r \\times r$ diagonal matrix for some $r$ not exceeding the smaller of $N$ and $P$, we could simplify $S$ and the correspoding $V$ and $U$. Specifically, for $X$ with more samples than features $N \u0026gt; P$ (tall and thin), we ignore the last $N - P$ columns of U\n and for $X$ with more features than examples $N \u0026lt; P$ (short and fat), we ignore the last $P - N$ rows of $V^T$\n Linear Regression Revisited In the previous post Linear Regression 02, we introduced pseudo-inverse $A^+$\n$$ \\bold {\\hat w} = A^+ \\bold y = (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y = V (S^TS)^{-1}S^TU^T \\bold y = VS^+U^T \\bold y $$\nwhere the elements of $S^+$ are the reciprocal of the singular values,\n$$ S^+ = (S^TS)^{-1}S^T = \\begin{bmatrix}s_1^{-1}\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ 0\u0026amp;s_2^{-1} \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ \u0026hellip; \\\\ 0\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; s_p^{-1} \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\end{bmatrix} $$\nThis means if any of the singular values of $X$ are small, then $S^{-1}$ will magnify component in that direction. Thus, little change in $\\bold y$ will lead to a greatly different model and eventually a poor generalization.\nOne way to tackle this is regularisation. Ridge Regression is one variant of linear regression by adding a regulariser $\\lambda ||\\bold w||^2$ to the loss function,\n$$ L = ||\\bold X \\bold w - \\bold y||^2 + \\lambda ||\\bold w||^2 $$\nThe estimate $\\bold w$ is given by\n$$ \\bold {\\hat w} = V(S^TS + \\lambda I)^{-1} S^TU^T \\bold y $$\nwhere\n$$ (S^TS + \\lambda I)^{-1} S^T = \\begin{bmatrix}\\frac{s_1}{s_1^2+\\lambda}\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ 0\u0026amp;\\frac{s_2}{s_2^2+\\lambda} \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ \u0026hellip; \\\\ 0\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; \\frac{s_p}{s_p^2+\\lambda} \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\end{bmatrix} $$\n if $s_i = 0$, then $\\frac{s_i}{s_i^2 + \\lambda} =0$ and the \u0026lsquo;inverse\u0026rsquo; is defined if $s_i \u0026laquo; \\lambda$, then $\\frac{s_i}{s_i^2 + \\lambda} \\simeq \\frac{1}{\\lambda }$ if $si \u0026raquo; \\lambda$, then $\\frac{s_i}{s_i^2 + \\lambda}\\simeq s_i^{-1}$  Therefore, adding a regulariser can make our model much more stable.\nPCA Principal component analysis(PCA) is often used to reduce dimentionality. The idea of PCA is to find directions along which data has the largest variation. The variation can be computed by projecting data onto that direction. Mathematically, we want to find a vector $v$ with $||v||=1$ to maximise\n$$ \\sigma^2 = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 $$\nThere are two things to notice here:\n $v$ is an unit vector since we are care about the direction only data are centralized first for simple computation; centralizing data doesn\u0026rsquo;t change the distribution of data  We can solve the above equation by introducing Lagrange multiplier\n$$ L = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 - \\lambda (||v||^2 - 1) $$\n$$ = \\frac{1}{n-1}\\sum_i^n \\bold v^T (\\bold x_i - \\bold \\mu)(\\bold x_i - \\bold \\mu)^T\\bold v - \\lambda (||v||^2 - 1) $$\n$$ = \\bold v^T \\bold C \\bold v - \\lambda (\\bold v^T\\bold v - 1) $$\nwhere $\\bold C$ is the covariance matrix of $X$. Then we take the derivative of $L$ w.r.t $\\bold v$ , and then set it to $0$\n$$ \\frac{\\partial L}{\\partial \\bold v} = 2(C \\bold v - \\lambda \\bold v) = 0 $$\nso $\\bold v$ is the eigenvector of $\\bold C$, and the variance along this direction is,\n$$ \\sigma^2 = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 = \\bold v^T \\bold C \\bold v = \\lambda \\bold v^T \\bold v = \\lambda $$\nProperties of Covariance Matrix We know that the quadratic form of a vector and a matrix is defined as\n$$ x^T M x $$\nThus, the quadratic form of $C$ is\n$$ x^TCx = x^T XX^Tx=u^Tu \\ge 0 $$\nso $C$ is positive semi-definite, which means all eigenvalues of $C$ are greater than or equal to zero. Why? Suppose $\\mu$ is an eigenvector of $C$, since $\\mu^TC\\mu\\ge0$ and $||\\mu||\u0026gt;0$, then we have\n$$ \\mu^TC\\mu = \\mu^T \\lambda \\mu =\u0026gt; \\lambda = \\frac{\\mu^TC\\mu }{||\\mu||^2 } \\ge 0 $$\nzero eigenvalue What if $C$ has a zero eigenvalue? Well, a zero eigenvalue means that there is no variation in the direction of the corresponding eigenvector. So when will we have zero eigenvalues? Based on the definition of eigenvalue, we have\n$$ Cx = 0x = 0 $$\nSince $x$ is nonzero vector, $C$ is singular or non-invertible. And this will inevitably happen if the number of features is much more than the number of examples. Conversely, if $C$ is invertible, it has no zero eigenvalues and is said to be positive definite (since all eigenvalues are greater than 0).\nGeometry of PCA Since $C$ is a $p \\times p$ symmetric matrix, it can be orthogonally diagonalized as $C = PDP^T$, where $P$ is an orthogonal matrix and $D$ is a diagonal matrix. Thus, the above formula can also be written as follows,\n$$ \\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 = \\sum_i^n \\bold v^T (\\bold x_i - \\bold \\mu)(\\bold x_i - \\bold \\mu)^T\\bold v = \\bold v^T C \\bold v $$\n$$ = \\bold v^T PDP^T \\bold v = \\bold y^T D \\bold y $$\nwhere $\\bold y = P^T \\bold v$. Mathematically, $\\bold v^T C \\bold v$ and $\\bold y^T D \\bold y$ yield the same result, which represents the sum of deviation of each sample from the original point along the direction determined by this vector as shown in Figure 2.\n  Figure 2: Change of variable in $x^T A x$ (Introduction to Linear Algebra[1])  Geometrically, $P^T_{y \\larr v}$ is the change coordinate matrix from $v$ to $y$, which finally transform the shape of the quadratic form $\\bold v^TC\\bold v$ into the standard position, such as the shapes in the figure below.\n  Figure 3: An ellipse and a hyperbola in standard position (Introduction to Linear Algebra[1])  Projection For a data set with $p$ features, we want to reduce its dimension to $k$, there are a few steps to follow\n  Construct a $p\\times p$ covariance matrix $C$ using centralized data\n  Find all the eigenvectors $v_i$ and eigenvalues $\\lambda_i$ of $C$\n  Construct a $p \\times k$ projection matrix $P$ with $k$ eigenvectors determined by the $k$ largest eigenvalues (principal components)\n  Project the original data into the space spanned by the principal components\n  $$ \\bold z = P^T(\\bold x - \\bold \\mu) $$\nwhere $\\bold z$ is our new inputs.\nUsually, there are two common ways to find the eigenvalues:\n eigendecomposition SVD  Eigen-decomposition follows the above steps, and we can see it from the following code,\ndef pca(X): # Data matrix X, assumes 0-centered P, N = X.shape # Compute covariance matrix, where X is a P by N matrix C = np.dot(X, X.T) / (N-1) # Eigen decomposition eigen_vals, eigen_vecs = np.linalg.eig(C) # Project X onto eigen space X_pca = np.dot(eigen_vecs.T, X) return X_pca, eigen_vals, eigen_vecs Instead, SVD decomposes $X$ directly. Besides, we don\u0026rsquo;t need to centralize data first in SVD, though most people will do.\ndef svd(X): N, P = X.shape # In practice, we usually subtract the mean from data and then perform SVD X_c = X - np.mean(X, axis=0) # the columns of Vt are the eigenvalues of X^TX U, Sigma, Vt = np.linalg.svd(X_c) # Project X onto eigen space X_pca = np.dot(X, Vt.T) return U, Sigma, Vt So why do we use SVD? Simply put, If the number of features are much more than the number of example, i.e. $P\u0026raquo;N$, it\u0026rsquo;s not easy to compute eigenvalues using eigen-decomposition. But in SVD, the algorithm will compute eigenvalues by choosing the smaller of two matrice $X^TX$ and $XX^T$. In this case, $X^TX$ has less elements( $N * N$ ) than $XX^T$($ P * P$). More details can be found in the following section \u0026lsquo;PCA for image\u0026rsquo;.\nReconstruction From the perspective of projection, the projection onto a vector $\\bold v_j$ can be seen as approximating the inputs by\n$$ \\hat {\\bold x_i} = \\bold \\mu + \\sum_j^k z_j^i \\bold v_j $$\n$$ z_j^i = \\bold v_j^T(\\bold x_i - \\bold \\mu) $$\nSo our goal is to minimize the error\n$$ E[ ||\\hat {\\bold x_i} - \\bold x_i ||^2] = \\frac{1}{n} \\sum_{n=1}^n ||\\sum_{i=1}^k \\bold u_i^T\\bold x_n \\bold u_i + \\sum_{i=k+1}^p \\bold u_i^T\\bold x_n \\bold u_i - \\sum_{j=1}^k \\bold v_j^T\\bold x_n \\bold v_j = \\frac{1}{n} \\sum_{n=1}^n \\sum_{i=k+1}^p ||\\bold u_i^T\\bold x_n \\bold u_i||^2 $$\n$$ = \\frac{1}{n}\\sum_{n=1}^n\\sum_{i=k+1}^p (\\bold u_i^T\\bold x_n) \\bold u_i^T \\cdot (\\bold u_i^T\\bold x_n) \\bold u_i =\\frac{1}{n} \\sum_{n=1}^n\\sum_{i=k+1}^p (\\bold u_i^T\\bold x_n)^2 $$\n$$ = \\frac{1}{n} \\sum_{n=1}^n\\sum_{i=k+1}^p \\bold u_i^T\\bold x_n \\bold x_n^T \\bold u_i= \\sum_{i=k+1}^p \\bold u_i^T (\\frac{1}{n} \\sum_{n=1}^n\\bold x_n \\bold x_n^T )\\bold u_i $$\n$$ \\sum_{i=k+1}^p \\bold u_i^T S \\bold u_i = \\sum_{i=k+1}^p \\lambda_i \\bold u_i^T \\bold u_i = \\sum_{i=k+1}^p \\lambda_i $$\nWe can see that the error is exactly the sum of the eigenvalues in the directions that are discarded.\nPCA for images Suppose we have an image with the size of $256 \\times 256$, so it has nearly $64K$ pixels or features. Then we could create a covariance matrix $C$ with more than $4\\times10^9$ elements using PCA. However, this huge matrix is not easy to compute eigenvalues. To make this problem tractable, we usually work in a dual space instead of a feature space. The dual space we choose is spanned by $n$ vectors, which are exactly the sample images. Specifically, if we have $n$ images, the subpace of $R^n$ has at most $n-1$ dimensions, and usually $n$ is much smaller than $p$. But how do we find the eigenvalues of $C$ in this dual space?\nWe\u0026rsquo;ve known that $C = XX^T$ is a $p\\times p$ matrix, where $X$ is a $p \\times n $ matrix. Now we construct another matrix $D=X^TX$ with $n \\times n$ elements, which is also a symmetric matrix. Suppose $v$ is the eigenvalue of $D$, then we have\n$$ Dv = \\lambda v $$\n$$ XX^TXv = \\lambda Xv $$\n$$ CXv = \\lambda Xv $$\n$$ Cu = \\lambda u $$\nwhere $u = Xv$. We find that $C$ and $D$ has the same eigenvalues. Thus, we can use the dual $n \\times n$ matrix $D$ to find eigenvalues and eigenvectors of $C$.\nFind K Components The last question is how to decide the number of components. Instead of guessing the number of dimensions that we want to keep, we choose the right $k$ components along which the sum of the explained variance ratio is greater than a threshold. The explained variance ratio of each component indicates how much the variance explained along component. In Sklearn, it can be accessed via explained_variance_ratio_.\nfrom sklearn.decomposition import PCA pca = PCA(n_components=2) X2D = pca.fit_transform(X) pca.explained_variance_ratio_ The following code shows how to find $k$ components with a variance ratio of $0.95$ using Sklearn.\nfrom sklearn.decomposition import PCA pca = PCA() pca.fit(X_train) cumsum = np.cumsum(pca.explained_variance_ratio_) d = np.argmax(cumsum\u0026gt;0.95) + 1 # or simply pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_train) pca.n_components_ Also, we can plot the explained variance ratio as a function of the number of dimensions, and the elbow in the curve is where the appropriate $k$ lies.\n  Figure 4: Explained variance as a function of k (Hands-on machine learning, 2019)  References [1]\tG. Strang, Introduction to Linear Algebra, 5th ed. Wellesley, MA: Wellesley-Cambridge Press, 2016.\n[2] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/end2end-project-01/",
                "title": "An E2E Project - EDA",
                "section": "post",
                "date" : "2021.04.25",
                "body": "So far, we have discussed many algorithms, such as linear regression and ensemble methods. It\u0026rsquo;s time to learn how to build a real machine learning project to enhance our understanding of these models.\nOverview The book \u0026ldquo;Hands-on Machine Learning\u0026rdquo; has summarised the below 7 steps as a guidance to do an end-to-end machine learning project,\n Frame the problem Get the data Explore the data Prepapre the data Explore many different models Fine-tune the model Deploy the system  In this post, we will cover the first 3 parts. The complete code can be found here.\nFrame the problem The first step is to define you problem. People are unlikely to do things without reasons, right? So ask yourself, what problem do you want to solve? Why are you interested in them? What\u0026rsquo;s your objective? Are there any solutions out there?\nAs an example, we are going to predict used car prices because accurate prices prediction can help both buyers and sellers. For buyers, it can ensure the money that customers invest on used cars to be worthy. For used car dealers, they might want to know which factors influence car prices most so as to adjust sales strategy and offer a better prediction to customers. Therefore, there is a necessity for building a used car price prediction system.\nGet the data There are many ways to get the desired data. You can write a web crawler to download the data from related websites or you can get data freely from some public data platforms. Among them, Kaggle is one of the most popular data platforms. In this toy project, the data is downloaded from Used Cars Dataset - Kaggle.\n!kaggle datasets download -d austinreese/craigslist-carstrucks-data -p data !unzip data/craigslist-carstrucks-data.zip -d data Explore the data Exploring the data or EDA is the very first and important step when doing a machine learning project. The goal of EDA is to get insights from the data so as to clean and prepare data for building models. Generally, it involves the following steps,\n Identify variables Examine data quality Univariate analysis Bivariate analysis  Identify variables After getting data, we should have a peek at data first. Specifically, we need to answer the following questions,\n  How many observations and variables do we have?\n  What variables are your predictors and target?\n  How about data types and memory usage?\n  What\u0026rsquo;s the descriptive statistics about data?\n  Luckily, Pandas provides convenient functions to answer these questions.\ndf_vehicles.shape df_vehicles.info() df_vehicles.describe() df_vehicles.head() The following tables shows the distribution of data types and some toy data.\n   Data Type Variable Name     Numerical (6) id, year, price, odometer, lat, long   Object (19) url, image_url, region_url, manufacturer, model, condition, cylinders, fuel, title_status, transmission, drive, type, paint_color, region, state, posting_date, description, size, VIN      Table 1: Some example data  Comments:\n There are 426,880 rows and 25 attributes. The variable price is our target variable and the remaining are our predictors. id, url, region_url, image_url, post_date, and VIN have nothing to do with car prices. Thus, we need to remove them. There are numerous categorical variables plus 6 numerical variables.  Examine data quality The raw data is unlikely to use directly because it\u0026rsquo;s inevitable to introduce errors like duplication, null values or extreme values when collecting data. The higher the data quality is, the better model\u0026rsquo;s performance is. So we need to identify and solve these problems to obtain a clean data set. Below are some common techniques to check whether we have redundant data, missing values or outliers.\n1) duplication # return true if we have duplicate rows df.duplicated() # remove the duplicated rows, keep the first row by default df.drop_duplicates() Fortunately, there are no duplicated rows. But it depends.\n2) missing value Heatmap enables us to identify the distribution of missing values visually.\nsns.heatmap(df.isna())   Figure 1: A heatmap for visualising the distribution of null values  Figure 1 shows that size, condition, VIN, cylinders, drive and paint_color have a significant number of null values. To quantify the number of missing values, we can use a table shown below.\n  Table 2: A table that shows the percentage of null values  3) outlier In the previous post Descriptive Statistics, we talked about the descriptive statistics like mean, variance, and range. These values can be obtained easily using the following code,\ndf_vehicles.describe()   Table 3: A table that shows descriptive statistics  From above table, we can see that the lowest car price is 0 while the highest car price is up to 3,600,000,000. The mean of the car prices is greater than the median, which means the distribution of prices is not symmetrical and there are abnormal values in prices. And the same goes for odometer. To verify our belief further, we can plot boxplots for year and odometer shown in Figure 2.\n  Figure 2: Boxplots for year and odometer  Okay, let\u0026rsquo;s put it all together. From the above initial examination, we can conclude that,\n There are 426,880 rows and 25 attributes in our data set, and price is our target variable. A great deal of categorical variables need to be encoded. There are no redundant rows. There are some useless columns to be dropped, such as url, image_url, region_url, id, post_date, and VIN . Nearly all features contain a great deal of missing values. We can remove features directly or find ways to impute them. price and odometer have extreme values.  Univariate analysis In univariate analysis, we will look at variables one by one. The statistics and visualization methods depend on the data types. Typically, we divide data into 2 types: numerical variable and categorical variable.\n1) numerical variables For numerical variables, we measure the central tendency and dispersion of the data, which are discussed earlier. To visualize data, we can use histogram, boxplot, or other suitable charts. You might find that this is also exactly what we did before for detecting missing values and outliers.\n   measurement statistics     central tendency mean, median, mode   dispersion range, quantile, variance, skewness   visualization histogram, boxplot, bar chart    For example, Figure 3 shows the distribution of the variable year. It can be seen that most cars were made after 2000. Besides, this distribution has a long left tail, which means the mean is lower than the median. From Figure 4, we can find that the mean is 2010 while the median is 2013. This is because the feature year contains some extreme small values.\n  Figure 3: The distribution of the year  2) categorical variables For categorical variables, we usually plot bar charts to understand the distribution of each category, as shown in Figure 4.\n  Figure 4: Bar charts for some categorical variables  Bivariate analysis Analyzing one predictor seems a bit monotonous. In fact, we are more interested in the relationships between predictors and our goal. Generally speaking, we can do conduct an analysis\n between numerical and numerical between numerical and categorical between categorical and categorical  1) numerical vs numerical Scatter plots provide a nice way to find the relationships between continuous variables. It\u0026rsquo;s easy to plot them with the help of seaborn. From Figure 5, we can see that there is a negative relationship between odometer and car prices, which means the car prices will decrease as the odometer increases.\n  Figure 5: The relationship between odometer and price  However, scatter plot cannot tell us how strong this relationship is. The solution is to calculate correlation. Figure 6 shows that the car prices have a moderate positive relation with year and a negative relation with odometer. On the contrary, latitude and longitude have a weak relation with car prices.\n  Figure 6: The correlation among continuous variables  2) categorical vs numerical For categorical features, we group data by category and compare them side by side using boxplot or line chart. Basically, we just divide the whole data into several groups and the following analysis is the same as we did before.\n3) categorical vs categorical Conclusion In summary, we introduced some common data analysis techniques to gain insights from your data in this post. Hopefully it can give you an idea of how to explore data. If you\u0026rsquo;d like to explore more excellent analysis methods and visualizations, Kaggle is a great place to enhance your skills.\nAs for the remaining parts, we will talk about them in the following posts.\nReferences [1] https://www.kaggle.com/austinreese/craigslist-carstrucks-data\n[2] https://www.scribbr.com/statistics/statistical-tests/\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/ensemble-methods/",
                "title": "Ensemble Methods",
                "section": "post",
                "date" : "2021.04.20",
                "body": "Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.\nOverview In real life, we often take advices from others. For example, suppose you want to know whether a movie is worthwhile to watch, you may ask your friends who\u0026rsquo;ve watched to give the movie a score(out of 5), say 3, 3, 2, 2, 2, 4. Since 5 people gave a score that is lower than 4, you may think about choosing another movie, let\u0026rsquo;s say Avatar. Then you ask your friends again and have some scores like this 5, 5, 5, 5, 5. Wow! All of your friends think that Avatar is an amazing movie that should certainly not be missed. You agree with their opinons and decide to watch Avatar finally. From this example, we can see that gathering plenty of opinions from different people are likely to make an informed decision.\nHere, I highlight the words different people. It makes little sense if we only asks for people who have the same interests. Therefore, the more diverse the people are, the more sensible our decisions are. Basically, the idea behind it is the wisdom of collaborating.\nVoting The method used to decide whether to watch a movie or not in this example is known as voting. For classification, there are 2 types of voting named hard voting and soft voting.\n Hard voting returns the most popular class shown in Figure 1. Soft voting averages the probability of each class and then return the class that has the maximum probability.  Hard Voting   Figure 1: Hard voting classifier predicitons (Hands-on machine learning, 2019)  Figure 1 can also be illustrated in a mathematical way,\n$$ y' = mode(C_1(x), C_2(x), \u0026hellip;, C_n(x)) $$\nFor example, {0, 1, 0, 1, 1} are the class labels predicted by our 5 different classifiers for a data point $x$. By hard voting, the final class label is class 1 .\nC1 -\u0026gt; 0 C2 -\u0026gt; 1 C3 -\u0026gt; 0 C4 -\u0026gt; 1 C5 -\u0026gt; 1 Weighted Hard Voting Hard voting works nice, but in some cases, some people might be more professional than others. Hence, their opinions are much more significant. How to distinguish professionals and common people?\nWe assign weights to them. Specifically, we assign higher weights to professionals while common people have lower weights. Then we calculate weighted sum of occurrence of each class label and find the class label that has the maximum value.\n$$ y' = \\operatorname*{argmax}_i w_j\\sum_j [C_j == i] $$\nwhere $[C_j == i] = 1$ if classifier $j$ predicts class label i and 0 otherwise.\nFor example, if we assign the following weights to the previous 5 classifiers, then we will have 0.7 for class 0 and0.5 for class 1. Thus, class 0 wins because 0.7 is greater than 0.5.\n0.4, C1 -\u0026gt; 0 0.1, C2 -\u0026gt; 1 0.3, C3 -\u0026gt; 0 0.2, C4 -\u0026gt; 1 0.2, C5 -\u0026gt; 1 Soft Voting Instead of predicting the class label directly, some classifiers like logistic regression can predict the probability of each class label that $x$ belongs to. Then we simply average these probabilities for each class label. Certainly, you can assign weights to classifiers.\n$$ y' = \\operatorname*{argmax}_i \\frac{1}{n} \\sum_j^n w_j p_{ij} $$\nwhere $p_{ij}$ is the probability of class label $i$ that $x$ belongs to when using classifier $C_j$.\nAverage for Regression We simply average the predictions of different machines for a regression task.\n$$ y' = \\frac{1}{n} \\sum_j^n w_j C_j $$\nBagging In order to make our models different from each other, we use various algorithms to train the same data, as discussed above. Another way to have a set of diverse models is to train the same model on different data sets. But usually we only have one training data set. Where do other data sets come from? Well, they are sampled with replacement from the original data set, which is known as bootstrapping.\n  Figure 2: The process of bagging (Hands-on machine learning, 2019)  Specifically, given a training data set $D=(x_i, y_i)_i^n$ of the size $N$, we build an ensemble model of size $m$ according to the following steps:\n  For $i=1, 2, 3, \u0026hellip;, m$\n draw $N'(N' \\le N)$ samples with replacement from $D$, which is denoted by $D^*_i$ build a model (e.g. decision tree) $T^*_i$ based on $D_i^*$    For an unseen data, aggregate the predictions of all $T^*$\n perform a majority vote for classification average the predictions for regression    from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier ensemble_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=300, max_sample=100, bootstrap=True) Random Forest Bagging can be used for any models. Among them random forest is the special one. As its name suggests, it is exclusively designed for decision trees. Besides, it introduces extra randomness when growing trees.\n  Figure 3: A random subset of features at each split for each tree (Reference [2])  Specifically, it randomly choose a subset of $m'$ of the features at each split instead of using all features shown in Figure 3. By doing so, all trees can have much different training data set further, so they are less similar to each other, which results in more significant predictions.\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier from sklearn.tree import DecisionTreeClassifier random_forest_clf = BaggingClassifier(splitter=\u0026#39;random\u0026#39;, DecisionTreeClassifier(), max_leaf_nodes=16,n_estimators=300, max_sample=100, bootstrap=True) ## is equivalent to this random_forest_clf=RandomForestClassifier(n_estimators=300, max_leaf_nodes=16) Extra-Trees TODO\nWhy Bagging works Take random forests as an example, each decision tree is a machine learned from a data set. Based on the theory of bias and variance, we know that the mean meachine can be expressed as $f'_m=E_D[f'(x|D)]$. Thus, $f'(x|D)$ can be interpreted as a random variable $X$, and $f'_m$ can be described as $\\mu = E(X)$.\nSince we have $N$ decision trees in a random forest, there are $N$ random variables $X_i$, where $\\mu = E(X_i)$. We can construct a new random variable $Z = \\frac{1}{n}\\sum_i^nX_i$, which represents the mean of $n$ random independent variables $X_i$.\nThe expected value of $Z$ is given by\n$$ E[Z] = E[ \\frac{1}{n}\\sum_i^nX_i ] =\\frac{1}{n} E[\\sum_i^nX_i] = \\frac{1}{n} nE[X_i] = \\mu $$\nThe variance is\n$$ E[(Z - E[Z])^2] = E[(\\frac{1}{n}\\sum_i^nX_i - \\mu)^2] = \\frac{1}{n^2} E[(\\sum_i^n (X_i - \\mu))^2] $$\n$$ = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2 + \\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] $$\nSince $X_i$ is independent of $X_j$ ( $i\\ne j$ ),\n$$ E[\\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] = 0 $$\nwe are left with\n$$ E[(Z - E[Z])^2] = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2] = \\frac{1}{n} \\sigma^2 $$\nwhere $E[(X_i-\\mu)^2]=\\sigma^2$.\nFrom the above euqation, we can see that ensemble methods reduce variances as $n$ increases when our models are uncorrelated.\nOn the contrary, if our models are correlated with the correlation coefficient $\\rho$\n$$ \\rho = \\frac{E[(X_i - \\mu)(X_j - \\mu)]} {\\sqrt{\\sigma^2(X_i)}\\sqrt{\\sigma^2(X_j)}} $$\nThe variance is\n$$ E[(Z - E[Z])^2] = \\frac{1}{n} \\sigma^2 + \\frac{n-1}{n} \\rho \\sigma^2 = \\rho \\sigma^2 + \\frac{(1 - \\rho)}{n} \\sigma^2 $$\nAs $n$ increases, the second term vanishes and we are left with the first term. Therefore, if we want the ensemble methods to do well, we need our models to be uncorrelated.\nRandom forests do a good job because of both the randomness of training data sets sampled via bootstrapping and the randomness of features considered at each split. The algorithm results in much less correlated trees, which trades a higher bias for a lower variance, generally yielding better results.\nFeature Importance Like decision tree, random forests can tell us the relative importance of each feature. It is measured by calculating the sum of the reduction in impurity over all the nodes that are split on that feature.\nrandom_forest_clf.feature_importances_ Boosting In boosting, we combine a group of weak learners into a strong learner.\nWhat\u0026rsquo;s the weak learners? They are the learning machines that do a little better than chance. Thus, they have high bias but low variance. The goal of boosting is to reduce the bias by combining them.\nHow to construct a weak learner? Well, one of the widely used types of weak learner are very shallow trees, for example, the stump with only one depth.\nAdaBoost and GradientBoost are two popular algorithms for boosting. Let\u0026rsquo;s lookt at AdaBoost first.\nAdaBoost AdaBoost is a classic algorithm for binary classification. Suppose we have a data set $D=(x^i,y^i)_i^N$ with $y^i \\in {-1, 1}$, and our weak learner $h(x)$ provides a prediction $h(x^i) \\in {-1, 1 }$. The goal of AdaBoost is to construct a strong learner by combining all the weak learners, which can be written as a weighted sum of weak learners,\n$$ C_n(x) = \\sum_i \\alpha_i h_i(x) $$\nHow to find $\\alpha_i$ and $h_i(x)$? Well, it\u0026rsquo;s difficult to find all the coefficients for one time, so we will solve this equation greedily,\n$$ C_n(x) = C_{n-1}(x) + \\alpha_n h_n(x) $$\nwhere $C_{n-1}(x)$ is the current ensemble model that fit the training data best and $h_n(x)$ is the weak learner we are going to add.\nExponential Loss The second step is to find an appropriate loss function to optimize. How do we measure the performance of a classification model? One of the most widely used loss functions is 0-1 loss,\n$$ L = \\sum_i^N [y_i \\ne y'_i] $$\nwhere $[y_i \\ne y'_i] = 1$ if $x_i$ is classified incorrectly and 0 otherwise. However, it\u0026rsquo;s not-convex and difficult to optimize shown in Figure 4. In AdaBoost, we use exponential loss.\n$$ L = \\sum_i^n e^{-y^iC_n(x^i)} $$\nFrom Figure 4, it can be seen that data that are classified correctly have lower value while misclassfication observations have much larger values, which means exponential loss punishes examples classified incorrecly much more than correct classifications.\n  Figure 4: Exponential loss in AdaBoost  Intuition To minimize the loss, we plug the previous $C_n(x)$ into the loss function\n$$ L = \\sum_i^n e^{-y^i (C_{n-1}(x^i) + \\alpha_n h_n(x^i))} = \\sum_i^n e^{-y^i C_{n-1}(x^i)} e^{-y^i \\alpha_n h_n(x^i)} =\\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} + \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} $$\n$$ = \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} - \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} $$\n$$ = e^{-\\alpha_n} \\sum_i^n w_n^i + (e^{\\alpha_n} - e^{-\\alpha_n}) \\sum_{y^i\\ne h_n(x^i) }^n w_n^i $$\nThen we find that minimizing the loss is equivalent to minimizing the sum of weights of each data that $h_n(x)$ misclassified, and that the value of weights depend on the current ensemble model $C_{n-1}(x)$.\n$$ \\sum_{y^i\\ne h_n(x^i) }^n w_n^i = \\sum_{y^i\\ne h_n(x^i) }^n e^{-y^i C_{n-1}(x^i)} $$\n If the data misclassified by $C_{n-1}(x)$ are still classified incorrectly by $h_n(x)$, then $w_n^i$ is extremely large. If the data classified correcly by $C_{n-1}(x)$ are misclassified by $h_n(x)$, then $w_n^i$ is small.  Simply put, misclassified data points will get high weights while correctly classified data points will get their weights decreased.\nTherefore, we are finding some weak learner that tries to correct the errors the previous learners made. Furthermore, we also notice that we need to update $w_n^i$ for the next weak learner $h_{n+1}(x)$,\n$$ w_{n+1}^i = e^{-y^i C_{n}(x^i)} = e^{-y^i (C_{n-1}(x^i) + \\alpha_nh_n(x^i))} = w_n^i e^{-y^i\\alpha_nh_n(x^i)} $$\nSo the new weight of each data depends on the last weight of that data, the weight of the previous weak learner $h_n(x)$ and itself. But wait, what\u0026rsquo;s the initial weight of each data? We simply initialize weights $w_1^i = \\frac{1}{N}$ for every training sample.\nOkay, now we are only left with $\\alpha_n$. To find $\\alpha_n$, we take the derivative of $L$ with respect to $\\alpha_n$\n$$ \\frac{\\partial L}{\\partial \\alpha_n} = e^{\\alpha_n} \\sum_{y^i\\ne h_n(x^i)}^n w_n^i - e^{-\\alpha_n} \\sum_{y^i= h_n(x^i) }^n w_n^i $$\nThat is\n$$ \\alpha_n = \\frac{1}{2} In\\frac{\\sum_{y^i= h_n(x^i) }^n w_n^i}{\\sum_{y^i\\ne h_n(x^i) }^n w_n^i} $$\nAlgorithm Let\u0026rsquo;s put it all together. The algorithm of AdaBoost can be summarised as below,\n  Given a data set $D=(x^i,y^i)_i^N$ with $y^i \\in \\{-1, 1\\}$ and a group of weak learners $h(x)$ of size $T$\n  Associate a weight $w_1^i = \\frac{1}{N}$ with every data point $(x^i, y^i)$\n  For $t = 1$ to $T$\n Train a weak learner $h_t(x)$ that minimises $\\sum_{y^i\\ne h_t(x^i) }^n w_t^i $ Update the weight of this learner, $\\alpha_t = \\frac{1}{2} In\\frac{\\sum_{y^i= h_t(x^i) }^n w_t^i}{\\sum_{y^i\\ne h_t(x^i) }^n w_t^i}$ Update weights for each training point, $w_{t+1}^i = w_t^i e^{-y^i\\alpha_th_t(x^i)}$    Make a prediction\n $C_n(x) = sign[\\sum_t^T \\alpha_t h_t(x)]$    Example Step 1: Initialisation\nHere we have 8 rows with 3 predictors chest_pain, blocked_arteries and weight and 1 target variable heart_disease. Each data point is initialised with an equal weight 0.125.\n  Figure 5: Toy data from \u0026#39;StatQuest with Josh Starmer\u0026#39;  Step 2: Find the weak learner\nHere, we use stump as our weak learner and Figure 6 shows the first optimal tree where we only misclassified one observation.\nfrom sklearn import tree X = df.drop([\u0026#39;heart_disease\u0026#39;, \u0026#39;weights\u0026#39;], axis=1) y = df[\u0026#39;heart_disease\u0026#39;] clf = tree.DecisionTreeClassifier(max_depth=1) clf = clf.fit(X, y) tree.plot_tree(clf.fit(X, y)) plt.show()   Figure 6: The first stump  Step 3: Update weights\nSince we have only one misclassification, the error rate is 1/8 and $\\alpha_1$ is 0.97. Then we update weights for each data point using $\\alpha_1$.\ndef cal_alpha(error): return 0.5*np.log((1 - error)/error) alpha_1 = cal_alpha(1/8) correct_samples = df[clf.predict(X) == y] df.loc[clf.predict(X) == y, \u0026#39;weights\u0026#39;] = correct_samples[\u0026#39;weights\u0026#39;] * np.exp(-alpha_1) misclassified_samples = df[clf.predict(X) != y] df.loc[clf.predict(X) != y, \u0026#39;weights\u0026#39;] = misclassified_samples[\u0026#39;weights\u0026#39;] * np.exp(alpha_1) print(alpha_1, df)   Figure 7: Updated weigts after training the first stump  Step 4: Go back to Step 2 until the desired number of learners is reached.\nAdjusted impurity Recall that Gini Index is written as\n$$ Q_m^g(L) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nand entropy discussed in Decision Tree as\n$$ Q_m^e(L) = -p_c(L)logp_c(L) $$\nwhere $p_c(L)$ is the fraction of the observations belong to class $c$. In order to use the weight of each data in AdaBoost, we need to change it slightly.\n$$ p_c(L) = \\frac{\\sum_{x^j \\in C} w_n^j I[y_j == C]}{\\sum_{x^i \\in L} w_n^i} $$\nWhy this works? Remember that the lower the impurity is, the better the split is. And a higher fraction leads to a lower impurity or entropy.\n If we classify the misclassified example in the node $L$ correctly, then the denominator of $p_c(L)$ becomes smaller and then $p_c(L)$ becomes larger. On the contratry, if this split works so bad, then we will have many observations that classified incorrectly, resulting in smaller $p_c(L)$ due to a small numerator and large denominator.  Thus, we are finding the best split that can correctly classify the examples that previous learners failed as much as possible.\nReferences [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2]\tT. Yiu, “Understanding random forest - towards data science,” Towards Data Science, 12-Jun-2019. [Online]. Available: https://towardsdatascience.com/understanding-random-forest-58381e0602d2. [Accessed: 23-Apr-2021].\n[3]\tJ. Rocca, “Ensemble methods: bagging, boosting and stacking - Towards Data Science,” Towards Data Science, 23-Apr-2019. [Online]. Available: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205. [Accessed: 23-Apr-2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/decision-tree/",
                "title": "Decision Tree",
                "section": "post",
                "date" : "2021.04.19",
                "body": "The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as \u0026lsquo;Is it a fiction movie? Is it directed by David Fincher?\u0026rsquo;\n  Figure 1: The process of movie selection  From Figure 1, we can see that a decision tree builds a binary tree to partion the data. Each node is a decision rule based on a feature and the tree can grow endlessly. Two questions then arise:\n How is the decision rule made? How deep is the decision tree?  Decision Rule Since there are so many features and each feature can have different values, we have to loop over all of them and find the best split. Well, how to measure the performance of a split? There are 3 measures widely used — Gini Impurity, Entropy, and Information Gain.\nGini Index Gini Index, also known as Gini Impurity, measures how often a randomly selected element from the set is classified incorrectly.\n$$ Q_m^g(L) = \\sum_{c \\in C } p_c(L) (1 - p_c(L)) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nwhere $L$ is short for the children node and $p_c(L)$ is the faction of class $c$ in the leaf node $L$,\n$$ p_c(L) = \\frac{1}{|L|} \\sum_{x, y \\in L} [y == c] $$\nwhere $[y == c] = 1$ if $y$ belongs to class $c$ and 0 otherwise.\n  Figure 2: The plot of Gini Index  Figure 2 shows that the value of Gini Index is the lowest at the start and end of the x-axis and maximum at the middle of the x-axis. In other words,\n If the leaf node only has one class, then this node is a pure node and the gini impurity of this leaf node is 0. On the other hand, if all elements in this leaf node belong to an individual class, then the Gini Index of this node has the maximum value of $ 1 - 1/len(L) $.  Entropy Entropy is a concept of Information Theory. Before introducing the entropy, we should have a little understanding of information.\nInformation is related to the surprise in some way. For example, if I told you that you will go to work tomorrow. Well, that\u0026rsquo;s not surprising because you work every day. However, if I told you that tomorrow is the end of the world, you are likely to be shocked because it is an breaking news.\nWe can measure this surprise by the following equation\n$$ I(x) = -log(p(x)) $$\nIf an event is unlikely to happen, then $p(x)$ is close to 0 and $I(x)$ tends to be infinity, which means it conveys much information since impossible things happened and it must have a significant implication behind it.\nThen what\u0026rsquo;s the entropy? The previous equation calculate the information contained in one outcome. However, it\u0026rsquo;s quite often to have many outcomes for a random variable $X$. Therefore, the expected information over all outcomes is defined as the entropy.\n$$ H(X) = E_{x \\in X}[-log(p(x)] $$\nIn this case, the classes in the leaf node is the random variable $X$ and the probability of each outcome is the fraction of each class, which is expressed as\n$$ Q_m^e(L) = \\sum_{c \\in C}- p_c(L) log(p_c(L)) $$\nIn a word, entropy measures the randomness of a set. Lower entropy means a purer set while higher entropy means there are more other classes that is not the class $c$ in a set.\nInformation Gain Information Gain is simply the difference between the impurity of the parent node $A$ before splitting and the sum of impurity of all children nodes after splitting. It measures how much the impurity of a set $A$ were reduced after splitting. The larger the information gain is, the better the split is.\n$$ IG(A) = H(A) - \\sum_{L \\in children \\ nodes} p(L) H(L) $$\nIn summary, when a decisiton tree makes a split on a feature, it tries to achieve,\n a lower impurity a lower entropy a higher information gain  CART Algorithm CART is short for classification and regression tree algorithm, which is used to train Decisioin Trees. CART tries to find the best split that produces purest subsets by calculating the weighted Gini Impurity on each split made by each feature $k$ from the parent node\u0026rsquo;s training sample and corresponding threshold $t_k$.\nSpecifically, it tries to minimize the following loss function for a classification task,\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}Q_m(C_m^L) + \\frac{|C_m^R|}{|N_m|} Q_m(C_m^R) $$\nwhere $C_m^L$ and $C_m^R$ are children nodes splitted on node $N_m$.\nFor a regression task, it minimizes the sum of squred error\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}SSE(C_m^L) + \\frac{|C_m^R|}{|N_m|} SSE(C_m^R) $$\nwhere $SSE(subset)$ is defined as\n$$ SSE(subset) = \\sum_{n \\in subset} (y_n - \\overline y)^2 $$\n$$ \\overline y = \\frac{1}{|subset|} \\sum_{n \\in subset} y_n $$\nPrunning Now let\u0026rsquo;s address the second problem, i.e. when to stop growing the tree. You can either stop growing the tree when you build the tree or trim the tree after building, which are known as pre-pruning and post-pruning respectively.\nPre-pruning Scikit-learn provides several hyperparameters to do a pre-pruning:\n the maximum depth the minimum number of the samples a node must have to split the minimum number of the samples in a leaf node the maximum number of leaf nodes  All of these parameters can be tuned with cross-validation.\nTODO\nPost-pruning Though pre-pruning is straightforward, it is a bit short-sighted since it doesn\u0026rsquo;t build a full tree and there might be some splits works better later on. Therefore, it would be better to have a large and full-size tree and then we trim some less important branches to avoid overfitting. Cost complexity pruning, also known as weakest link pruning, is one way to achieve this.\nPros and Cons Advantages:\n Decision trees are simple and intutive to interpret, and we can easily visualize the process of decision making. They can be used for both classification and regression. There is no need to normalize or scale data. They can help to understand what features are most important.  Disadvantages:\n They can be easy to overfit. They are sensitive to variations of training data. If you rotate the same data, you will get a completely different tree because all splits are perpendicular to an axis.    Figure 3: senstivity to variations of training set (Hands-on machine learning 2019)  References [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2] arunmohan, “Pruning decision trees,” Kaggle.com, 02-Sep-2020. [Online]. Available: https://www.kaggle.com/arunmohan003/pruning-decision-trees. [Accessed: 22-Apr-2021].\n[3] “How to choose α in cost-complexity pruning?,” Stackexchange.com. [Online]. Available: https://stats.stackexchange.com/questions/193538/how-to-choose-alpha-in-cost-complexity-pruning. [Accessed: 22-Apr-2021].\n[4] “Cost-complexity pruning - ML wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Cost-Complexity_Pruning. [Accessed: 22-Apr-2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-02/",
                "title": "Linear Regression 02",
                "section": "post",
                "date" : "2021.04.17",
                "body": "In the previous post, we talked about simple linear regression. However, we only considered one predictor. It\u0026rsquo;s quite common to have multiple predictors for real-world problems. For example, if we want to predict car prices, we should consider many factors like car sizes, manufacturers and fuel types. The simple linear regression is not suitable for this case. Therefore, we need to extend it to accommodate the multiple predictors.\nMultiple Linear Regression Suppose we have a data set with the size of $n$, and each data point has $d$ dimensions. Then the input data is denoted by $X \\in R^{n \\times d}$, and the parameters and targets are denoted by $\\bold w \\in R^d$, $\\bold y \\in R^n$ respectively. Thus, the loss function can be written by the following equation:\n$$ L = \\sum_i^{n} (\\bold x_i \\bold w - \\bold y_i)^2 = (\\bold X \\bold w - \\bold y)^T(\\bold X \\bold w - \\bold y) $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - \\bold y^T \\bold X \\bold w - \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - 2 \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\nThen we take the derivative of $L$ with respect to $\\bold w$ as simple linear regression before. Well, we need to know a little bit about the matrix calculus\n$$ \\frac{\\partial}{\\partial \\bold x} \\bold x^TA\\bold x = (A + A^T)\\bold x $$\n$$ \\frac{\\partial}{\\partial \\bold x} A^T \\bold x = A $$\nThe gradient of $L$ can be seen easily\n$$ \\frac{\\partial}{\\partial \\bold x } L = (2\\bold X^T\\bold X)\\bold w - 2 \\bold X^T\\bold y $$\nSetting this gradient to zero,\n$$ \\bold w= (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y $$\nHowever, this equation is unlikely to work if $\\bold X^T\\bold X$ is not invertible(singular), such as if the number of features are more than the number of observations($n \u0026lt; d$). One way to solve this equation is to use SVD.\npseudoinverse SVD technique can decompose any matrix $A$ into the multiplication of three matrices, i.e. $U\\Sigma V^T$. Thus the above equation can be written in the following form,\n$$ \\bold w = A^+y $$\n$$ A^+ = (\\bold X^T\\bold X)^{-1} \\bold X^T = V\\Sigma^{-1}U^T $$\nIn practice, the algorithm will set the elements of $\\Sigma$ that less than a smaller threshold to zero, then take the inverse of all nozero values, and finally transpose the resulting matrix i.e. $(U\\Sigma V^T)^{-1}$\nQR factorisation TODO\ncomparison of algorithms TODO\nProbabilistic Interpretation Assumption It\u0026rsquo;s inevitable to introduce errors when we collect data. The error could be systematic errors, human errors or something else. We define the error to be $\\epsilon_i$ for each observation.\n$$ y_i = a + bx_i + \\epsilon_i $$\nThe assumption of linear regression is that the expected error is zero. Specifically, the error follows the Gaussian distribution with the mean of zero and variance of $\\sigma^2$.\n$$ \\epsilon_i \\sim N(0, \\sigma^2) $$\nThus, the probability of $y_i$ is defined by the predictors $x_i$ and the paramters $a, b, \\sigma^2$.\nMLE We have found the parameters by minimizing the loss, but now we are going to use another method to derive the same result, which is known as maximum likelihood estimation(MLE).\nThe basic idea of MLE is that if the data were generated from some model, then what\u0026rsquo;s the parameters of the model were most likely to make this happen? In other words, we are finding the parameters that maximize the probability of the data $D$ that we\u0026rsquo;ve seen.\nSuppose we have a data set of inputs $X={x^{(1)}, x^{(2)}, \u0026hellip;, x^{(N)}}$ and corresponding target variables ${y_1, y_2, .., y_N}$ with a Gaussian noise $\\epsilon$. Then we can construct the likelihood of all data points,\n$$ L(\\theta|D) = \\prod_{n=1}^N p(y_i|x_i, a, b, \\sigma^2) $$\nUsually, we will take the log likelihood to make computation more simpler,\n$$ In(L(\\theta|D)) =\\sum_i^n In(\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y - a - bx_i)^2}{2\\sigma^2}}) $$\n$$ = \\frac{N}{2}In\\sigma - \\frac{N}{2}In2\\pi - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(y_i - a - bx_i)^2 $$\nFrom above equation, we can see that maximizing the likelihood is equivalent to minimizing the sum of squared error.\nGeometry of Linear Regression In this section, we will look at the geometry of the linear regression. In $N$-dimensional space whose axes are the values of $y_1, y_2, \u0026hellip;, y_n$ , the least-squares solution is obtained by finding the orthogonal projection of the target vector $y$ onto the subspace spanned by the columns of $X$.\n  Figure 1: Geometry interpretation of the least-squares solution (PRML 2006)  From the following matrix form, we can see that the predicted value $\\bold y'$ lies the column space of $X$. If the true target value $\\bold y$ also lies in this space, then the loss of linear regression is zero, which is never the case in real life.\n$$ \\displaystyle{\\bold y' = \\bold X \\bold w = \\begin{bmatrix}1\u0026amp;x_{11} \u0026amp; x_{12} \u0026amp; \u0026hellip; \u0026amp; x_{1d}\\\\ 1\u0026amp;x_{21} \u0026amp; x_{22} \u0026amp; \u0026hellip; \u0026amp; x_{2d}\\\\ \u0026hellip; \\\\ 1\u0026amp;x_{n1} \u0026amp; x_{n2} \u0026amp; \u0026hellip; \u0026amp; x_{nd} \\end{bmatrix} \\begin{bmatrix}w_0\\\\ w_1\\\\ w_2\\\\ \u0026hellip;\\\\ w_d \\end{bmatrix} } $$\nNon-linear Data Fitting Polynomials Transformation Dependent Data TODO\nOutliers TODO\nMulticollinearity Dummy Variables References [1] C. Bishop, Pattern Recognition and Machine Learning. 2006.\n[2] \u0026ldquo;Interaction Effects in Regression\u0026rdquo;, Stattrek.com, 2021. [Online]. Available: https://stattrek.com/multiple-regression/interaction.aspx?tutorial=reg. [Accessed: 11- May- 2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/bias-variance-dilemma/",
                "title": "Bias-Variance dilemma",
                "section": "post",
                "date" : "2021.04.15",
                "body": "When you learn more about machine learning, you must hear people talking about high bias, high variance or something like that. What do they mean by \u0026lsquo;high bias\u0026rsquo; or \u0026lsquo;high variance\u0026rsquo;?\nActually, when I first heard these terms, I was completely confused. Even though I tried to find the answer on Google, I still had no idea until I took the Advanced Machine Learning module in semester 2. Therefore, I\u0026rsquo;m writing this post to try to explain this. I hope this post can help people who are still struggling with them to understand the two most important concepts clearly.\nGeneralization Error The underlying assumption of machine learning is that there are some relationships between data. However, we are not able to know this true function, otherwise there is no need to learn it.\nSuppose we have a true realtionship denoted by $f(x)$ (the red dot in Figure 1), and we want to construct a machine denoted by $f'(x)$ to approximate the true function based on the data $D$ sampled from the population $\\chi$.\nThe training loss is defined by the following equation, where $f'(x|D)$ is the machine we learn from this particular data set $D$\n$$ L_T(D) = \\sum_{x\\in D}(f'(x|D) - f(x))^2 $$\nHowever, our goal is to know how well this machine works on unseen data, which is known as generalization. The generalization loss is expressed as\n$$ L_G(D) = \\sum_{x\\in \\chi} p(x) (f'(x|D) - f(x))^2 $$\nIf we have another data set $D_1$, then we will get another machine $f'(x|D_1)$ and another generalization loss $L_G(D_1)$ shown in Figure 1.\n  Figure 1: Generalisation error  We can see that the generalization loss is depend on our training data. Thus, the generalization loss for a particular data set doesn\u0026rsquo;t make much sense. Instead, the average generalization loss over all the data set with the same size of $n$ is what we expect.\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\nMean Machine We have already known that there is a different machine $f'(x|D)$ for a given data set $D$. Thus, for an unseen data $x$, we will have many predictions of many different machines, which are represented in blut dots shown in Figure 2.\n  Figure 2: Bias and variance  The average prediction for an unseen data is the mean prediction(the yellow dot in Figure 2).\n$$ f'_m(x) = E_D[f'(x|D)] $$\nBias Bias is the distance between the mean prediction(the yellow dot) and the true value(the red dot) shown in Figure 2. High bias implies that our model is too simple and the prediction value is much far away from the true value.\n$$ B = \\sum_{x \\in \\chi} p(x) (f\u0026rsquo;m - f(x))^2 $$\nVariance Variance measures the variation in the prediction of the machine when we change different data set we train on. If we have a complex machine, as mentioned earlier, the machine will try its best to match every data in training data set. In other words, the machine memorized the trainining data and a little change in data set will cause significant variation in prediction.\n$$ V = \\sum_{x \\in \\chi}p(x) E_D[ (f'(x|D) - f\u0026rsquo;m)^2 ] $$\nBias-Variance dilemma Now it\u0026rsquo;s time to decompose the average generalisation error. Let\u0026rsquo;s plug the $f'_m(x)$ into the previous equation\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x) (f'(x|D) - f_m' + f_m' - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x){(f'(x|D) - f_m')^2 + (f_m' - f(x))^2 + 2(f'(x|D) - f_m')(f_m' - f(x)) }] $$\nIt\u0026rsquo;s noticeable that the cross-term will cancel out because $f\u0026rsquo;m$ and $f(x)$ are constants no matter what data set $D$ is.\n$$ E_D[\\sum_{x\\in \\chi}p(x)2(f'(x|D) - f_m')(f_m' - f(x))] $$\n$$ = \\sum_{x\\in \\chi}p(x) (2E_D[f'(x|D)]-f\u0026rsquo;m)(f\u0026rsquo;m-f(x)) = 0 $$\nTherefore, we are left with\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi}p(x)(f'(x|D) - f_m')^2 + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2] $$\n$$ = \\sum_{x\\in \\chi}p(x) E_D[(f'(x|D) - f_m')^2] + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2 $$\n$$ = V + B $$\nIn summary,\n If our machine is too simple, then we might not be able to fit the training data. Since the machine knows little about the data, it\u0026rsquo;s unlikely to work well on unseen data. This means our model has a high bias. If our machine is too complex, then we might be able to fit the training data perfectly. It means that the machine knows too much about the data, even the noise that it should not learn. Thus, it\u0026rsquo;s too sensitive to training data so that a little change in data will cause a great variance. This means our model has a high variance.  Throughout the world of machine learning, we are always trying to find a balance between bias and variance.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-01/",
                "title": "Linear Regression 01",
                "section": "post",
                "date" : "2021.04.14",
                "body": "There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand. Perhaps it is the first algorithm that most people learn in machine learning. So let\u0026rsquo;s go!\nProblem Statement Suppose you are a teacher, and you recorded some data about the hours students spent on study and the grades they achieved. Below are some sample data you collected. Then you want to predict the score for someone according to the hours he spent.\n   Hours 0.5 1 2 3 4     Grade 20 21 22 24 25    How to make a prediction? Since there are only two variables, let\u0026rsquo;s plot them to see if there is a relation betwee grade and hours.\n  Figure 1: A scatter plot of hours and grade  Figure 1 shows that grade is positively related to hours. For simplicity, we can use a straight line(the red line) to approximate this relation. And this is exactly our first linear model.\nSimple Linear Regression Remember the equation of a straight line is typically written as,\n$$ y = ax + b $$\nIn this example, $x$ is the variable hours and $y$ is the variable grade, which we\u0026rsquo;ve already known. So the problem is to find the parameter $a, b$ such that the red line is as close as possible to the blue data points. Technically, this is called parameter estimation. Usually, there are two ways to do this: minimising the loss and maximising the likelihood. Now we focus on the former.\nResidual So how to measure the closeness mentioned above? The most common way is to measure the residual, which is the difference between the estimated value and our true value shown in Figure 2. For a single data point, the residual is defined as below, where $y_i$ is the true value and $y_i'$ is our predicted value.\n$$ e = y_i - y'_i = y_i - ax_i - b $$\n  Figure 2: Residual/Error is the difference between the observed value and the predicted value. (Bradthiessen.com 2021)  error Since we have many data points, we need to sum them up to evaluate the overall errors. Unfortunately, some error terms will cancel out when you do the this calculation directly.\n$$ L = \\sum_i^n (y_i - y'_i) = \\sum_i^n (y_i - ax_i - b) $$\nthe absolute value of error One way to tackle this is to take the absolute value of the error terms. However, the absoulte value of $x$ is not differentiable at $0$.\n$$ L = \\sum_i^n |y_i - y_i'| $$\nthe squared value of error Instead of taking the absolute value, we square all the errors and sum them up, which is known as the Residual Sum of Squares(RSS) or Sum of Squared Error (SSE).\n$$ L = \\sum_i^n (y_i - y_i')^2 $$\nClosed-form solution Finally, we find a function to measure the loss. Next, we need to find the parameters that minimize the squared error. The good news is that our loss function is differentiable and convex! It means that we have the global minimum value and can be calculated directly by taking derivatives.\nLet\u0026rsquo;s take the first derivative w.r.t $b$\n$$ \\frac{\\partial L}{\\partial b} = \\sum_i^n -2(y_i - ax_i-b) $$\nand then set this equation to $0$,\n$$ -2(\\sum_i^ny_i -a\\sum_i^nx_i - \\sum_i^nb) = -2(n\\overline y-an\\overline x - nb) = 0 $$\n$$ \\hat b = \\overline y - \\hat a \\overline x $$\nSimilarly, let\u0026rsquo;s take the first derivative w.r.t $a$\n$$ \\frac{\\partial L}{\\partial a} = \\sum_i^n -2x_i(y_i - ax_i-b) $$\nplug $b$ into this equaiton and set this equation to 0 again,\n$$ \\sum_i^n -2x_i(y_i - ax_i-\\overline y+a\\overline x) = \\sum_i^n -2x_i[(y_i-\\overline y)- a(x_i -\\overline x)] $$\n$$ \\hat a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} $$\nHere, we use a slight algebra trick,\n$$ c\\sum_i^n(x_i - \\overline x_i) = 0 $$\nplug the above equation into the previous equation\n$$ \\hat a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} = \\frac{\\sum_i^nx_i(y_i-\\overline y) - \\sum_i^n\\overline x(y_i - \\overline y)}{\\sum_i^nx_i(x_i -\\overline x) - \\sum_i^n\\overline x(x_i - \\overline x)} $$\n$$ = \\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2} $$\n$$ = \\frac{Cov(x, y)}{Var(x)} $$\nFinally, we find the best estimators for our simple linear regression.\nTest statistics The above formulas give us the best estimation for the parameters $a, b$ of the linear regression model. If we generate different data sets from the population, we will have different linear models and different values of dependent variable. If we average these values, we\u0026rsquo;ll find that the average value is pretty close to the true value. Mathematically, this can be expressed as follows,\n$$ E_D(\\hat ax_i + \\hat b|D) \\simeq ax_i+b $$\nThe idea behind the above equation is analogous to the Central Limit Theorem for Sample Mean. The population mean of the random variable $Y_i$ (the true line) can be estimated by the expected value of the sample mean(the estimated line). CLT tells us the distribution of the sample mean follows the Gaussian Distribution with the mean of the population mean as the sample size gets larger.\nStandard error But in practice, we can only have one data set, so how accurate is the parameters $a, b$ calculated from the above equations? In other words, a single sample mean may overestimate or underestimate the population mean, but to what extent is this deviation? We use the standard error of sample mean to measure it, which can be obtained by the following equation, where $\\sigma$ is the population standard deviation and $n$ is the sample size.\n$$ SE^2(\\hat {\\overline \\mu}) = Var(\\hat {\\overline \\mu}) = \\frac{\\sigma^2}{n} $$\nSimilarly, we can calculate the standard error associated with the parameters $a$ and $b$.\n$$ SE^2(\\hat a) = Var(\\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2}) = Var(\\frac{\\sum_i^n(x_i-\\overline x)y_i - \\sum_i^n(x_i-\\overline x)\\overline y}{\\sum_i^n(x_i -\\overline x)^2}) $$\n$$ = Var(\\frac{\\sum_i^n(x_i-\\overline x)(ax_i + b + \\epsilon_i)}{\\sum_i^n(x_i -\\overline x)^2}) $$\n$$ = \\frac{\\sum_i^n(x_i-\\overline x)^2}{(\\sum_i^n(x_i -\\overline x)^2)^2} Var(ax_i + b + \\epsilon_i) = \\frac{\\sum_i^n(x_i-\\overline x)^2}{(\\sum_i^n(x_i -\\overline x)^2)^2} Var(\\epsilon_i) $$\n$$ = \\frac{Var(\\epsilon_i)}{\\sum_i^n(x_i -\\overline x)^2} $$\nSince $x_i, y_i$ are known and $a, b$ are the true parameters, the only thing unknown is $\\epsilon_i$. In other words, they are all indepentdent of $\\epsilon_i$. In addition, we use a bit tricks to derive the above formula,\n$$ Var(c) = 0 $$\n$$ Var(cX) = c^2Var(X) $$\n$$ Var(c + X) = Var(X) $$\nIn summary, what we should know is that the standard error of $\\hat a$ tells us how far away this estimate $\\hat a$ is from the true value $a$ or how far away the predicated value $\\hat y$ is from the observed value $y$.\nFurthermore, since the predicted value obtained from a given sample is different from the true value, we cannot say we are sure that the estimated value is exactly the true value. But we could say we are 90% confident that the true value lies somewhere around the predicted value. And this \u0026lsquo;somewhere\u0026rsquo; can be computed by confident intervals.\np-value To investigate whether the estimated parameters are statistically significant, we perform hypothesis tests. A statistical significant result means it\u0026rsquo;s unlikely to happen by chance. In the simple linear regression, the null and alternative hypotheses are defined as\n$H_0$: There is no relationship between $X$ and $Y$\n$H_a$: There is some relationship between $X$ and $Y$\nMathematically, it is equvalent to testing\n$H_0$: $a=0$\n$H_a$: $a\\ne0$\nSo in order to test the null hypothesis, we need to demonstrate that $\\hat a$ is sufficiently far away from $0$. Thus, we can be confident that $a$ is not equal to $0$ and reject the null hypothesis. To quantify the distance between $\\hat a$ and $0$, we calculate t-score\n$$ t = \\frac{\\hat a - 0}{SE(\\hat a)} $$\nThe higher the $t$ is, the farther the distance is. But wait, what\u0026rsquo;s the probability of getting this estimate $\\hat a$ or more extreme? In other words, what\u0026rsquo;s the p-value? How to interpret this probability? A higher p-value doesn\u0026rsquo;t provide much information. In contrast, a smaller p-value means it\u0026rsquo;s unlikely to observe this t-score due to chance under the assumption that $H_0$ is true. You can interpret that a small p-value indicates a strong evidence against the null hypothesis. But how small is enough? Typically, we set the threshold value of $0.05$. If p-value is smaller than $0.05$, we reject $H_0$, otherwise we accept it.\nModel Evaluation Next question is how to evaluate our model? How good is it? There are two common metrics to measure the quality of a linear regression model: RSE and $R^2$.\nRSE Residual standard error measures the average deviation between the estimated value and the true value. It can be calculated using the following fomula, where n-2 is the degree of freedom(df) of the residual.\nQ: Why do we divide by $n-2$ not $n$?\nA: This is because the latter tends to underestimate variance. Rememer we divide by $n-1$ when calculating the sample variance. This is the same reason here.\nQ: But why n-2?\nA: We know that 2 points determine a line, which means there is no other line fitting the data and the residual of each data point is fixed. If we add a third point, there could be many lines fitting these points and thus different residuals. That means the third point increases the flexiblity of the value of residuals. We say we have one free observation. Therefore, the degree of freedom of the residual is $n-2$ in simple linear regression.\n$$ RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - y_i')^2} $$\nA smaller RSE indicates that our model fit the data well. However, RSE is measured in the unit of $Y$. For some data sets, RSE would be small, e.g $3.2$. But for other data sets, it would be very large, e.g $1200$. So it\u0026rsquo;s a bit confusing. That\u0026rsquo;s why $R^2$ comes. $R^2$ measures the fraction of variance explained, which is independent of the unit of $Y$.\n$R^2$ Coefficient of determination or $R^2$ is another metric to measure the goodness of a linear regression model. Let\u0026rsquo;s rewrite the previous equation by multiplying both the denominator and numerator by $\\sqrt {\\sum_i^n(y_i-\\overline y)^2}$\n$$ a = \\frac{\\sum_i^n (x_i - \\overline x)(y_i - \\overline y) \\sqrt {\\sum_i^n(y_i-\\overline y)^2}}{\\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(y_i-\\overline y)^2}} $$\n$$ a = R\\frac{s_y}{s_x} $$\nwhere\n$$ R = \\frac{Cov(x, y)}{\\sqrt{var(x)} \\sqrt{var(y)}} $$\n$$ s_y = \\sqrt{Var(y)} $$\n$$ s_x = \\sqrt{Var(x)} $$\nRemember that the error is defined as $e_i = y_i' - y_i$, so the mean of $e$ is\n$$ E(e) = \\frac{1}{N} \\sum_i^n e_i = \\frac{1}{N} \\sum_i^n b + ax_i - y_i =b + a\\overline x - \\overline y = 0 $$\nand the variance is\n$$ var(e) = \\sum_i^n (e_i - \\overline e)^2 = \\sum_i^n (y_i - b - ax_i)^2 = \\sum_i^n (y_i - \\overline y + a\\overline x - ax_i)^2 $$\nLet\u0026rsquo;s plug $a$ into this equation\n$$ var(e) = \\sum_i^n [(y_i - \\overline y) - R\\frac{s_y}{s_x}( x_i - \\overline x)]^2 = var(y) (1-R^2) $$\nOr you might be more familiar with this equation\n$$ R^2 = 1 - \\frac{var(e)}{var(y)} = 1 - \\frac{RSS}{TSS} $$\nTherefore, $R^2$ tells us how much the variance of $y$ has been explained by our models. The higher the $R^2$ is, the better our model is.\nReferences [1] Bradthiessen.com, 2021. [Online]. Available: https://www.bradthiessen.com/html5/docs/ols.pdf. [Accessed: 14- Apr- 2021].\n[2] “Linear Regression - ML Wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Linear_Regression. [Accessed: 14-Apr-2021].\n[3] K. Base and S. statistics, \u0026ldquo;Standard Error | What It Is, Why It Matters, and How to Calculate\u0026rdquo;, Scribbr, 2021. [Online]. Available: https://www.scribbr.com/statistics/standard-error/. [Accessed: 12- May- 2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/descriptive-statistics/",
                "title": "Descriptive Statistics",
                "section": "post",
                "date" : "2021.04.13",
                "body": "Statistics is one of the most important skills required to be a data scientist. There are two main branches of statistics:\n  descriptive statistics tells us the statistics about data like mean, mode and standard deviation, which you\u0026rsquo;ve learned in high school.\n  inferential statistics, on the other hand, uses a random dataset sampled from the population to make inferences about the population.\n  In this post, we will focus on descriptive statistics and other basic concepts in statistics.\nCentral tendency Central tendency measures the center of the data. Mean, mode, and median are three mainly used measures of central tendency.\nMean Mean is the average of data and calculated by summing up all data values and then dividing them by the number of data. For instance, say we have data 1,2,3,4,5,6, the mean is 3.5.\n$$ \\overline x = \\frac{\\sum_i^nx_i}{n} $$\nMedian Though mean is widely used, it\u0026rsquo;s sensitive to outliers. Suppose you have a set of data 1,2,3,4,5,6,100, after some calculations, you find that the mean is 17.28. However, it seems a bit strange since most of the data values are less than 10 except for one extreme value 100, which stretches the distribution of the whole data set to the right. This is why the median comes.\nTo get the median, firstly we need to sort the data in ascending order and then find the middle number that separates the data into two groups of the same size. In this example, the median is 4.\nMode Mode is the most frequent value. There could be one, two or more data values with the same frequency, and that frequency is the largest.\nDispersion Dispersion measures how spread out a given dataset is.\nRange Range is the distance between the maximum value and the minimum value. Again, the range is sensitive to outliers.\n$$ r=max - min $$\nQuantile Quantiles are used to divide data into several equal-sized groups. The most widely used cut points are 0, 25, 50, 75, 100, denoted by min, Q1, Q2, Q3, max, respectively.\nIQR or interquartile range measures where the central 50% of the data is.\n$$ IQR = Q3 - Q1 $$\nIQR can be used to detect outliers. Data that is greater than the upper boundary or less than the lower boundary can be considered as an outlier.\n$$ upper \\ boundary = Q3 + 1.5*IQR $$\n$$ lower \\ boundary = Q1 - 1.5*IQR $$\nVariance The deviation is the distance between a given data point and the mean. Since there are many data points, the variance calculates the average deviation from the mean. But when you do this, you will always get a zero due to the definition of the mean.\n$$ \\frac{1}{n}\\sum_i^n d_i = \\frac{1}{n}\\sum_i^n (x_i - \\overline x) = \\frac{1}{n}\\sum_i^nx_i - n\\overline x = \\frac{1}{n} (n\\overline x - n\\overline x) = 0 $$\nLet\u0026rsquo;s try to ignore the sign and use the absolute value of the deviation.\n$$ \\frac{1}{n-1} \\sum_i^n d_i = \\frac{1}{n-1} \\sum_i^n |x - \\overline x| $$\nThough it works, the most widely used method is to calculate the square of the deviation.\n$$ s^2 = \\frac{1}{n-1} \\sum_i^n (x_i - \\overline x)^2 $$\nHowever, the squared value is a bit hard to interpret. For instance, below are 15 records of fish size measured in kilogram, and the variance is 30.97. It means that if we randomly catch a fish, its weight would be 30.97 squared kilograms far away from the average weight. Emm\u0026hellip;, a squared kilogram is an odd unit.\n2.1, 2.4, 2.4, 2.4, 2.4, 2.6, 2.9, 3.2, 3.2, 3.9, 4.5, 6.3, 8.2, 12.8, 23.5 Standard Deviation It\u0026rsquo;s simple to solve this problem by taking the square root of the variance, which is the standard deviation. The standard deviation in the previous example is 5.56kg, which makes much sense.\n$$ s = \\sqrt{\\frac{\\sum (x - \\overline x)^2}{n-1}} $$\nDistribution Skewness Skewness measures the symmetry of a distribution. It\u0026rsquo;s quite common to have non-symmetric distributions shown in Figure 1.\n  Figure 1: Non-symmetric distributions  A left-/negative-skewed distribution\n a long left tail mean \u0026lt; median  A right-/positive-skewed distribution\n a long right tail mean \u0026gt; median  A skewed distribution implies that some special values are much larger/smaller than the common values. Let\u0026rsquo;s see an example.\n Figure 3.2 shows the histogram of the fish sizes gathered from a fisherman. We can see that this distribution is right-skewed. The majority of fish sizes are between 0 and 3kg, but a few fishes weigh over 3.5kg. It also can be seen that the average weight(1.67kg) is a bit higher than the median(1.62kg) shown in Table 3.2.\nCorrelation So far we have introduced different methods of characterizing the distribution of a single variable. But what about two or more variables? You might want to find the relationships between them. An intuitive way is to visualise data using a scatterplot. For example, you might find that car prices tend to increase as car ages decrease.\nStatistically, we can measure both the direction and the strength of this tendency using correlation coefficients. And Pearson correlation coefficients is widely used to measure the strength of a linear relationship. It can be calculated as follows,\n$$ r = \\frac{Cov(X, Y)}{\\sqrt{V(X)}\\sqrt{V(Y)} } $$\nwhere $Cov(X, Y)$ is the covariance between $X$ and $Y$, and $V(X), V(Y)$ are the standard deviation of $X, Y$ respectively. The value of $r$ ranges between -1 and 1.\n a negative value represents a negative relationship a positive value represents a positive relationship $0$ means there is no relationship between $X$ and $Y$  But to what extent is the value of $r$ large means a strong relationship? Below are some values that could give you an insight into how strong your correlation $r$ is.\n 0 - 0.3: weak relationship 0.3 - 0.6: moderate relationship 0.6 - 0.9: strong relationship 0.9 - 1.0: very relationship  Data Types Now let\u0026rsquo;s look at some characteristics of data. Roughly, we classify data into two categories:\n numerical/quantitative data categorical/qualitative data  Categorical data refers to data that cannot be measured, for example, the color of car or the quality of service. There are two subcategories, namely nominal data and ordinal data. The difference between them is that ordinal data can have order, such as 0=bad, 1=good, 2=better, 3=excellent.\nNumerical data refers to data that can be measured. There are also two subcategories, namely discrete and continuous. Discrete data describes values that cannot be divided into smaller individual parts, e.g. the number of people. You cannot say, \u0026lsquo;There are 1.5 people\u0026rsquo;. However, we can say, \u0026lsquo;He is 1.87 meters tall\u0026rsquo;. So height is a continuous variable.\nMeasurement Scales At a lower level, we can also classify data from the perspective of measurement scales, which capture the characteristics of data used to determine the types of variables. There are four main levels of measurement scales:\n nominal ordinal interval ratio  Each of them satisfies one or more of the following properties of measurement.\n Identity  each value have a unique meaning $=, \\cancel{=}$ nominal scale, e.g. car color   Maginitue  values have an ordered relationship to one another $\\lt, \\le, \\gt, \\ge$ ordinal scale, e.g. service quality   Equal interval  the different between data points A and B will be equal to the different between data points C and D $+, -$ interval scale, e.g. time   A minimum value of zero  the scale has a true zero point, below which no value exists ratio scale, e.g. height    References [1] B. al., \u0026ldquo;Introduction to Statistics | Simple Book Production\u0026rdquo;, Courses.lumenlearning.com, 2021. [Online]. Available: https://courses.lumenlearning.com/introstats1. [Accessed: 14- Apr- 2021].\n[2]\u0026ldquo;Measurement Scales\u0026rdquo;, Stattrek.com, 2021. [Online]. Available: https://stattrek.com/statistics/measurement-scales.aspx?tutorial=reg. [Accessed: 11- May- 2021].\n"
            }
        
    
]