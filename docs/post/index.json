[
    
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/decision-tree/",
                "title": "Decision Tree",
                "section": "post",
                "date" : "2021.04.18",
                "body": "The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as 'Is it a fiction movie? Is it directed by David Fincher?'\nFigure 1: The process of movie selection\nFrom Figure 1, we can see that a decision tree builds a binary tree to partion the data. Each node is a decision rule based on a feature and the tree can grow endlessly. Two questions then arise:\n How is the decision rule made? How deep is the decision tree?  Splitting Rule Since there are so many features and each feature can have different values, we have to loop over all of them and find the best splitting point. Well, how to measure the performance of a splitting? 3 measures mainly used are Gini Impurity, Entropy, Information Gain.\nGini Index Gini Index, also known as Gini Impurity, measures how often a randomly selected element from the set is classified incorrectly, which is defined as,\n\\[ Q_m^g(L) = \\sum_{c \\in C } p_c(L) (1 - p_c(L)) = 1 - \\sum_{c \\in C } p_c(L)^2 \\]\n\\(p_c(L)\\) is defined as\n\\[ p_c(L) = \\frac{1}{len(L)} \\sum_{x, y \\in L} [y == c] \\]\nwhere \\([y == c] = 1\\) if \\(y\\) belongs to class \\(c\\) and \\(0\\) otherwise.\nFigure 2: Gini Index.\nFigure 1 shows that the value of Gini Index is the lowest at the start and end of the x-axis and maximum at the middle of the x-axis. In other words, if the leaf node only has one class, then this node is a pure node and the gini impurity of this leaf node is \\(0\\). On the other hand, if all elements in this leaf node belong to an individual class, then the maximum value of Gini Index is \\( 1 - 1/len(L) \\).\nEntropy Entropy is a concept of Information Theory. Before introducing the entropy, we should have a little understanding of information.\nInformation is related to the surprise in some way. For example, if I told you that you will go to work tomorrow. Well, that's not surprising because you work every day. However, if I told you that tomorrow is the end of the world, you are likely to be shocked because it is an breaking news.\nWe can measure this surprise by the following equation\n\\[ I(x) = -log(p(x)) \\]\nIf an event is unlikely to happen, then \\(p(x)\\) is close to 0 and \\(I(x)\\) tends to be infinity, which means it conveys much information since impossible things happened and it must have a significant implication behind it.\nThen what's the entropy? The previous equation calculate the information of one outcome. However, it's quite often to have many outcomes for a random variable \\(X\\). Therefore, the expected information over all outcomes is defined as the entropy.\n\\[ H(X) = E_{x \\in X}[-log(p(x)] \\]\nIn this case, it is expressed as\n\\[ Q_m^e(L) = \\sum_{c \\in C}- p_c(L) log(p_c(L)) \\]\nInformation Gain Information Gain is simply the difference between the impurity of the parent node before splitting and the sum of impurity of all children nodes after splitting.\n\\[ IG(A) = H(A) - \\sum_{L \\in Leave Nodes} p(L) H(L) \\]\nIn words, it measures how much impurity of a set \\(A\\) were reduced after splitting. The larger the information gain is, the better the splitting is.\nIn summary, when decisiton tree makes a split on a feature, it tries to achieve:\n lower impurity lower entropy higher information gain  Hyperparameters Now let's address the second problem, i.e. when to stop growing the tree. Scikit-learn provides several hyperparameters to avoid overfitting:\n maximum depth minimum number of the samples a node must have to split minimum number of the samples in a leaf node minimum leaf nodes  All of these parameters can be tuned by cross-validation.\nPros and Cons Advantages:\n Decision trees are simple and intutive to interpret because we can easily visualize the process of decision making. They can be used for both classification and regression. They can handle missing data. The missing data is either on the left or right depends on maximum purity. There is no need to normalize or scale data.  Disadvantages:\n They can have high bias because they split the data based on a single variable. They can be easy to overfit. They are sensitive to variations of training data. If you rotate the same data, you will get a completely different tree because all splits are perpendicular to an axis.  Figure 3: senstivity to variations of training set (Hands-on machine learning 2019)\nReferences  [1]A. GÃ©ron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O'Reilly Media, 2019.  "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-02/",
                "title": "Linear Regression 02",
                "section": "post",
                "date" : "2021.04.17",
                "body": "In the last post, we talked about simple linear regression. However, we only considered one predictor. In fact, it's quite common to have mulitple attributes in real-world problems. For example, if we want to predict the price of a car, we have to consider many factors like car size, manufacturer and fuel type. Clearly, the simple linear regression is not suitable for this case. Therefore, we need to extend it to accommodate multiple predictors.\nMultiple Linear Regression Suppose we have a data set with the size of \\(n\\) , and each data point has \\(d\\) dimensions. Then the input data is denoted by \\(X \\in R^{n \\times d}\\), and the parameters and targets are denoted by \\(\\bold w \\in R^d\\), \\(\\bold y \\in R^n\\) respectively. Thus, the loss function can be written by the following equation:\n\\[ L = \\sum_i^{n} (\\bold x_i \\bold w - \\bold y_i)^2 = (\\bold X \\bold w - \\bold y)^T(\\bold X \\bold w - \\bold y)\\\\ = \\bold w^T\\bold X^T \\bold X \\bold w - \\bold y^T \\bold X \\bold w - \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y \\\\ = \\bold w^T\\bold X^T \\bold X \\bold w - 2 \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y \\]\nThen we take the derivative of \\(L\\) with respect to \\(\\bold w\\) as simple linear regression before. Well, we need to know a little bit about the matrix calculus\n\\[ \\frac{\\partial}{\\partial \\bold x} \\bold x^TA\\bold x = (A + A^T)\\bold x \\\\ \\frac{\\partial}{\\partial \\bold x} A^T \\bold x = A \\]\nThe gradient of \\(L\\) can be seen easily\n\\[ \\frac{\\partial}{\\partial \\bold x } L = (2\\bold X^T\\bold X)\\bold w - 2 \\bold X^T\\bold y \\]\nSetting this gradient to zero,\n\\[ \\bold w= (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y \\]\nHowever, this equation is unlikely to work if \\(\\bold X^T\\bold X\\) is not invertible(singular), such as if the number of features are more than the number of observations(\\(n ). One way to solve this equation is to use SVD.\npseudoinverse SVD technique can decompose any matrix \\(A\\) into the matrix multiplication of three matrices \\(U\\Sigma V^T\\). Thus the above equation can be written in the following form\n\\[ \\bold w = A^+y \\\\ A^+ = (\\bold X^T\\bold X)^{-1} \\bold X^T = V\\Sigma^{-1}U^T \\]\nIn practice, the algorithm will set the elements of \\(\\Sigma\\) that less than a smaller threshold to zero, then take the inverse of all nozero values, and finally transpose the resulting matrix i.e. \\((U\\Sigma V^T)^{-1}\\)\nProbabilistic Interpretation It's inevitable to introduce errors when we collect data. The error could be systematic errors, human errors or something else. We can define the error to be \\(\\epsilon_i\\) for each observation.\n\\[ y_i = a + bx_i + \\epsilon_i \\]\nThe assumption of linear regression is that the expected error is zero. Specifically, the error follows the Gaussian distribution with the mean of zero and variance of \\(\\sigma^2\\).\n\\[ \\epsilon_i \\sim N(0, \\sigma^2) \\]\nThus, the probability of \\(y_i\\) is defined by the predictors \\(x_i\\) and the paramters \\(a, b, \\sigma^2\\).\nMLE We have found the parameters by minimizing the loss, but now we are going to use another method to derive the same result, which is known as maximum likelihood estimation(MLE).\nThe basic idea of MLE is that if the data were generated from some model, then what's the parameters of the model were most likely to make this happen? In other words, we are finding the parameters that maximize the probability of the data \\(D\\) that we've seen.\nSuppose we have a data set of inputs \\(X={x^{(1)}, x^{(2)}, ..., x^{(N)}}\\) and corresponding target variables \\({y_1, y_2, .., y_N}\\) with a Gaussian noise \\(\\epsilon\\). Then we can construct the likelihood of all data points,\n\\[ L(\\theta|D) = \\prod_{n=1}^N p(y_i|x_i, a, b, \\sigma^2) \\]\nUsually, we will take the log likelihood to make computation more simpler,\n\\[ In(L(\\theta|D)) =\\sum_i^n In(\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y - a - bx_i)^2}{2\\sigma^2}})\\\\ = \\frac{N}{2}In\\sigma - \\frac{N}{2}In2\\pi - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(y_i - a - bx_i)^2 \\]\nFrom above equation, we can see that maximizing the likelihood is equivalent to minimizing the sum of squared error.\nGeometry of Linear Regression In this section, we will look at the geometry of the linear regression. In \\(N\\)-dimensional space whose axes are the values of \\(y_1, y_2, ..., y_n\\) , the least-squares solution is obtained by finding the orthogonal projection of the target vector \\(y\\) onto the subspace spanned by the columns of \\(X\\).\nFigure 1: Geometry interpretation of the least-squares solution. (PRML 2006)\nFrom the following matrix form, we can see that the predicted value \\(\\bold y'\\) lies the column space of \\(X\\). If the true target value \\(\\bold y\\) also lies in this space, then the loss of linear regression is zero, which is never the case in real life.\n\\[ \\bold y' = \\bold X \\bold w = \\begin{bmatrix} 1\u0026x_{11} \u0026 x_{12} \u0026 ... \u0026 x_{1d} \\\\ 1\u0026x_{21} \u0026 x_{22} \u0026 ... \u0026 x_{2d} \\\\ ... \\\\ 1\u0026x_{n1} \u0026 x_{n2} \u0026 ... \u0026 x_{nd} \\end{bmatrix} \\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ ...\\\\ w_d \\end{bmatrix} \\]\nReferences [1]C. Bishop, Pattern Recognition and Machine Learning. 2006.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/bias-variance-dilemma/",
                "title": "Bias-Variance dilemma",
                "section": "post",
                "date" : "2021.04.15",
                "body": "When you learn more about machine learning, you must hear people talking about high bias or high variance something like that. What does they mean by 'high bias' or 'high variance'? Actually, when I first heard these terms, I was completely confused. Even though I tried to find the answer on Google, I still had no idea until I took the Advanced Machine Learning module in semester 2. Therefore, I'm writing this post to try to explain this. I hope this post can help people who are still struggling with them understand the two most important concepts clearly.\nGeneralization Before diving into it further, we should know what the generalization is.\nGeneralization measures how well our machine works on unseen data.\n If our machine is too simple, then we might not be able to fit the training data. Since the machine knows little about the data, it's unlikely to work well on unseen data. If our machine is too complex, then we might be able to fit the training data perfectly. It means that the machine knows too much about the data, even the noise that it should not learn. Thus, it's too sensitive to data so that a little change in data will cause a great variance.  Generalization Error The underlying assumption of machine learning is that there are some relationships between data. However, we are not able to know this true function, otherwise there is no need to learn it.\nSuppose we have a true realtionship denoted by \\(f(x)\\) (the red dot in Figure 2), and we want to construct a machine denoted by \\(f'(x)\\) to approximate the true function based on the data \\(D\\) sampled from the population \\(\\chi\\). Then the training loss is defined by the following equation, where \\(f'(x|D)\\) is the machine we learn from this particular data set \\(D\\)\n\\[ L_T(D) = \\sum_{x\\in D}(f'(x|D) - f(x))^2 \\]\nOkay, now we want to know how well this machine works on unseen data, which can be measured by generalization loss.\n\\[ L_G(D) = \\sum_{x\\in \\chi} p(x) (f'(x|D) - f(x))^2 \\]\nIf we have another data set \\(D_1\\), then we will get another machine \\(f'(x|D_1)\\) and another generalization loss \\(L_G(D_1)\\) shown in Figure 1.\nFigure 1\nWe can see that the generalization loss is depend on our training data. Thus, the generalization loss for a particular data set doesn't make much sense. Instead, the average generalization loss over all the data set with the same size of \\(n\\) is what we expect.\n\\[ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] \\]\nMean Machine We have already known that there is a different machine \\(f'(x|D)\\) for a given data set \\(D\\). Thus, for an unseen data \\(x\\), we will have many predictions of many different machines, which are represented in blut dots shown in Figure 2.\nFigure 2: Bias and variance.\nThe average prediction for an unseen data is the mean prediction(the yellow dot in Figure 2).\n\\[ f'_m(x) = E_D[f'(x|D)] \\]\nBias Bias is the distance between the mean prediction(the yellow dot) and the true value(the red dot) shown in Figure 2. High bias implies that our model is too simple and the prediction value is much far away from the true value.\n\\[ B = \\sum_{x \\in \\chi} p(x) (f'm - f(x))^2 \\]\nVariance Variance measures the variation in the prediction of the machine when we change different data set we train on. If we have a complex machine, as mentioned earlier, the machine will try its best to match every data in training data set. In other words, the machine memorized the trainining data and a little change in data set will cause significant variation in prediction.\n\\[ V = \\sum_{x \\in \\chi}p(x) E_D[ (f'(x|D) - f'm)^2 ] \\]\nBias-Variance dilemma Now it's time to decompose the average generalisation error. Let's plug the \\(f'_m(x)\\) into the previous equation\n\\[ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] \\\\ = E_D[\\sum_{x\\in \\chi}p(x) (f'(x|D) - f_m' + f_m' - f(x))^2] \\\\ = E_D[\\sum_{x\\in \\chi}p(x)\\{(f'(x|D) - f_m')^2 + (f_m' - f(x))^2 + 2(f'(x|D) - f_m')(f_m' - f(x)) \\}] \\]\nIt's noticeable that the cross-term will cancel out because \\(f'm\\) and \\(f(x)\\) are constants no matter what data set \\(D\\) is.\n\\[ E_D[\\sum_{x\\in \\chi}p(x)2(f'(x|D) - f_m')(f_m' - f(x))] = \\sum_{x\\in \\chi}p(x) (2E_D[f'(x|D)]-f'm)(f'm-f(x)) = 0 \\]\nTherefore, we are left with\n\\[ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi}p(x)(f'(x|D) - f_m')^2 + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2] \\\\ = \\sum_{x\\in \\chi}p(x) E_D[(f'(x|D) - f_m')^2] + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2 \\\\ = V + B \\]\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-01/",
                "title": "Linear Regression 01",
                "section": "post",
                "date" : "2021.04.14",
                "body": "There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and maybe it's the first algorithm that most of people learn in the world of machine learning. So let's go!\nProblem statement Suppose you are a teacher, and you record some data about the hours students spent on study and the grades they achieved. Then you want to predict the grade for given hours that someone spent. Here are some sample data you collected:\n   Hours 0.5 1 2 3 4     Grade 20 21 22 24 25    Since there are only two variables, let's plot them.\nFigure 1: The scatter plot of hours and grade\nWell, we can see that the variable \\(grade\\) is positive related to the variable \\(hours\\). For simplicity, we can use a simple line(the red line in this figure) to approximate this relationship. And this is exactly our first simple linear model.\nSimple Linear Regression Remember a line equation is written in this way:\n\\[ y = ax + b \\]\nIn this example, \\(x\\) is the variable \\(hours\\) and \\(y\\) is the variable \\(grade\\) , which we already know. So the problem is how to calculate the parameter \\(a, b\\). Technically, this is called parameter estimation. Usually, there are two ways to do this: minimising the loss and maximising likelihood. Now we focus on minimising the loss.\nLoss What is the loss? Basically, it's the error between the esitmated value and our true value. Minimising the error is simply to make the estimated value as close as possible to the true value.\nFigure 2: The error for a single data (Bradthiessen.com 2021)\nFor a single data point, the loss function is defined below, where \\(y\\) is the true value and \\(y'\\) is our estimated value for a given \\(a, b\\).\n\\[ error = y_i - y'_i = y_i - ax_i - b \\]\nSince we have many data points, we need to sum up them all to evaluate the overall errors.\n1. error Unfortunately, some error terms will cancel out when you do this calculation directly.\n\\[ L = \\sum_i^n (y_i - y'_i) = \\sum_i^n (y_i - ax_i - b) \\]\n2. the absolute value of error One way to tackle this is taking the absolute value of the error terms.\n\\[ L = \\sum_i^n |y_i - y_i'| \\]\nHowever, the absoulte value of \\(x\\) is not differentiable at \\(0\\).\n3. the squared value of error Instead of taking absolute value, we will square all the errors. One reason is that the errors will become larger and can be distinguished easily when squaring them. It looks like the errors are zoomed in and we can find them and minimize them quickly. It is also known as Residual Sum of Squares(RSS) or Sum of Squared Error (SSE)\n\\[ L = \\sum_i^n (y_i - y_i')^2 \\]\nPS: We will revisit the squared error later from the perspective of MLE.\nClosed-form solution Okay, finally we find a function to measure the loss. Next we need to find the parameters that minimize the squared error. Good news is that our loss function is differentiable and convex! It means that we have a global minimial value and can be calculated directly by taking derivatives.\nLet's take the first derivatve of \\(b\\)\n\\[ \\frac{\\partial L}{\\partial b} = \\sum_i^n -2(y_i - ax_i-b) \\]\nand then set this equation to \\(0\\),\n\\[ -2(\\sum_i^ny_i -a\\sum_i^nx_i - \\sum_i^nb) = -2(n\\overline y-an\\overline x - nb) = 0 \\\\ b = \\overline y-a\\overline x \\]\nLet's take the first derivatve of \\(a\\)\n\\[ \\frac{\\partial L}{\\partial a} = \\sum_i^n -2x_i(y_i - ax_i-b) \\]\nand then plug \\(b\\) into this equaiton and set this equation to \\(0\\) again,\n\\[ \\sum_i^n -2x_i(y_i - ax_i-\\overline y+a\\overline x) = \\sum_i^n -2x_i[(y_i-\\overline y)- a(x_i -\\overline x)]\\\\ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} \\]\nHere, we use a slight algebra trick,\n\\[ a\\sum_i^n(x_i - \\overline x_i) = 0 \\]\nThen we plug this into the previous equation\n\\[ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} = \\frac{\\sum_i^nx_i(y_i-\\overline y) - \\sum_i^n\\overline x(y_i - \\overline y)}{\\sum_i^nx_i(x_i -\\overline x) - \\sum_i^n\\overline x(x_i - \\overline x)}\\\\ = \\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2}\\\\ = \\frac{Cov(x, y)}{Var(x)} \\]\nFinally, we find the best estimators for simple linear regression.\nReference [1] Bradthiessen.com, 2021. [Online]. Available: https://www.bradthiessen.com/html5/docs/ols.pdf. [Accessed: 14- Apr- 2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/descriptive-statistics/",
                "title": "Descriptive Statistics",
                "section": "post",
                "date" : "2021.04.13",
                "body": "In this post , I'm going to go through some basic concepts of statistics required in Data Science.\nThere are two main branches of statistics :\n descriptive statistics tells us the statistics about data like mean, mode and standard deviation, which you've learned in high school.\n inferential statistics, on the other hand, uses a random dataset sampled from population to make inferences about population.\n  We will firstly focus on descriptive statistics, specifically, the central tendency and dispersion. Central tendency measures the center of the data while dispersion measures how spread out a given data is.\nCentral tendency Mean, mode and median are three mainly used measures of central tendency.\nMean Mean is the average of the data and calculated by summing up all data values and then dividing them by the number of data.\n\\[ \\overline x = \\frac{\\sum_i^nx_i}{n} \\]\nFor instance, say we have a group of data 1,2,3,4,5,6, the mean is 3.5.\nMedian Though mean is widely used, it's sensitive to outliers. Suppose you have a set of data 1,2,3,4,5,6,100, after some calculations, you find that the mean is 17.28. However, it seems a bit strange since most of the data values is less than 10 except one extreme value 100, which stretches the distribution of the whole data set to the right. This is why median comes.\nTo get the median, firstly we need to sort the data in ascending order and then find the middle number that separate the data into two groups with the same size. In this example, the median is 4.\nMode Mode is the most frequent value. There could be one, two or more data values that have the same frequency and that freqency is the highest.\nDispersion Range Range is the distance between the maximum value and the minimum value. Again, range is sensitive to outliers.\n\\[ r=max - min \\]\nQuantile Quantiles are used to divide data into several equal-sized groups. The most widely used cut points are 0, 25, 50, 75, 100, denoted by min, Q1, Q2, Q3, max respectively.\nIQR or interquartile range measures where the central 50% of the data is.\n\\[ IQR = Q3 - Q1 \\]\nIQR can be used to detect outliers. Data that is greater than upper boundary or less than lower boundary can be considered as an outlier.\n\\[ upper \\ boundary = Q3 + 1.5*IQR \\\\ \\]\n\\[ lower \\ boundary = Q1 - 1.5*IQR \\]\nVariance Deviation is the distance between a given data point and the mean. Since there are many data points, the variance calculates the average deviation from the mean.\nBut when you do this, you will always get a zero due to the definition of the mean.\n\\[ \\sum_i^n d_i = \\sum_i^n (x_i - \\overline x) = \\sum_i^nx_i - n\\overline x = n\\overline x - n\\overline x = 0 \\]\nFor simplicity, I omit the denominator n.\nOkay, let's ignore the sign and use the absolute value of the deviation.\n\\[ d_i = |x - \\overline x| \\]\nThough it works, the most popularly used method is calculate the square of the deviation.\n\\[ s^2 = \\frac{\\sum_i^n (x_i - \\overline x)^2}{n-1} \\]\nThough variance works fine, the value is a bit hard to interpret. For instance, we have 15 records of fish size measured in kilogram:\n[ 2.1, 2.4, 2.4, 2.4, 2.4, 2.6, 2.9, 3.2, 3.2, 3.9, 4.5, 6.3, 8.2, 12.8, 23.5 ].\nThe variance is 30.97. It means that if we randomly catch a fish, its weight would be 30.97 squared kilogram far away from the average weight. Emm...squared kilogram is an odd unit.\nStandard Deviation It's simple to solve this problem by taking the square root of the variance, which is standard deviation.\n\\[ s = \\sqrt{\\frac{\\sum (x - \\overline x)^2}{n-1}} \\]\nSo the standard deviation in the previous example is 5.56kg, which makes much sense.\nDistribution Skewness Skewness measures the symmetry of a distribution. It's quite common to have non-symmetric distributions.\n\nA left-/negative-skewed distribution\n it has a long left tail the mean is on the left of the median  A right-/positive-skewed distribution\n it has a long right tail the mean is on the right of the median  A skewed distribution implies that there are some special values that are larger/smaller than the common values. Let's see an example.\n\nFigure 3.2 shows the histogram of the fish sizes gathered from a fisherman. We can see that this distribution is right-skewed. The majority of fish sizes are between 0 and 3kg and there are a few special fishes that weigh over 3.5kg. It also can be seen that the average weight(1.67kg) is a bit higher than the median(1.62kg) shown in Table 3.2.\nReference [1] B. al., \u0026quot;Introduction to Statistics | Simple Book Production\u0026quot;, Courses.lumenlearning.com, 2021. [Online]. Available: https://courses.lumenlearning.com/introstats1. [Accessed: 14- Apr- 2021].\n"
            }
        
    
]