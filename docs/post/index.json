[
    
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/constrained-optimisation/",
                "title": "Constrained Optimisation",
                "section": "post",
                "date" : "2021.06.03",
                "body": "When I first learned machine learning, I was scared by the complicated formulas. Actually, I spent much time going over subjects like Linear Algebra and Calculus since I\u0026rsquo;d already forgotten them. But with time, I feel more and more confident in understanding them, though they sometimes still confuse me. Anyway, in my opinion, there is no need to know every detail about each equation, after all, we are not mathematicians. Instead, learning how to use these math formulas to solve real-wold problems is the key.\nAs we all know, the main effort in machine learning is to find a loss function and optimise it, i.e. find the miminum or maximum point, and this is the question of optimisation. However, we may only find local optimisation because of some constraints. Even without constraints, there is still chance that we can reach local optimisation only. From this, we can see that there are two main situations we need to consider: unconstrained optimisation and constrained optimisation. Furthermore, constrained optimisation can be divided into two parts: euqality constraints and inequality constraints. And today we will talk about all of them in detail.\nUnconstrained Optimisation TODO\nEquality Constraints Suppose we want to minimise a function $f(x)$ subject to an equality constraint, $g(x) = 0$. This can be solved by introducing Lagrange multiplier $\\alpha$\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nThen set the gradient of $\\mathcal{L}$ equal to the zero vector\n$$ \\nabla_x \\mathcal{L} = \\nabla_x f(x) - \\alpha \\nabla_xg(x) = 0 $$\n$$ \\frac{\\partial \\mathcal L}{\\partial \\alpha} = -g(x) = 0 $$\nFinally, solving the above equations will give us the minimum point we are seeking.\nMultiple Constraints If we have multiple constraints, then multiple Lagrange multipliers are introduced,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\sum_i^m \\alpha_i g_i(x) $$ And the corresponding solutions are given by,\n$$ \\nabla_x f(x) = \\sum_i^m \\alpha_i \\nabla_xg_i(x) $$\n$$ \\frac{\\partial \\mathcal L}{\\partial \\alpha_i} = -g_i(x) = 0 $$\nLagrange But what\u0026rsquo;s the rationale behind these formulas? Though we are not required to know everything, basic understanding of Lagrange is still needed. Remember that the question is to find a point $x^*$ that satisfies both $g(x^*) = 0$ and $f(x^*) =m $, where $m$ is the minimum value of $f(x)$ given the constraint.\nThe line represented by $f(x) = c$ is known as a contour line. Since $f(x)$ can have many values, we can plot many contour lines with equal intervals between lines in ascending or descending order from inner to outer. Graphically, we want to find a point that lies on the line of $g(x) = 0$ and the line of $f(x)$ with the minimum value simultaneously as shown in Figure 1. The green point is the point we are looking for.\n  Figure 1: Illustration of equality constraints  But we still need to figure out equations to calculate the position of the gree point. Let\u0026rsquo;s start from $x_0$, the magenta point in Figure 1. Mathematically, the above process of finding $x^*$ can be described as follows,\n$$ f(x_0 + \\delta x) \u0026lt; f(x_0) $$\n$$ g(x_0) = g(x_0 + \\delta x) = 0 $$\nSo in which direction should we move at $x_0$? With the aid of Taylor expansion, we have $$ g(x_0 + \\delta x) = g(x_1) = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) + \\frac{1}{2} (x_1 - x_0)^T H (x_1 - x_0) $$\n$$ = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) + O(||x_1 - x_0||^2) $$\nwhere $x_1 = x_0 + \\delta x$ and $H$ is a matrix of second derivative of $g(x)$ known as Hessian. The third term tends to be zero if $\\delta x$ is small enough, then we are left with\n$$ g(x_0 + \\delta x) = g(x_0) + (x_1 - x_0)^T\\nabla_xg(x_0) = g(x_0) $$\nThus,\n$$ (x_1 - x_0)^T\\nabla_xg(x_0) = 0 $$\nwhich means that the moving direction from $x_0$ should be perpedicular to $\\nabla_xg(x_0)$. But we are not done, because not all $(\\delta x = x_1 - x_0)$ point to the right direction along which $f(x)$ decreases. In order to ensure this, we require that $\\delta x$ must satisfy\n$$ (x_1 - x_0)^T ( - \\nabla_x f(x)) \u0026gt; 0 $$\nwhere $- \\nabla_x f(x)$ indicates the descent direction. Therefore, as long as the value of the dot product is greater than zero, the moving of point will continue unless the value becomes zero. If so, it means that the direction of $\\nabla_x f(x)$ is parallel to $\\nabla_x g(x)$, i.e.\n$$ -\\nabla_x f(x) = \\lambda \\nabla_x g(x) $$\nWe can rewrite this by replacing $\\lambda$ with $\\alpha = -\\lambda $\n$$ \\nabla_x f(x) = \\alpha \\nabla_x g(x) $$\nFinally, we come to the method of Lagrange multipliers.\nInequality Constraints Now we consider another situation where we have inequality constraints, i.e. $g(x) \\ge 0$. It looks a little complicated, but if we think about it for a while, we can find that only two things could happen, as shown in Figure 2,\n either the optimum point satisfies $g(x) \\gt 0$ or the (local) optimum point lies on the boundary, $g(x) = 0$    Figure 2: Two possible outcomes of inequality constraints  Again, we use Lagrange to solve it,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nIn the first case where the optimum point lies in the interior of the constraint, i.e. $g(x) \u0026gt; 0$, which is filled in red in Figure 2 a). We set $\\alpha = 0$, which means the constrains has no influence on $f(x)$.\nAs for the second case, it is exactly the same as the equality constraints, i.e.\n$$ \\nabla_x f(x) = \\alpha \\nabla_xg(x) $$\nbut with an additional constraint $\\alpha \u0026gt; 0$. So why do we set $\\alpha \u0026gt; 0$ here?\n$\\alpha \\ge 0$ Visually, it can be seen From Figure 2 b) that both the magenta and blue points seem to be the right point we are seeking. But in fact, only the bule one is in a lower position. And we find that $\\nabla_x f(x) $ and $\\nabla_x g(x)$ point to the same direction at the blue point. Thus, $\\alpha$ is positive.\nIn theory, the area defined by $g(x) \u0026gt; 0$ is known as feasible region, i.e. any value of $x$ inside this region is valid. Besides, We can see that $\\nabla_x f(x)$ (colored in blue) points in towards the interior of the region. Conversely, the negative gradient $-\\nabla_x f(x)$, which is the direction along which $f(x)$ decreases the most quickly, points away from the feasible region(I don\u0026rsquo;t plot it).\nIf we are at a point where $-\\nabla_x f(x)$ points to the feasible region, it means that a point with smaller value of $f(x)$ can be found in the feasible region, which is contradictory to the assumption that we can only move along the boundary of the region. In other words, this is not the optimal point.\nIf $-\\nabla_x f(x)$ at some point points to the exterior of the feasible region, then we are in the right position because the outer of the feasible region is invalid and we cannot move forward any further.\nIt is noticeable that $\\nabla_x g(x)$ points in towards the feasible region. Therefore, we conclude that $\\nabla_x f(x)$ and $\\nabla_x g(x)$ have the same direction. Thus, $\\alpha$ is positive.\nFrom above, we can also draw another conclusion shown below\n$$ \\alpha \\nabla_x g(x) = 0 $$\nKKT Conditions To sum up, we want to minimise $f(x)$ subject $g(x) \\ge 0$, and the Lagrangian function is defined as follows,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\alpha g(x) $$\nThen we can find a local mininum $x^*$ s.t.\n $\\nabla_x f(x) = \\alpha \\nabla_xg(x)$ $\\alpha \\ge 0$  $\\alpha = 0$, the solution is in the interior or $\\alpha \\gt 0$ and $g(x) = 0$, i.e. the solution is on the boundary   $\\alpha \\nabla_x g(x) = 0$  These are the Karush-Kuhn-Tucker (KKT) conditions.\nMany Inequalities Once again, if we have many ineuqality constraints, then Lagrangian function is given by,\n$$ \\mathcal{L}(x, \\alpha) = f(x) - \\sum_i^m \\alpha_i g_i(x) $$\nAnd the corresponding solutions are given by,\n$$ \\nabla_x f(x) = \\sum_i^m \\alpha_i \\nabla_xg_i(x) $$\nplus the constraints that\n either $\\alpha_i = 0$ or $\\alpha_i \\gt 0$ and $g_i(x) = 0$  Duality Convex Jensen’s inequality References  https://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/probabilistic-model/",
                "title": "Probabilistic Model",
                "section": "post",
                "date" : "2021.05.26",
                "body": "In the previous post Descriptive Statistics, we focused on summary statistics on a particular data set. However, in machine learning, we usually attempt to make inference, i.e. infer the unknown parameters from the given data. For unknown things, we use probability to describe its uncertainty. For inference, we use Bayes' rule to invert it into a forward process.\nProbability At the beginning, let\u0026rsquo;s have a quick refresh on probability. Conventionly, we use a capital letter, such as $X$ or $Y$, to represent a random variable. Suppose we have two random variables of interest, $X$ and $Y$,\n  The joint probability of $X$ that takes the value of $x$ and $Y$ that takes the value of $y$ is written as $P(X = x, Y=y)$, which means that the probability of $x$ and $y$ happening at the same time\n  Given $Y=y$, the conditional probability of $X$ given $Y=y$ is denoted by $P(X|Y=y)$\n  There are two major rules of probability that we should remember,\n the sum rule, i.e. the marginal probability of $X$ that takes the value of $x$, irrespective of the value of $Y$  $$ P(X=x) = \\sum_{y \\in Y} P(X=x, Y=y) $$\n the product rule, i.e. the joint probability of $X$ and $Y$ can be written as the product of the conditional probability and the marginal probability  $$ P(X, Y) = P(Y|X) P(X) = P(X|Y)P(Y) $$\nFrom the above formula, we can deduce the following equation, which is also known as Bayes' Rule,\n$$ P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)} $$\nBayes' Inference In machine learning, our goal is to learn a model $\\Theta$ from a given data set $D$, but $\\Theta$ is uncertain. One way to measure uncertainty is probability, so the question becomes how to compute the probablity of $\\Theta$ given the data $D$, i.e. $P(\\Theta|D)$. But the only thing we know is the observed data, so how can we do it? The answer is the Bayes' Rule, which helps us convert this into a forward problem, where parameters are known and thus we can draw data from the distribution determined by that paramaters. The formula is defined as follows,\n$$ P(\\Theta|D) = \\frac{P(D|\\Theta)P(\\Theta)}{P(D)} $$\n $P(\\Theta)$ is called the prior probability, i.e. the best guess about $\\Theta$ before we see the data. You might have a good estimate on it or simply have no idea at all $P(D|\\Theta)$ is the likelihood of the data given the parameters $\\Theta$ $P(D)$ is the evidence or marginal probability denoted by $P(D) = \\sum_{\\theta \\in \\Theta }P(D, \\theta)$ $P(\\Theta|D)$ is the posterior probability, i.e. the updated probability of $\\theta_i$ after we see the data  Pros and cons Pros\n Bayes' rule provides us a full probabilistic description of the parameters It doesn\u0026rsquo;t overfit since we are not choosing the best parameters that fit the data perfectly  Cons\n However, we need to compute $P(D)$, whichsometimes is not reasonable, e.g. there are too many possible values of $\\Theta$ Posterior might not be described as a nice probability function  MAP If we ignore the evidence $P(D)$ (after all, it\u0026rsquo;s just a constant for any $\\theta$), we are only left with the numerator. An easy way to compute $P(\\Theta|D$) is to find the maximum value shown below, though it\u0026rsquo;s not a strictly probability\n$$ {\\argmax}_{\\Theta} log (P(D|\\Theta)) + log (P(\\Theta)) $$\nThis method is called Maximum A Posterior(MAP). However, it can overfit because we are finding the parameters that maximise the likelihood of the observed data. Thus, we are likely to get a model that fit the data with no errors.\nMLE Furthermore, if we ignore the prior $P(\\Theta)$, the MAP is just maximising the likelihood(MLE), which is widely used statistics in machine learning.\nConjugate Prior In some cases, the likelihood of the observed data $D$ is simple to compute, and the posterior would have the same form as the prior if we could find a right prior. Such a likelihood and prior distribution are said to be \u0026lsquo;conjugate\u0026rsquo;. Here we consider two common distributions that a prior might follow: Bernoulli and Poisson.\nBernoulli Distribution Suppose we have a binary random variable $X \\in \\{0, 1 \\}$, and $X_i=1$ if the ith trial is a head and 0 otherwise. Then the likelihood of $X_i$ given the probability of a head $\\mu$ is\n$$ P(X_i = 1|\\mu) = \\mu $$\n$$ P(X_i = 0|\\mu) = 1 - \\mu $$\nOr we can write it in this form\n$$ P(X_i|\\mu) = \\mu^{X_i} (1-\\mu)^{1-X_i} $$\nSuppose we have a data set $ D = \\{ x_1, x_2, \u0026hellip;, x_n \\}$, where $x_i \\in \\{0, 1 \\}$, assuming these observations are drawn independently, then the likelihood of $D$ can be computed as follows,\n$$ L(D;\\mu) = \\prod_{i=1}^N P(X_i|\\mu) = \\prod_{i=1}^N \\mu^{X_i} (1-\\mu)^{1-X_i} = \\mu^{N_h} (1-\\mu)^{N-N_h} $$ where $N_h = \\sum_i X_i$, i.e number of heads.\nBeta The next step is to choose the prior $P(\\Theta)$. In the case of Bernoulli, it would be better if we can find a function that has a simliar exponential parts that appeared in the above likelihood function to model the probability of every single value of $\\mu$. Luckily, Beta distribution shown below is the right function we are looking for.\n$$ P(\\mu) = Beta(\\mu|a, b) = \\frac{\\mu^{a-1} (1-\\mu)^{b-1}}{B(a, b)} $$\nwhere $B(a, b)$ is a normalisation constant\n$$ B(a, b) = \\int_0^1 \\mu^{a-1} (1-\\mu)^{b-1} d\\mu $$\nSo how to choose a, b? It depends. If we have no idea about $\\mu$, it\u0026rsquo;s natural to assume that there are equal chances to take all vaules of $\\mu$. This corresponds to a beta distribution with $a = b = 1$.\nThe last step is to plug the prior and likelihood into the Bayes' rules,\n$$ P(\\mu|D) = \\frac{P(D|\\mu)P(\\mu)}{P(D)} = \\frac{\\mu^{N_h} (1-\\mu)^{N-N_h} \\mu^{a-1} (1-\\mu)^{b-1}}{P(D) B(a, b)} = \\frac{\\mu^{N_h+a-1} (1-\\mu)^{N+b-N_h-1}}{P(D) B(a, b)} $$\nand\n$$ P(D) = \\int_0^1 \\frac{\\mu^{N_h+a-1} (1-\\mu)^{N+b-N_h-1}} {B(a, b)} d\\mu = \\frac{B(N_h +a, N+b-N_h)}{B(a, b)} $$\nSo we have\n$$ P(\\mu|D) = Beta(\\mu| N_h + a, N+b-N_h) $$\nIn summary, before we see data, we have some beliefs about $\\mu$ governed by $Beta(\\mu|a, b)$. After seeing the data, the probability of $\\mu$ now is updated via $Beta(\\mu| N_h + a, N+b-N_h)$, which can be served as the prior for the next new observations.\nIncremental Updating For independent data we can update the hyperparamters incrementally, we consider an individual data at a time so that,\n$$ P(\\mu|X_1) = \\frac{P(X_1|\\mu)P(\\mu)}{P(X_1)} $$\n$$ P(\\mu|X_2, X_1) = \\frac{P(X_2, X_1|\\mu)P(\\mu)}{P(X_2, X_1)} = \\frac{P(X_2|\\mu)P(X_1|\\mu)P(\\mu)}{P(X_2)P(X_1)} = \\frac{P(X_2|\\mu)P(\\mu|X_1)}{P(X_2)} $$\nIt\u0026rsquo;s clear that the previous posterior now becomes the prior for the next piece of data.\nPoisson Distribution Poisson distribution measures the probability of a given number of events occuring in a specific time range, which is given by,\n$$ Pois(N;\\theta) = \\frac{e^{-\\theta}\\theta^N}{N!} $$\nwhere $N$ is the number of occurences in a time slot and $\\theta$ is the parameter of interest. For example, we want to know the rate of traffic along a road between 8am and 9am, so $N$ is the number of cars and $\\mu$ is the rate of traffic per hour.\nGamma Then we have Gamma distribution as our prior,\n$$ P(\\theta) = \\Gamma(\\theta|a, b) = \\frac{b^a \\theta^{a-1} e^{-b\\theta}}{\\Gamma(a)} $$\nso the posterior after seeing the first piece of data is\n$$ P(\\theta|N_1) = \\frac{P(N_1|\\theta)P(\\theta)}{P(N_1)} = \\frac{b^a}{\\Gamma(a) N_1! P(N_1) }e^{-(b+1)\\theta} \\theta^{N+a-1} \\propto e^{-(b+1)\\theta} \\theta^{N_1+a-1} $$\nwe can see that the posterior is also a Gamma distribution with $a_1 = a + N_1$ and $b_1 = b + 1$.\nMultinominal Distribution In the case of Bernoulli, we only consider two outcomes: 0 or 1. But what if we have 3 or more outcomes? Well, we use multinominal distribution, which is the generalization of the binominal distribution. Suppose we have a $k$-sided dice with the probability of $\\mu_k$ for $x^k = 1$, and we roll the dice $N$ times, so there are $N$ independent observations $x_1, x_2, \u0026hellip;, x_n$, the multinominal distribution are given by,\n$$ M(m_1, m_2, \u0026hellip;, m_k|\\mu, N) = \\frac{N!}{m_1!m_2!\u0026hellip;m_k!} \\prod_{k=1}^K \\mu_k^{m_k} $$\nwhere $m_k=\\sum_n^Nx_n^k$ represents the number of $x_n^k = 1$.\nDirichlet TODO\nDiscriminal vs Generative Models Take the problem mentioned in the introduction as an example, suppose we have 2 classes, \u0026lsquo;cat\u0026rsquo; and \u0026lsquo;dog\u0026rsquo;, and we want to know which class the new image belong to. The problem is to find the probability $P(C=cat|x)$ and $P(C=dog|x)$, and then we classify the image into the class with the largest probability.\nUsually, we think of our observations as given and the predictions as random variables, and we model the target variable as a function of the predictors. This is known as discriminal model. In this example, we could use logistic regression to make a classification, and the corresponding probability can be computed using a sigmoid function\n$$ P(C=cat|X)=\\frac{1}{1 + e^{-(wx + b)}} $$\nHowever, we can also consider both features and target variables at the same time. Using the Bayes' rule below, we can calculate $P(Y|X)$ in another way,\n$$ P(C=cat|X) \\propto P(X, C=cat)= P(X|C=cat)P(C=cat) $$\nThis is known as generative model, where we use Bayes' rule to turn $P(X|Y)$ into $P(Y|X)$.\nIn discriminal model, we don\u0026rsquo;t need the prior of classes, so there are less parameters to be determined. Also, it might have a better performance if the estimate for that prior is far away from the true distribution.\nGraphical Models Given 2 random variables, $X$ and $Y$, they could be dependent directly or not. Even if they are not dependent directly, typically there is still correlation between them. If they are correlated, then\n X could affect Y Y could affect X X has no direct effect on Y, but they could be both affected by another random variable Z  We can describe the above relationships using a graph, where each node represents a random variable and the links between nodes show that relationship. The above three relationships are captured by the following figure,\n  Figure 1: Directed graphical models representing the above three conditions   Such a graph is known as Bayesian networks where we use a directed graph to show causal relationships between random variables. Specifically, we add directed links between $X$ and $Y$ if $X$ directly influences $Y$ shown in the left graph of Figure 1.\nConditional Independence The left and middle graphs of Figure 1 are easy to understand since they only have two variables, and they are denoted by $P(Y|X), P(X|Y)$ respectively. However, the last one with three variables, $X, Y, Z$, is a bit tricky. Let\u0026rsquo;s first consider the conditional distribution of $X$ given Z and Y, we have\n$$ P(X|Z, Y) = P(X|Z) $$\nIf we further consider the joint distribution of X and Y conditioned on Z, then we have\n$$ P(X, Y|Z) = \\frac{P(X, Y, Z)}{P(Z)} = \\frac{P(Z) P(Y|Z)P(X|Z, Y)}{P(Z)} = P(X|Z)P(Y|Z) $$\nThis is called conditional independence, which means that X and Y are statistically independent, given Z. From the view of a graphical model, we can see that there is no direct link between X and Y shown in the right graph of Figure 1.\nLatent Dirichlet Allocation Now let\u0026rsquo;s learn a topic modeling method that utilises the knowledge we\u0026rsquo;ve talked about so far, Latent Dirichlet Allocation(LDA). Note this is not linear discriminant analysis, which is also abbrivated to LDA. Here, LDA is an unsupervised learning method that is used to model topics within a set of documents. Speaking of \u0026lsquo;topic\u0026rsquo;, it means that when you find a group of words occuring many times in an article, such as \u0026lsquo;banana, apple, fruit, vegetable\u0026rsquo;, you will relate them to an area, like \u0026lsquo;Food\u0026rsquo; in this case.\nLatent In LDA, documents are represented as a fixed group of topics, which are unknown as latent variables. And these topics are characterized by a small specific words. For example, words like \u0026lsquo;teacher, student, school, exam, marks\u0026rsquo; should occur more frequently in the area of Education than topics like Sports. This means different topics have different word distribution, as shown in Figure 2.\n  Figure 2: Word distribution varies in different topics  If a document contains more words like \u0026lsquo;teacher, school\u0026rsquo;, it\u0026rsquo;s likely to be identified as \u0026lsquo;Education\u0026rsquo;. But it could contain other topics. For example, if this is a document regarding a sports contest held in a school, then it\u0026rsquo;s much likely to be associated with \u0026lsquo;Sports\u0026rsquo; rather than \u0026lsquo;Education\u0026rsquo;. Thus, we can see that a document can also contain different topics with different weights, i.e. each document has its own topic distribution.\nFrom above, we know that a document consists of a group of topics, and each topic has its special words. To generate a document, we can randomly choose N topics for N words in a document and then randomly choose a word from the corresponding topic. Of course, such an article contains little meaning since we ignore the order and semantics of words. But it\u0026rsquo;s reasonable for LDA since LDA treats documents just as a bag of words(BOW).\nDirichlet So the topic-word distribution and doc-topic distribution are the unknown parameters we want to find, but there are many possible values for them. Again, we need a right prior to describe this uncertainty of the values, but which prior should we use?\nRemember that we draw a topic from $K$ topics for each word in a document with $N$ words, it means there are $K$ possbile outcomes available for each word, and we repeat this process $N$ times, so it\u0026rsquo;s a multinominal distribution. And this is true for drawing a word from that topic with $V$ words. Furthermore, we\u0026rsquo;ve known that the cojugate prior of the multinominal distribution is Dirichlet distribution, so Dirichlet distribution is chosen as our prior, where\n doc-topic distribution follows $\\theta^d \\sim Dir(\\alpha)$ topic-word distribution follows $\\phi^t \\sim Dir(\\beta)$  Allocation To sum up, the whole process of generating a document in LDA is illustrated in Figure 3,\n  Figure 3: Graphical model for LDA  The figure looks a bit scary, well, let\u0026rsquo;s start with some notations first,\n $M=|D|$, the number of documents  $D={d_1, d_2, \u0026hellip;, d_M}$, where $d_i$ represents $i_{th}$ document   $V=|W|$, the number of vocabulary appeared in all documents  $W={w_1, w_2, \u0026hellip;, w_V}$, where $w_i$ represents $i_{th}$ word   $K = |T|$, the number of topics  $T={t_1, t_2, \u0026hellip;, t_K}$, where $t_k$ represents $k_{th}$ topic   $N_{d}$, the number of words in a document $d$ $\\bold w^{d}={w_1^{d}, w_2^{d}, \u0026hellip;, w_{N_{d}}^{d}}$, where $w_i^{d}$ represents $i_{th}$ word in a document $d$ $\\theta^d$ is a probability vector, which represents the distribution of topics in a document $d$  $\\Theta = (\\theta^d|d \\in D)$   $\\phi^t$ is a probability vector, which represents the distribution of words associated with a topic $t$  $\\Phi = (\\phi^t|t \\in T)$   $\\bold t^{d} = t_1^{d}, t_2^{d}, \u0026hellip;, t_{N_{d}}^{d}$, where $t_i^{d}$ represents a topic drawn from $\\theta^d$  From Figure 3, the joint distribution of the hidden and observed variables for a single document is given by,\n$$ p(\\bold t^d, \\bold w^d, \\theta^d, \\Phi|\\alpha, \\beta) = P(\\theta^d|\\alpha) P(\\Phi|\\beta) P(\\bold t^d, \\bold w^d|\\theta^d, \\Phi) = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P( t_n^d, w_n^d|\\theta^d, \\Phi) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d |\\theta^d, \\Phi) P(w_n^d|t_n^d, \\theta^d, \\Phi) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\n$$ = P(\\theta^d|\\alpha) \\prod_t^TP(\\phi^t|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\nOur goal is to maximise this equation, but before that we need to eliminate the hidden variable $\\bold t^d$,\n$$ p(\\bold w^d, \\theta^d, \\Phi|\\alpha, \\beta) = \\int_{t^d} P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\n$$ = P(\\theta^d|\\alpha) P(\\Phi|\\beta) \\prod_n^{N_d} \\sum_{t_n^d=t_1}^{t_K} P(t_n^d|\\theta^d) P(w_n^d|\\phi^{t_n^d}) $$\nThere are two common ways to compute this: Gibbs sampling and variational inference.\nGibbs Sampling TODO\nVariational inference TODO\nReferences  http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/  "
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/semanticweb/",
                "title": "Semantic Web",
                "section": "post",
                "date" : "2021.05.08",
                "body": "In semester 2, I took a module called Semantic Web Technologies. There are some reasons why I choose this module. At first glance, the name itself sounds not appealing. Actually, it does. But it\u0026rsquo;s still on my shortlist because I\u0026rsquo;ve been working on the web for many years and I wondered what the semantic web was.\nOn the other hand, I was not interested in modules like Advanced Databases since I\u0026rsquo;ve learnt database during my undergraduate degree in Computer Science. Besides, I hope I could have more free time to do other things, and I don\u0026rsquo;t think I can deal with a more difficult module like Reinforcement Learning though it\u0026rsquo;s indeed useful and interesting. Maybe I could learn it later when needed.\nOkay, let\u0026rsquo;s go back to this post. The goal of this post is to go through the most important parts of this module for the final exam preparation.\nWeb for machine We all know that the current web pages are written in HTML. You can use \u0026lt;p\u0026gt; to define a paragraph like this.\n\u0026lt;p\u0026gt;Hello world\u0026lt;/p\u0026gt; HTML looks more like a natural language as it is designed to be readable and understandable for human, not machines. However, we hope machines can understand information as well. But why? Because we want machines to do reasoning about data rather than save data only.\nWeb pages can be linked using the markup \u0026lt;a\u0026gt;, but not all relevant documents are connected. Anyone can publish a new web page anytime. Moreover, not everyone uses the same language to describe the same thing. Thus, the interconnected web pages are simply a limited bundle of documents. More importantly, the data are not connected, and we don\u0026rsquo;t have explicit schemas that enable machines to understand documents.\nOn the contrary, the goal of Semantic Web is to make data readable and understandable for machines. To achieve this, the first step is to encourage people to share their data. Then things are denoted by their unique identifiers. Thus, data from various contexts can be connected through the identifiers, making a huge net finally (also known as Linked Data). Therefore, in Semantic Web, machines learn from data directly instead of documents that are written for people.\nWhat are the applications of Semantic web? Well, we can utilise the vocabulary of a domain to build a knowledge graph, which encodes the semantics of domain knowledge in this network. Search engines can augment their search results from such knowledge graphs since they provide domain-specific knowledge.\nRDF RDF is short for the Resource Description Framework. It\u0026rsquo;s a triple-based data model for knowledge representation. Figure 1 shows a triple composed of 3 components, which means Bob has a friend named Alice.\n  Bob is the subject\n  Alice is the object\n  hasFriend is the predicate that connects them\n    Figure 1: A triple-based data model  The above model is an abstract framework. Subjects and predicates can be anything. To describe the specific subjects and the relationship between them, we need to find a way to identify them explicitly. We also need a data format that defines and parses this model.\nURI In the World Wide Web, URI is used to define a unique object. Here are some examples,\nhttps://www.example.com mailto:aaa@xxx.com urn:isbn:123456 Well, URI can also be followed by an optional fragment identifier like this,\nhttps://www.example.com/index.html#introduction Namespace The above method has one problem. A term may have different meanings in different domains. If we include them all in a file, it will cause name conflicts. We can solve it using namespace. Specifically, a resource can be represented by a namespace, a colon and a local name.\n\u0026lt;http://example.org/ontology#hasFriend\u0026gt; // is equivalent to @prefix d: \u0026lt;http://example.org/ontology#\u0026gt; . d:hasFriend Turtle The last thing is to define a data structure so that machines can parse and serialize. One of the popular data formats is Turtle. The following are some syntaxes defined by Turtle,\n Resource URI are written in angle brackets Literal values are written in double quotes Triples end with a full stop  \u0026lt;http://example.org/data#Bob\u0026gt; \u0026lt;http://example.org/ontology#hasFriend\u0026gt; \u0026lt;http://example.org/data#Alice\u0026gt; . RDF/XML Apart from Turtle, we can also express RDF using RDF/XML, which is similar to XML. But it\u0026rsquo;s not easy for humans to read compared to Turtle.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;rdf:RDF xmlns:rdf=\u0026#34;http://www.w3.org/1999/02/22-rdf-syntax-ns#\u0026#34; xmlns:ns0=\u0026#34;http://example.org/ontology#\u0026#34;\u0026gt; \u0026lt;rdf:Description rdf:about=\u0026#34;http://example.org/data#Bob\u0026#34;\u0026gt; \u0026lt;ns0:hasFriend rdf:resource=\u0026#34;http://example.org/data#Alice\u0026#34;/\u0026gt; \u0026lt;/rdf:Description\u0026gt; \u0026lt;/rdf:RDF\u0026gt; Tools The following are some useful tools for understanding RDF.\n EasyRDF Converter  Tool for converting between different RDF syntaxes   W3C RDF Validator  Validator for RDF/XML, which can also visualise RDF as a graph    Generally, we are required to know how to read them and write some simple statements in this module.\nRDFS RDF only tells us some specific instances. For example, Bob knows Alice, Bob works for Google, etc. However, they don\u0026rsquo;t define the vocabulary used in those triples. From the perspective of object-oriented programming, we want to define the classes that generate these instances. Strictly speaking, we want to define the ontology of a domain. Moreover, if we define some rules in this domain, say Person worksFor Company, then we can infer that Bob is a person and Google is a company under the defined rule.\nRDFS and OWL are two common ontology languages to do this. We will talk RDFS first in this section. RDFS allows us to define classes and properties as well as the characteristics of classes and properties.\nclass definition In OOP, we use keyword class to declare a class, for example,\nclass Person {} In RDFS, we use predefined classes, such as rdfs:Class, rdfs:Property, to declare the corresponding object.\nex:Person rdf:type rdfs:Class .   Figure 2: Declare a class  Furthermore, we use rdfs:subClassOf to declare that a class is a subclass of another class. It\u0026rsquo;s clear that rdfs:subClassOf is transitive. For example, if Teacher is a subclass of UniStaff and UniStaff is a subclass of Person , then Teacher is a subclass of Person, as shown in Figure 3.\nex:Teacher rdf:type rdfs:Class ; rdf:subClassOf ex:UniStaff . ex:UniStaff rdf:type rdfs:Class ; rdf:subClassOf ex:Person .   Figure 3: RDFS class semantics   On the other hand, rdf:type distributes over rdf:subClassOf. This means that if C rdf:type A and A rdfs:subClassOf B , then C rdf:type B. For instance, Figure 4 illustrates that if Bob is an instance of Teacher, then Bob is also an instance of Unistaff and Person.\n  Figure 4: rdf:type distributes over rdfs:subClassOf  property definition Property declaration is similar to class declaration. The following code declares a property named ex:hasStreet.\nex:hasStreet rdf:type rdfs:Property . But not every object has an addresss. Besides, we sometimes want to limit the domain and range of a property. In this case, we would say, only people can have an address.\nex:hasStreet rdf:type rdfs:Property ; rdf:domain ex:Person . We also notice that ex:hasStreet is a more specific attribute of ex:hasAddress since addresses are usually composed of several components like towns and streets. So we define ex:hasStreet as a subproperty of ex:hasAddress.\nex:hasStreet rdf:type rdfs:Property ; rdf:domain ex:Person ; rdf:subPropertyOf ex:hasAddress . Figure 5 illustrates that if Bob lives in a street named West Road, it implies that West Road is Bob\u0026rsquo;s address. Furthermore, we have already stated that only Person can have street names, which means Bob must be a person under this semantics.\n  Figure 5: RDF Schema property semantics   SPARQL So far, we are able to represent data. But how do we retrieve it from databases? Well, SPARQL is a SQL-like query language that allows us to make a query.\nbasic syntax SELECT \u0026lt;variable\u0026gt; FROM \u0026lt;graph\u0026gt; WHERE { \u0026lt;triple patterns\u0026gt; }  variables are prefixed with ? triple patterns are expressed in Turtle  For example, the following codes return the names of the people who have street addresses.\nSELECT ?name WHERE { ?name ex:hasStreet ?street . } FILTER We can also query the names of the people who live on the streets only containing \u0026lsquo;Baker\u0026rsquo;.\nSELECT ?name WHERE { ?person ex:hasStreet ?street . FILTER regex(?street, \u0026#34;Baker\u0026#34;) ?person ex:hasName ?name . } Another example is to filter all the items whose prices are lower than 5 pounds.\nSELECT ?name WHERE { ?item ex:hasPrice ?price . FILTER (?price \u0026gt;= 5) ?item ex:hasName ?name . } UNION UNION allows us to either match this condition or that one, which is equivalent to the operation of OR\nSELECT ?name WHERE { { ?person ex:hasStreet \u0026#34;Baker Street\u0026#34; . } UNION { ?person ex:hasStreet \u0026#34;West Road\u0026#34; . } ?person ex:hasName ?name . } We won\u0026rsquo;t dive into SPARQL deeper because SPARQL has quite similar syntaxes to SQL. If you are familiar with SQL, you\u0026rsquo;ll know how to use SPARQL instantly.\nDescription Logic (DL) TODO\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/svd/",
                "title": "Singular Value Decomposition",
                "section": "post",
                "date" : "2021.05.04",
                "body": "Singular Value Decomposition(SVD) is an important concept in Linear Algebra. Any matrix can be decomposed into the multiplication of three matrices using SVD. In machine learning, SVD is typically used to reduce dimensionality. Another popular dimension reduction technique is PCA. We will cover both of them in this post.\nChange of Basis Suppose there is a point in the 2D space, how do you describe it? The common way is to use the Cartesian coordinate system, which is composed of two fixed perpendicular oriented axes, measured in the same unit of length. The two perpendicular axes are just a special set of vectors served as the basis of the 2D space. Actually, there are many other sets of vectors that can be the basis for the 2D space. For example, in Figure 1, the position of the red point is (-4, 1) when using the standard basis (1, 0), (0, 1) (colored in grey). If we change the basis to (2, 1), (-1, 1) (colored in blue), the position is (-1, 2).\n  Figure 1: The same point with different coordinates in two different coordinate spaces  From Figure 1, we can see that the absolute position of the red point always stay the same. However, the relative positions to the different bases are different. Mathematically, the red point can be described from the perspective of basis as follows,\n$$ x = P_b[x]_b = c_1 \\bold b_1 + c_2 \\bold b_2 + \u0026hellip; + c_n \\bold b_n $$\n$$ P_b = [\\bold b_1, \\bold b_2, \u0026hellip; , \\bold b_n ] $$\n$$ [x]_b = [c_1, c_2, \u0026hellip; c_n] $$\nwhere\n $[x]_b$ is a set of scalars, which represent the length of projection onto each axis of the current coordinate system $P_b$ is the corresponding basis of the current coordinate system  Let\u0026rsquo;s plug the above point and the basis (1, 0), (0, 1) (colored in grey) into the equation,\n$$ P_b = [ (1, 0), (0, 1)] $$\n$$ [x]_b = (-4, 1) $$\n$$ x_b = -4 \\begin{bmatrix}1\\\\ 0 \\end{bmatrix} + 1 \\begin{bmatrix}0\\\\ 1 \\end{bmatrix} = \\begin{bmatrix}-4\\\\ 1 \\end{bmatrix} $$\nLet\u0026rsquo;s do the same calculation with another basis (colored in blue),\n$$ P_b = [ (2, 1), (-1, 1)] $$\n$$ [x]_b = (-1, 2) $$\n$$ x_b = -1 \\begin{bmatrix}2\\\\ 1 \\end{bmatrix} + 2 \\begin{bmatrix}-1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix}-4\\\\ 1 \\end{bmatrix} $$\nAs expected, they yield the same result. And the second one essentially changes the basis of $R^2$ from (2,1),(-1,1) to(1,0),(0,1), which is the standard basis of $R^2$.\nActually, this example is a special case of the change of basis, where the new basis is the standard basis. More generally, $P_{c \\larr b}$ is known as **the change of coordinate matrix from the old basis $b$ to the new basis $c$, which we are going to switch to** in $R^n$.\nSay we are in the basis $b$ and $[x]_b$ is known, the corresponding coodinates of $x$ under the new basis $c$ can be computed as follows,\n$$ x_c = P_{c \\larr b} x_b $$\nSince $P_{c \\larr b}$ is invertible, we have\n$$ (P_{c \\larr b})^{-1}x_c = x_b $$\nwhich is the inverse operation of change of basis from $b$ to $c$. We can generalize this to any number of points and dimensions.\n$$ A=US\\\\(D,N)= (D, M) \\times (M, N) $$\n$$ U^{-1} A = S\\\\(M, D) \\times (D, N) = (M, N) $$\nwhere\n $A$ is a $D\\times N$ matrix with $D$ dimensions and $N$ points $S$ is a $M\\times N$ matrix with $M$ dimensions and $N$ points described in a new vector space decided by another basis $U$ is the change of coordinate matrix from $S$ to $A$ $U^{-1}$ is the the change of coordinate matrix from $A$ to $S$  If we do some transformation for a point in the standard coordinate system, what\u0026rsquo;re the new coordinates of the same point in another system? This problem can be solved by the following equation,\n$$ x_s' = U^{-1}TUx_s $$\nwhere $T$ represents the transformation matrix. If $T=I$, $x_s'$ is exactly $x_s$.\nSVD SVD is a technique in linear algebra that can be used to decompose any $N \\times P$ matrix\n$$ X = U S V^T $$\n $U$ is an $N \\times N$ orthogonal matrix, where the columns of $U$ are the eigenvectors of $XX^T$ $S$ is an $N \\times P$ diagonal matrix whose diagonal entries are the sorted singluar values, which are square roots of eigenvalues of $XX^T$ or $X^TX$ $V$ is a $P \\times P$ orthogonal matrix, where the columns of $V$ are the eigenvectors of $X^TX$  $$ C = X^TX = (USV^T )^T USV^T = VS^TU^T USV^T = VSS^TV^T $$\n$$ D = XX^T = USV^T (USV^T )^T = USS^TU^T $$\n$$ =\u0026gt; C [\\bold v_1, \\bold v_2, \u0026hellip;, \\bold v_p] = [\\lambda_1 \\bold v_1, \\lambda_2\\bold v_2, \u0026hellip;, \\lambda_p \\bold v_p] $$\n$$ =\u0026gt; D [\\bold u_1, \\bold u_2, \u0026hellip;, \\bold u_n] = [\\lambda_1 \\bold u_1, \\lambda_2\\bold u_2, \u0026hellip;, \\lambda_n \\bold u_n] $$\nTherefore, $V$ and $U$ are matrices of eigenvectors for $X^TX$ and $XX^T$.\nIf we look at the formula of SVD from the view of \u0026lsquo;the change of basis\u0026rsquo; discussed above, we will find that\n $V^T$ is the change of matrix from, say basis A, to the standard basis $S$ is a scaling matrix $U$ is another change of matrix from the standard basis to basis A  Geometrally, the multiplication between a matrix $A$ and a vector $x$ represents a linear transformation, where we using another basis of $R^n$ to represent the same point and $A$ is the change of matrix between bases. By decomposing $A$, we can clearly see that this transformation is composed of three transformations:\n $ V^T$: rotation $S$: scaling $U$: rotation  Economical Forms of SVD Since $S$ is an $r \\times r$ diagonal matrix for some $r$ not exceeding the smaller of $N$ and $P$, we could simplify $S$ and the correspoding $V$ and $U$. Specifically, for $X$ with more samples than features $N \u0026gt; P$ (tall and thin), we ignore the last $N - P$ columns of U\n and for $X$ with more features than examples $N \u0026lt; P$ (short and fat), we ignore the last $P - N$ rows of $V^T$\n Linear Regression Revisited In the previous post Linear Regression 02, we introduced pseudo-inverse $A^+$\n$$ \\bold {\\hat w} = A^+ \\bold y = (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y = V (S^TS)^{-1}S^TU^T \\bold y = VS^+U^T \\bold y $$\nwhere the elements of $S^+$ are the reciprocal of the singular values,\n$$ S^+ = (S^TS)^{-1}S^T = \\begin{bmatrix}s_1^{-1}\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ 0\u0026amp;s_2^{-1} \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ \u0026hellip; \\\\ 0\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; s_p^{-1} \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\end{bmatrix} $$\nThis means if any of the singular values of $X$ are small, then $S^{-1}$ will magnify component in that direction. Thus, little change in $\\bold y$ will lead to a greatly different model and eventually a poor generalization.\nOne way to tackle this is regularisation. Ridge Regression is one variant of linear regression by adding a regulariser $\\lambda ||\\bold w||^2$ to the loss function,\n$$ L = ||\\bold X \\bold w - \\bold y||^2 + \\lambda ||\\bold w||^2 $$\nThe estimate $\\bold w$ is given by\n$$ \\bold {\\hat w} = V(S^TS + \\lambda I)^{-1} S^TU^T \\bold y $$\nwhere\n$$ (S^TS + \\lambda I)^{-1} S^T = \\begin{bmatrix}\\frac{s_1}{s_1^2+\\lambda}\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ 0\u0026amp;\\frac{s_2}{s_2^2+\\lambda} \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\\\ \u0026hellip; \\\\ 0\u0026amp;0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; \\frac{s_p}{s_p^2+\\lambda} \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \u0026amp; 0\\end{bmatrix} $$\n if $s_i = 0$, then $\\frac{s_i}{s_i^2 + \\lambda} =0$ and the \u0026lsquo;inverse\u0026rsquo; is defined if $s_i \u0026laquo; \\lambda$, then $\\frac{s_i}{s_i^2 + \\lambda} \\simeq \\frac{1}{\\lambda }$ if $si \u0026raquo; \\lambda$, then $\\frac{s_i}{s_i^2 + \\lambda}\\simeq s_i^{-1}$  Therefore, adding a regulariser can make our model much more stable.\nPCA Principal component analysis(PCA) is often used to reduce dimentionality. The idea of PCA is to find directions along which data has the largest variation. The variation can be computed by projecting data onto that direction. Mathematically, we want to find a vector $v$ with $||v||=1$ to maximise\n$$ \\sigma^2 = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 $$\nThere are two things to notice here:\n $v$ is an unit vector since we are care about the direction only data are centralized first for simple computation; centralizing data doesn\u0026rsquo;t change the distribution of data  We can solve the above equation by introducing Lagrange multiplier\n$$ L = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 - \\lambda (||v||^2 - 1) $$\n$$ = \\frac{1}{n-1}\\sum_i^n \\bold v^T (\\bold x_i - \\bold \\mu)(\\bold x_i - \\bold \\mu)^T\\bold v - \\lambda (||v||^2 - 1) $$\n$$ = \\bold v^T \\bold C \\bold v - \\lambda (\\bold v^T\\bold v - 1) $$\nwhere $\\bold C$ is the covariance matrix of $X$. Then we take the derivative of $L$ w.r.t $\\bold v$ , and then set it to $0$\n$$ \\frac{\\partial L}{\\partial \\bold v} = 2(C \\bold v - \\lambda \\bold v) = 0 $$\nso $\\bold v$ is the eigenvector of $\\bold C$, and the variance along this direction is,\n$$ \\sigma^2 = \\frac{1}{n-1}\\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 = \\bold v^T \\bold C \\bold v = \\lambda \\bold v^T \\bold v = \\lambda $$\nProperties of Covariance Matrix We know that the quadratic form of a vector and a matrix is defined as\n$$ x^T M x $$\nThus, the quadratic form of $C$ is\n$$ x^TCx = x^T XX^Tx=u^Tu \\ge 0 $$\nso $C$ is positive semi-definite, which means all eigenvalues of $C$ are greater than or equal to zero. Why? Suppose $\\mu$ is an eigenvector of $C$, since $\\mu^TC\\mu\\ge0$ and $||\\mu||\u0026gt;0$, then we have\n$$ \\mu^TC\\mu = \\mu^T \\lambda \\mu =\u0026gt; \\lambda = \\frac{\\mu^TC\\mu }{||\\mu||^2 } \\ge 0 $$\nzero eigenvalue What if $C$ has a zero eigenvalue? Well, a zero eigenvalue means that there is no variation in the direction of the corresponding eigenvector. So when will we have zero eigenvalues? Based on the definition of eigenvalue, we have\n$$ Cx = 0x = 0 $$\nSince $x$ is nonzero vector, $C$ is singular or non-invertible. And this will inevitably happen if the number of features is much more than the number of examples. Conversely, if $C$ is invertible, it has no zero eigenvalues and is said to be positive definite (since all eigenvalues are greater than 0).\nGeometry of PCA Since $C$ is a $p \\times p$ symmetric matrix, it can be orthogonally diagonalized as $C = PDP^T$, where $P$ is an orthogonal matrix and $D$ is a diagonal matrix. Thus, the above formula can also be written as follows,\n$$ \\sum_i^n(\\bold v^T (\\bold x_i - \\bold \\mu))^2 = \\sum_i^n \\bold v^T (\\bold x_i - \\bold \\mu)(\\bold x_i - \\bold \\mu)^T\\bold v = \\bold v^T C \\bold v $$\n$$ = \\bold v^T PDP^T \\bold v = \\bold y^T D \\bold y $$\nwhere $\\bold y = P^T \\bold v$. Mathematically, $\\bold v^T C \\bold v$ and $\\bold y^T D \\bold y$ yield the same result, which represents the sum of deviation of each sample from the original point along the direction determined by this vector as shown in Figure 2.\n  Figure 2: Change of variable in $x^T A x$ (Introduction to Linear Algebra[1])  Geometrically, $P^T_{y \\larr v}$ is the change coordinate matrix from $v$ to $y$, which finally transform the shape of the quadratic form $\\bold v^TC\\bold v$ into the standard position, such as the shapes in the figure below.\n  Figure 3: An ellipse and a hyperbola in standard position (Introduction to Linear Algebra[1])  Projection For a data set with $p$ features, we want to reduce its dimension to $k$, there are a few steps to follow\n  Construct a $p\\times p$ covariance matrix $C$ using centralized data\n  Find all the eigenvectors $v_i$ and eigenvalues $\\lambda_i$ of $C$\n  Construct a $p \\times k$ projection matrix $P$ with $k$ eigenvectors determined by the $k$ largest eigenvalues (principal components)\n  Project the original data into the space spanned by the principal components\n  $$ \\bold z = P^T(\\bold x - \\bold \\mu) $$\nwhere $\\bold z$ is our new inputs.\nUsually, there are two common ways to find the eigenvalues:\n eigendecomposition SVD  Eigen-decomposition follows the above steps, and we can see it from the following code,\ndef pca(X): # Data matrix X, assumes 0-centered P, N = X.shape # Compute covariance matrix, where X is a P by N matrix C = np.dot(X, X.T) / (N-1) # Eigen decomposition eigen_vals, eigen_vecs = np.linalg.eig(C) # Project X onto eigen space X_pca = np.dot(eigen_vecs.T, X) return X_pca, eigen_vals, eigen_vecs Instead, SVD decomposes $X$ directly. Besides, we don\u0026rsquo;t need to centralize data first in SVD, though most people will do.\ndef svd(X): N, P = X.shape # In practice, we usually subtract the mean from data and then perform SVD X_c = X - np.mean(X, axis=0) # the columns of Vt are the eigenvalues of X^TX U, Sigma, Vt = np.linalg.svd(X_c) # Project X onto eigen space X_pca = np.dot(X, Vt.T) return U, Sigma, Vt So why do we use SVD? Simply put, If the number of features are much more than the number of example, i.e. $P\u0026raquo;N$, it\u0026rsquo;s not easy to compute eigenvalues using eigen-decomposition. But in SVD, the algorithm will compute eigenvalues by choosing the smaller of two matrice $X^TX$ and $XX^T$. In this case, $X^TX$ has less elements( $N * N$ ) than $XX^T$($ P * P$). More details can be found in the following section \u0026lsquo;PCA for image\u0026rsquo;.\nReconstruction From the perspective of projection, the projection onto a vector $\\bold v_j$ can be seen as approximating the inputs by\n$$ \\hat {\\bold x_i} = \\bold \\mu + \\sum_j^k z_j^i \\bold v_j $$\n$$ z_j^i = \\bold v_j^T(\\bold x_i - \\bold \\mu) $$\nSo our goal is to minimize the error\n$$ E[ ||\\hat {\\bold x_i} - \\bold x_i ||^2] = \\frac{1}{n} \\sum_{n=1}^n ||\\sum_{i=1}^k \\bold u_i^T\\bold x_n \\bold u_i + \\sum_{i=k+1}^p \\bold u_i^T\\bold x_n \\bold u_i - \\sum_{j=1}^k \\bold v_j^T\\bold x_n \\bold v_j = \\frac{1}{n} \\sum_{n=1}^n \\sum_{i=k+1}^p ||\\bold u_i^T\\bold x_n \\bold u_i||^2 $$\n$$ = \\frac{1}{n}\\sum_{n=1}^n\\sum_{i=k+1}^p (\\bold u_i^T\\bold x_n) \\bold u_i^T \\cdot (\\bold u_i^T\\bold x_n) \\bold u_i =\\frac{1}{n} \\sum_{n=1}^n\\sum_{i=k+1}^p (\\bold u_i^T\\bold x_n)^2 $$\n$$ = \\frac{1}{n} \\sum_{n=1}^n\\sum_{i=k+1}^p \\bold u_i^T\\bold x_n \\bold x_n^T \\bold u_i= \\sum_{i=k+1}^p \\bold u_i^T (\\frac{1}{n} \\sum_{n=1}^n\\bold x_n \\bold x_n^T )\\bold u_i $$\n$$ \\sum_{i=k+1}^p \\bold u_i^T S \\bold u_i = \\sum_{i=k+1}^p \\lambda_i \\bold u_i^T \\bold u_i = \\sum_{i=k+1}^p \\lambda_i $$\nWe can see that the error is exactly the sum of the eigenvalues in the directions that are discarded.\nPCA for images Suppose we have an image with the size of $256 \\times 256$, so it has nearly $64K$ pixels or features. Then we could create a covariance matrix $C$ with more than $4\\times10^9$ elements using PCA. However, this huge matrix is not easy to compute eigenvalues. To make this problem tractable, we usually work in a dual space instead of a feature space. The dual space we choose is spanned by $n$ vectors, which are exactly the sample images. Specifically, if we have $n$ images, the subpace of $R^n$ has at most $n-1$ dimensions, and usually $n$ is much smaller than $p$. But how do we find the eigenvalues of $C$ in this dual space?\nWe\u0026rsquo;ve known that $C = XX^T$ is a $p\\times p$ matrix, where $X$ is a $p \\times n $ matrix. Now we construct another matrix $D=X^TX$ with $n \\times n$ elements, which is also a symmetric matrix. Suppose $v$ is the eigenvalue of $D$, then we have\n$$ Dv = \\lambda v $$\n$$ XX^TXv = \\lambda Xv $$\n$$ CXv = \\lambda Xv $$\n$$ Cu = \\lambda u $$\nwhere $u = Xv$. We find that $C$ and $D$ has the same eigenvalues. Thus, we can use the dual $n \\times n$ matrix $D$ to find eigenvalues and eigenvectors of $C$.\nFind K Components The last question is how to decide the number of components. Instead of guessing the number of dimensions that we want to keep, we choose the right $k$ components along which the sum of the explained variance ratio is greater than a threshold. The explained variance ratio of each component indicates how much the variance explained along component. In Sklearn, it can be accessed via explained_variance_ratio_.\nfrom sklearn.decomposition import PCA pca = PCA(n_components=2) X2D = pca.fit_transform(X) pca.explained_variance_ratio_ The following code shows how to find $k$ components with a variance ratio of $0.95$ using Sklearn.\nfrom sklearn.decomposition import PCA pca = PCA() pca.fit(X_train) cumsum = np.cumsum(pca.explained_variance_ratio_) d = np.argmax(cumsum\u0026gt;0.95) + 1 # or simply pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_train) pca.n_components_ Also, we can plot the explained variance ratio as a function of the number of dimensions, and the elbow in the curve is where the appropriate $k$ lies.\n  Figure 4: Explained variance as a function of k (Hands-on machine learning, 2019)  References [1]\tG. Strang, Introduction to Linear Algebra, 5th ed. Wellesley, MA: Wellesley-Cambridge Press, 2016.\n[2] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/end2end-project-01/",
                "title": "An E2E Project - EDA",
                "section": "post",
                "date" : "2021.04.25",
                "body": "So far, we have discussed many algorithms, such as linear regression and ensemble methods. It\u0026rsquo;s time to kick off a project from scratch to learn how a real machine learning project works.\nOverview The book \u0026ldquo;Hands-on Machine Learning\u0026rdquo; has summarised the below 8 steps as a guidance to do an end-to-end machine learning project,\n Frame the problem Get the data Explore the data Prepapre the data Explore many different models Fine-tune the model Deploy the system  In this post, we will cover the first 3 parts. The complete code can be found here.\nFrame the problem The first step is to define you problem. People are unlikely to do things without reasons, right?. So ask yourself, what problem do you want to solve? Why are you interested in them? What\u0026rsquo;s your objective? Are there any solutions out there?\nAs an example, we are going to predict used car prices because accurate prices prediction can help both buyers and sellers. For buyers, it can ensure the money that customers invest on used cars to be worthy. For used car dealers, they might want to know which factors influence car prices most so as to adjust sales strategy and offer a better prediction to customers. Therefore, there is a necessity for building a used car price prediction system.\nGet the data There are many ways to get the desired data. You can write a web crawler to download the data from related websites or you can get data freely from some public data platforms. Among them, Kaggle is one of the most popular platforms that enables us to achieve our data science goals. In this example project, the data is downloaded from Used Cars Dataset - Kaggle.\n!kaggle datasets download -d austinreese/craigslist-carstrucks-data -p data !unzip data/craigslist-carstrucks-data.zip -d data EDA EDA stands for Exploratory Data Analysis, which is an important part throughout the project. The goal of EDA is to get insights from the data so as to clean and prepare data for building models. Generally speaking, it involves the following steps,\n Identify variables Examine the quality of the data Univariate analysis Bivariate analysis  Identify variables Before any further analysis, we need to have a look at the data generally. More specifically, you need to answer the following questions,\n  How many observations and variables do you have?\n  Which variables are your predictors and target?\n  What\u0026rsquo;s the data type and memory usage?\n  What\u0026rsquo;s the descriptive statistics about the data?\n  Luckily, Pandas provides convenient functions to answer these questions.\ndf_vehicles.shape df_vehicles.info() df_vehicles.describe() df_vehicles.head() The following table shows the distribution of data types and Figure 1 shows some example data.\n   Data Type Variable Name     Numerical (5) id, price, odometer, lat, long   Object (20) \u0026lsquo;url\u0026rsquo;, image_url, region_url, \u0026lsquo;manufacturer\u0026rsquo;, model, \u0026lsquo;condition\u0026rsquo;, \u0026lsquo;cylinders\u0026rsquo;, \u0026lsquo;fuel\u0026rsquo;, \u0026lsquo;title_status\u0026rsquo;, \u0026lsquo;transmission\u0026rsquo;, \u0026lsquo;drive\u0026rsquo;, \u0026lsquo;type\u0026rsquo;, \u0026lsquo;paint_color\u0026rsquo;, \u0026lsquo;region\u0026rsquo;, \u0026lsquo;state\u0026rsquo;,posting_date, description, image_url, size, VIN,      Figure 1: Some example data  In our example, we have 458,213 rows and 25 attributes. The variable price is our target variable and the remaining are our predictors. However, not all of them are related to car prices. For instance, url, image_url, region_url, id, post_date, and VIN have nothing to do with car prices. Thus, we need to remove them.\nExamine data quality The raw data is unlikely to use directly because it\u0026rsquo;s inevitable to introduce errors like duplication, null values and extreme values when collecting data. The higher the data quality is, the better the model performance is. So we need to identify and solve these problems to obtain a clean data set. Below are some common techniques to check whether we have redundant data, missing values and outliers.\n1) duplication # return true if we have duplicate rows df.duplicated() # remove the duplicated rows, keep the first row by default df.drop_duplicates() Fortunately, there are no duplicated rows for now. But it depends.\n2) missing value Heatmap enables us to identify the distribution of missing values visually.\nsns.heatmap(df.isna())   Figure 2: A heatmap for visualising the distribution of null values  Figure 2 shows that size, condition, VIN, cylinders, drive and paint_color have a significant number of null values. To quantify the number of missing values, we can use tables shown below.\n  Figure 3: A table that shows the percentage of null values  3) outlier In the previous post Descriptive Statistics, we talked about the descriptive statistics like mean, variance, and range. These values can be obtained easily using the following code,\ndf_vehicles.describe()   Figure 4: A table that shows descriptive statistics  From Figure 4, it can be seen that the lowest car price is 0 while the highest car price is up to 3,600,000,000. The mean of the car prices is greater than the median, which means the distribution of prices is not symmetrical and there are abnormal values in prices. And the same goes for odometer. To verify our belief further, we can plot boxplots for year and odometer shown in Figure 5.\n  Figure 5: Boxplots for year and odometer  Okay, let\u0026rsquo;s put it all together. From the above initial examination, we can conclude that,\n There are 458,213 rows and 25 attributes in our data set, and price is our target variable. There are some useless columns to be dropped, such as url, image_url, region_url, id, post_date, and VIN . So far, there are no redundant rows. But we have many variables like size that contain a great deal of missing values and two variables ( price and odometer ) that have extreme values.  Univariate analysis In univariate analysis, we will look at variables one by one. The statistics and visualization methods depend on the data types. Typically, we divide data into 2 types: numerical variable and categorical variable.\n1) numerical variables For numerical variables, we measure the central tendency and dispersion of the data, which are discussed earlier. To visualize data, we can use histogram, boxplot, or other suitable charts. You might find that this is also exactly what we did before for detecting missing values and outliers.\n   measurement statistics     central tendency mean, median, mode   dispersion range, quantile, variance, skewness   visualization histogram, boxplot, bar chart    For example, Figure 6 shows the distribution of the variable year. It can be seen that most cars were made after 2000. Besides, this distribution has a long left tail, which means the mean is lower than the median. From Figure 4, we can find that the mean is 2010 while the median is 2013. This is because the feature year contains some extreme small values.\n  Figure 6: The distribution of the year  2) categorical variables For categorical variables, we usually plot bar charts to understand the distribution of each category, as shown in Figure 7.\n  Figure 7: Bar charts for some categorical variables  Bivariate analysis Analyzing one predictor seems a bit monotonous. In fact, we are more interested in the relationships between predictors and our goal. Generally speaking, we can do conduct an analysis\n between numerical and numerical between numerical and categorical between categorical and categorical  1) numerical vs numerical Scatter plots provide a nice way to find the relationships between continuous variables. It\u0026rsquo;s easy to plot them with the help of seaborn. From Figure 8, we can see that there is a negative relationship between odometer and car prices, which means the car prices will decrease as the odometer increases.\n  Figure 8: The relationship between odometer and price  However, scatter plot cannot tell us how strong this relationship is. The solution is to calculate correlation. Figure 9 shows that the car prices have a moderate positive relation with year and a negative relation with odometer. On the contrary, latitude and longitude have a weak relation with car prices.\n  Figure 9: The correlation among continuous variables  2) categorical vs numerical For categorical features, we group data by category and compare them side by side using boxplot or line chart. Basically, we just divide the whole data into several groups and the following analysis is the same as we did before.\nPS: Aside from visualisation methods talked above, we can also do a statistic test like Chi-square test to find out whether there is a statistically significant relationship between variables. We will talk about this later.\nConclusion In summary, we introduced some common data analysis techniques to gain insights from your data in this post. Hopefully it can give you an idea of how to explore data. If you\u0026rsquo;d like to explore more excellent analysis methods and visualizations, Kaggle is a great place to enhance your skills.\nAs for the remaining parts, we will talk about them in the following posts.\nReferences [1] https://www.kaggle.com/austinreese/craigslist-carstrucks-data\n[2] https://www.scribbr.com/statistics/statistical-tests/\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/ensemble-methods/",
                "title": "Ensemble Methods",
                "section": "post",
                "date" : "2021.04.20",
                "body": "Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.\nOverview In real life, we often take advices from others. For example, suppose you want to know whether a movie is worthwhile to watch, you may ask your friends who\u0026rsquo;ve watched to give the movie a score(out of 5), say 3, 3, 2, 2, 2, 4. Since 5 people gave a score that is lower than 4, you may think about choosing another movie, let\u0026rsquo;s say Avatar. Then you ask your friends again and have some scores like this 5, 5, 5, 5, 5. Wow! All of your friends think that Avatar is an amazing movie that should certainly not be missed. You agree with their opinons and decide to watch Avatar finally. From this example, we can see that gathering plenty of opinions from different people are likely to make an informed decision.\nHere, I highlight the words different people. It makes little sense if we only asks for people who have the same interests. Therefore, the more diverse the people are, the more sensible our decisions are. Basically, the idea behind it is the wisdom of collaborating.\nVoting The method used to decide whether to watch a movie or not in this example is known as voting. For classification, there are 2 types of voting named hard voting and soft voting.\n Hard voting returns the most popular class shown in Figure 1. Soft voting averages the probability of each class and then return the class that has the maximum probability.  Hard Voting   Figure 1: Hard voting classifier predicitons (Hands-on machine learning, 2019)  Figure 1 can also be illustrated in a mathematical way,\n$$ y' = mode(C_1(x), C_2(x), \u0026hellip;, C_n(x)) $$\nFor example, {0, 1, 0, 1, 1} are the class labels predicted by our 5 different classifiers for a data point $x$. By hard voting, the final class label is class 1 .\nC1 -\u0026gt; 0 C2 -\u0026gt; 1 C3 -\u0026gt; 0 C4 -\u0026gt; 1 C5 -\u0026gt; 1 Weighted Hard Voting Hard voting works nice, but in some cases, some people might be more professional than others. Hence, their opinions are much more significant. How to distinguish professionals and common people?\nWe assign weights to them. Specifically, we assign higher weights to professionals while common people have lower weights. Then we calculate weighted sum of occurrence of each class label and find the class label that has the maximum value.\n$$ y' = \\operatorname*{argmax}_i w_j\\sum_j [C_j == i] $$\nwhere $[C_j == i] = 1$ if classifier $j$ predicts class label i and 0 otherwise.\nFor example, if we assign the following weights to the previous 5 classifiers, then we will have 0.7 for class 0 and0.5 for class 1. Thus, class 0 wins because 0.7 is greater than 0.5.\n0.4, C1 -\u0026gt; 0 0.1, C2 -\u0026gt; 1 0.3, C3 -\u0026gt; 0 0.2, C4 -\u0026gt; 1 0.2, C5 -\u0026gt; 1 Soft Voting Instead of predicting the class label directly, some classifiers like logistic regression can predict the probability of each class label that $x$ belongs to. Then we simply average these probabilities for each class label. Certainly, you can assign weights to classifiers.\n$$ y' = \\operatorname*{argmax}_i \\frac{1}{n} \\sum_j^n w_j p_{ij} $$\nwhere $p_{ij}$ is the probability of class label $i$ that $x$ belongs to when using classifier $C_j$.\nAverage for Regression We simply average the predictions of different machines for a regression task.\n$$ y' = \\frac{1}{n} \\sum_j^n w_j C_j $$\nBagging In order to make our models different from each other, we use various algorithms to train the same data, as discussed above. Another way to have a set of diverse models is to train the same model on different data sets. But usually we only have one training data set. Where do other data sets come from? Well, they are sampled with replacement from the original data set, which is known as bootstrapping.\n  Figure 2: The process of bagging (Hands-on machine learning, 2019)  Specifically, given a training data set $D=(x_i, y_i)_i^n$ of the size $N$, we build an ensemble model of size $m$ according to the following steps:\n  For $i=1, 2, 3, \u0026hellip;, m$\n draw $N'(N' \\le N)$ samples with replacement from $D$, which is denoted by $D^*_i$ build a model (e.g. decision tree) $T^*_i$ based on $D_i^*$    For an unseen data, aggregate the predictions of all $T^*$\n perform a majority vote for classification average the predictions for regression    from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier ensemble_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=300, max_sample=100, bootstrap=True) Random Forest Bagging can be used for any models. Among them random forest is the special one. As its name suggests, it is exclusively designed for decision trees. Besides, it introduces extra randomness when growing trees.\n  Figure 3: A random subset of features at each split for each tree (Reference [2])  Specifically, it randomly choose a subset of $m'$ of the features at each split instead of using all features shown in Figure 3. By doing so, all trees can have much different training data set further, so they are less similar to each other, which results in more significant predictions.\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier from sklearn.tree import DecisionTreeClassifier random_forest_clf = BaggingClassifier(splitter=\u0026#39;random\u0026#39;, DecisionTreeClassifier(), max_leaf_nodes=16,n_estimators=300, max_sample=100, bootstrap=True) ## is equivalent to this random_forest_clf=RandomForestClassifier(n_estimators=300, max_leaf_nodes=16) Extra-Trees TODO\nWhy Bagging works Take random forests as an example, each decision tree is a machine learned from a data set. Based on the theory of bias and variance, we know that the mean meachine can be expressed as $f'_m=E_D[f'(x|D)]$. Thus, $f'(x|D)$ can be interpreted as a random variable $X$, and $f'_m$ can be described as $\\mu = E(X)$.\nSince we have $N$ decision trees in a random forest, there are $N$ random variables $X_i$, where $\\mu = E(X_i)$. We can construct a new random variable $Z = \\frac{1}{n}\\sum_i^nX_i$, which represents the mean of $n$ random independent variables $X_i$.\nThe expected value of $Z$ is given by\n$$ E[Z] = E[ \\frac{1}{n}\\sum_i^nX_i ] =\\frac{1}{n} E[\\sum_i^nX_i] = \\frac{1}{n} nE[X_i] = \\mu $$\nThe variance is\n$$ E[(Z - E[Z])^2] = E[(\\frac{1}{n}\\sum_i^nX_i - \\mu)^2] = \\frac{1}{n^2} E[(\\sum_i^n (X_i - \\mu))^2] $$\n$$ = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2 + \\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] $$\nSince $X_i$ is independent of $X_j$ ( $i\\ne j$ ),\n$$ E[\\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] = 0 $$\nwe are left with\n$$ E[(Z - E[Z])^2] = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2] = \\frac{1}{n} \\sigma^2 $$\nwhere $E[(X_i-\\mu)^2]=\\sigma^2$.\nFrom the above euqation, we can see that ensemble methods reduce variances as $n$ increases when our models are uncorrelated.\nOn the contrary, if our models are correlated with the correlation coefficient $\\rho$\n$$ \\rho = \\frac{E[(X_i - \\mu)(X_j - \\mu)]} {\\sqrt{\\sigma^2(X_i)}\\sqrt{\\sigma^2(X_j)}} $$\nThe variance is\n$$ E[(Z - E[Z])^2] = \\frac{1}{n} \\sigma^2 + \\frac{n-1}{n} \\rho \\sigma^2 = \\rho \\sigma^2 + \\frac{(1 - \\rho)}{n} \\sigma^2 $$\nAs $n$ increases, the second term vanishes and we are left with the first term. Therefore, if we want the ensemble methods to do well, we need our models to be uncorrelated.\nRandom forests do a good job because of both the randomness of training data sets sampled via bootstrapping and the randomness of features considered at each split. The algorithm results in much less correlated trees, which trades a higher bias for a lower variance, generally yielding better results.\nFeature Importance Like decision tree, random forests can tell us the relative importance of each feature. It is measured by calculating the sum of the reduction in impurity over all the nodes that are split on that feature.\nrandom_forest_clf.feature_importances_ Boosting In boosting, we combine a group of weak learners into a strong learner.\nWhat\u0026rsquo;s the weak learners? They are the learning machines that do a little better than chance. Thus, they have high bias but low variance. The goal of boosting is to reduce the bias by combining them.\nHow to construct a weak learner? Well, one of the widely used types of weak learner are very shallow trees, for example, the stump with only one depth.\nAdaBoost and GradientBoost are two popular algorithms for boosting. Let\u0026rsquo;s lookt at AdaBoost first.\nAdaBoost AdaBoost is a classic algorithm for binary classification. Suppose we have a data set $D=(x^i,y^i)_i^N$ with $y^i \\in {-1, 1}$, and our weak learner $h(x)$ provides a prediction $h(x^i) \\in {-1, 1 }$. The goal of AdaBoost is to construct a strong learner by combining all the weak learners, which can be written as a weighted sum of weak learners,\n$$ C_n(x) = \\sum_i \\alpha_i h_i(x) $$\nHow to find $\\alpha_i$ and $h_i(x)$? Well, it\u0026rsquo;s difficult to find all the coefficients for one time, so we will solve this equation greedily,\n$$ C_n(x) = C_{n-1}(x) + \\alpha_n h_n(x) $$\nwhere $C_{n-1}(x)$ is the current ensemble model that fit the training data best and $h_n(x)$ is the weak learner we are going to add.\nExponential Loss The second step is to find an appropriate loss function to optimize. How do we measure the performance of a classification model? One of the most widely used loss functions is 0-1 loss,\n$$ L = \\sum_i^N [y_i \\ne y'_i] $$\nwhere $[y_i \\ne y'_i] = 1$ if $x_i$ is classified incorrectly and 0 otherwise. However, it\u0026rsquo;s not-convex and difficult to optimize shown in Figure 4. In AdaBoost, we use exponential loss.\n$$ L = \\sum_i^n e^{-y^iC_n(x^i)} $$\nFrom Figure 4, it can be seen that data that are classified correctly have lower value while misclassfication observations have much larger values, which means exponential loss punishes examples classified incorrecly much more than correct classifications.\n  Figure 4: Exponential loss in AdaBoost  Intuition To minimize the loss, we plug the previous $C_n(x)$ into the loss function\n$$ L = \\sum_i^n e^{-y^i (C_{n-1}(x^i) + \\alpha_n h_n(x^i))} = \\sum_i^n e^{-y^i C_{n-1}(x^i)} e^{-y^i \\alpha_n h_n(x^i)} =\\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} + \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} $$\n$$ = \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} - \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} $$\n$$ = e^{-\\alpha_n} \\sum_i^n w_n^i + (e^{\\alpha_n} - e^{-\\alpha_n}) \\sum_{y^i\\ne h_n(x^i) }^n w_n^i $$\nThen we find that minimizing the loss is equivalent to minimizing the sum of weights of each data that $h_n(x)$ misclassified, and that the value of weights depend on the current ensemble model $C_{n-1}(x)$.\n$$ \\sum_{y^i\\ne h_n(x^i) }^n w_n^i = \\sum_{y^i\\ne h_n(x^i) }^n e^{-y^i C_{n-1}(x^i)} $$\n If the data misclassified by $C_{n-1}(x)$ are still classified incorrectly by $h_n(x)$, then $w_n^i$ is extremely large. If the data classified correcly by $C_{n-1}(x)$ are misclassified by $h_n(x)$, then $w_n^i$ is small.  Simply put, misclassified data points will get high weights while correctly classified data points will get their weights decreased.\nTherefore, we are finding some weak learner that tries to correct the errors the previous learners made. Furthermore, we also notice that we need to update $w_n^i$ for the next weak learner $h_{n+1}(x)$,\n$$ w_{n+1}^i = e^{-y^i C_{n}(x^i)} = e^{-y^i (C_{n-1}(x^i) + \\alpha_nh_n(x^i))} = w_n^i e^{-y^i\\alpha_nh_n(x^i)} $$\nSo the new weight of each data depends on the last weight of that data, the weight of the previous weak learner $h_n(x)$ and itself. But wait, what\u0026rsquo;s the initial weight of each data? We simply initialize weights $w_1^i = \\frac{1}{N}$ for every training sample.\nOkay, now we are only left with $\\alpha_n$. To find $\\alpha_n$, we take the derivative of $L$ with respect to $\\alpha_n$\n$$ \\frac{\\partial L}{\\partial \\alpha_n} = e^{\\alpha_n} \\sum_{y^i\\ne h_n(x^i)}^n w_n^i - e^{-\\alpha_n} \\sum_{y^i= h_n(x^i) }^n w_n^i $$\nThat is\n$$ \\alpha_n = \\frac{1}{2} In\\frac{\\sum_{y^i= h_n(x^i) }^n w_n^i}{\\sum_{y^i\\ne h_n(x^i) }^n w_n^i} $$\nAlgorithm Let\u0026rsquo;s put it all together. The algorithm of AdaBoost can be summarised as below,\n  Given a data set $D=(x^i,y^i)_i^N$ with $y^i \\in \\{-1, 1\\}$ and a group of weak learners $h(x)$ of size $T$\n  Associate a weight $w_1^i = \\frac{1}{N}$ with every data point $(x^i, y^i)$\n  For $t = 1$ to $T$\n Train a weak learner $h_t(x)$ that minimises $\\sum_{y^i\\ne h_t(x^i) }^n w_t^i $ Update the weight of this learner, $\\alpha_t = \\frac{1}{2} In\\frac{\\sum_{y^i= h_t(x^i) }^n w_t^i}{\\sum_{y^i\\ne h_t(x^i) }^n w_t^i}$ Update weights for each training point, $w_{t+1}^i = w_t^i e^{-y^i\\alpha_th_t(x^i)}$    Make a prediction\n $C_n(x) = sign[\\sum_t^T \\alpha_t h_t(x)]$    Example Step 1: Initialisation\nHere we have 8 rows with 3 predictors chest_pain, blocked_arteries and weight and 1 target variable heart_disease. Each data point is initialised with an equal weight 0.125.\n  Figure 5: Toy data from \u0026#39;StatQuest with Josh Starmer\u0026#39;  Step 2: Find the weak learner\nHere, we use stump as our weak learner and Figure 6 shows the first optimal tree where we only misclassified one observation.\nfrom sklearn import tree X = df.drop([\u0026#39;heart_disease\u0026#39;, \u0026#39;weights\u0026#39;], axis=1) y = df[\u0026#39;heart_disease\u0026#39;] clf = tree.DecisionTreeClassifier(max_depth=1) clf = clf.fit(X, y) tree.plot_tree(clf.fit(X, y)) plt.show()   Figure 6: The first stump  Step 3: Update weights\nSince we have only one misclassification, the error rate is 1/8 and $\\alpha_1$ is 0.97. Then we update weights for each data point using $\\alpha_1$.\ndef cal_alpha(error): return 0.5*np.log((1 - error)/error) alpha_1 = cal_alpha(1/8) correct_samples = df[clf.predict(X) == y] df.loc[clf.predict(X) == y, \u0026#39;weights\u0026#39;] = correct_samples[\u0026#39;weights\u0026#39;] * np.exp(-alpha_1) misclassified_samples = df[clf.predict(X) != y] df.loc[clf.predict(X) != y, \u0026#39;weights\u0026#39;] = misclassified_samples[\u0026#39;weights\u0026#39;] * np.exp(alpha_1) print(alpha_1, df)   Figure 7: Updated weigts after training the first stump  Step 4: Go back to Step 2 until the desired number of learners is reached.\nAdjusted impurity Recall that Gini Index is written as\n$$ Q_m^g(L) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nand entropy discussed in Decision Tree as\n$$ Q_m^e(L) = -p_c(L)logp_c(L) $$\nwhere $p_c(L)$ is the fraction of the observations belong to class $c$. In order to use the weight of each data in AdaBoost, we need to change it slightly.\n$$ p_c(L) = \\frac{\\sum_{x^j \\in C} w_n^j I[y_j == C]}{\\sum_{x^i \\in L} w_n^i} $$\nWhy this works? Remember that the lower the impurity is, the better the split is. And a higher fraction leads to a lower impurity or entropy.\n If we classify the misclassified example in the node $L$ correctly, then the denominator of $p_c(L)$ becomes smaller and then $p_c(L)$ becomes larger. On the contratry, if this split works so bad, then we will have many observations that classified incorrectly, resulting in smaller $p_c(L)$ due to a small numerator and large denominator.  Thus, we are finding the best split that can correctly classify the examples that previous learners failed as much as possible.\nReferences [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2]\tT. Yiu, “Understanding random forest - towards data science,” Towards Data Science, 12-Jun-2019. [Online]. Available: https://towardsdatascience.com/understanding-random-forest-58381e0602d2. [Accessed: 23-Apr-2021].\n[3]\tJ. Rocca, “Ensemble methods: bagging, boosting and stacking - Towards Data Science,” Towards Data Science, 23-Apr-2019. [Online]. Available: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205. [Accessed: 23-Apr-2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/decision-tree/",
                "title": "Decision Tree",
                "section": "post",
                "date" : "2021.04.19",
                "body": "The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as \u0026lsquo;Is it a fiction movie? Is it directed by David Fincher?\u0026rsquo;\n  Figure 1: The process of movie selection  From Figure 1, we can see that a decision tree builds a binary tree to partion the data. Each node is a decision rule based on a feature and the tree can grow endlessly. Two questions then arise:\n How is the decision rule made? How deep is the decision tree?  Decision Rule Since there are so many features and each feature can have different values, we have to loop over all of them and find the best split. Well, how to measure the performance of a split? There are 3 measures widely used — Gini Impurity, Entropy, and Information Gain.\nGini Index Gini Index, also known as Gini Impurity, measures how often a randomly selected element from the set is classified incorrectly.\n$$ Q_m^g(L) = \\sum_{c \\in C } p_c(L) (1 - p_c(L)) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nwhere $L$ is short for the children node and $p_c(L)$ is the faction of class $c$ in the leaf node $L$,\n$$ p_c(L) = \\frac{1}{|L|} \\sum_{x, y \\in L} [y == c] $$\nwhere $[y == c] = 1$ if $y$ belongs to class $c$ and 0 otherwise.\n  Figure 2: The plot of Gini Index  Figure 2 shows that the value of Gini Index is the lowest at the start and end of the x-axis and maximum at the middle of the x-axis. In other words,\n If the leaf node only has one class, then this node is a pure node and the gini impurity of this leaf node is 0. On the other hand, if all elements in this leaf node belong to an individual class, then the Gini Index of this node has the maximum value of $ 1 - 1/len(L) $.  Entropy Entropy is a concept of Information Theory. Before introducing the entropy, we should have a little understanding of information.\nInformation is related to the surprise in some way. For example, if I told you that you will go to work tomorrow. Well, that\u0026rsquo;s not surprising because you work every day. However, if I told you that tomorrow is the end of the world, you are likely to be shocked because it is an breaking news.\nWe can measure this surprise by the following equation\n$$ I(x) = -log(p(x)) $$\nIf an event is unlikely to happen, then $p(x)$ is close to 0 and $I(x)$ tends to be infinity, which means it conveys much information since impossible things happened and it must have a significant implication behind it.\nThen what\u0026rsquo;s the entropy? The previous equation calculate the information contained in one outcome. However, it\u0026rsquo;s quite often to have many outcomes for a random variable $X$. Therefore, the expected information over all outcomes is defined as the entropy.\n$$ H(X) = E_{x \\in X}[-log(p(x)] $$\nIn this case, the classes in the leaf node is the random variable $X$ and the probability of each outcome is the fraction of each class, which is expressed as\n$$ Q_m^e(L) = \\sum_{c \\in C}- p_c(L) log(p_c(L)) $$\nIn a word, entropy measures the randomness of a set. Lower entropy means a purer set while higher entropy means there are more other classes that is not the class $c$ in a set.\nInformation Gain Information Gain is simply the difference between the impurity of the parent node $A$ before splitting and the sum of impurity of all children nodes after splitting. It measures how much the impurity of a set $A$ were reduced after splitting. The larger the information gain is, the better the split is.\n$$ IG(A) = H(A) - \\sum_{L \\in children \\ nodes} p(L) H(L) $$\nIn summary, when a decisiton tree makes a split on a feature, it tries to achieve,\n a lower impurity a lower entropy a higher information gain  CART Algorithm CART is short for classification and regression tree algorithm, which is used to train Decisioin Trees. CART tries to find the best split that produces purest subsets by calculating the weighted Gini Impurity on each split made by each feature $k$ from the parent node\u0026rsquo;s training sample and corresponding threshold $t_k$.\nSpecifically, it tries to minimize the following loss function for a classification task,\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}Q_m(C_m^L) + \\frac{|C_m^R|}{|N_m|} Q_m(C_m^R) $$\nwhere $C_m^L$ and $C_m^R$ are children nodes splitted on node $N_m$.\nFor a regression task, it minimizes the sum of squred error\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}SSE(C_m^L) + \\frac{|C_m^R|}{|N_m|} SSE(C_m^R) $$\nwhere $SSE(subset)$ is defined as\n$$ SSE(subset) = \\sum_{n \\in subset} (y_n - \\overline y)^2 $$\n$$ \\overline y = \\frac{1}{|subset|} \\sum_{n \\in subset} y_n $$\nPrunning Now let\u0026rsquo;s address the second problem, i.e. when to stop growing the tree. You can either stop growing the tree when you build the tree or trim the tree after building, which are known as pre-pruning and post-pruning respectively.\nPre-pruning Scikit-learn provides several hyperparameters to do a pre-pruning:\n the maximum depth the minimum number of the samples a node must have to split the minimum number of the samples in a leaf node the maximum number of leaf nodes  All of these parameters can be tuned with cross-validation.\nTODO\nPost-pruning Though pre-pruning is straightforward, it is a bit short-sighted since it doesn\u0026rsquo;t build a full tree and there might be some splits works better later on. Therefore, it would be better to have a large and full-size tree and then we trim some less important branches to avoid overfitting. Cost complexity pruning, also known as weakest link pruning, is one way to achieve this.\nPros and Cons Advantages:\n Decision trees are simple and intutive to interpret, and we can easily visualize the process of decision making. They can be used for both classification and regression. There is no need to normalize or scale data. They can help to understand what features are most important.  Disadvantages:\n They can be easy to overfit. They are sensitive to variations of training data. If you rotate the same data, you will get a completely different tree because all splits are perpendicular to an axis.    Figure 3: senstivity to variations of training set (Hands-on machine learning 2019)  References [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2] arunmohan, “Pruning decision trees,” Kaggle.com, 02-Sep-2020. [Online]. Available: https://www.kaggle.com/arunmohan003/pruning-decision-trees. [Accessed: 22-Apr-2021].\n[3] “How to choose α in cost-complexity pruning?,” Stackexchange.com. [Online]. Available: https://stats.stackexchange.com/questions/193538/how-to-choose-alpha-in-cost-complexity-pruning. [Accessed: 22-Apr-2021].\n[4] “Cost-complexity pruning - ML wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Cost-Complexity_Pruning. [Accessed: 22-Apr-2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-02/",
                "title": "Linear Regression 02",
                "section": "post",
                "date" : "2021.04.17",
                "body": "In the previous post, we talked about simple linear regression. However, we only considered one predictor. It\u0026rsquo;s quite common to have multiple predictors for real-world problems. For example, if we want to predict car prices, we should consider many factors like car sizes, manufacturers and fuel types. The simple linear regression is not suitable for this case. Therefore, we need to extend it to accommodate the multiple predictors.\nMultiple Linear Regression Suppose we have a data set with the size of $n$, and each data point has $d$ dimensions. Then the input data is denoted by $X \\in R^{n \\times d}$, and the parameters and targets are denoted by $\\bold w \\in R^d$, $\\bold y \\in R^n$ respectively. Thus, the loss function can be written by the following equation:\n$$ L = \\sum_i^{n} (\\bold x_i \\bold w - \\bold y_i)^2 = (\\bold X \\bold w - \\bold y)^T(\\bold X \\bold w - \\bold y) $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - \\bold y^T \\bold X \\bold w - \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - 2 \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\nThen we take the derivative of $L$ with respect to $\\bold w$ as simple linear regression before. Well, we need to know a little bit about the matrix calculus\n$$ \\frac{\\partial}{\\partial \\bold x} \\bold x^TA\\bold x = (A + A^T)\\bold x $$\n$$ \\frac{\\partial}{\\partial \\bold x} A^T \\bold x = A $$\nThe gradient of $L$ can be seen easily\n$$ \\frac{\\partial}{\\partial \\bold x } L = (2\\bold X^T\\bold X)\\bold w - 2 \\bold X^T\\bold y $$\nSetting this gradient to zero,\n$$ \\bold w= (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y $$\nHowever, this equation is unlikely to work if $\\bold X^T\\bold X$ is not invertible(singular), such as if the number of features are more than the number of observations($n \u0026lt; d$). One way to solve this equation is to use SVD.\npseudoinverse SVD technique can decompose any matrix $A$ into the multiplication of three matrices, i.e. $U\\Sigma V^T$. Thus the above equation can be written in the following form,\n$$ \\bold w = A^+y $$\n$$ A^+ = (\\bold X^T\\bold X)^{-1} \\bold X^T = V\\Sigma^{-1}U^T $$\nIn practice, the algorithm will set the elements of $\\Sigma$ that less than a smaller threshold to zero, then take the inverse of all nozero values, and finally transpose the resulting matrix i.e. $(U\\Sigma V^T)^{-1}$\nQR factorisation TODO\ncomparison of algorithms TODO\nProbabilistic Interpretation Assumption It\u0026rsquo;s inevitable to introduce errors when we collect data. The error could be systematic errors, human errors or something else. We define the error to be $\\epsilon_i$ for each observation.\n$$ y_i = a + bx_i + \\epsilon_i $$\nThe assumption of linear regression is that the expected error is zero. Specifically, the error follows the Gaussian distribution with the mean of zero and variance of $\\sigma^2$.\n$$ \\epsilon_i \\sim N(0, \\sigma^2) $$\nThus, the probability of $y_i$ is defined by the predictors $x_i$ and the paramters $a, b, \\sigma^2$.\nMLE We have found the parameters by minimizing the loss, but now we are going to use another method to derive the same result, which is known as maximum likelihood estimation(MLE).\nThe basic idea of MLE is that if the data were generated from some model, then what\u0026rsquo;s the parameters of the model were most likely to make this happen? In other words, we are finding the parameters that maximize the probability of the data $D$ that we\u0026rsquo;ve seen.\nSuppose we have a data set of inputs $X={x^{(1)}, x^{(2)}, \u0026hellip;, x^{(N)}}$ and corresponding target variables ${y_1, y_2, .., y_N}$ with a Gaussian noise $\\epsilon$. Then we can construct the likelihood of all data points,\n$$ L(\\theta|D) = \\prod_{n=1}^N p(y_i|x_i, a, b, \\sigma^2) $$\nUsually, we will take the log likelihood to make computation more simpler,\n$$ In(L(\\theta|D)) =\\sum_i^n In(\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y - a - bx_i)^2}{2\\sigma^2}}) $$\n$$ = \\frac{N}{2}In\\sigma - \\frac{N}{2}In2\\pi - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(y_i - a - bx_i)^2 $$\nFrom above equation, we can see that maximizing the likelihood is equivalent to minimizing the sum of squared error.\nGeometry of Linear Regression In this section, we will look at the geometry of the linear regression. In $N$-dimensional space whose axes are the values of $y_1, y_2, \u0026hellip;, y_n$ , the least-squares solution is obtained by finding the orthogonal projection of the target vector $y$ onto the subspace spanned by the columns of $X$.\n  Figure 1: Geometry interpretation of the least-squares solution (PRML 2006)  From the following matrix form, we can see that the predicted value $\\bold y'$ lies the column space of $X$. If the true target value $\\bold y$ also lies in this space, then the loss of linear regression is zero, which is never the case in real life.\n$$ \\displaystyle{\\bold y' = \\bold X \\bold w = \\begin{bmatrix}1\u0026amp;x_{11} \u0026amp; x_{12} \u0026amp; \u0026hellip; \u0026amp; x_{1d}\\\\ 1\u0026amp;x_{21} \u0026amp; x_{22} \u0026amp; \u0026hellip; \u0026amp; x_{2d}\\\\ \u0026hellip; \\\\ 1\u0026amp;x_{n1} \u0026amp; x_{n2} \u0026amp; \u0026hellip; \u0026amp; x_{nd} \\end{bmatrix} \\begin{bmatrix}w_0\\\\ w_1\\\\ w_2\\\\ \u0026hellip;\\\\ w_d \\end{bmatrix} } $$\nNon-linear Data Fitting Polynomials Transformation Dependent Data TODO\nOutliers TODO\nMulticollinearity Dummy Variables References [1] C. Bishop, Pattern Recognition and Machine Learning. 2006.\n[2] \u0026ldquo;Interaction Effects in Regression\u0026rdquo;, Stattrek.com, 2021. [Online]. Available: https://stattrek.com/multiple-regression/interaction.aspx?tutorial=reg. [Accessed: 11- May- 2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/bias-variance-dilemma/",
                "title": "Bias-Variance dilemma",
                "section": "post",
                "date" : "2021.04.15",
                "body": "When you learn more about machine learning, you must hear people talking about high bias, high variance or something like that. What do they mean by \u0026lsquo;high bias\u0026rsquo; or \u0026lsquo;high variance\u0026rsquo;?\nActually, when I first heard these terms, I was completely confused. Even though I tried to find the answer on Google, I still had no idea until I took the Advanced Machine Learning module in semester 2. Therefore, I\u0026rsquo;m writing this post to try to explain this. I hope this post can help people who are still struggling with them to understand the two most important concepts clearly.\nGeneralization Error The underlying assumption of machine learning is that there are some relationships between data. However, we are not able to know this true function, otherwise there is no need to learn it.\nSuppose we have a true realtionship denoted by $f(x)$ (the red dot in Figure 1), and we want to construct a machine denoted by $f'(x)$ to approximate the true function based on the data $D$ sampled from the population $\\chi$.\nThe training loss is defined by the following equation, where $f'(x|D)$ is the machine we learn from this particular data set $D$\n$$ L_T(D) = \\sum_{x\\in D}(f'(x|D) - f(x))^2 $$\nHowever, our goal is to know how well this machine works on unseen data, which is known as generalization. The generalization loss is expressed as\n$$ L_G(D) = \\sum_{x\\in \\chi} p(x) (f'(x|D) - f(x))^2 $$\nIf we have another data set $D_1$, then we will get another machine $f'(x|D_1)$ and another generalization loss $L_G(D_1)$ shown in Figure 1.\n  Figure 1: Generalisation error  We can see that the generalization loss is depend on our training data. Thus, the generalization loss for a particular data set doesn\u0026rsquo;t make much sense. Instead, the average generalization loss over all the data set with the same size of $n$ is what we expect.\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\nMean Machine We have already known that there is a different machine $f'(x|D)$ for a given data set $D$. Thus, for an unseen data $x$, we will have many predictions of many different machines, which are represented in blut dots shown in Figure 2.\n  Figure 2: Bias and variance  The average prediction for an unseen data is the mean prediction(the yellow dot in Figure 2).\n$$ f'_m(x) = E_D[f'(x|D)] $$\nBias Bias is the distance between the mean prediction(the yellow dot) and the true value(the red dot) shown in Figure 2. High bias implies that our model is too simple and the prediction value is much far away from the true value.\n$$ B = \\sum_{x \\in \\chi} p(x) (f\u0026rsquo;m - f(x))^2 $$\nVariance Variance measures the variation in the prediction of the machine when we change different data set we train on. If we have a complex machine, as mentioned earlier, the machine will try its best to match every data in training data set. In other words, the machine memorized the trainining data and a little change in data set will cause significant variation in prediction.\n$$ V = \\sum_{x \\in \\chi}p(x) E_D[ (f'(x|D) - f\u0026rsquo;m)^2 ] $$\nBias-Variance dilemma Now it\u0026rsquo;s time to decompose the average generalisation error. Let\u0026rsquo;s plug the $f'_m(x)$ into the previous equation\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x) (f'(x|D) - f_m' + f_m' - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x){(f'(x|D) - f_m')^2 + (f_m' - f(x))^2 + 2(f'(x|D) - f_m')(f_m' - f(x)) }] $$\nIt\u0026rsquo;s noticeable that the cross-term will cancel out because $f\u0026rsquo;m$ and $f(x)$ are constants no matter what data set $D$ is.\n$$ E_D[\\sum_{x\\in \\chi}p(x)2(f'(x|D) - f_m')(f_m' - f(x))] $$\n$$ = \\sum_{x\\in \\chi}p(x) (2E_D[f'(x|D)]-f\u0026rsquo;m)(f\u0026rsquo;m-f(x)) = 0 $$\nTherefore, we are left with\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi}p(x)(f'(x|D) - f_m')^2 + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2] $$\n$$ = \\sum_{x\\in \\chi}p(x) E_D[(f'(x|D) - f_m')^2] + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2 $$\n$$ = V + B $$\nIn summary,\n If our machine is too simple, then we might not be able to fit the training data. Since the machine knows little about the data, it\u0026rsquo;s unlikely to work well on unseen data. This means our model has a high bias. If our machine is too complex, then we might be able to fit the training data perfectly. It means that the machine knows too much about the data, even the noise that it should not learn. Thus, it\u0026rsquo;s too sensitive to training data so that a little change in data will cause a great variance. This means our model has a high variance.  Throughout the world of machine learning, we are always trying to find a balance between bias and variance.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-01/",
                "title": "Linear Regression 01",
                "section": "post",
                "date" : "2021.04.14",
                "body": "There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand. Perhaps it is the first algorithm that most people learn in the world of machine learning. So let\u0026rsquo;s go!\nProblem Statement Suppose you are a teacher, and you recorded some data about the hours students spent on study and the grades they achieved. Below are some sample data you collected. Then you want to predict the score for someone according to the hours he spent.\n   Hours 0.5 1 2 3 4     Grade 20 21 22 24 25    How to make a prediction? Since there are only two variables, let\u0026rsquo;s plot them to see if there is a relation betwee grade and hours.\n  Figure 1: A scatter plot of hours and grade  Figure 1 shows that grade is positively related to hours. For simplicity, we can use a straight line(the red line) to approximate this relation. And this is exactly our first linear model.\nSimple Linear Regression Remember the equation of a straight line is typically written as,\n$$ y = ax + b $$\nIn this example, $x$ is the variable hours and $y$ is the variable grade, which we\u0026rsquo;ve already known. So the problem is to find the parameter $a, b$ such that the red line is as close as possible to the blue data points. Technically, this is called parameter estimation. Usually, there are two ways to do this: minimising the loss and maximising the likelihood. Now we focus on the former.\nResidual So how to measure the closeness mentioned above? The most common way is to measure the residual, which is the difference between the estimated value and our true value shown in Figure 2. For a single data point, the residual is defined as below, where $y_i$ is the true value and $y_i'$ is our predicted value.\n$$ e = y_i - y'_i = y_i - ax_i - b $$\n  Figure 2: Residual/Error is the difference between the observed value and the predicted value. (Bradthiessen.com 2021)  error Since we have many data points, we need to sum them up to evaluate the overall errors. Unfortunately, some error terms will cancel out when you do the this calculation directly.\n$$ L = \\sum_i^n (y_i - y'_i) = \\sum_i^n (y_i - ax_i - b) $$\nthe absolute value of error One way to tackle this is to take the absolute value of the error terms. However, the absoulte value of $x$ is not differentiable at $0$.\n$$ L = \\sum_i^n |y_i - y_i'| $$\nthe squared value of error Instead of taking the absolute value, we square all the errors and sum them up, which is known as the Residual Sum of Squares(RSS) or Sum of Squared Error (SSE).\n$$ L = \\sum_i^n (y_i - y_i')^2 $$\nClosed-form solution Finally, we find a function to measure the loss. Next, we need to find the parameters that minimize the squared error. The good news is that our loss function is differentiable and convex! It means that we have the global minimum value and can be calculated directly by taking derivatives.\nLet\u0026rsquo;s take the first derivative w.r.t $b$\n$$ \\frac{\\partial L}{\\partial b} = \\sum_i^n -2(y_i - ax_i-b) $$\nand then set this equation to $0$,\n$$ -2(\\sum_i^ny_i -a\\sum_i^nx_i - \\sum_i^nb) = -2(n\\overline y-an\\overline x - nb) = 0 $$\n$$ \\hat b = \\overline y - \\hat a \\overline x $$\nSimilarly, let\u0026rsquo;s take the first derivative w.r.t $a$\n$$ \\frac{\\partial L}{\\partial a} = \\sum_i^n -2x_i(y_i - ax_i-b) $$\nplug $b$ into this equaiton and set this equation to 0 again,\n$$ \\sum_i^n -2x_i(y_i - ax_i-\\overline y+a\\overline x) = \\sum_i^n -2x_i[(y_i-\\overline y)- a(x_i -\\overline x)] $$\n$$ \\hat a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} $$\nHere, we use a slight algebra trick,\n$$ c\\sum_i^n(x_i - \\overline x_i) = 0 $$\nplug the above equation into the previous equation\n$$ \\hat a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} = \\frac{\\sum_i^nx_i(y_i-\\overline y) - \\sum_i^n\\overline x(y_i - \\overline y)}{\\sum_i^nx_i(x_i -\\overline x) - \\sum_i^n\\overline x(x_i - \\overline x)} $$\n$$ = \\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2} $$\n$$ = \\frac{Cov(x, y)}{Var(x)} $$\nFinally, we find the best estimators for our simple linear regression.\nTest statistics The above formulas give us the best estimation for the parameters $a, b$ of the linear regression model. If we generate different data sets from the population, we will have different linear models and different values of dependent variable. If we average these values, we\u0026rsquo;ll find that the average value is pretty close to the true value. Mathematically, this can be expressed as follows,\n$$ E_D(\\hat ax_i + \\hat b|D) \\simeq ax_i+b $$\nThe idea behind the above equation is analogous to the Central Limit Theorem for Sample Mean. The population mean of the random variable $Y_i$ (the true line) can be estimated by the expected value of the sample mean(the estimated line). CLT tells us the distribution of the sample mean follows the Gaussian Distribution with the mean of the population mean as the sample size gets larger.\nStandard error But in practice, we can only have one data set, so how accurate is the parameters $a, b$ calculated from the above equations? In other words, a single sample mean may overestimate or underestimate the population mean, but to what extent is this deviation? We use the standard error of sample mean to measure it, which can be obtained by the following equation, where $\\sigma$ is the population standard deviation and $n$ is the sample size.\n$$ SE^2(\\hat {\\overline \\mu}) = Var(\\hat {\\overline \\mu}) = \\frac{\\sigma^2}{n} $$\nSimilarly, we can calculate the standard error associated with the parameters $a$ and $b$.\n$$ SE^2(\\hat a) = Var(\\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2}) = Var(\\frac{\\sum_i^n(x_i-\\overline x)y_i - \\sum_i^n(x_i-\\overline x)\\overline y}{\\sum_i^n(x_i -\\overline x)^2}) $$\n$$ = Var(\\frac{\\sum_i^n(x_i-\\overline x)(ax_i + b + \\epsilon_i)}{\\sum_i^n(x_i -\\overline x)^2}) $$\n$$ = \\frac{\\sum_i^n(x_i-\\overline x)^2}{(\\sum_i^n(x_i -\\overline x)^2)^2} Var(ax_i + b + \\epsilon_i) = \\frac{\\sum_i^n(x_i-\\overline x)^2}{(\\sum_i^n(x_i -\\overline x)^2)^2} Var(\\epsilon_i) $$\n$$ = \\frac{Var(\\epsilon_i)}{\\sum_i^n(x_i -\\overline x)^2} $$\nSince $x_i, y_i$ are known and $a, b$ are the true parameters, the only thing unknown is $\\epsilon_i$. In other words, they are all indepentdent of $\\epsilon_i$. In addition, we use a bit tricks to derive the above formula,\n$$ Var(c) = 0 $$\n$$ Var(cX) = c^2Var(X) $$\n$$ Var(c + X) = Var(X) $$\nIn summary, what we should know is that the standard error of $\\hat a$ tells us how far away this estimate $\\hat a$ is from the true value $a$ or how far away the predicated value $\\hat y$ is from the observed value $y$.\nFurthermore, since the predicted value obtained from a given sample is different from the true value, we cannot say we are sure that the estimated value is exactly the true value. But we could say we are 90% confident that the true value lies somewhere around the predicted value. And this \u0026lsquo;somewhere\u0026rsquo; can be computed by confident intervals.\np-value To investigate whether the estimated parameters are statistically significant, we perform hypothesis tests. A statistical significant result means it\u0026rsquo;s unlikely to happen by chance. In the simple linear regression, the null and alternative hypotheses are defined as\n$H_0$: There is no relationship between $X$ and $Y$\n$H_a$: There is some relationship between $X$ and $Y$\nMathematically, it is equvalent to testing\n$H_0$: $a=0$\n$H_a$: $a\\ne0$\nSo in order to test the null hypothesis, we need to demonstrate that $\\hat a$ is sufficiently far away from $0$. Thus, we can be confident that $a$ is not equal to $0$ and reject the null hypothesis. To quantify the distance between $\\hat a$ and $0$, we calculate t-score\n$$ t = \\frac{\\hat a - 0}{SE(\\hat a)} $$\nThe higher the $t$ is, the farther the distance is. But wait, what\u0026rsquo;s the probability of getting this estimate $\\hat a$ or more extreme? In other words, what\u0026rsquo;s the p-value? How to interpret this probability? A higher p-value doesn\u0026rsquo;t provide much information. In contrast, a smaller p-value means it\u0026rsquo;s unlikely to observe this t-score due to chance under the assumption that $H_0$ is true. You can interpret that a small p-value indicates a strong evidence against the null hypothesis. But how small is enough? Typically, we set the threshold value of $0.05$. If p-value is smaller than $0.05$, we reject $H_0$, otherwise we accept it.\nModel Evaluation Next question is how to evaluate our model? How good is it? There are two common metrics to measure the quality of a linear regression model: RSE and $R^2$.\nRSE Residual standard error measures the average deviation between the estimated value and the true value. It can be calculated using the following fomula, where n-2 is the degree of freedom(df) of the residual.\nQ: Why do we divide by $n-2$ not $n$?\nA: This is because the latter tends to underestimate variance. Rememer we divide by $n-1$ when calculating the sample variance. This is the same reason here.\nQ: But why n-2?\nA: We know that 2 points determine a line, which means there is no other line fitting the data and the residual of each data point is fixed. If we add a third point, there could be many lines fitting these points and thus different residuals. That means the third point increases the flexiblity of the value of residuals. We say we have one free observation. Therefore, the degree of freedom of the residual is $n-2$ in simple linear regression.\n$$ RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - y_i')^2} $$\nA smaller RSE indicates that our model fit the data well. However, RSE is measured in the unit of $Y$. For some data sets, RSE would be small, e.g $3.2$. But for other data sets, it would be very large, e.g $1200$. So it\u0026rsquo;s a bit confusing. That\u0026rsquo;s why $R^2$ comes. $R^2$ measures the fraction of variance explained, which is independent of the unit of $Y$.\n$R^2$ Coefficient of determination or $R^2$ is another metric to measure the goodness of a linear regression model. Let\u0026rsquo;s rewrite the previous equation by multiplying both the denominator and numerator by $\\sqrt {\\sum_i^n(y_i-\\overline y)^2}$\n$$ a = \\frac{\\sum_i^n (x_i - \\overline x)(y_i - \\overline y) \\sqrt {\\sum_i^n(y_i-\\overline y)^2}}{\\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(y_i-\\overline y)^2}} $$\n$$ a = R\\frac{s_y}{s_x} $$\nwhere\n$$ R = \\frac{Cov(x, y)}{\\sqrt{var(x)} \\sqrt{var(y)}} $$\n$$ s_y = \\sqrt{Var(y)} $$\n$$ s_x = \\sqrt{Var(x)} $$\nRemember that the error is defined as $e_i = y_i' - y_i$, so the mean of $e$ is\n$$ E(e) = \\frac{1}{N} \\sum_i^n e_i = \\frac{1}{N} \\sum_i^n b + ax_i - y_i =b + a\\overline x - \\overline y = 0 $$\nand the variance is\n$$ var(e) = \\sum_i^n (e_i - \\overline e)^2 = \\sum_i^n (y_i - b - ax_i)^2 = \\sum_i^n (y_i - \\overline y + a\\overline x - ax_i)^2 $$\nLet\u0026rsquo;s plug $a$ into this equation\n$$ var(e) = \\sum_i^n [(y_i - \\overline y) - R\\frac{s_y}{s_x}( x_i - \\overline x)]^2 = var(y) (1-R^2) $$\nOr you might be more familiar with this equation\n$$ R^2 = 1 - \\frac{var(e)}{var(y)} = 1 - \\frac{RSS}{TSS} $$\nTherefore, $R^2$ tells us how much the variance of $y$ has been explained by our models. The higher the $R^2$ is, the better our model is.\nReferences [1] Bradthiessen.com, 2021. [Online]. Available: https://www.bradthiessen.com/html5/docs/ols.pdf. [Accessed: 14- Apr- 2021].\n[2] “Linear Regression - ML Wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Linear_Regression. [Accessed: 14-Apr-2021].\n[3] K. Base and S. statistics, \u0026ldquo;Standard Error | What It Is, Why It Matters, and How to Calculate\u0026rdquo;, Scribbr, 2021. [Online]. Available: https://www.scribbr.com/statistics/standard-error/. [Accessed: 12- May- 2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/descriptive-statistics/",
                "title": "Descriptive Statistics",
                "section": "post",
                "date" : "2021.04.13",
                "body": "Statistics is one of the most important skills required to be a data scientist. There are two main branches of statistics:\n  descriptive statistics tells us the statistics about data like mean, mode and standard deviation, which you\u0026rsquo;ve learned in high school.\n  inferential statistics, on the other hand, uses a random dataset sampled from the population to make inferences about the population.\n  In this post, we will focus on descriptive statistics and other basic concepts in statistics.\nCentral tendency Central tendency measures the center of the data. Mean, mode, and median are three mainly used measures of central tendency.\nMean Mean is the average of data and calculated by summing up all data values and then dividing them by the number of data. For instance, say we have data 1,2,3,4,5,6, the mean is 3.5.\n$$ \\overline x = \\frac{\\sum_i^nx_i}{n} $$\nMedian Though mean is widely used, it\u0026rsquo;s sensitive to outliers. Suppose you have a set of data 1,2,3,4,5,6,100, after some calculations, you find that the mean is 17.28. However, it seems a bit strange since most of the data values are less than 10 except for one extreme value 100, which stretches the distribution of the whole data set to the right. This is why the median comes.\nTo get the median, firstly we need to sort the data in ascending order and then find the middle number that separates the data into two groups of the same size. In this example, the median is 4.\nMode Mode is the most frequent value. There could be one, two or more data values with the same frequency, and that frequency is the largest.\nDispersion Dispersion measures how spread out a given dataset is.\nRange Range is the distance between the maximum value and the minimum value. Again, the range is sensitive to outliers.\n$$ r=max - min $$\nQuantile Quantiles are used to divide data into several equal-sized groups. The most widely used cut points are 0, 25, 50, 75, 100, denoted by min, Q1, Q2, Q3, max, respectively.\nIQR or interquartile range measures where the central 50% of the data is.\n$$ IQR = Q3 - Q1 $$\nIQR can be used to detect outliers. Data that is greater than the upper boundary or less than the lower boundary can be considered as an outlier.\n$$ upper \\ boundary = Q3 + 1.5*IQR $$\n$$ lower \\ boundary = Q1 - 1.5*IQR $$\nVariance The deviation is the distance between a given data point and the mean. Since there are many data points, the variance calculates the average deviation from the mean. But when you do this, you will always get a zero due to the definition of the mean.\n$$ \\frac{1}{n}\\sum_i^n d_i = \\frac{1}{n}\\sum_i^n (x_i - \\overline x) = \\frac{1}{n}\\sum_i^nx_i - n\\overline x = \\frac{1}{n} (n\\overline x - n\\overline x) = 0 $$\nLet\u0026rsquo;s try to ignore the sign and use the absolute value of the deviation.\n$$ \\frac{1}{n-1} \\sum_i^n d_i = \\frac{1}{n-1} \\sum_i^n |x - \\overline x| $$\nThough it works, the most widely used method is to calculate the square of the deviation.\n$$ s^2 = \\frac{1}{n-1} \\sum_i^n (x_i - \\overline x)^2 $$\nHowever, the squared value is a bit hard to interpret. For instance, below are 15 records of fish size measured in kilogram, and the variance is 30.97. It means that if we randomly catch a fish, its weight would be 30.97 squared kilograms far away from the average weight. Emm\u0026hellip;, a squared kilogram is an odd unit.\n2.1, 2.4, 2.4, 2.4, 2.4, 2.6, 2.9, 3.2, 3.2, 3.9, 4.5, 6.3, 8.2, 12.8, 23.5 Standard Deviation It\u0026rsquo;s simple to solve this problem by taking the square root of the variance, which is the standard deviation. The standard deviation in the previous example is 5.56kg, which makes much sense.\n$$ s = \\sqrt{\\frac{\\sum (x - \\overline x)^2}{n-1}} $$\nDistribution Skewness Skewness measures the symmetry of a distribution. It\u0026rsquo;s quite common to have non-symmetric distributions shown in Figure 1.\n  Figure 1: Non-symmetric distributions  A left-/negative-skewed distribution\n a long left tail mean \u0026lt; median  A right-/positive-skewed distribution\n a long right tail mean \u0026gt; median  A skewed distribution implies that some special values are much larger/smaller than the common values. Let\u0026rsquo;s see an example.\n Figure 3.2 shows the histogram of the fish sizes gathered from a fisherman. We can see that this distribution is right-skewed. The majority of fish sizes are between 0 and 3kg, but a few fishes weigh over 3.5kg. It also can be seen that the average weight(1.67kg) is a bit higher than the median(1.62kg) shown in Table 3.2.\nCorrelation So far we have introduced different methods of characterizing the distribution of a single variable. But what about two or more variables? You might want to find the relationships between them. An intuitive way is to visualise data using a scatterplot. For example, you might find that car prices tend to increase as car ages decrease.\nStatistically, we can measure both the direction and the strength of this tendency using correlation coefficients. And Pearson correlation coefficients is widely used to measure the strength of a linear relationship. It can be calculated as follows,\n$$ r = \\frac{Cov(X, Y)}{\\sqrt{V(X)}\\sqrt{V(Y)} } $$\nwhere $Cov(X, Y)$ is the covariance between $X$ and $Y$, and $V(X), V(Y)$ are the standard deviation of $X, Y$ respectively. The value of $r$ ranges between -1 and 1.\n a negative value represents a negative relationship a positive value represents a positive relationship $0$ means there is no relationship between $X$ and $Y$  But to what extent is the value of $r$ large means a strong relationship? Below are some values that could give you an insight into how strong your correlation $r$ is.\n 0 - 0.3: weak relationship 0.3 - 0.6: moderate relationship 0.6 - 0.9: strong relationship 0.9 - 1.0: very relationship  Data Types Now let\u0026rsquo;s look at some characteristics of data. Roughly, we classify data into two categories:\n numerical/quantitative data categorical/qualitative data  Categorical data refers to data that cannot be measured, for example, the color of car or the quality of service. There are two subcategories, namely nominal data and ordinal data. The difference between them is that ordinal data can have order, such as 0=bad, 1=good, 2=better, 3=excellent.\nNumerical data refers to data that can be measured. There are also two subcategories, namely discrete and continuous. Discrete data describes values that cannot be divided into smaller individual parts, e.g. the number of people. You cannot say, \u0026lsquo;There are 1.5 people\u0026rsquo;. However, we can say, \u0026lsquo;He is 1.87 meters tall\u0026rsquo;. So height is a continuous variable.\nMeasurement Scales At a lower level, we can also classify data from the perspective of measurement scales, which capture the characteristics of data used to determine the types of variables. There are four main levels of measurement scales:\n nominal ordinal interval ratio  Each of them satisfies one or more of the following properties of measurement.\n Identity  each value have a unique meaning $=, \\cancel{=}$ nominal scale, e.g. car color   Maginitue  values have an ordered relationship to one another $\\lt, \\le, \\gt, \\ge$ ordinal scale, e.g. service quality   Equal interval  the different between data points A and B will be equal to the different between data points C and D $+, -$ interval scale, e.g. time   A minimum value of zero  the scale has a true zero point, below which no value exists ratio scale, e.g. height    References [1] B. al., \u0026ldquo;Introduction to Statistics | Simple Book Production\u0026rdquo;, Courses.lumenlearning.com, 2021. [Online]. Available: https://courses.lumenlearning.com/introstats1. [Accessed: 14- Apr- 2021].\n[2]\u0026ldquo;Measurement Scales\u0026rdquo;, Stattrek.com, 2021. [Online]. Available: https://stattrek.com/statistics/measurement-scales.aspx?tutorial=reg. [Accessed: 11- May- 2021].\n"
            }
        
    
]