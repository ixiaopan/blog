[
    
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/bias-variance-dilemma/",
                "title": "Bias-Variance dilemma",
                "section": "post",
                "date" : "2021.04.15",
                "body": "When you learn more about machine learning, you must hear people talking about high bias or high variance something like that. What does they mean by 'high bias' or 'high variance'? Actually, when I first heard these terms, I was completely confused. Even though I tried to find the answer on Google, I still had no idea until I took the Advanced Machine Learning module in semester 2. Therefore, I'm writing this post to try to explain this. I hope this post can help people who are still struggling with them understand the two most important concepts clearly.\nGeneralization Before diving into it further, we should know what the generalization is.\nGeneralization measures how well our machine works on unseen data.\n If our machine is too simple, then we might not be able to fit the training data. Since the machine knows little about the data, it's unlikely to work well on unseen data. If our machine is too complex, then we might be able to fit the training data perfectly. It means that the machine knows too much about the data, even the noise that it should not learn. Thus, it's too sensitive to data so that a little change in data will cause a great variance.  Generalization Error The underlying assumption of machine learning is that there are some relationships between data. However, we are not able to know this true function, otherwise there is no need to learn it.\nSuppose we have a true realtionship denoted by \\(f(x)\\) (the red dot in Figure 2), and we want to construct a machine denoted by \\(f'(x)\\) to approximate the true function based on the data \\(D\\) sampled from the population \\(\\chi\\). Then the training loss is defined by the following equation, where \\(f'(x|D)\\) is the machine we learn from this particular data set \\(D\\)\n\\[ L_T(D) = \\sum_{x\\in D}(f'(x|D) - f(x))^2 \\]\nOkay, now we want to know how well this machine works on unseen data, which can be measured by generalization loss.\n\\[ L_G(D) = \\sum_{x\\in \\chi} p(x) (f'(x|D) - f(x))^2 \\]\nIf we have another data set \\(D_1\\), then we will get another machine \\(f'(x|D_1)\\) and another generalization loss \\(L_G(D_1)\\) shown in Figure 1.\nFigure 1\nWe can see that the generalization loss is depend on our training data. Thus, the generalization loss for a particular data set doesn't make much sense. Instead, the average generalization loss over all the data set with the same size of \\(n\\) is what we expect.\n\\[ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] \\]\nMean Machine We have already known that there is a different machine \\(f'(x|D)\\) for a given data set \\(D\\). Thus, for an unseen data \\(x\\), we will have many predictions of many different machines, which are represented in blut dots shown in Figure 2.\nFigure 2: Bias and variance.\nThe average prediction for an unseen data is the mean prediction(the yellow dot in Figure 2).\n\\[ f'_m(x) = E_D[f'(x|D)] \\]\nBias Bias is the distance between the mean prediction(the yellow dot) and the true value(the red dot) shown in Figure 2. High bias implies that our model is too simple and the prediction value is much far away from the true value.\n\\[ B = \\sum_{x \\in \\chi} p(x) (f'm - f(x))^2 \\]\nVariance Variance measures the variation in the prediction of the machine when we change different data set we train on. If we have a complex machine, as mentioned earlier, the machine will try its best to match every data in training data set. In other words, the machine memorized the trainining data and a little change in data set will cause significant variation in prediction.\n\\[ V = \\sum_{x \\in \\chi}p(x) E_D[ (f'(x|D) - f'm)^2 ] \\]\nBias-Variance dilemma Now it's time to decompose the average generalisation error. Let's plug the \\(f'_m(x)\\) into the previous equation\n\\[ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] \\\\ = E_D[\\sum_{x\\in \\chi}p(x) (f'(x|D) - f_m' + f_m' - f(x))^2] \\\\ = E_D[\\sum_{x\\in \\chi}p(x)\\{(f'(x|D) - f_m')^2 + (f_m' - f(x))^2 + 2(f'(x|D) - f_m')(f_m' - f(x)) \\}] \\]\nIt's noticeable that the cross-term will cancel out because \\(f'm\\) and \\(f(x)\\) are constants no matter what data set \\(D\\) is.\n\\[ E_D[\\sum_{x\\in \\chi}p(x)2(f'(x|D) - f_m')(f_m' - f(x))] = \\sum_{x\\in \\chi}p(x) (2E_D[f'(x|D)]-f'm)(f'm-f(x)) = 0 \\]\nTherefore, we are left with\n\\[ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi}p(x)(f'(x|D) - f_m')^2 + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2] \\\\ = \\sum_{x\\in \\chi}p(x) E_D[(f'(x|D) - f_m')^2] + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2 \\\\ = V + B \\]\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression/",
                "title": "Linear Regression 01",
                "section": "post",
                "date" : "2021.04.14",
                "body": "There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and maybe it's the first algorithm that most of people learn in the world of machine learning. So let's go!\nProblem statement Suppose you are a teacher, and you record some data about the hours students spent on study and the grades they achieved. Then you want to predict the grade for given hours that someone spent. Here are some sample data you collected:\n   Hours 0.5 1 2 3 4     Grade 20 21 22 24 25    Since there are only two variables, let's plot them.\nFigure 1: The scatter plot of hours and grade\nWell, we can see that the variable \\(grade\\) is positive related to the variable \\(hours\\). For simplicity, we can use a simple line(the red line in this figure) to approximate this relationship. And this is exactly our first simple linear model.\nSimple Linear Regression Remember a line equation is written in this way:\n\\[ y = ax + b \\]\nIn this example, \\(x\\) is the variable \\(hours\\) and \\(y\\) is the variable \\(grade\\) , which we already know. So the problem is how to calculate the parameter \\(a, b\\). Technically, this is called parameter estimation. Usually, there are two ways to do this: minimising the loss and maximising likelihood. Now we focus on minimising the loss.\nLoss What is the loss? Basically, it's the error between the esitmated value and our true value. Minimising the error is simply to make the estimated value close to the true value as much as possible.\nFigure 2: The error for a single data (Bradthiessen.com 2021)\nFor a single data point, the loss function is defined below, where \\(y\\) is the true value and \\(y'\\) is our estimated value for a given \\(a, b\\).\n\\[ error = y_i - y'_i = y_i - ax_i - b \\]\nSince we have many data points, we need to sum up them all to evaluate the overall errors.\n1. error Unfortunately, some error terms will cancel out when you do this calculation directly.\n\\[ total \\ error = \\sum_i^n (y_i - y'_i) = \\sum_i^n (y_i - ax_i - b) \\]\n2. the absolute value of error One way to tackle this is taking the absolute value of the error terms.\n\\[ total \\ error = \\sum_i^n |y_i - y_i'| \\]\nHowever, the absoulte value of \\(x\\) is not differentiable at \\(0\\).\n3. the squared value of error Instead of taking absolute value, we will square all the errors. One reason is that the errors will become larger and can be distinguished easily when squaring them. It looks like the errors are zoomed in and we can find them and minimize them quickly.\n\\[ L = \\sum_i^n (y_i - y_i')^2 \\]\nPS: We will revisit the squared error later from the perspective of MLE.\nClosed-form solution Okay, finally we find a function to measure the loss. Next we need to find the parameters that minimize the squared error. Good news is that our loss function is differentiable and convex! It means that we have a global minimial value and can be calculated directly by taking derivatives.\nLet's take the first derivatve of \\(b\\)\n\\[ \\frac{\\partial L}{\\partial b} = \\sum_i^n -2(y_i - ax_i-b) \\]\nand then set this equation to \\(0\\),\n\\[ -2(\\sum_i^ny_i -a\\sum_i^nx_i - \\sum_i^nb) = -2(n\\overline y-an\\overline x - nb) = 0 \\\\ b = \\overline y-a\\overline x \\]\nLet's take the first derivatve of \\(a\\)\n\\[ \\frac{\\partial L}{\\partial a} = \\sum_i^n -2x_i(y_i - ax_i-b) \\]\nand then plug \\(b\\) into this equaiton and set this equation to \\(0\\) again,\n\\[ \\sum_i^n -2x_i(y_i - ax_i-\\overline y+a\\overline x) = \\sum_i^n -2x_i[(y_i-\\overline y)- a(x_i -\\overline x)]\\\\ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} \\]\nHere, we use a slight algebra trick,\n\\[ a\\sum_i^n(x_i - \\overline x_i) = 0 \\]\nThen we plug this into the previous equation\n\\[ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} = \\frac{\\sum_i^nx_i(y_i-\\overline y) - \\sum_i^n\\overline x(y_i - \\overline y)}{\\sum_i^nx_i(x_i -\\overline x) - \\sum_i^n\\overline x(x_i - \\overline x)}\\\\ = \\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2}\\\\ = \\frac{Cov(x, y)}{Var(x)} \\]\nFinally, we find the best estimators for simple linear regression.\nReference [1] Bradthiessen.com, 2021. [Online]. Available: https://www.bradthiessen.com/html5/docs/ols.pdf. [Accessed: 14- Apr- 2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/descriptive-statistics/",
                "title": "Descriptive Statistics",
                "section": "post",
                "date" : "2021.04.13",
                "body": "In this post , I'm going to go through some basic concepts of statistics required in Data Science.\nThere are two main branches of statistics :\n descriptive statistics tells us the statistics about data like mean, mode and standard deviation, which you've learned in high school.\n inferential statistics, on the other hand, uses a random dataset sampled from population to make inferences about population.\n  We will firstly focus on descriptive statistics, specifically, the central tendency and dispersion. Central tendency measures the center of the data while dispersion measures how spread out a given data is.\nCentral tendency Mean, mode and median are three mainly used measures of central tendency.\nMean Mean is the average of the data and calculated by summing up all data values and then dividing them by the number of data.\n\\[ \\overline x = \\frac{\\sum_i^nx_i}{n} \\]\nFor instance, say we have a group of data 1,2,3,4,5,6, the mean is 3.5.\nMedian Though mean is widely used, it's sensitive to outliers. Suppose you have a set of data 1,2,3,4,5,6,100, after some calculations, you find that the mean is 17.28. However, it seems a bit strange since most of the data values is less than 10 except one extreme value 100, which stretches the distribution of the whole data set to the right. This is why median comes.\nTo get the median, firstly we need to sort the data in ascending order and then find the middle number that separate the data into two groups with the same size. In this example, the median is 4.\nMode Mode is the most frequent value. There could be one, two or more data values that have the same frequency and that freqency is the highest.\nDispersion Range Range is the distance between the maximum value and the minimum value. Again, range is sensitive to outliers.\n\\[ r=max - min \\]\nQuantile Quantiles are used to divide data into several equal-sized groups. The most widely used cut points are 0, 25, 50, 75, 100, denoted by min, Q1, Q2, Q3, max respectively.\nIQR or interquartile range measures where the central 50% of the data is.\n\\[ IQR = Q3 - Q1 \\]\nIQR can be used to detect outliers. Data that is greater than upper boundary or less than lower boundary can be considered as an outlier.\n\\[ upper \\ boundary = Q3 + 1.5*IQR \\\\ \\]\n\\[ lower \\ boundary = Q1 - 1.5*IQR \\]\nVariance Deviation is the distance between a given data point and the mean. Since there are many data points, the variance calculates the average deviation from the mean.\nBut when you do this, you will always get a zero due to the definition of the mean.\n\\[ \\sum_i^n d_i = \\sum_i^n (x_i - \\overline x) = \\sum_i^nx_i - n\\overline x = n\\overline x - n\\overline x = 0 \\]\nFor simplicity, I omit the denominator n.\nOkay, let's ignore the sign and use the absolute value of the deviation.\n\\[ d_i = |x - \\overline x| \\]\nThough it works, the most popularly used method is calculate the square of the deviation.\n\\[ s^2 = \\frac{\\sum_i^n (x_i - \\overline x)^2}{n-1} \\]\nThough variance works fine, the value is a bit hard to interpret. For instance, we have 15 records of fish size measured in kilogram:\n[ 2.1, 2.4, 2.4, 2.4, 2.4, 2.6, 2.9, 3.2, 3.2, 3.9, 4.5, 6.3, 8.2, 12.8, 23.5 ].\nThe variance is 30.97. It means that if we randomly catch a fish, its weight would be 30.97 squared kilogram far away from the average weight. Emm...squared kilogram is an odd unit.\nStandard Deviation It's simple to solve this problem by taking the square root of the variance, which is standard deviation.\n\\[ s = \\sqrt{\\frac{\\sum (x - \\overline x)^2}{n-1}} \\]\nSo the standard deviation in the previous example is 5.56kg, which makes much sense.\nDistribution Skewness Skewness measures the symmetry of a distribution. It's quite common to have non-symmetric distributions.\n\nA left-/negative-skewed distribution\n it has a long left tail the mean is on the left of the median  A right-/positive-skewed distribution\n it has a long right tail the mean is on the right of the median  A skewed distribution implies that there are some special values that are larger/smaller than the common values. Let's see an example.\n\nFigure 3.2 shows the histogram of the fish sizes gathered from a fisherman. We can see that this distribution is right-skewed. The majority of fish sizes are between 0 and 3kg and there are a few special fishes that weigh over 3.5kg. It also can be seen that the average weight(1.67kg) is a bit higher than the median(1.62kg) shown in Table 3.2.\nReference [1] B. al., \u0026quot;Introduction to Statistics | Simple Book Production\u0026quot;, Courses.lumenlearning.com, 2021. [Online]. Available: https://courses.lumenlearning.com/introstats1. [Accessed: 14- Apr- 2021].\n"
            }
        
    
]