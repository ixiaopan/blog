[
    
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/end2end-project/",
                "title": "An E2E Project - EDA",
                "section": "post",
                "date" : "2021.04.24",
                "body": "We have discussed many algorithms, such as linear regression and ensemble methods. It\u0026rsquo;s time to kick off a project from scratch to learn the pipeline of a machine learning project.\nOverview There are 8 steps discussed in the book \u0026ldquo;Hands-on Machine Learning\u0026rdquo; to do a machine learning project,\n Frame the problem Get the data Explore the data Prepapre the data Explore many different models Fine-tune Deploy  In this post, we will cover the first 3 parts.\nFrame the problem The first step is to define you problem. People are unlikely to do things without reasons, right?. So ask yourself, what problem do you want to solve? Why are they interested in you? What\u0026rsquo;s your objective? Are there any solutions out there?\nAs an example, we are going to predict used car prices because accurate prices prediction can help both buyers and sellers. For buyers, it can ensure the money that customers invest on used cars to be worthy. For used car dealers, they might want to know which factors influence car prices most so as to adjust sales strategy and offer a better prediction to customers. Therefore, there is a necessity for building a used car price prediction system.\nGet the data There are many ways to get the desired data. You can write a scaper to download the data from related websites or you can search related data from some public databases. And Kaggle is one of the most popular platforms that enables us to achieve our data science goals. The data we used in this project is Used Cars Dataset - Kaggle.\nEDA EDA stands for Exploratory Data Analysis, which is an important part throughout the project. In this step, we are trying to get insights from the data. Specifically, we will study the characteristics of each feature, find the correlation between them, visualize the data, identify the quality of the data like outlier detection.\nReferences [1] https://www.kaggle.com/austinreese/craigslist-carstrucks-data\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/ensemble-methods/",
                "title": "Ensemble Methods",
                "section": "post",
                "date" : "2021.04.20",
                "body": "Ensemble means a group of people or a collection of things.Thus, ensemble methods means rather than using a single model, we will use a group of different models to gain a better prediction. In fact, ensemble methods often outperform other models in Kaggle competitions. In this post, we will talk about the most popular ensemble methods , including voting, bagging, and boosting.\nOverview In real life, we often take advices from others. For example, suppose you want to know whether a movie is worthwhile to watch, you may ask your friends who\u0026rsquo;ve watched to give the movie a score(out of 5), say 3, 3, 2, 2, 2, 4. Since 5 people gave a score that is lower than 4, you may think about choosing another movie, let\u0026rsquo;s say Avatar. Then you ask your friends again and have some scores like this 5, 5, 5, 5, 5. Wow! All of your friends think that Avatar is an amazing movie that should certainly not be missed. You agree with their opinons and decide to watch Avatar finally. From this example, we can see that gathering plenty of opinions from different people are likely to make an informed decision.\nHere, I highlight the words different people. It makes little sense if we only asks for people who have the same interests. Therefore, the more diverse the people are, the more sensible our decisions are. Basically, the idea behind it is the wisdom of collaborating.\nVoting The method used to decide whether to watch a movie or not in this example is known as voting. For classification, there are 2 types of voting named hard voting and soft voting.\n Hard voting returns the most popular class shown in Figure 1. Soft voting averages the probability of each class and then return the class that has the maximum probability.  Hard Voting   Figure 1: Hard voting classifier predicitons (Hands-on machine learning, 2019)  Figure 1 can also be illustrated in a mathematical way,\n$$ y' = mode(C_1(x), C_2(x), \u0026hellip;, C_n(x)) $$\nFor example, {0, 1, 0, 1, 1} are the class labels predicted by our 5 different classifiers for a data point $x$. By hard voting, the final class label is class 1 .\nC1 -\u0026gt; 0 C2 -\u0026gt; 1 C3 -\u0026gt; 0 C4 -\u0026gt; 1 C5 -\u0026gt; 1 Weighted Hard Voting Hard voting works nice, but in some cases, some people might be more professional than others. Hence, their opinions are much more significant. How to distinguish professionals and common people?\nWe assign weights to them. Specifically, we assign higher weights to professionals while common people have lower weights. Then we calculate weighted sum of occurrence of each class label and find the class label that has the maximum value.\n$$ y' = \\operatorname*{argmax}_i w_j\\sum_j [C_j == i] $$\nwhere $[C_j == i] = 1$ if classifier $j$ predicts class label i and 0 otherwise.\nFor example, if we assign the following weights to the previous 5 classifiers, then we will have 0.7 for class 0 and0.5 for class 1. Thus, class 0 wins because 0.7 is greater than 0.5.\n0.4, C1 -\u0026gt; 0 0.1, C2 -\u0026gt; 1 0.3, C3 -\u0026gt; 0 0.2, C4 -\u0026gt; 1 0.2, C5 -\u0026gt; 1 Soft Voting Instead of predicting the class label directly, some classifiers like logistic regression can predict the probability of each class label that $x$ belongs to. Then we simply average these probabilities for each class label. Certainly, you can assign weights to classifiers.\n$$ y' = \\operatorname*{argmax}_i \\frac{1}{n} \\sum_j^n w_j p_{ij} $$\nwhere $p_{ij}$ is the probability of class label $i$ that $x$ belongs to when using classifier $C_j$.\nAverage for Regression We simply average the predictions of different machines for a regression task.\n$$ y' = \\frac{1}{n} \\sum_j^n w_j C_j $$\nBagging In order to make our models different from each other, we use various algorithms to train the same data, as discussed above. Another way to have a set of diverse models is to train the same model on different data sets. But usually we only have one training data set. Where do other data sets come from? Well, they are sampled with replacement from the original data set, which is known as bootstrapping.\n  Figure 2: The process of bagging (Hands-on machine learning, 2019)  Specifically, given a training data set $D=(x_i, y_i)_i^n$ of the size $N$, we build an ensemble model of size $m$ according to the following steps:\n  For $i=1, 2, 3, \u0026hellip;, m$\n draw $N'(N' \\le N)$ samples with replacement from $D$, which is denoted by $D^*_i$ build a model (e.g. decision tree) $T^*_i$ based on $D_i^*$    For an unseen data, aggregate the predictions of all $T^*$\n perform a majority vote for classification average the predictions for regression    from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier ensemble_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=300, max_sample=100, bootstrap=True) Random Forest Bagging can be used for any models. Among them random forest is the special one. As its name suggests, it is exclusively designed for decision trees. Besides, it introduces extra randomness when growing trees.\n  Figure 3: A random subset of features at each split for each tree (Reference [2])  Specifically, it randomly choose a subset of $m'$ of the features at each split instead of using all features shown in Figure 3. By doing so, all trees can have much different training data set further, so they are less similar to each other, which results in more significant predictions.\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier from sklearn.tree import DecisionTreeClassifier random_forest_clf = BaggingClassifier(splitter=\u0026#39;random\u0026#39;, DecisionTreeClassifier(), max_leaf_nodes=16,n_estimators=300, max_sample=100, bootstrap=True) ## is equivalent to this random_forest_clf=RandomForestClassifier(n_estimators=300, max_leaf_nodes=16) Extra-Trees TODO\nWhy Bagging works Take random forests as an example, each decision tree is a machine learned from a data set. Based on the theory of bias and variance, we know that the mean meachine can be expressed as $f'_m=E_D[f'(x|D)]$. Thus, $f'(x|D)$ can be interpreted as a random variable $X$, and $f'_m$ can be described as $\\mu = E(X)$.\nSince we have $N$ decision trees in a random forest, there are $N$ random variables $X_i$, where $\\mu = E(X_i)$. We can construct a new random variable $Z = \\frac{1}{n}\\sum_i^nX_i$, which represents the mean of $n$ random independent variables $X_i$.\nThe expected value of $Z$ is given by\n$$ E[Z] = E[ \\frac{1}{n}\\sum_i^nX_i ] =\\frac{1}{n} E[\\sum_i^nX_i] = \\frac{1}{n} nE[X_i] = \\mu $$\nThe variance is\n$$ E[(Z - E[Z])^2] = E[(\\frac{1}{n}\\sum_i^nX_i - \\mu)^2] = \\frac{1}{n^2} E[(\\sum_i^n (X_i - \\mu))^2] $$\n$$ = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2 + \\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] $$\nSince $X_i$ is independent of $X_j$ ( $i\\ne j$ ),\n$$ E[\\sum_i^n\\sum_{j=1,i \\ne j}^n (X_i -\\mu)(X_j -\\mu)] = 0 $$\nwe are left with\n$$ E[(Z - E[Z])^2] = \\frac{1}{n^2}E[\\sum_i^n(X_i - \\mu)^2] = \\frac{1}{n} \\sigma^2 $$\nwhere $E[(X_i-\\mu)^2]=\\sigma^2$.\nFrom the above euqation, we can see that ensemble methods reduce variances as $n$ increases when our models are uncorrelated.\nOn the contrary, if our models are correlated with the correlation coefficient $\\rho$\n$$ \\rho = \\frac{E[(X_i - \\mu)(X_j - \\mu)]} {\\sqrt{\\sigma^2(X_i)}\\sqrt{\\sigma^2(X_j)}} $$\nThe variance is\n$$ E[(Z - E[Z])^2] = \\frac{1}{n} \\sigma^2 + \\frac{n-1}{n} \\rho \\sigma^2 = \\rho \\sigma^2 + \\frac{(1 - \\rho)}{n} \\sigma^2 $$\nAs $n$ increases, the second term vanishes and we are left with the first term. Therefore, if we want the ensemble methods to do well, we need our models to be uncorrelated.\nRandom forests do a good job because of both the randomness of training data sets sampled via bootstrapping and the randomness of features considered at each split. The algorithm results in much less correlated trees, which trades a higher bias for a lower variance, generally yielding better results.\nFeature Importance Like decision tree, random forests can tell us the relative importance of each feature. It is measured by calculating the sum of the reduction in impurity over all the nodes that are split on that feature.\nrandom_forest_clf.feature_importances_ Boosting In boosting, we combine a group of weak learners into a strong learner.\nWhat\u0026rsquo;s the weak learners? They are the learning machines that do a little better than chance. Thus, they have high bias but low variance. The goal of boosting is to reduce the bias by combining them.\nHow to construct a weak learner? Well, one of the widely used types of weak learner are very shallow trees, for example, the stump with only one depth.\nAdaBoost and GradientBoost are two popular algorithms for boosting. Let\u0026rsquo;s lookt at AdaBoost first.\nAdaBoost AdaBoost is a classic algorithm for binary classification. Suppose we have a data set $D=(x^i,y^i)_i^N$ with $y^i \\in {-1, 1}$, and our weak learner $h(x)$ provides a prediction $h(x^i) \\in {-1, 1 }$. The goal of AdaBoost is to construct a strong learner by combining all the weak learners, which can be written as a weighted sum of weak learners,\n$$ C_n(x) = \\sum_i \\alpha_i h_i(x) $$\nHow to find $\\alpha_i$ and $h_i(x)$? Well, it\u0026rsquo;s difficult to find all the coefficients for one time, so we will solve this equation greedily,\n$$ C_n(x) = C_{n-1}(x) + \\alpha_n h_n(x) $$\nwhere $C_{n-1}(x)$ is the current ensemble model that fit the training data best and $h_n(x)$ is the weak learner we are going to add.\nExponential Loss The second step is to find an appropriate loss function to optimize. How do we measure the performance of a classification model? One of the most widely used loss functions is 0-1 loss,\n$$ L = \\sum_i^N [y_i \\ne y'_i] $$\nwhere $[y_i \\ne y'_i] = 1$ if $x_i$ is classified incorrectly and 0 otherwise. However, it\u0026rsquo;s not-convex and difficult to optimize shown in Figure 4. In AdaBoost, we use exponential loss.\n$$ L = \\sum_i^n e^{-y^iC_n(x^i)} $$\nFrom Figure 4, it can be seen that data that are classified correctly have lower value while misclassfication observations have much larger values, which means exponential loss punishes examples classified incorrecly much more than correct classifications.\n  Figure 4: Exponential loss in AdaBoost  Intuition To minimize the loss, we plug the previous $C_n(x)$ into the loss function\n$$ L = \\sum_i^n e^{-y^i (C_{n-1}(x^i) + \\alpha_n h_n(x^i))} = \\sum_i^n e^{-y^i C_{n-1}(x^i)} e^{-y^i \\alpha_n h_n(x^i)} =\\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} + \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} $$\n$$ = \\sum_{y^i= h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} - \\sum_{y^i\\ne h_n(x^i) }^n w_n^i e^{-\\alpha_n} + \\sum_{y^i\\ne h_n(x^i)}^n w_n^i e^{\\alpha_n} $$\n$$ = e^{-\\alpha_n} \\sum_i^n w_n^i + (e^{\\alpha_n} - e^{-\\alpha_n}) \\sum_{y^i\\ne h_n(x^i) }^n w_n^i $$\nThen we find that minimizing the loss is equivalent to minimizing the sum of weights of each data that $h_n(x)$ misclassified, and that the value of weights depend on the current ensemble model $C_{n-1}(x)$.\n$$ \\sum_{y^i\\ne h_n(x^i) }^n w_n^i = \\sum_{y^i\\ne h_n(x^i) }^n e^{-y^i C_{n-1}(x^i)} $$\n If the data misclassified by $C_{n-1}(x)$ are still classified incorrectly by $h_n(x)$, then $w_n^i$ is extremely large. If the data classified correcly by $C_{n-1}(x)$ are misclassified by $h_n(x)$, then $w_n^i$ is small.  Simply put, misclassified data points will get high weights while correctly classified data points will get their weights decreased.\nTherefore, we are finding some weak learner that tries to correct the errors the previous learners made. Furthermore, we also notice that we need to update $w_n^i$ for the next weak learner $h_{n+1}(x)$,\n$$ w_{n+1}^i = e^{-y^i C_{n}(x^i)} = e^{-y^i (C_{n-1}(x^i) + \\alpha_nh_n(x^i))} = w_n^i e^{-y^i\\alpha_nh_n(x^i)} $$\nSo the new weight of each data depends on the last weight of that data, the weight of the previous weak learner $h_n(x)$ and itself. But wait, what\u0026rsquo;s the initial weight of each data? We simply initialize weights $w_1^i = \\frac{1}{N}$ for every training sample.\nOkay, now we are only left with $\\alpha_n$. To find $\\alpha_n$, we take the derivative of $L$ with respect to $\\alpha_n$\n$$ \\frac{\\partial L}{\\partial \\alpha_n} = e^{\\alpha_n} \\sum_{y^i\\ne h_n(x^i)}^n w_n^i - e^{-\\alpha_n} \\sum_{y^i= h_n(x^i) }^n w_n^i $$\nThat is\n$$ \\alpha_n = \\frac{1}{2} In\\frac{\\sum_{y^i= h_n(x^i) }^n w_n^i}{\\sum_{y^i\\ne h_n(x^i) }^n w_n^i} $$\nAlgorithm Let\u0026rsquo;s put it all together. The algorithm of AdaBoost can be summarised as below,\n  Given a data set $D=(x^i,y^i)_i^N$ with $y^i \\in \\{-1, 1\\}$ and a group of weak learners $h(x)$ of size $T$\n  Associate a weight $w_1^i = \\frac{1}{N}$ with every data point $(x^i, y^i)$\n  For $t = 1$ to $T$\n Train a weak learner $h_t(x)$ that minimises $\\sum_{y^i\\ne h_t(x^i) }^n w_t^i $ Update the weight of this learner, $\\alpha_t = \\frac{1}{2} In\\frac{\\sum_{y^i= h_t(x^i) }^n w_t^i}{\\sum_{y^i\\ne h_t(x^i) }^n w_t^i}$ Update weights for each training point, $w_{t+1}^i = w_t^i e^{-y^i\\alpha_th_t(x^i)}$    Make a prediction\n $C_n(x) = sign[\\sum_t^T \\alpha_t h_t(x)]$    Example Step 1: Initialisation\nHere we have 8 rows with 3 predictors chest_pain, blocked_arteries and weight and 1 target variable heart_disease. Each data point is initialised with an equal weight 0.125.\n  Figure 5: Toy data from \u0026#39;StatQuest with Josh Starmer\u0026#39;  Step 2: Find the weak learner\nHere, we use stump as our weak learner and Figure 6 shows the first optimal tree where we only misclassified one observation.\nfrom sklearn import tree X = df.drop([\u0026#39;heart_disease\u0026#39;, \u0026#39;weights\u0026#39;], axis=1) y = df[\u0026#39;heart_disease\u0026#39;] clf = tree.DecisionTreeClassifier(max_depth=1) clf = clf.fit(X, y) tree.plot_tree(clf.fit(X, y)) plt.show()   Figure 6: The first stump  Step 3: Update weights\nSince we have only one misclassification, the error rate is 1/8 and $\\alpha_1$ is 0.97. Then we update weights for each data point using $\\alpha_1$.\ndef cal_alpha(error): return 0.5*np.log((1 - error)/error) alpha_1 = cal_alpha(1/8) correct_samples = df[clf.predict(X) == y] df.loc[clf.predict(X) == y, \u0026#39;weights\u0026#39;] = correct_samples[\u0026#39;weights\u0026#39;] * np.exp(-alpha_1) misclassified_samples = df[clf.predict(X) != y] df.loc[clf.predict(X) != y, \u0026#39;weights\u0026#39;] = misclassified_samples[\u0026#39;weights\u0026#39;] * np.exp(alpha_1) print(alpha_1, df)   Figure 7: Updated weigts after training the first stump  Step 4: Go back to Step 2 until the desired number of learners is reached.\nAdjusted impurity Recall that Gini Index is written as\n$$ Q_m^g(L) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nand entropy discussed in Decision Tree as\n$$ Q_m^e(L) = -p_c(L)logp_c(L) $$\nwhere $p_c(L)$ is the fraction of the observations belong to class $c$. In order to use the weight of each data in AdaBoost, we need to change it slightly.\n$$ p_c(L) = \\frac{\\sum_{x^j \\in C} w_n^j I[y_j == C]}{\\sum_{x^i \\in L} w_n^i} $$\nWhy this works? Remember that the lower the impurity is, the better the split is. And a higher fraction leads to a lower impurity or entropy.\n If we classify the misclassified example in the node $L$ correctly, then the denominator of $p_c(L)$ becomes smaller and then $p_c(L)$ becomes larger. On the contratry, if this split works so bad, then we will have many observations that classified incorrectly, resulting in smaller $p_c(L)$ due to a small numerator and large denominator.  Thus, we are finding the best split that can correctly classify the examples that previous learners failed as much as possible.\nReferences [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2]\tT. Yiu, “Understanding random forest - towards data science,” Towards Data Science, 12-Jun-2019. [Online]. Available: https://towardsdatascience.com/understanding-random-forest-58381e0602d2. [Accessed: 23-Apr-2021].\n[3]\tJ. Rocca, “Ensemble methods: bagging, boosting and stacking - Towards Data Science,” Towards Data Science, 23-Apr-2019. [Online]. Available: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205. [Accessed: 23-Apr-2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/decision-tree/",
                "title": "Decision Tree",
                "section": "post",
                "date" : "2021.04.19",
                "body": "The way decision tree works is similar to the way we make decisions in real life. For example,when you are going to watch a movie, you might have some questions on your head, such as \u0026lsquo;Is it a fiction movie? Is it directed by David Fincher?\u0026rsquo;\n  Figure 1: The process of movie selection  From Figure 1, we can see that a decision tree builds a binary tree to partion the data. Each node is a decision rule based on a feature and the tree can grow endlessly. Two questions then arise:\n How is the decision rule made? How deep is the decision tree?  Decision Rule Since there are so many features and each feature can have different values, we have to loop over all of them and find the best split. Well, how to measure the performance of a split? There are 3 measures widely used — Gini Impurity, Entropy, and Information Gain.\nGini Index Gini Index, also known as Gini Impurity, measures how often a randomly selected element from the set is classified incorrectly.\n$$ Q_m^g(L) = \\sum_{c \\in C } p_c(L) (1 - p_c(L)) = 1 - \\sum_{c \\in C } p_c(L)^2 $$\nwhere $L$ is short for the children node and $p_c(L)$ is the faction of class $c$ in the leaf node $L$,\n$$ p_c(L) = \\frac{1}{|L|} \\sum_{x, y \\in L} [y == c] $$\nwhere $[y == c] = 1$ if $y$ belongs to class $c$ and 0 otherwise.\n  Figure 2: The plot of Gini Index  Figure 2 shows that the value of Gini Index is the lowest at the start and end of the x-axis and maximum at the middle of the x-axis. In other words,\n If the leaf node only has one class, then this node is a pure node and the gini impurity of this leaf node is 0. On the other hand, if all elements in this leaf node belong to an individual class, then the Gini Index of this node has the maximum value of $ 1 - 1/len(L) $.  Entropy Entropy is a concept of Information Theory. Before introducing the entropy, we should have a little understanding of information.\nInformation is related to the surprise in some way. For example, if I told you that you will go to work tomorrow. Well, that\u0026rsquo;s not surprising because you work every day. However, if I told you that tomorrow is the end of the world, you are likely to be shocked because it is an breaking news.\nWe can measure this surprise by the following equation\n$$ I(x) = -log(p(x)) $$\nIf an event is unlikely to happen, then $p(x)$ is close to 0 and $I(x)$ tends to be infinity, which means it conveys much information since impossible things happened and it must have a significant implication behind it.\nThen what\u0026rsquo;s the entropy? The previous equation calculate the information contained in one outcome. However, it\u0026rsquo;s quite often to have many outcomes for a random variable $X$. Therefore, the expected information over all outcomes is defined as the entropy.\n$$ H(X) = E_{x \\in X}[-log(p(x)] $$\nIn this case, the classes in the leaf node is the random variable $X$ and the probability of each outcome is the fraction of each class, which is expressed as\n$$ Q_m^e(L) = \\sum_{c \\in C}- p_c(L) log(p_c(L)) $$\nIn a word, entropy measures the randomness of a set. Lower entropy means a purer set while higher entropy means there are more other classes that is not the class $c$ in a set.\nInformation Gain Information Gain is simply the difference between the impurity of the parent node $A$ before splitting and the sum of impurity of all children nodes after splitting. It measures how much the impurity of a set $A$ were reduced after splitting. The larger the information gain is, the better the split is.\n$$ IG(A) = H(A) - \\sum_{L \\in children \\ nodes} p(L) H(L) $$\nIn summary, when a decisiton tree makes a split on a feature, it tries to achieve,\n a lower impurity a lower entropy a higher information gain  CART CART is short for classification and regression tree algorithm, which is used to train Decisioin Trees. CART tries to find the best split that produces purest subsets by calculating the weighted Gini Impurity on each split made by each feature $k$ from the parent node\u0026rsquo;s training sample and corresponding threshold $t_k$.\nSpecifically, it tries to minimize the following loss function for a classification task,\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}Q_m(C_m^L) + \\frac{|C_m^R|}{|N_m|} Q_m(C_m^R) $$\nwhere $C_m^L$ and $C_m^R$ are children nodes splitted on node $N_m$.\nFor a regression task, it minimizes the sum of squred error\n$$ L(k, t_k) = \\frac{|C_m^L|}{|N_m|}SSE(C_m^L) + \\frac{|C_m^R|}{|N_m|} SSE(C_m^R) $$\nwhere $SSE(subset)$ is defined as\n$$ SSE(subset) = \\sum_{n \\in subset} (y_n - \\overline y)^2 $$\n$$ \\overline y = \\frac{1}{|subset|} \\sum_{n \\in subset} y_n $$\nPrunning Now let\u0026rsquo;s address the second problem, i.e. when to stop growing the tree. You can either stop growing the tree when you build the tree or trim the tree after building, which are known as pre-pruning and post-pruning respectively.\nPre-pruning Scikit-learn provides several hyperparameters to do a pre-pruning:\n the maximum depth the minimum number of the samples a node must have to split the minimum number of the samples in a leaf node the maximum number of leaf nodes  All of these parameters can be tuned with cross-validation.\nPost Pruning Though pre-pruning is straightforward, it is a bit short-sighted since it doesn\u0026rsquo;t build a full tree and there might be some splits works better later on. Therefore, it would be better to have a large and full-size tree and then we trim some useless branches to get a better substree. Cost complexity pruning, also known as weakest link pruning, is one way to do this.\nPros and Cons Advantages:\n Decision trees are simple and intutive to interpret, and we can easily visualize the process of decision making. They can be used for both classification and regression. There is no need to normalize or scale data. They can help to understand what features are most important.  Disadvantages:\n They can be easy to overfit. They are sensitive to variations of training data. If you rotate the same data, you will get a completely different tree because all splits are perpendicular to an axis.    Figure 3: senstivity to variations of training set (Hands-on machine learning 2019)  References [1] A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow. Sebastopol (CA): O\u0026rsquo;Reilly Media, 2019.\n[2] arunmohan, “Pruning decision trees,” Kaggle.com, 02-Sep-2020. [Online]. Available: https://www.kaggle.com/arunmohan003/pruning-decision-trees. [Accessed: 22-Apr-2021].\n[3] “How to choose α in cost-complexity pruning?,” Stackexchange.com. [Online]. Available: https://stats.stackexchange.com/questions/193538/how-to-choose-alpha-in-cost-complexity-pruning. [Accessed: 22-Apr-2021].\n[4] “Cost-complexity pruning - ML wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Cost-Complexity_Pruning. [Accessed: 22-Apr-2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-02/",
                "title": "Linear Regression 02",
                "section": "post",
                "date" : "2021.04.17",
                "body": "In the last post, we talked about simple linear regression. However, we only considered one predictor. In fact, it\u0026rsquo;s quite common to have mulitple attributes in real-world problems. For example, if we want to predict the price of a car, we have to consider many factors like car size, manufacturer and fuel type. Clearly, the simple linear regression is not suitable for this case. Therefore, we need to extend it to accommodate multiple predictors.\nMultiple Linear Regression Suppose we have a data set with the size of $n$, and each data point has $d$ dimensions. Then the input data is denoted by $X \\in R^{n \\times d}$, and the parameters and targets are denoted by $\\bold w \\in R^d$, $\\bold y \\in R^n$ respectively. Thus, the loss function can be written by the following equation:\n$$ L = \\sum_i^{n} (\\bold x_i \\bold w - \\bold y_i)^2 = (\\bold X \\bold w - \\bold y)^T(\\bold X \\bold w - \\bold y) $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - \\bold y^T \\bold X \\bold w - \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\n$$ = \\bold w^T\\bold X^T \\bold X \\bold w - 2 \\bold w^T \\bold X^T \\bold y + \\bold y^T \\bold y $$\nThen we take the derivative of $L$ with respect to $\\bold w$ as simple linear regression before. Well, we need to know a little bit about the matrix calculus\n$$ \\frac{\\partial}{\\partial \\bold x} \\bold x^TA\\bold x = (A + A^T)\\bold x $$\n$$ \\frac{\\partial}{\\partial \\bold x} A^T \\bold x = A $$\nThe gradient of $L$ can be seen easily\n$$ \\frac{\\partial}{\\partial \\bold x } L = (2\\bold X^T\\bold X)\\bold w - 2 \\bold X^T\\bold y $$\nSetting this gradient to zero,\n$$ \\bold w= (\\bold X^T\\bold X)^{-1} \\bold X^T \\bold y $$\nHowever, this equation is unlikely to work if $\\bold X^T\\bold X$ is not invertible(singular), such as if the number of features are more than the number of observations($n \u0026lt; d$). One way to solve this equation is to use SVD.\npseudoinverse SVD technique can decompose any matrix $A$ into the matrix multiplication of three matrices $U\\Sigma V^T$. Thus the above equation can be written in the following form\n$$ \\bold w = A^+y $$\n$$ A^+ = (\\bold X^T\\bold X)^{-1} \\bold X^T = V\\Sigma^{-1}U^T $$\nIn practice, the algorithm will set the elements of $\\Sigma$ that less than a smaller threshold to zero, then take the inverse of all nozero values, and finally transpose the resulting matrix i.e. $(U\\Sigma V^T)^{-1}$\nProbabilistic Interpretation It\u0026rsquo;s inevitable to introduce errors when we collect data. The error could be systematic errors, human errors or something else. We can define the error to be $\\epsilon_i$ for each observation.\n$$ y_i = a + bx_i + \\epsilon_i $$\nThe assumption of linear regression is that the expected error is zero. Specifically, the error follows the Gaussian distribution with the mean of zero and variance of $\\sigma^2$.\n$$ \\epsilon_i \\sim N(0, \\sigma^2) $$\nThus, the probability of $y_i$ is defined by the predictors $x_i$ and the paramters $a, b, \\sigma^2$.\nMLE We have found the parameters by minimizing the loss, but now we are going to use another method to derive the same result, which is known as maximum likelihood estimation(MLE).\nThe basic idea of MLE is that if the data were generated from some model, then what\u0026rsquo;s the parameters of the model were most likely to make this happen? In other words, we are finding the parameters that maximize the probability of the data $D$ that we\u0026rsquo;ve seen.\nSuppose we have a data set of inputs $X={x^{(1)}, x^{(2)}, \u0026hellip;, x^{(N)}}$ and corresponding target variables ${y_1, y_2, .., y_N}$ with a Gaussian noise $\\epsilon$. Then we can construct the likelihood of all data points,\n$$ L(\\theta|D) = \\prod_{n=1}^N p(y_i|x_i, a, b, \\sigma^2) $$\nUsually, we will take the log likelihood to make computation more simpler,\n$$ In(L(\\theta|D)) =\\sum_i^n In(\\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y - a - bx_i)^2}{2\\sigma^2}}) $$\n$$ = \\frac{N}{2}In\\sigma - \\frac{N}{2}In2\\pi - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(y_i - a - bx_i)^2 $$\nFrom above equation, we can see that maximizing the likelihood is equivalent to minimizing the sum of squared error.\nGeometry of Linear Regression In this section, we will look at the geometry of the linear regression. In $N$-dimensional space whose axes are the values of $y_1, y_2, \u0026hellip;, y_n$ , the least-squares solution is obtained by finding the orthogonal projection of the target vector $y$ onto the subspace spanned by the columns of $X$.\n  Figure 1: Geometry interpretation of the least-squares solution (PRML 2006)  From the following matrix form, we can see that the predicted value $\\bold y'$ lies the column space of $X$. If the true target value $\\bold y$ also lies in this space, then the loss of linear regression is zero, which is never the case in real life.\n$$ \\displaystyle{\\bold y' = \\bold X \\bold w = \\begin{bmatrix}1\u0026amp;x_{11} \u0026amp; x_{12} \u0026amp; \u0026hellip; \u0026amp; x_{1d}\\\\ 1\u0026amp;x_{21} \u0026amp; x_{22} \u0026amp; \u0026hellip; \u0026amp; x_{2d}\\\\ \u0026hellip; \\\\ 1\u0026amp;x_{n1} \u0026amp; x_{n2} \u0026amp; \u0026hellip; \u0026amp; x_{nd} \\end{bmatrix} \\begin{bmatrix}w_0\\\\ w_1\\\\ w_2\\\\ \u0026hellip;\\\\ w_d \\end{bmatrix} } $$\nReferences [1] C. Bishop, Pattern Recognition and Machine Learning. 2006.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/bias-variance-dilemma/",
                "title": "Bias-Variance dilemma",
                "section": "post",
                "date" : "2021.04.15",
                "body": "When you learn more about machine learning, you must hear people talking about high bias or high variance something like that. What does they mean by \u0026lsquo;high bias\u0026rsquo; or \u0026lsquo;high variance\u0026rsquo;?\nActually, when I first heard these terms, I was completely confused. Even though I tried to find the answer on Google, I still had no idea until I took the Advanced Machine Learning module in semester 2. Therefore, I\u0026rsquo;m writing this post to try to explain this. I hope this post can help people who are still struggling with them understand the two most important concepts clearly.\nGeneralization Error The underlying assumption of machine learning is that there are some relationships between data. However, we are not able to know this true function, otherwise there is no need to learn it.\nSuppose we have a true realtionship denoted by $f(x)$ (the red dot in Figure 1), and we want to construct a machine denoted by $f'(x)$ to approximate the true function based on the data $D$ sampled from the population $\\chi$.\nThe training loss is defined by the following equation, where $f'(x|D)$ is the machine we learn from this particular data set $D$\n$$ L_T(D) = \\sum_{x\\in D}(f'(x|D) - f(x))^2 $$\nHowever, our goal is to know how well this machine works on unseen data, which is known as generalization. The generalization loss is expressed as\n$$ L_G(D) = \\sum_{x\\in \\chi} p(x) (f'(x|D) - f(x))^2 $$\nIf we have another data set $D_1$, then we will get another machine $f'(x|D_1)$ and another generalization loss $L_G(D_1)$ shown in Figure 1.\n  Figure 1: Generalisation error  We can see that the generalization loss is depend on our training data. Thus, the generalization loss for a particular data set doesn\u0026rsquo;t make much sense. Instead, the average generalization loss over all the data set with the same size of $n$ is what we expect.\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\nMean Machine We have already known that there is a different machine $f'(x|D)$ for a given data set $D$. Thus, for an unseen data $x$, we will have many predictions of many different machines, which are represented in blut dots shown in Figure 2.\n  Figure 2: Bias and variance  The average prediction for an unseen data is the mean prediction(the yellow dot in Figure 2).\n$$ f'_m(x) = E_D[f'(x|D)] $$\nBias Bias is the distance between the mean prediction(the yellow dot) and the true value(the red dot) shown in Figure 2. High bias implies that our model is too simple and the prediction value is much far away from the true value.\n$$ B = \\sum_{x \\in \\chi} p(x) (f\u0026rsquo;m - f(x))^2 $$\nVariance Variance measures the variation in the prediction of the machine when we change different data set we train on. If we have a complex machine, as mentioned earlier, the machine will try its best to match every data in training data set. In other words, the machine memorized the trainining data and a little change in data set will cause significant variation in prediction.\n$$ V = \\sum_{x \\in \\chi}p(x) E_D[ (f'(x|D) - f\u0026rsquo;m)^2 ] $$\nBias-Variance dilemma Now it\u0026rsquo;s time to decompose the average generalisation error. Let\u0026rsquo;s plug the $f'_m(x)$ into the previous equation\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi} p(x)(f'(x|D) - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x) (f'(x|D) - f_m' + f_m' - f(x))^2] $$\n$$ = E_D[\\sum_{x\\in \\chi}p(x){(f'(x|D) - f_m')^2 + (f_m' - f(x))^2 + 2(f'(x|D) - f_m')(f_m' - f(x)) }] $$\nIt\u0026rsquo;s noticeable that the cross-term will cancel out because $f\u0026rsquo;m$ and $f(x)$ are constants no matter what data set $D$ is.\n$$ E_D[\\sum_{x\\in \\chi}p(x)2(f'(x|D) - f_m')(f_m' - f(x))] $$\n$$ = \\sum_{x\\in \\chi}p(x) (2E_D[f'(x|D)]-f\u0026rsquo;m)(f\u0026rsquo;m-f(x)) = 0 $$\nTherefore, we are left with\n$$ E_G = E_D[L_G(D)] = E_D[\\sum_{x\\in \\chi}p(x)(f'(x|D) - f_m')^2 + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2] $$\n$$ = \\sum_{x\\in \\chi}p(x) E_D[(f'(x|D) - f_m')^2] + \\sum_{x\\in \\chi}p(x)(f_m' - f(x))^2 $$\n$$ = V + B $$\nIn summary,\n If our machine is too simple, then we might not be able to fit the training data. Since the machine knows little about the data, it\u0026rsquo;s unlikely to work well on unseen data. This means our model has a high bias. If our machine is too complex, then we might be able to fit the training data perfectly. It means that the machine knows too much about the data, even the noise that it should not learn. Thus, it\u0026rsquo;s too sensitive to training data so that a little change in data will cause a great variance. This means our model has a high variance.  Throughout the world of machine learning, we are always trying to find a balance between bias and variance.\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/linear-regression-01/",
                "title": "Linear Regression 01",
                "section": "post",
                "date" : "2021.04.14",
                "body": "There are two main tasks in machine learning: regression and classification. Today we will talk about regression, more specifically, linear regression. Linear regression is simple and easy to understand and possibly the first algorithm that most people learn in the world of machine learning. So let\u0026rsquo;s go!\nProblem statement Suppose you are a teacher, and you record some data about the hours students spent on study and the grades they achieved. Then you want to predict the grade for given hours that someone spent. Here are some sample data you collected:\n   Hours 0.5 1 2 3 4     Grade 20 21 22 24 25    Since there are only two variables, let\u0026rsquo;s plot them.\n  Figure 1: The scatter plot of hours and grade  Well, we can see that the variable grade is positive related to the variable hours. For simplicity, we can use a simple line(the red line in this figure) to approximate this relationship. And this is exactly our first simple linear model.\nSimple Linear Regression Remember a line equation is written in this way:\n$$ y = ax + b $$\nIn this example, $x$ is the variable hours and $y$ is the variable grade , which we already know. So the problem is how to calculate the parameter $a, b$. Technically, this is called parameter estimation. Usually, there are two ways to do this: minimising the loss and maximising the likelihood. Now we focus on the former.\nLoss What is the loss? Basically, it\u0026rsquo;s the error between the esitmated value and our true value. Minimising the error is simply to make the estimated value as close to the true value as possible.\n  Figure 2: The error of a single data (Bradthiessen.com 2021)  For a single data point, the loss function is defined below, where $y$ is the true value and $y'$ is our estimated value for a given $a, b$.\n$$ error = y_i - y'_i = y_i - ax_i - b $$\nSince we have many data points, we need to sum up them all to evaluate the overall errors.\n1. error Unfortunately, some error terms will cancel out when you do this calculation directly.\n$$ L = \\sum_i^n (y_i - y'_i) = \\sum_i^n (y_i - ax_i - b) $$\n2. the absolute value of error One way to tackle this is taking the absolute value of the error terms.\n$$ L = \\sum_i^n |y_i - y_i'| $$\nHowever, the absoulte value of $x$ is not differentiable at $0$.\n3. the squared value of error Instead of taking absolute value, we will square all the errors. One reason is that the errors will become larger and can be distinguished easily when squaring them. It looks like the errors are zoomed in and we can find them and minimize them quickly. It is also known as Residual Sum of Squares(RSS) or Sum of Squared Error (SSE).\n$$ L = \\sum_i^n (y_i - y_i')^2 $$\nPS: We will revisit the squared error later from the perspective of MLE.\nClosed-form solution Okay, finally we find a function to measure the loss. Next we need to find the parameters that minimize the squared error. Good news is that our loss function is differentiable and convex! It means that we have a global minimial value and can be calculated directly by taking derivatives.\nLet\u0026rsquo;s take the first derivatve of $b$\n$$ \\frac{\\partial L}{\\partial b} = \\sum_i^n -2(y_i - ax_i-b) $$\nand then set this equation to $0$,\n$$ -2(\\sum_i^ny_i -a\\sum_i^nx_i - \\sum_i^nb) = -2(n\\overline y-an\\overline x - nb) = 0 $$\n$$ b = \\overline y - a\\overline x $$\nLet\u0026rsquo;s take the first derivatve of $a$\n$$ \\frac{\\partial L}{\\partial a} = \\sum_i^n -2x_i(y_i - ax_i-b) $$\nand then plug $b$ into this equaiton and set this equation to 0 again,\n$$ \\sum_i^n -2x_i(y_i - ax_i-\\overline y+a\\overline x) = \\sum_i^n -2x_i[(y_i-\\overline y)- a(x_i -\\overline x)] $$\n$$ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} $$\nHere, we use a slight algebra trick,\n$$ a\\sum_i^n(x_i - \\overline x_i) = 0 $$\nThen we plug this into the previous equation\n$$ a = \\frac{\\sum_i^nx_i(y_i-\\overline y)}{\\sum_i^nx_i(x_i -\\overline x)} = \\frac{\\sum_i^nx_i(y_i-\\overline y) - \\sum_i^n\\overline x(y_i - \\overline y)}{\\sum_i^nx_i(x_i -\\overline x) - \\sum_i^n\\overline x(x_i - \\overline x)} $$\n$$ = \\frac{\\sum_i^n(x_i-\\overline x)(y_i-\\overline y)}{\\sum_i^n(x_i -\\overline x)^2} $$\n$$ = \\frac{Cov(x, y)}{Var(x)} $$\nFinally, we find the best estimators for simple linear regression.\n$R^2$ So how to evaluate our model? In other words, how good is it? We can use $R^2$ to measure our model. Let\u0026rsquo;s rewrite the previous equation by multiplying both the denominator and numerator by $\\sqrt {\\sum_i^n(y_i-\\overline y)^2}$\n$$ a = \\frac{\\sum_i^n (x_i - \\overline x)(y_i - \\overline y) \\sqrt {\\sum_i^n(y_i-\\overline y)^2}}{\\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(x_i-\\overline x)^2} \\sqrt {\\sum_i^n(y_i-\\overline y)^2}} $$\n$$ a = R\\frac{s_y}{s_x} $$\nwhere\n$$ R = \\frac{Cov(x, y)}{\\sqrt{var(x)} \\sqrt{var(y)}} $$\n$$ s_y = \\sqrt{Var(y)} $$\n$$ s_x = \\sqrt{Var(x)} $$\nRemember that the error is defined as $e_i = y_i' - y_i$, so the mean of $e$ is\n$$ E(e) = \\frac{1}{N} \\sum_i^n e_i = \\frac{1}{N} \\sum_i^n b + ax_i - y_i =b + a\\overline x - \\overline y = 0 $$\nand the variance is\n$$ var(e) = \\sum_i^n (e_i - \\overline e)^2 = \\sum_i^n (y_i - b - ax_i)^2 = \\sum_i^n (y_i - \\overline y + a\\overline x - ax_i)^2 $$\nLet\u0026rsquo;s plug $a$ into this equation\n$$ var(e) = \\sum_i^n [(y_i - \\overline y) - R\\frac{s_y}{s_x}( x_i - \\overline x)]^2 = var(y) (1-R^2) $$\nOr you might be more familiar with this equation\n$$ R^2 = 1 - \\frac{var(e)}{var(y)} = 1 - \\frac{RSS}{TSS} $$\nTherefore, $R^2$ tells us how much variance of $y$ has been explained by our models. The higher the $R^2$ is, the better our model is.\nReferences [1] Bradthiessen.com, 2021. [Online]. Available: https://www.bradthiessen.com/html5/docs/ols.pdf. [Accessed: 14- Apr- 2021].\n[2] “Linear Regression - ML Wiki,” Mlwiki.org. [Online]. Available: http://mlwiki.org/index.php/Linear_Regression. [Accessed: 14-Apr-2021].\n"
            }
        
    ,
        
            {
                "ref": "https://ixiaopan.github.io/blog/post/descriptive-statistics/",
                "title": "Descriptive Statistics",
                "section": "post",
                "date" : "2021.04.13",
                "body": "In this post , I\u0026rsquo;m going to go through some basic concepts of statistics required in Data Science.\nThere are two main branches of statistics :\n  descriptive statistics tells us the statistics about the data like mean, mode and standard deviation, which you\u0026rsquo;ve learned in high school.\n  inferential statistics, on the other hand, uses a random dataset sampled from population to make inferences about population.\n  We will firstly focus on descriptive statistics, specifically, the central tendency and dispersion. Central tendency measures the center of the data while dispersion measures how spread out a given data is.\nCentral tendency Mean, mode and median are three mainly used measures of central tendency.\nMean Mean is the average of the data and calculated by summing up all data values and then dividing them by the number of data.\n$$ \\overline x = \\frac{\\sum_i^nx_i}{n} $$\nFor instance, say we have a group of data 1,2,3,4,5,6, the mean is 3.5.\nMedian Though mean is widely used, it\u0026rsquo;s sensitive to outliers. Suppose you have a set of data 1,2,3,4,5,6,100, after some calculations, you find that the mean is 17.28. However, it seems a bit strange since most of the data values is less than 10 except one extreme value 100, which stretches the distribution of the whole data set to the right. This is why median comes.\nTo get the median, firstly we need to sort the data in ascending order and then find the middle number that separate the data into two groups with the same size. In this example, the median is 4.\nMode Mode is the most frequent value. There could be one, two or more data values that have the same frequency and that freqency is the highest.\nDispersion Range Range is the distance between the maximum value and the minimum value. Again, range is sensitive to outliers.\n$$ r=max - min $$\nQuantile Quantiles are used to divide data into several equal-sized groups. The most widely used cut points are 0, 25, 50, 75, 100, denoted by min, Q1, Q2, Q3, max respectively.\nIQR or interquartile range measures where the central 50% of the data is.\n$$ IQR = Q3 - Q1 $$\nIQR can be used to detect outliers. Data that is greater than upper boundary or less than lower boundary can be considered as an outlier.\n$$ upper \\ boundary = Q3 + 1.5*IQR $$\n$$ lower \\ boundary = Q1 - 1.5*IQR $$\nVariance Deviation is the distance between a given data point and the mean. Since there are many data points, the variance calculates the average deviation from the mean.\nBut when you do this, you will always get a zero due to the definition of the mean.\n$$ \\sum_i^n d_i = \\sum_i^n (x_i - \\overline x) = \\sum_i^nx_i - n\\overline x = n\\overline x - n\\overline x = 0 $$\nFor simplicity, I omit the denominator n.\nOkay, let\u0026rsquo;s ignore the sign and use the absolute value of the deviation.\n$$ d_i = |x - \\overline x| $$\nThough it works, the most popularly used method is calculate the square of the deviation.\n$$ s^2 = \\frac{\\sum_i^n (x_i - \\overline x)^2}{n-1} $$\nThough variance works fine, the value is a bit hard to interpret. For instance, we have 15 records of fish size measured in kilogram:\n[ 2.1, 2.4, 2.4, 2.4, 2.4, 2.6, 2.9, 3.2, 3.2, 3.9, 4.5, 6.3, 8.2, 12.8, 23.5 ].\nThe variance is 30.97. It means that if we randomly catch a fish, its weight would be 30.97 squared kilogram far away from the average weight. Emm\u0026hellip;squared kilogram is an odd unit.\nStandard Deviation It\u0026rsquo;s simple to solve this problem by taking the square root of the variance, which is standard deviation.\n$$ s = \\sqrt{\\frac{\\sum (x - \\overline x)^2}{n-1}} $$\nSo the standard deviation in the previous example is 5.56kg, which makes much sense.\nDistribution Skewness Skewness measures the symmetry of a distribution. It\u0026rsquo;s quite common to have non-symmetric distributions.\n A left-/negative-skewed distribution\n it has a long left tail the mean is on the left of the median  A right-/positive-skewed distribution\n it has a long right tail the mean is on the right of the median  A skewed distribution implies that there are some special values that are larger/smaller than the common values. Let\u0026rsquo;s see an example.\n Figure 3.2 shows the histogram of the fish sizes gathered from a fisherman. We can see that this distribution is right-skewed. The majority of fish sizes are between 0 and 3kg and there are a few special fishes that weigh over 3.5kg. It also can be seen that the average weight(1.67kg) is a bit higher than the median(1.62kg) shown in Table 3.2.\nReferences [1] B. al., \u0026ldquo;Introduction to Statistics | Simple Book Production\u0026rdquo;, Courses.lumenlearning.com, 2021. [Online]. Available: https://courses.lumenlearning.com/introstats1. [Accessed: 14- Apr- 2021].\n"
            }
        
    
]