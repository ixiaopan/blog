<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Deep Learning - CNN - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" />
  <meta name="description" content="Convolutional Neural Networks (CNN) are widely used for image classification, object detection and other tasks related to images or videos in the field of computer vision. Plenty of architectures based on convolution operation have been proposed in recent years, such as AlexNet and ResNet. So, how do CNNs work? Why not use MLP? In this post, we will go through the fundamentals of CNN and several ConvNets to develop a big picture of CNN.
" />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/dl/cnn/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Deep Learning - CNN" />
<meta property="og:description" content="Convolutional Neural Networks (CNN) are widely used for image classification, object detection and other tasks related to images or videos in the field of computer vision. Plenty of architectures based on convolution operation have been proposed in recent years, such as AlexNet and ResNet. So, how do CNNs work?  Why not use MLP? In this post, we will go through the fundamentals of CNN and several ConvNets to develop a big picture of CNN." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/dl/cnn/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-09-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-09-23T00:00:00+00:00" />

<meta itemprop="name" content="Deep Learning - CNN">
<meta itemprop="description" content="Convolutional Neural Networks (CNN) are widely used for image classification, object detection and other tasks related to images or videos in the field of computer vision. Plenty of architectures based on convolution operation have been proposed in recent years, such as AlexNet and ResNet. So, how do CNNs work?  Why not use MLP? In this post, we will go through the fundamentals of CNN and several ConvNets to develop a big picture of CNN."><meta itemprop="datePublished" content="2021-09-23T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-09-23T00:00:00+00:00" />
<meta itemprop="wordCount" content="3142">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning - CNN"/>
<meta name="twitter:description" content="Convolutional Neural Networks (CNN) are widely used for image classification, object detection and other tasks related to images or videos in the field of computer vision. Plenty of architectures based on convolution operation have been proposed in recent years, such as AlexNet and ResNet. So, how do CNNs work?  Why not use MLP? In this post, we will go through the fundamentals of CNN and several ConvNets to develop a big picture of CNN."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/tags/">Tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Deep Learning - CNN</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-09-23">
      2021-09-23
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/deep-learning/"> Deep Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>Convolutional Neural Networks (CNN) are widely used for image classification, object detection and other tasks related to images or videos in the field of computer vision. Plenty of architectures based on convolution operation have been proposed in recent years, such as AlexNet and ResNet. So, how do CNNs work?  Why not use MLP? In this post, we will go through the fundamentals of CNN and several ConvNets to develop a big picture of CNN.</p>
<h2 id="convolution">Convolution</h2>
<h3 id="convolution-operation">Convolution Operation</h3>
<p>As its name suggests, CNN employes a special mathematical operation called convolution rather than matrix multiplication in the networks. So, what&rsquo;s the convolution? Mathematically, the convolution operation is defined as</p>
<p>$$
s(t) = (x \ast w)(t) = \int x(a) w(t - a)da
$$
where $x$ is the real valued function of $t$, $w$ is a weighting function that measures the weight of $x$ at the point of $t$. So, convolution is a weighted average operation. For example, $x$ returns the real-time position of a plane, but the returned positions have some noises. A solution is to average the positions of recent moments. Obviously, the most recent moment has the highest weight. For functions that return discrete values, the discrete convolution operation is defined as</p>
<p>$$
s(t) = (x \ast w)(t) = \sum_{a = -\infin}^{\infin} x(a) w(t - a)
$$</p>
<h3 id="cross-correlation">Cross-correlation</h3>
<p>The above equation considers only one variable. However, in machine learning, we often deal with multi-dimensional data. In this case, the convolution is simply the sum of the products from all dimensions,</p>
<p>$$
S(i, j) = (I \ast K)(i, j) = \sum_{m}\sum_{n} I(m, n)K(i - m, j - n)
$$</p>
<p>where $I$ is a two-dimensional input array, and $K$ is a two-dimensional weighting matrix, also known as a kernel. Careful people will notice that $K$ is flipped when multiplying $I$. Suppose $I$ represents an image whose width is $m$ and height is $n$, as $m$ increases, the index of $I$ increases while the index of $K$ decreases. That is, the left part of the image multiplies the right part of the kernel.</p>
<p>In deep learning, flipping a kernel or not is unnecessary, since the kernel is obtained through learning. If it should be flipped, then the learned kernel is flipped. Thus, there is no need to explicitly specify a flipped kernel, let alone we do not know what it is exactly. In fact, many deep learning frameworks implement a similar function and call it as convolution. This function is cross-correlation, which is the same as convolution but without flippling</p>
<p>$$
S(i, j) = (I \ast K)(i, j) = \sum_{m}\sum_{n} I(i+m, j+n)K(m, n)
$$
In CNN, $I$ and $K$ are often referred to as the input and the kernel, respectively. The output is often referred to as the feature map.</p>
<h3 id="receptive-field">Receptive Field</h3>
<p>In CNN, the scalar obtained from the cross correlation is referred to a neuron. The area enclosed by relevant inputs is called the receptive field of that neuron. So, convolution is simply an element-wise multiplication of learned weights (kernel) across a receptive field, which is repeated at various positions across the input. Normally, we also add a bias term for each filter.</p>
<p><img src="/blog/post/images/receptive-field.png" alt="" title="Figure 1: Receptive Field of a neuron"></p>
<h3 id="why-cnn">Why CNN</h3>
<p>Okay, but we still do not know why to use CNN instead?</p>
<h4 id="sparse-connectivity">Sparse connectivity</h4>
<p>In MLP, all input variables are connected to a hidden neuron. Suppose there are $m$ features and $h$ hidden units, we have $m * h$ parameters to learn. If we restrict the number of connections, say $k$, then there are only $k * h$ parameters. Generally, $k$ is far smaller than $m$, which greatly improves the efficiency of learning. This technique can be easily achieved in CNN through a kernel whose size is smaller than the input. So, sparse connectivity in CNN means</p>
<ul>
<li>reduce memory space to store the parameters</li>
<li>reduce the number of computation operations</li>
<li>increase the efficiency of learning</li>
</ul>
<h4 id="parameter-sharing">Parameter sharing</h4>
<p>Another difference is that kernels in CNN are shared. Instead of different $k$ parameters for each hidden neuron in the traditional neural networks, there are only $k$ shared parameters for each element in each feature map (for now, we only consider one channel).</p>
<h4 id="translation-equivariance">Translation equivariance</h4>
<p>Translation equivariance indicates that CNN should get the same result from the same pattern, no matter where the pattern is. Mathematically, it can be expressed as,</p>
<p>$$
I(T(x)) = T(I(x))
$$
where $T$ represents a transformation function, and $I$ is the mapping function. If the object of interest in the inputs moves, the corresponding representation should move the same amount in the feature map due to parameter sharing.</p>
<p>This works based on assumption that we want to detect a specific pattern in an image, irrelevant to locations. Suppose there are two of the same cats but in different positions in an image, it is redundant to learn the same parameters to classify them as cats. As a human, we know the second one is also a cat, because we see that it has whiskers, furs, ears, tails and other characters of being a cat. Thus, we hope the kernels or convolutions will also be able to extract these features, regardless of positions. As long as an area contains these features, it will be classified as cats.</p>
<h3 id="padding-and-stride">Padding and Stride</h3>
<p>Now let&rsquo;s see how a kernel really works. Figure 2 depicts the convolution operation between a two-dimensional array and a kernel with the size of $2 \times 2$ .</p>
<p><img src="/blog/post/images/conv-example.png" alt="" title="Figure 2: Convolution Operation"></p>
<p>The output tensor is derived as follows</p>
<p>$$
1 * 1 + 2 * 3 + 4 * 2 + 5 * 4 = 35 \\ 2 * 1 + 3 * 3 + 5 * 2 + 6 * 4 = 45 \\ 4 * 1 + 5 * 3 + 7 * 2 + 8 * 4 = 36 \\ 5 * 1 + 6 * 3 + 8 * 2 + 9 * 4 = 75
$$</p>
<p>We also noticed that the size of the output tensor is $2 \times 2$, which is smaller than the input size ($3 \times 3$). In fact, the output size is determined by the shape of the input and kernel, as shown below</p>
<p>$$
( I_h - K_h + 1 ) \times ( I_w - K_w + 1 )
$$</p>
<p>where $I$ is the input size, and $K$ is the size of the kernel. In this example, the output size is $ (3 - 2 + 1) \times ( 3 - 2  + 1) = 2 \times 2 $.</p>
<h4 id="why-padding">Why padding</h4>
<p>Padding and stride are two commonly used techniques in CNN. But why?</p>
<p>If we further apply another kernel to the output, we will get a scalar in the end, which is not always the desired result. Sometimes we want to preserve the shape of the input. So, we often use zero-padding to retain the size.</p>
<p>Besides, each time we perform convolution operation, we will lose pixels on the boundaries of the image as we only scan the border only once. In the above example, we can see that pixels on the top and bottom border of the image ($1, 3, 7, 9$) appear only once in the convolution operation, while the inner part of the image (2,4,5,6,8) present twice.</p>
<h4 id="why-stride">Why stride</h4>
<p>On the other hand, the step that the kernel moves each time either horizontally or vertically is 1, which is expensive. Sometimes we might want to quickly obtain a reduced output, a solution is to increase step.</p>
<p>With padding and stride, the output shape is given as</p>
<p>$$
\lfloor (I_h + P_h - K_h) / S_h + 1 \rfloor \times \lfloor (I_w + P_w - K_w) / S_w + 1 \rfloor
$$</p>
<p>where $P$ and $S$ indicates the padding and stride, respectively. From the above equation, we can see that</p>
<ul>
<li>the output size increases as $P_h$ and $P_w$ increases when $S$ stays the same</li>
<li>if $P_h = K_h - S_h$ and $P_w = K_w - S_h$, the shape of output is $I_h/S_h \times I_w / S_w$</li>
<li>if $S_h = 1$ and $S_w = 1$
<ul>
<li>an odd value of $K_h$ will pad the same rows in both sides of the height;</li>
<li>an even value of $K_h$, well, one solution is to pad $\lfloor P_h/2 \rfloor$ rows on the top of the image and $\lceil P_h/2 \rceil$ rows on the bottom</li>
</ul>
</li>
</ul>
<p>That&rsquo;s why you often see odd kernels in CNN (stride is 1 by default). Besides, in practice, we often set $P_h = P_w$ and $S_h = S_w$.</p>
<h3 id="multiple-channels">Multiple Channels</h3>
<p>So far, we only considered a single-layer input. As we know, an image has three channels —— R, G, B. That is, we need to perform convolution at each channel and then add the respective feature map together to obtain the final output, which is depicted in Figure 3.</p>
<p><img src="/blog/post/images/conv-input-channel.png#full" alt="" title="Figure 3: Convolution with multi-layer inputs"></p>
<p>Figure 3 shows that the number of kernels are the same as the number of channels in the input data. To make a clear distinction between one kernel and multiple kernels, we note that a kernel is a two-dimensional array (a slice of a filter) while a set of kernels are called a filter. In fact, many articles use them interchangeably. So far so good. But we still get one output. How do we get multiple outputs? —— The answer is to use multiple filters.</p>
<p>Now let&rsquo;s focus on the number of parameters. Let $C_I$ and $C_O$ be the number of input and output channels, and $K_w$ and $K_h$ be the width and height of each kernel, the total number of parameter needed in convolution layer is given as (bias is ignored)</p>
<p>$$
C_O * C_I * K_w * K_w
$$</p>
<p>In summary, the core parameters of a convolution layer are</p>
<ul>
<li>the dimensionality of the input</li>
<li>the spatial extent of the kernel</li>
<li>the number of kernels ( output channels )</li>
</ul>
<h3 id="data-types">Data Types</h3>
<p>Convolution can be applied to many dimensionalities and types of data, for example</p>
<table>
<thead>
<tr>
<th></th>
<th>Single Channel</th>
<th>Multiple Channels</th>
</tr>
</thead>
<tbody>
<tr>
<td>1D</td>
<td>Audio</td>
<td>multiple sensor data over time</td>
</tr>
<tr>
<td>2D</td>
<td>greyscale images</td>
<td>Colour image data</td>
</tr>
<tr>
<td>3D</td>
<td>Volumetric data</td>
<td>Colour Video Data</td>
</tr>
</tbody>
</table>
<h3 id="1-x-1-kernel">1 x 1 Kernel</h3>
<p>The minimum size of a kernel is $1 \times 1$. It seems meaningless at first, since such a kernel does not correlate neighbouring pixels (capture local spatial information). Well, perhaps the only reason we&rsquo;d like to use it is to change the dimension of the input channel ( the number of feature maps ).</p>
<p>$1 \times 1$ kernel will map each input element, so the output size is the same as the size of the input. However, the number of feature map depends on the number of filters we apply.</p>
<h2 id="pooling">Pooling</h2>
<p>A typical CNN contains three layers</p>
<ul>
<li>convolution layer</li>
<li>activation layer</li>
<li>pooling layer</li>
</ul>
<h3 id="maxavg-pooling">Max/Avg Pooling</h3>
<p>We have introduced the previous two layers, so what does pooling do? Pooling acts more like a window, where it reports some statistics from a group of data enclosed by that window, such as the maximum value or the average value, as shown in Figure 4 and Figure 5.</p>
<p><img src="/blog/post/images/max-pooling.png" alt="" title="Figure 4: Max Pooling"></p>
<p><img src="/blog/post/images/avg-pooling.png" alt="" title="Figure 5: Average Pooling"></p>
<p>Unlike kernels, the pooling layer does not contain any parameter. But, why Pooling?</p>
<ul>
<li>decrease output dimension
<ul>
<li>reduce memory space for storing parameters</li>
<li>improve computational efficiency</li>
</ul>
</li>
<li>make feature representation approximately invariant to small translations of the input</li>
</ul>
<blockquote>
<p>In all cases, pooling helps to make the representation approximately invariant to small translations of the input. Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.</p>
<p>— Deep Learning, P336</p>
</blockquote>
<h3 id="local-vs-global-pooling">Local VS Global Pooling</h3>
<p>The above pooling operations are local pooling, because they result in a smaller feature map. On the contrary, global pooling reduces a feature map to a scalar, which often used near the end of networks to flatten feature mapts into a feature vector that can be fed into a MLP.</p>
<h2 id="convnet">ConvNet</h2>
<h3 id="lenet">LeNet</h3>
<p><a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">LeNet</a> was the earliest CNN proposed for handwritten digit recognition in 1998. As the oldest CNN architecture, it is a good starting point to learn how CNNs work before we move on to more complex networks.</p>
<h4 id="architecture">Architecture</h4>
<p><img src="/blog/post/images/leNet-5.png#full" alt="" title="Figure 6: Architecture of LeNet-5. Source: GradientBased Learning Applied to Document Recognition"></p>
<p>Figure 6 shows that LeNet has 7 layers consisting of</p>
<ul>
<li>3 convolutional layers</li>
<li>2 sub-sampling layers</li>
<li>2 fully-connected layers</li>
</ul>
<p>which are denoted by $C_x$,  $S_x$, and $F_x$, respectively. Based on the formulas discussed above, we can derive the shape of the intermediate ouputs and learning parameters in each layer in LeNet, which is shown below.</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Kernel/Padding/Pooling</th>
<th>Output</th>
<th>Num of Paramaters</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>1@32$\times$32</td>
<td></td>
</tr>
<tr>
<td>C1</td>
<td>6@5 $\times$ 5, 0,1</td>
<td>6@28$\times$28</td>
<td>5*5*1*6+6 = 156</td>
</tr>
<tr>
<td>S2</td>
<td>2$\times$2,0,2</td>
<td>6@14$\times$14</td>
<td>6*2=12</td>
</tr>
<tr>
<td>C3</td>
<td>16@5 $\times$ 5,0,1</td>
<td>16@10$\times$10</td>
<td>(5*5*3*6 + 6) + (5*5*4*9 + 9)+(5*5*6*1 + 1)  =1516</td>
</tr>
<tr>
<td>S4</td>
<td>2$\times$2, 0, 2</td>
<td>16@5$\times$5</td>
<td>16*2=32</td>
</tr>
<tr>
<td>C5</td>
<td>120@5 $\times$ 5</td>
<td>120</td>
<td>5*5*16*120 + 120 = 48120</td>
</tr>
<tr>
<td>F6</td>
<td>NaN</td>
<td>84</td>
<td>120*84+84 = 10164</td>
</tr>
<tr>
<td>Output</td>
<td>NaN</td>
<td>10</td>
<td>84 * 10 + 10 = 850</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>60,850</td>
</tr>
</tbody>
</table>
<p style="text-align:center">Table 1: The shape of the output and learning parameters in LeNet.</p>
<p>LeNet differs from modern CNNs in several ways</p>
<ul>
<li>the activation function is sigmoid function rather than reLU</li>
<li>LeNet used subsampling (similar to average pooling) to reduce output dimentionality while modern CNNs use max pooling</li>
</ul>
<h4 id="implementation">Implementation</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LeNet</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, num_class):
        super(LeNet, self)<span style="color:#f92672">.</span>__init__()
   
        <span style="color:#75715e"># in_channel, output_channel, kernel_size, stride, padding</span>
        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">5</span>)
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">16</span>,<span style="color:#ae81ff">5</span>)
        
        <span style="color:#75715e"># self.conv3 = nn.Conv2d(16, 120, 5)</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">120</span>) 
        
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">120</span>, <span style="color:#ae81ff">84</span>) 
        self<span style="color:#f92672">.</span>fc3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">84</span>, num_class) 

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>sigmoid(self<span style="color:#f92672">.</span>conv1(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>avg_pool2d(x, <span style="color:#ae81ff">2</span>)
        
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>sigmoid(self<span style="color:#f92672">.</span>conv2(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>avg_pool2d(x, <span style="color:#ae81ff">2</span>)

        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>flatten(x, <span style="color:#ae81ff">1</span>)
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>sigmoid(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>sigmoid(self<span style="color:#f92672">.</span>fc2(x))
        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc3(x)

        probs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

        <span style="color:#66d9ef">return</span> logits, probs
</code></pre></div><h3 id="alexnet">AlexNet</h3>
<p><a href="https://image-net.org/challenges/LSVRC/index.php">ImageNet</a> is a publicly large image database for computer vision and deep learning research. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was an annual image classification competition held from 2010 to 2017 using 1.3 million images in 1000 classes from the ImageNet dataset. AlexNet was the winner of ILSVRC 2012, achieving a top-5 error rate of 15.3%.</p>
<h4 id="architecture-1">Architecture</h4>
<p><img src="/blog/post/images/AlexNet.png#full" alt="" title="Figure 7: Architecture of AlexNet. Source: [ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3065386)"></p>
<p>As shown in Figure 7, AlexNet consists of 8 layers</p>
<ul>
<li>5 convolutional layers</li>
<li>3 fully connected layers</li>
</ul>
<p>Each layer and the corresponding number of learning parameters are shown below,</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Kernel/Padding/Pooling</th>
<th>Output</th>
<th>Num of Paramaters</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>3@227$\times$227</td>
<td></td>
</tr>
<tr>
<td>C1</td>
<td>96@11 $\times$11, 0, 4</td>
<td>96@55$\times$55 (=(227-11)/4+1)</td>
<td>11*11*3*96+96 =</td>
</tr>
<tr>
<td>P2</td>
<td>3$\times$3, 0, 2</td>
<td>96@27$\times$27 (=(55-3)/2+1)</td>
<td>0</td>
</tr>
<tr>
<td>C3</td>
<td>256@5 $\times$ 5,2,1</td>
<td>256@27$\times$27 (=(27-5+2*2)/1 + 1)</td>
<td>5*5*96*256 + 256 =</td>
</tr>
<tr>
<td>P4</td>
<td>3$\times$3, 0, 2</td>
<td>256@13$\times$13 (=(27-3)/2+1)</td>
<td>0</td>
</tr>
<tr>
<td>C5</td>
<td>384@3 $\times$ 3, 1, 1</td>
<td>384@13$\times$13 (=(13-3+1*2)/1+1)</td>
<td>3*3*256*384 + 384 =</td>
</tr>
<tr>
<td>C6</td>
<td>384@3 $\times$ 3, 1, 1</td>
<td>384@13$\times$13 (=(13-3+1*2)/1+1)</td>
<td>3*3*384*384 + 384 =</td>
</tr>
<tr>
<td>C7</td>
<td>256@3 $\times$ 3, 1, 1</td>
<td>256@13$\times$13 (=(13-3+1*2)/1+1)</td>
<td>3*3*384*256 + 256 =</td>
</tr>
<tr>
<td>P8</td>
<td>3$\times$3, 0, 2</td>
<td>256@6$\times$6 (=(13-3)/2+1)</td>
<td>0</td>
</tr>
<tr>
<td>F9</td>
<td>NaN</td>
<td>4096</td>
<td>6*6*256*4096 + 4096</td>
</tr>
<tr>
<td>F10</td>
<td>NaN</td>
<td>4096</td>
<td>4096*4096 +4096</td>
</tr>
<tr>
<td>Output</td>
<td>NaN</td>
<td>1000</td>
<td>4096*1000 + 1000</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>62,378,344</td>
</tr>
</tbody>
</table>
<p style="text-align:center">Table 1: The shape of the output and learning parameters in LeNet.</p>
<p>The main differences with LeNet includes</p>
<ul>
<li>reLU is used as the activation function</li>
<li>overlapping max pooling is used to downsample the output</li>
<li>GPU optimization</li>
</ul>
<h4 id="avoid-overfitting">Avoid overfitting</h4>
<ul>
<li>Data Augmentation</li>
<li>Dropout</li>
</ul>
<h4 id="implementation-1">Implementation</h4>
<p><a href="https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py">PyTorch AlexNet - Github</a></p>
<h3 id="vgg">VGG</h3>
<p><a href="https://arxiv.org/abs/1409.1556">VGG</a> is short for Visual Geometry Group, which aimed to investigate the effect of convolutional network depth on large-scale image recognition. They increased the depth of network by using a small filter (3 $\times$ 3) instead of 5 $\times$ 5 or 7 $\times$ 7, showing significant improvements by adding the depth to 16-19. Typically, when we speak of VGG, we are talking about VGG-16 or VGG-19. The complete experimental ConvNet configurations are shown in Figure 8.</p>
<h4 id="architecture-2">Architecture</h4>
<p><img src="/blog/post/images/vgg.png#full" alt="" title="Figure 8: Architecture of VGG. Source: Very Deep Convolutional Networks for Large-Scale Image Recognition"></p>
<p>The major difference to the previous networks is that the authors employed a stack of 3 $\times$ 3 filters to have receptive fields of 5 $\times$ 5, 7 $\times$ 7 or 11 $\times$ 11 rather than using larger filters that were the same size as the receptive fields. So, there are two questions,</p>
<ul>
<li>How it works?</li>
<li>Why do we use it this way?</li>
</ul>
<p>Figure 9 shows that we can use two 3 $\times$ 3 filters instead of a 5 $\times$ 5 filter to have the same receptive field. Similarly, three such filters will have a 7 × 7 effective receptive field.</p>
<p><img src="/blog/post/images/vgg-33cov.png#full" alt="" title="Figure 9: Replacing the 5X5 filter with a small 3X3 conv filter. "></p>
<p>What do we gain from the smaller filters?</p>
<ul>
<li>more non-linear activations
<ul>
<li>each filter is followed by a non-linear activation function, which makes the decision function more discriminative</li>
</ul>
</li>
<li>decrease the number of parameters
<ul>
<li>suppose both the input and output have $C$ channels, a single 5 $\times$ 5 filter has $5*5*C^2= 25C^2$ while the stacked two 3$\times$3 filters has $2*3*3*C^2=18C^2$</li>
</ul>
</li>
</ul>
<h4 id="drawbacks">Drawbacks</h4>
<p>Though VGG outperfoms other networks in ILSVRC, it&rsquo;s difficult to train VGG from scratch because of</p>
<ul>
<li>long traning time</li>
<li>huge number of weights
<ul>
<li>much deeper network depth</li>
<li>more fully-connected layers</li>
</ul>
</li>
</ul>
<h3 id="resnet">ResNet</h3>
<p><a href="https://arxiv.org/abs/1512.03385">ResNet</a>, short for Residual Network, was the winner of the ILSVRC 2015 classification challenge. Like VGG, there are many variants of ResNet, such as ResNet-18, ResNet-34, ResNet-50 and so on. Similar to other ConvNets, ResNet also has Conv layer, pooling, activation and FC layers. The difference is that ResNet introduced the identity connection, which is shown in Figure 10.</p>
<p><img src="/blog/post/images/resnet-identity.png" alt="" title="Figure 10: The core block of ResNet."></p>
<h4 id="motivation">Motivation</h4>
<p>In my opinion, the main idea of ResNet is similar to gradient boosting. We know that the idea of gradient boosting is to learn residuals. Similarly, ResNet tries to learn the residual function rather than the mapping function directly. What does this mean? First, neural networks are function approximators. Normally, we build a neural network to learn a specific function, say $h(x)$. Now, we suppose that $h(x)$ can be decomposed into two parts</p>
<p>$$
f(x) + x = h(x)
$$</p>
<p>From Figure 10, we see that $f(x)$ is the target function that we want to learn while $x$ is the identity connection, If the output is $x$ ($h(x) =x$), then $f(x) = 0$. In other words, as learning progresses, $f(x)$ should approach $0$. That&rsquo;s why $f(x)$ is called the residual function.</p>
<h4 id="the-effect-of-identity-connection">The effect of Identity Connection</h4>
<p>VGG has demenstrated that as the depth of networks increases, the generalisation becomes better. However, the gradients are prone to shrink to zero if the network is too deep due to the chain rule, so some weights are never updated during learning. This is known as vanishing gradients. With ResNet, gradients can flow directly through the identity connection to the previous layers because of the addition operation, even if the graident in the residual block is very small.</p>
<h4 id="architecture-3">Architecture</h4>
<p><img src="/blog/post/images/resnet.png" alt="" title="Figure 11: ResNet 34 from original paper[Deep Residual Learning for Image Recognition]"></p>
<p><img src="/blog/post/images/resnet-size.png#full" alt="" title="Figure 12: The size of building blocks in different layers of ResNet. Source: Deep Residual Learning for Image Recognition"></p>
<p>One problem is that $f(x)$ might have different shape with $x$. In this case, we employ 1 $\times$ 1 kernel to change the input shape, which is depicted by the dotted line in Figure 11. Mathematically, it can be expressed as</p>
<p>$$
f(x) + Wx = h(x)
$$</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a></li>
<li><a href="https://www.deeplearningbook.org/contents/convnets.html">Chapeter 9 Convolutional Networks - Deep Learning</a></li>
<li><a href="https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac">Simple Introduction to Convolutional Neural Networks</a></li>
<li><a href="https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/">A Gentle Introduction to 1×1 Convolutions to Manage Model Complexity</a></li>
<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">An Intuitive Explanation of Convolutional Neural Networks</a></li>
<li><a href="https://fabianfuchsml.github.io/equivariance1of2/">CNNs and Equivariance</a></li>
<li><a href="https://divsoni2012.medium.com/translation-invariance-in-convolutional-neural-networks-61d9b6fa03df">Translation Invariance in Convolutional Neural Networks</a></li>
<li><a href="https://learnopencv.com/understanding-alexnet/">Understanding AlexNet</a></li>
<li><a href="https://www.kaggle.com/blurredmachine/vggnet-16-architecture-a-complete-guide">VGG-16 Architecture: A Complete Guide</a></li>
<li><a href="https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8">Understading and visualizing ResNets</a></li>
<li><a href="http://pabloruizruiz10.com/resources/CNNs/ResNets.pdf">ResNets</a></li>
<li>K. He, X. Zhang, S. Ren and J. Sun, “Deep Residual Learning for Image Recognition,” in <em>CVPR</em>, 2016. <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></li>
</ul>
        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/dl/rnn/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Deep Learning - RNN</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/ml/model-selection/">
                <span class="next-text nav-default">Model Selection</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#convolution">Convolution</a>
      <ul>
        <li><a href="#convolution-operation">Convolution Operation</a></li>
        <li><a href="#cross-correlation">Cross-correlation</a></li>
        <li><a href="#receptive-field">Receptive Field</a></li>
        <li><a href="#why-cnn">Why CNN</a></li>
        <li><a href="#padding-and-stride">Padding and Stride</a></li>
        <li><a href="#multiple-channels">Multiple Channels</a></li>
        <li><a href="#data-types">Data Types</a></li>
        <li><a href="#1-x-1-kernel">1 x 1 Kernel</a></li>
      </ul>
    </li>
    <li><a href="#pooling">Pooling</a>
      <ul>
        <li><a href="#maxavg-pooling">Max/Avg Pooling</a></li>
        <li><a href="#local-vs-global-pooling">Local VS Global Pooling</a></li>
      </ul>
    </li>
    <li><a href="#convnet">ConvNet</a>
      <ul>
        <li><a href="#lenet">LeNet</a></li>
        <li><a href="#alexnet">AlexNet</a></li>
        <li><a href="#vgg">VGG</a></li>
        <li><a href="#resnet">ResNet</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.97e3349af25266740f15f60ba3a52fa1c247c87c6b73b9bca9290ec5c41c7cbe.js" integrity="sha256-l&#43;M0mvJSZnQPFfYLo6UvocJHyHxrc7m8qSkOxcQcfL4=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
