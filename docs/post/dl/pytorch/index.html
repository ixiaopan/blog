<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Deep Learning - PyTorch - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" />
  <meta name="description" content="The most popular deep learning frameworks nowadays are Tensorflow and PyTorch. Well, during my study, I use PyTorch more often. Recently, I am building the classic BiLSTM-CRF model using PyTorch. It&amp;rsquo;s a bit hard for me when operating matrices since it uses various advanced techniques about indexing and slicing. I think it&amp;rsquo;s necessary to explain these amazing functions. Therefore, I am going to write this post to revisit the most important aspects of PyTorch for future references.
" />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/dl/pytorch/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Deep Learning - PyTorch" />
<meta property="og:description" content="The most popular deep learning frameworks nowadays are Tensorflow and PyTorch. Well, during my study, I use PyTorch more often. Recently, I am building the classic BiLSTM-CRF model using PyTorch. It&rsquo;s a bit hard for me when operating matrices since it uses various advanced techniques about indexing and slicing. I think it&rsquo;s necessary to explain these amazing functions. Therefore, I am going to write this post to revisit the most important aspects of PyTorch for future references." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/dl/pytorch/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-07-19T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-07-19T00:00:00+00:00" />

<meta itemprop="name" content="Deep Learning - PyTorch">
<meta itemprop="description" content="The most popular deep learning frameworks nowadays are Tensorflow and PyTorch. Well, during my study, I use PyTorch more often. Recently, I am building the classic BiLSTM-CRF model using PyTorch. It&rsquo;s a bit hard for me when operating matrices since it uses various advanced techniques about indexing and slicing. I think it&rsquo;s necessary to explain these amazing functions. Therefore, I am going to write this post to revisit the most important aspects of PyTorch for future references."><meta itemprop="datePublished" content="2021-07-19T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-07-19T00:00:00+00:00" />
<meta itemprop="wordCount" content="2680">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning - PyTorch"/>
<meta name="twitter:description" content="The most popular deep learning frameworks nowadays are Tensorflow and PyTorch. Well, during my study, I use PyTorch more often. Recently, I am building the classic BiLSTM-CRF model using PyTorch. It&rsquo;s a bit hard for me when operating matrices since it uses various advanced techniques about indexing and slicing. I think it&rsquo;s necessary to explain these amazing functions. Therefore, I am going to write this post to revisit the most important aspects of PyTorch for future references."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Deep Learning - PyTorch</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-07-19">
      2021-07-19
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/deep-learning/"> Deep Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>The most popular deep learning frameworks nowadays are Tensorflow and PyTorch. Well, during my study, I use PyTorch more often. Recently, I am building the classic BiLSTM-CRF model using PyTorch. It&rsquo;s a bit hard for me when operating matrices since it uses various advanced techniques about indexing and slicing. I think it&rsquo;s necessary to explain these amazing functions. Therefore, I am going to write this post to revisit the most important aspects of PyTorch for future references.</p>
<h2 id="tensor">Tensor</h2>
<p>Tensor is the primary data structure in PyTorch. Simply put,</p>
<ul>
<li>A tensor refers to a multi-dimensional array where you can access an individual element by indexing</li>
<li>A tensor&rsquo;s rank tells us how many indexes are needed to refer to a specific element within the tensor. It can be zero, one, two, three and so on. If we have a rank-2 tensor, it means that this tensor have 2 axes or it&rsquo;s a matrix.</li>
<li>An axis of a tensor is a specific dimension of a tensor. In practice, we often do computation along a specifc axis.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">1.</span>)
torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])
torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">12</span>)<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>)
torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">12</span>)<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>)
</code></pre></div><p>A tensor&rsquo;s shape is very important because it contains all the information about the tensor like rank and the length of each axis. For example, in image classification, the data typically have the following shape. It&rsquo;s common to see <code>NCHW</code> rather than <code>BCHW</code>, where <code>B</code> is replaced by <code>N</code>. Common orderings are show below.</p>
<ul>
<li><code>NCHW</code></li>
<li><code>NHWC</code></li>
<li><code>CHWN</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">[Batch, Channels, Height, Width]
</code></pre></div><h3 id="storage">storage</h3>
<p>In fact, the tensor declared above is just a view of the underlying data. View means a kind of way to look at data. For example, you can look at an image from the top direction or the right direction. In the case of data, you could look at data in the original order or you could skip a fixed number of elements.</p>
<p>No matter how you view it, the data in memory stay the same. In fact, the real values are stored in a contiguous block of memory. In PyTorch, we can access it by invoking <code>tensor.storage()</code>. For example,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>]])
x<span style="color:#f92672">.</span>storage()
</code></pre></div><p>The result is shown below. You might find that the elements are sorted along the rows of the tensor $x$. Tthe difference is that they are stored in a one-dimensional array.</p>
<p><img src="/blog/post/images/torch_storage.png" alt="" title="Figure 1: The underlying data beneath the tensor x"></p>
<p>Here comes a question: How does the indexing operation $x[0, 1]$ work?</p>
<h3 id="stride">stride</h3>
<p>In oder to index an element, PyTorch needs to know meta data about a tensor, such as stride, which indicates the number of elements to be skipped along each dimension. In the above example, the stride is</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x<span style="color:#f92672">.</span>stride()
<span style="color:#75715e"># (3, 1)</span>
</code></pre></div><p>$3$ means we need to skip 3 elements to get to the next row, 1 means we just move one step so as to reach the next column. Mathematically, the indexing operation in 2D tensor are described as follows, where offset is usually zero</p>
<p>$$
x[i, j] = i * \text{stride}[0]  + j * \text{stride}[1] + \text{offset}
$$</p>
<p>Why is it designed like this? It&rsquo;s less expensive for some operations like transpose since there is no need to reallocate memory space. Instead, we just modify the stride.</p>
<p>For instance, $x^T$ is shown below</p>
<pre tabindex="0"><code>0 3 
1 4
2 5
</code></pre><p>If we look at the underlying data of $x^T$, we will find that $x^T$ and $x$ have the same storage shown in Figure 1.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x<span style="color:#f92672">.</span>data_ptr() <span style="color:#f92672">==</span> xt<span style="color:#f92672">.</span>data_ptr() <span style="color:#75715e"># True</span>
</code></pre></div><p>Let&rsquo;s have a look at the stride of $x^T$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xt<span style="color:#f92672">.</span>stride()
<span style="color:#75715e">#(1, 3)</span>
</code></pre></div><p>In this case, we simply move one step forward if we want to reach the next row while we need to move 3 steps to get to the next column.</p>
<p>Apart from this, knowing how storage and stride work will also help understand other PyTorch functions' behaviour. <a href="https://zhang-yang.medium.com/explain-pytorch-tensor-stride-and-tensor-storage-with-code-examples-50e637f1076d">This article</a> discusses the difference between <code>torch.expand()</code> and <code>torch.repeat()</code>. Accoding to the PyTorch document</p>
<blockquote>
<p>torch.expand()</p>
<p>​	Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the <code>stride</code> to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.</p>
<p>torch.repeat()</p>
<p>​	Unlike expand(), this function copies the tensor’s data.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">3</span>)
x<span style="color:#f92672">.</span>stride() <span style="color:#75715e"># 1</span>

y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>expand(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>)
y<span style="color:#f92672">.</span>stride() <span style="color:#75715e"># (0, 1)</span>

z<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>repeat(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
z<span style="color:#f92672">.</span>stride() <span style="color:#75715e"># (3, 1)</span>

</code></pre></div><p>From the results of strides, we can see that <code>torch.tensor.expand()</code> does not require extra memory space. This is essential when working with large data set.</p>
<h3 id="contiguous">contiguous</h3>
<p>In Pytorch, there is a concept called <strong>contiguous</strong>, indicating whether a tensor has the same values as the storage when counting from the innermost dimension. In other words, when moving along the rows in 2D tensors, the values sorted in this way are exactly the order of the underlying data. Visually, such an order is more comfortable and consistent.</p>
<p>For example, if we flatten $x$ along the innermost dimension, we will get
$$
0, 1, 2, 3, 4, 5
$$</p>
<p>If we flatten $x^T$ along the innermost dimension, we will get</p>
<p>$$
0, 3, 1, 4, 2, 5
$$</p>
<p>which is different from the storage shown in Figure 1. Therefore, we say $x$ is contiguous while $x^T$ is not. We can also check it in Pytorch shown below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x<span style="color:#f92672">.</span>is_contiguous() <span style="color:#75715e"># True</span>
xt<span style="color:#f92672">.</span>is_contiguous() <span style="color:#75715e"># False</span>
</code></pre></div><h4 id="why">Why</h4>
<p>But wait, why do we need contiguous tensor?</p>
<p>One reason is that we can exploit the benefit of <a href="https://stackoverflow.com/questions/16699247/what-is-a-cache-friendly-code/16699282#16699282">cache</a> to improve the speed of fetching data. In short, when fetching an element of a matrix, we can also get the neighboring elements at the same time, requiring less memory accesses.</p>
<p>Another reason is that some functions only work in the contiguous tensor, such as <code>view()</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xt<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>)
<span style="color:#75715e"># RuntimeError: view size is not compatible with input tensor&#39;s size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</span>
</code></pre></div><p>There are two approaches to fix it: call <code>contiguous()</code> to make the tensor contiguous or call <code>reshape()</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xt<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>)
xt<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>)
</code></pre></div><p>However, <code>reshape()</code> could return a view (if the tensor is contiguous) or a new tensor (if not).</p>
<h4 id="how">How</h4>
<p>How do we make $x^T$ contiguous too?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xtc <span style="color:#f92672">=</span> xt<span style="color:#f92672">.</span>contiguous()
</code></pre></div><p><img src="/blog/post/images/x_T.png" alt="" title="Figure 2: The underlying data beneath the tensor $x^T$"></p>
<p>Figure 2 shows the new storage. Thus, <code>contiguous()</code> reallocates a new memory and copy the original tensor into that memory space. We can check it using the code below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xt<span style="color:#f92672">.</span>data_ptr() <span style="color:#f92672">==</span> xtc<span style="color:#f92672">.</span>data_ptr() <span style="color:#75715e"># False</span>
</code></pre></div><p>Meanwhile, the stride has also been changed to adapt to this new storage.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xT<span style="color:#f92672">.</span>stride()
<span style="color:#75715e"># (2, 1)</span>
</code></pre></div><h2 id="torchnn">torch.nn</h2>
<h3 id="nnmodule">nn.module</h3>
<p><code>torch.nn</code> is the submodule that contains everything needed to build neural networks in PyTorch. We know that a neural network is composed of a stack of layers. In PyTorch, we call these layers <strong>modules</strong> and they are all subclasses of <code>nn.Module</code>.</p>
<p>I think we call them modules because a single layer is a layer while multiple layers can also be considered as a big layer. Thus, we refer to both of them as modules regardless how many layers there are. Moreover, a module can also have other submodules (subclasses of <code>nn.Module</code>) as their attributes and thus track their parameters automatically.</p>
<p>When I built the BiLSTM-CRF for NER, I split LSTM and CRF apart. However, I noticed that the BiLSTM model can still retrieve the parameters of the CRF layer. Now I found the reason.</p>
<p>First, both the <code>nn.Linear()</code> and <code>CRF()</code> are moules in PyTorch. Second, when the instance of such a module is assigned to an attribute of another <code>nn.Module</code> ( in this case, it&rsquo;s BiLSTM ), this module will be automatically registered as the submodule of  <code>BiLSTM</code>, and thus <code>BiLSTM</code> have access to the parameters of its submodules.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CRF</span>(nn<span style="color:#f92672">.</span>Module):
  <span style="color:#66d9ef">def</span> __init__(self):
		super(CRF, self)<span style="color:#f92672">.</span>__init__()


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTM</span>(nn<span style="color:#f92672">.</span>Module):
  <span style="color:#66d9ef">def</span> __init__(self):
    super(BiLSTM, self)<span style="color:#f92672">.</span>__init__()
    self<span style="color:#f92672">.</span>fc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>)
  	self<span style="color:#f92672">.</span>crf <span style="color:#f92672">=</span> CRF(self<span style="color:#f92672">.</span>num_of_tag)

</code></pre></div><h3 id="forward">forward</h3>
<p>When I first used PyTorch, I often confused why this model doesn&rsquo;t invoke <code>forward()</code>. For example, I have a simple forward network, I have seen two options shown below to move forward.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">self<span style="color:#f92672">.</span>fc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>)

self<span style="color:#f92672">.</span>fc(inputs)  <span style="color:#75715e"># option 1, the right way</span>
self<span style="color:#f92672">.</span>fc<span style="color:#f92672">.</span>forward(inputs) <span style="color:#75715e"># option 2, wrong</span>
</code></pre></div><p>Well, the reason is simple. In fact, the built-in PyTorch subclasses of <code>nn.Module</code> allows themselves to be called as a simple function call. In the <code>__call__</code>, it calls <code>forward()</code>. So what&rsquo;s the difference? The difference is that PyTorch will do other operations by provided hooks before calling <code>forward()</code>. Thus, it&rsquo;s possible to call <code>forward()</code> directly, but it shouldn&rsquo;t do this unless we don&rsquo;t provide any hooks. ( More details can be seen <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L1057">here</a> )</p>
<h3 id="parameter">parameter</h3>
<p>We have built our models, so how to access to the parameters? PyTorch provides <code>parameters()</code> function, which allows us to collect all parameters from the first layer to the last layer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters():
	print(param)
</code></pre></div><p>However, it&rsquo;s a bit hard to distinguish them when you have many layers. Thus, PyTorch provides another function <code>named_parameters()</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_parameters():
	print(name, param)
</code></pre></div><p>With the name of each layer, it&rsquo;s easy to retrieve individual parameter by accessing the corresponding name directly, such as</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>out_linear<span style="color:#f92672">.</span>weight
model<span style="color:#f92672">.</span>out_linear<span style="color:#f92672">.</span>bias
</code></pre></div><p>So far so good. But where are the names from?</p>
<h3 id="sequential">sequential</h3>
<p>PyTorch provide two ways to create neural networks. The first one is to use <code>nn.Sequential()</code>, as shown below</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
  nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>),
  nn<span style="color:#f92672">.</span>Tanh(),
  nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>)
)

<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">Sequential(
</span><span style="color:#e6db74">  (0): Linear(in_features=5, out_features=4, bias=True)
</span><span style="color:#e6db74">  (1): Tanh()
</span><span style="color:#e6db74">  (2): Linear(in_features=4, out_features=2, bias=True)
</span><span style="color:#e6db74">)
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p>Well, the names in this example are just numbers. It would be fine if there are few layers. To build a more semantic model structure, we can pass name for each layer,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> OrderedDict

model <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Sequential(OrderedDict([
  (<span style="color:#e6db74">&#39;hidden_layer&#39;</span>, torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>)),
  (<span style="color:#e6db74">&#39;active_func&#39;</span>, torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Tanh()),
  (<span style="color:#e6db74">&#39;out_layer&#39;</span>, torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>))
]))

model

<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">Sequential(
</span><span style="color:#e6db74">  (hidden_layer): Linear(in_features=5, out_features=4, bias=True)
</span><span style="color:#e6db74">  (active_func): Tanh()
</span><span style="color:#e6db74">  (out_layer): Linear(in_features=4, out_features=2, bias=True)
</span><span style="color:#e6db74">)
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p>As mentioned earlier, we can retrieve the parameters for an individual layer by accessing its name, such as</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>hidden_layer<span style="color:#f92672">.</span>bias
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">Parameter containing:
</span><span style="color:#e6db74">tensor([ 0.1177, -0.2876, -0.3861, -0.3367], requires_grad=True)
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p>Though <code>nn.Sequential()</code> is handy, the order of layers is fixed. That&rsquo;s where <code>nn.Module</code> comes into play. We rewrite the above code using <code>nn.Module</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimpleNet</span>(nn<span style="color:#f92672">.</span>Module):
	<span style="color:#66d9ef">def</span> __init__(self):
    super(SimpleNet, self)<span style="color:#f92672">.</span>__init__()

    self<span style="color:#f92672">.</span>hidden_layer <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>)
    self<span style="color:#f92672">.</span>out_layer <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>)
   
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, inputs):
  	outs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>hidden_layer(inputs)
  	outs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tanh(outs)
  	outs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>out_layer(outs)
  	<span style="color:#66d9ef">return</span> outs
  
net <span style="color:#f92672">=</span> SimpleNet()

<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">SimpleNet(
</span><span style="color:#e6db74">  (hidden_layer): Linear(in_features=5, out_features=4, bias=True)
</span><span style="color:#e6db74">  (out_layer): Linear(in_features=4, out_features=2, bias=True)
</span><span style="color:#e6db74">)
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><h2 id="basic-techniques">Basic Techniques</h2>
<h3 id="verify-torch">Verify torch</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print(<span style="color:#e6db74">&#39;torch version:&#39;</span>, torch<span style="color:#f92672">.</span>__version__)

<span style="color:#75715e"># verify GPU</span>
cuda_enabled <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available()
print(<span style="color:#e6db74">&#39;GPU available:&#39;</span>, cuda_enabled)
print(<span style="color:#e6db74">&#39;GPU version:&#39;</span>, torch<span style="color:#f92672">.</span>version<span style="color:#f92672">.</span>cuda)

<span style="color:#75715e"># using CUDA</span>
t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])
<span style="color:#66d9ef">if</span> cuda_enabled:
  t <span style="color:#f92672">=</span> t<span style="color:#f92672">.</span>cuda()
</code></pre></div><h3 id="creating-tensor">Creating Tensor</h3>
<h4 id="with-data">With Data</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">torch<span style="color:#f92672">.</span>Tensor(x)
torch<span style="color:#f92672">.</span>tensor(x)
torch<span style="color:#f92672">.</span>from_numpy(x)
torch<span style="color:#f92672">.</span>as_tensor(x)
</code></pre></div><p>What&rsquo;s the difference between them? Let&rsquo;s see an example.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">3</span>) <span style="color:#75715e"># construct a toy data using numpy</span>
print(x, x<span style="color:#f92672">.</span>dtype)
<span style="color:#75715e"># [0 1 2] int64</span>
o1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(x)
o2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(x)
o3 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>from_numpy(x)
o4 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>as_tensor(x)
<span style="color:#75715e"># tensor([0., 1., 2.]) torch.float32</span>
<span style="color:#75715e"># tensor([0, 1, 2]) torch.int64</span>
<span style="color:#75715e"># tensor([0, 1, 2]) torch.int64</span>
<span style="color:#75715e"># tensor([0, 1, 2]) torch.int64</span>

x[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
print(o1)
print(o2)
print(o3)
print(o4)
<span style="color:#75715e"># tensor([0., 1., 2.])</span>
<span style="color:#75715e"># tensor([0, 1, 2])</span>
<span style="color:#75715e"># tensor([0, 5, 2])</span>
<span style="color:#75715e"># tensor([0, 5, 2])</span>

</code></pre></div><p>After running the above code, you might have the following questions,</p>
<ul>
<li>What&rsquo;s the difference between <code>torch.Tensor</code> and <code>torch.tensor</code>?</li>
<li>inconsistent data type  — <code>torch.Tensor()</code> returns <code>float32</code> while others return <code>int</code>. Why?</li>
<li>Why do different methods behave so weird after changing the original data?</li>
</ul>
<p>Q1 difference between  <code>torch.Tensor()</code> and <code>torch.tensor()</code></p>
<ul>
<li><code>torch.Tensor()</code> is the constructor of the  <code>torch.Tensor </code> class</li>
<li><code>torch.tensor()</code> is a factory function that returns tensor object</li>
</ul>
<p>Q2 Why different dtypes?</p>
<ul>
<li><code>torch.Tensor()</code> uses the default dtype when creating the tensor. You can see the default dtype by invoking the following code.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">torch<span style="color:#f92672">.</span>get_default_dtype() <span style="color:#75715e"># float32</span>
</code></pre></div><ul>
<li>
<p>others use the dtype inferred from the input data.</p>
</li>
<li>
<p>You cannot specify a dtype in the <code>torch.Tensor()</code> constructor, but <code>dtype</code> can be explicitly passed as an argument when using other methods</p>
</li>
</ul>
<p>Q3 memory sharing</p>
<p>This is because <code>torch.tensor()</code> and <code>torch.Tensor()</code> copy the input data while the latter ones use the same data in memory. We see that <code>x.numpy()</code> also shares the same memory with the original tensor.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">5</span>, (<span style="color:#ae81ff">3</span>,)) <span style="color:#75715e"># tensor([3, 2, 2])</span>
a <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>numpy()

x[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>
print(x) <span style="color:#75715e"># tensor([3, 9, 2])</span>
print(a) <span style="color:#75715e"># array([3, 9, 2])</span>
</code></pre></div><h4 id="without-data">Without Data</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">torch<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">2</span>)
torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>)
torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>)
</code></pre></div><h3 id="loss-function">Loss function</h3>
<h4 id="mse">MSE</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
y_pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
y_true <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>)

F<span style="color:#f92672">.</span>mse_loss(y_pred, y_true) <span style="color:#75715e"># or</span>
mse_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>MSELoss()
mse_loss(y_pred, y_true)

</code></pre></div><h4 id="cross-entropy">Cross Entropy</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Cross-entropy Loss</span>
<span style="color:#75715e"># Example of target with class indices</span>
input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
target <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">5</span>, (<span style="color:#ae81ff">3</span>,), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(input, target) <span style="color:#75715e"># typically, either 1 or 0</span>
loss<span style="color:#f92672">.</span>backward()

<span style="color:#75715e"># Example of target with class probabilities</span>
input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
target <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>)<span style="color:#f92672">.</span>softmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(input, target)
loss<span style="color:#f92672">.</span>backward()

</code></pre></div><h4 id="bce">BCE</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#75715e"># Binary Cross-Entropy Loss (predicted prob, true prob [0,1])</span>
bce_loss<span style="color:#f92672">=</span>nn<span style="color:#f92672">.</span>BCELoss()
loss <span style="color:#f92672">=</span> bce_loss(torch<span style="color:#f92672">.</span>sigmoid(z), y_true)
</code></pre></div><h3 id="reshape">Reshape</h3>
<h2 id="advanced-techniques">Advanced Techniques</h2>
<h3 id="torchscatter_">torch.scatter_</h3>
<p>This function is often used to implement one-hot encoding. As its name suggests, <strong>scatter</strong> means to distribute a list of values over another tensor.</p>
<p>In the case of one-hot encoding, the matrix is a parse matrix whose element value is either zero or one. So one is the scalar to be scattered at the corresponding column index for each observation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">label <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">0</span>])<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>)
one_hot <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(len(label), <span style="color:#ae81ff">5</span>)
one_hot<span style="color:#f92672">.</span>scatter_(<span style="color:#ae81ff">1</span>, label, <span style="color:#ae81ff">1</span>)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">tensor([[0., 0., 1., 0., 0.],
</span><span style="color:#e6db74">        [0., 0., 0., 1., 0.],
</span><span style="color:#e6db74">        [0., 0., 0., 0., 1.],
</span><span style="color:#e6db74">        [1., 0., 0., 0., 0.]])
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>

</code></pre></div><p>Further, the scatterd item could be any multi-dimensional array rather than a simple scalar, for example,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([[<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">0</span>], [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>]]) <span style="color:#75715e"># 3 observations</span>
one_hot <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(len(labels), <span style="color:#ae81ff">6</span>)<span style="color:#f92672">.</span>long() <span style="color:#75715e"># 6 labels in total</span>
source <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>)<span style="color:#f92672">.</span>long()<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>)

one_hot<span style="color:#f92672">.</span>scatter_(<span style="color:#ae81ff">1</span>, labels, source)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">tensor([[0, 0, 1, 2, 0, 0],
</span><span style="color:#e6db74">        [4, 0, 0, 0, 3, 0],
</span><span style="color:#e6db74">        [0, 5, 0, 0, 0, 6]])
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p>From the above examples, we conclude that</p>
<ul>
<li><code>labels</code> and <code>source</code> should have the same size along the specified dimension (the first argument)</li>
<li>All tensors (one_hot, lables, source) should keep the same shape except the specified dimension</li>
</ul>
<h3 id="torchsqueeze">torch.squeeze()</h3>
<p><code>squeeze()</code> and <code>unsqueeze()</code> often appear in pairs to expand dimension of a tensor for broadcasting.</p>
<ul>
<li>
<p><code>squeeze()</code> remove all dimensions with the value of 1</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">8</span>)<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">4</span>)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">tensor([[[0, 1, 2, 3],
</span><span style="color:#e6db74">         [4, 5, 6, 7]]])
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>

x<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">0</span>)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">tensor([[0, 1, 2, 3],
</span><span style="color:#e6db74">        [4, 5, 6, 7]])
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div></li>
<li>
<p><code>unsqueeze()</code> add one dimension along the specified dimension</p>
<p>​</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])

x<span style="color:#f92672">.</span>unsqueeze(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#75715e"># tensor([[1, 2, 3]])</span>
x<span style="color:#f92672">.</span>unsqueeze(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) 
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">tensor([[1],
</span><span style="color:#e6db74">        [2],
</span><span style="color:#e6db74">        [3]])
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div></li>
</ul>
<h3 id="torchgather">torch.gather()</h3>
<p>I happened to meet this function when building BiLSTM+CRF NER models. It was hard to implement batch training for CRF layers until I found <code>torch.gather()</code>. Well, it&rsquo;s a bit similar to <code>torch.scatter_()</code> except that <code>torch.gather()</code> aims to fetch data while <code>toch.scatter_()</code> is used to write values.</p>
<p>For BilSTM+CRF models, we need to calculate the sentence score as follows,</p>
<p>$$
\text{Score} (D_j) = \sum_{i=0}^{|D_j|} T(y^{w_i} \rarr y^{w_{(i+1)}}) + E(w_{i+i}|y^{w_{(i+1)}})
$$</p>
<p>where emission scores are the outputs of BiLSTM and the transition scores are the parameters of the CRF layer. For each batch training, we need to calculate each sentence score in this batch.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">emission score: 
	(batch_size, max_seq_len, num_of_tag)
	[
		[
                   O   B   I
            [ w00 <span style="color:#ae81ff">0.1</span> <span style="color:#ae81ff">0.2</span> <span style="color:#ae81ff">0.7</span> ]
            [ w01 <span style="color:#ae81ff">0.2</span> <span style="color:#ae81ff">0.4</span> <span style="color:#ae81ff">0.4</span> ]
            <span style="color:#f92672">...</span>
		], <span style="color:#75715e"># sentence 1</span>

		[
                  O   B   I
            [ w10 <span style="color:#ae81ff">0.1</span> <span style="color:#ae81ff">0.2</span> <span style="color:#ae81ff">0.7</span> ]
            [ w11 <span style="color:#ae81ff">0.2</span> <span style="color:#ae81ff">0.4</span> <span style="color:#ae81ff">0.4</span> ]
            <span style="color:#f92672">...</span>
		] <span style="color:#75715e"># sentence 2</span>
	]
tags: (batch_size, max_seq_len)
	[ 
		[ w00<span style="color:#f92672">=&gt;</span>B, w01<span style="color:#f92672">=&gt;</span>I, <span style="color:#f92672">...</span>], <span style="color:#75715e"># sentence 1</span>
		[ w10<span style="color:#f92672">=&gt;</span>B, w11<span style="color:#f92672">=&gt;</span>O, <span style="color:#f92672">...</span>], <span style="color:#75715e"># sentence 2</span>
            <span style="color:#f92672">...</span>
	]
</code></pre></div><p>It&rsquo;s easy to fetch data from the specified index along the desired dimension using <code>torch.gather()</code>. In this case, we want to fetch data from the index that the true tag of each word belongs to along the innermost dimension of emission_score. For example, sentence 1 has two words with the labels <code>B</code> and <code>I</code>, so the corresponding scores in the emission score are <code>emission_score[0][0][1]</code> and <code>emission_score[0][1][2]</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">emission_score <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([
    [

        [  <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.7</span> ],
        [  <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.5</span> ]

    ], 

    [

        [ <span style="color:#ae81ff">0.35</span>, <span style="color:#ae81ff">0.25</span>, <span style="color:#ae81ff">0.4</span> ],
        [ <span style="color:#ae81ff">0.6</span>, <span style="color:#ae81ff">0.25</span>, <span style="color:#ae81ff">0.15</span> ]

    ]
])
true_labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([ 
    [ <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>],
    [ <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
    
])

torch<span style="color:#f92672">.</span>gather(emission_score, <span style="color:#ae81ff">2</span>, true_labels<span style="color:#f92672">.</span>unsqueeze(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>squeeze(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
<span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">tensor([[0.2000, 0.5000],
</span><span style="color:#e6db74">        [0.2500, 0.6000]]) =&gt; tensor([0.7000, 0.8500])
</span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</code></pre></div><p>which is exactly the second term of $\text{Score} (D_j) $.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://zhang-yang.medium.com/explain-pytorch-tensor-stride-and-tensor-storage-with-code-examples-50e637f1076d">https://zhang-yang.medium.com/explain-pytorch-tensor-stride-and-tensor-storage-with-code-examples-50e637f1076d</a></li>
<li><a href="https://deeplizard.com/learn/video/fCVuiW9AFzY">PyTorch - Deeplizard</a></li>
</ul>
        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/dl/pre/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Deep Learning - Preliminary</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/misc/sort/">
                <span class="next-text nav-default">Sorting</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#tensor">Tensor</a>
      <ul>
        <li><a href="#storage">storage</a></li>
        <li><a href="#stride">stride</a></li>
        <li><a href="#contiguous">contiguous</a></li>
      </ul>
    </li>
    <li><a href="#torchnn">torch.nn</a>
      <ul>
        <li><a href="#nnmodule">nn.module</a></li>
        <li><a href="#forward">forward</a></li>
        <li><a href="#parameter">parameter</a></li>
        <li><a href="#sequential">sequential</a></li>
      </ul>
    </li>
    <li><a href="#basic-techniques">Basic Techniques</a>
      <ul>
        <li><a href="#verify-torch">Verify torch</a></li>
        <li><a href="#creating-tensor">Creating Tensor</a></li>
        <li><a href="#loss-function">Loss function</a></li>
        <li><a href="#reshape">Reshape</a></li>
      </ul>
    </li>
    <li><a href="#advanced-techniques">Advanced Techniques</a>
      <ul>
        <li><a href="#torchscatter_">torch.scatter_</a></li>
        <li><a href="#torchsqueeze">torch.squeeze()</a></li>
        <li><a href="#torchgather">torch.gather()</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2023
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.4484b3f29dd568c80320701800e1b69704b179f367b8223a43c728d819f39b97.js" integrity="sha256-RISz8p3VaMgDIHAYAOG2lwSxefNnuCI6Q8co2Bnzm5c=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
