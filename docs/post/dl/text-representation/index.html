<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          NLP - Text Representation - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" />
  <meta name="description" content="In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&amp;rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation.
" />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/dl/text-representation/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="NLP - Text Representation" />
<meta property="og:description" content="In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/dl/text-representation/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-07-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-07-10T00:00:00+00:00" />

<meta itemprop="name" content="NLP - Text Representation">
<meta itemprop="description" content="In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation."><meta itemprop="datePublished" content="2021-07-10T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-07-10T00:00:00+00:00" />
<meta itemprop="wordCount" content="1997">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NLP - Text Representation"/>
<meta name="twitter:description" content="In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">NLP - Text Representation</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-07-10">
      2021-07-10
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/deep-learning/"> Deep Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called <strong>text representation</strong>.</p>
<p>There are two aspects to consider: the level of representation and the meaning of numbers. We know that a sentence is composed of words and each word consists of a group of characters. This means we can represent text at sentence level, character-level, or both. As for numbers, the simplest way is to count the number of occurrences of each word. The most common methods based on this idea includes one-hot encoding, bag-of-word and TF-IDF.</p>
<h2 id="frequency-based">Frequency-Based</h2>
<h3 id="one-hot">One-hot</h3>
<p>One-hot representation indicates whether a word/character is present in a sentence/word. If true, we assign the value of 1 to that word, otherwise 0.</p>
<h4 id="character-level">character-level</h4>
<p>Figure 1 shows a simple word representation at character level. The whole vocabulary contains 26 English letters. For each word, for example, the word <code>impossible</code></p>
<ul>
<li>each row represents a character in <code>impossible</code>, so there are 10 rows</li>
<li>The corresponding value of each row is a vector whose element value is either 0 or 1, indicating whether the corresponding letter is present in the given word</li>
</ul>
<p>Thus, we will get a <code>(10, 26)</code> matrix for the word <code>impossible</code>.</p>
<p><img src="/blog/post/images/word-vocab-matrix.png#full" alt="" title="Figure 1: character-vocabulary occurrence matrix with the shape of (|word|, |vocabulary|)"></p>
<p>Below are  simple codes to construct such a matrix.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Impossible Mr Bennet impossible when I am not acquainted with him&#39;</span>

line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
line, len(line)
<span style="color:#75715e"># (&#39;impossiblemrbennetimpossiblewheniamnotacquaintedwithhim&#39;, 55)</span>

<span style="color:#75715e"># each row represent a character in the sentence</span>
alphabet <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;abcdefghijklmnopqrstuvwxyz&#39;</span>
letter_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(len(line), len(alphabet)
<span style="color:#66d9ef">for</span> i, letter <span style="color:#f92672">in</span> enumerate(line):
    idx <span style="color:#f92672">=</span> alphabet<span style="color:#f92672">.</span>index(letter)
    letter_tensor[i][idx] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

</code></pre></div><h4 id="word-level">word-level</h4>
<p>From the view of sentences, the vocabulary is all the unique words in the corpus and each row represents each word. The implementation of one-hot encoding at word-level is similar to the above codes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Impossible Mr Bennet impossible when I am not acquainted with him&#39;</span>

<span style="color:#75715e"># build vocabulary</span>
vocabulary <span style="color:#f92672">=</span> set(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
word2idx <span style="color:#f92672">=</span> {w: idx <span style="color:#66d9ef">for</span> idx, w <span style="color:#f92672">in</span> enumerate(vocabulary)}
len(vocabulary), word2idx[<span style="color:#e6db74">&#39;impossible&#39;</span>]
<span style="color:#75715e"># 10, {&#39;impossible&#39;: 0, &#39;not&#39;: 1, &#39;i&#39;: 2, &#39;with&#39;: 3, &#39;acquainted&#39;: 4, &#39;bennet&#39;: 5, &#39;mr&#39;: 6, &#39;him&#39;: 7, &#39;am&#39;: 8, &#39;when&#39;: 9}</span>

<span style="color:#75715e"># each row represent a word in the sentence</span>
word_vector <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(line<span style="color:#f92672">.</span>split()), len(vocabulary)))
<span style="color:#66d9ef">for</span> idx, w <span style="color:#f92672">in</span> enumerate(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split()):
    j <span style="color:#f92672">=</span> word2idx[w]
    word_vector[idx][j] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

word_vector<span style="color:#f92672">.</span>shape
<span style="color:#75715e"># (11, 10)</span>
</code></pre></div><p>The results are shown below, we can see that this sentence can be represented as a  <code>11 x 10</code> matrix.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># (|sentence|, |vocabulary|)</span>
Impossible [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]
Mr         [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]
Bennet     [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]

</code></pre></div><h4 id="pros">Pros</h4>
<ul>
<li>simple and intuitive to understand and implement</li>
</ul>
<h4 id="cons">Cons</h4>
<ul>
<li>The size of a ont-hot vector is proportional to the size of vocabulary, resulting in a sparse representation when we have a large corpus</li>
<li>The representation matrix doesn&rsquo;t have fixed size. The dimension varies in sentences or words with different lengths.</li>
<li>Too naive to capture the similarity between words</li>
<li>It cannot deal with out-of-vocabulary(OOV) problem</li>
</ul>
<h3 id="bag-of-word">Bag of Word</h3>
<p>The idea of Bag-of-word(BOW) is that all the words in the corpus are in a bag without considering the orders and context. The intuition is similar to the concepts introduced in LDA — a topic is characterized by a  small specific set of words. Therefore, BOW is commonly used to classify documents. If two documents are similar (have the same words), they are likely to be classified into the same group. Each document is represented as a vector of <code>|V|</code> dimensions, where the element value of this vector is the frequency of the word in the corresponding doc. Say we have the following corpus,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">corpus <span style="color:#f92672">=</span> [
  <span style="color:#e6db74">&#39;cat eats meat and dog eats meat&#39;</span>,
  <span style="color:#e6db74">&#39;cat eats fish&#39;</span>,
  <span style="color:#e6db74">&#39;dog eats bones&#39;</span>
]
</code></pre></div><p>The vocabulary of this small corpus and the doc-term matrix are</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">bagofWord</span>(corpus):
    vocab <span style="color:#f92672">=</span> set(<span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(corpus)<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
    word2idx <span style="color:#f92672">=</span> {word: idx <span style="color:#66d9ef">for</span> idx, word <span style="color:#f92672">in</span> enumerate(vocab)}

    doc_term <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> corpus:
        doc_v <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>]<span style="color:#f92672">*</span>len(vocab)
        <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split():
            <span style="color:#66d9ef">if</span> word <span style="color:#f92672">in</span> vocab:
                doc_v[ word2idx[word]] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        doc_term<span style="color:#f92672">.</span>append(doc_v)

    print(word2idx)
    <span style="color:#66d9ef">for</span> i, doc_v <span style="color:#f92672">in</span> enumerate(doc_term):
        print(corpus[i], list(doc_v))

<span style="color:#75715e">#</span>
<span style="color:#75715e"># {&#39;eats&#39;: 0, &#39;and&#39;: 1, &#39;dog&#39;: 2, &#39;fish&#39;: 3, &#39;cat&#39;: 4, &#39;meat&#39;: 5, &#39;bones&#39;: 6}</span>
<span style="color:#75715e"># cat eats meat and dog eats meat [2, 1, 1, 0, 1, 2, 0]</span>
<span style="color:#75715e"># cat eats fish [1, 0, 0, 1, 1, 0, 0]</span>
<span style="color:#75715e"># dog eats bones [1, 0, 1, 0, 0, 0, 1]</span>
</code></pre></div><h4 id="one-hot-1">one-hot</h4>
<p>Sometimes, we don&rsquo;t care about the number of occurrence of words. Just like one-hot encoding, we only want to know whether a word is present in the sentence or not, which would be useful for sentiment analysis. Well, that&rsquo;s easy to implement using Sklearn</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># occurrence</span>
vectorizer <span style="color:#f92672">=</span> CountVectorizer(binary<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

X_train_dtm <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>fit_transform(X_train)
X_test_dtm <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>transform(X_test)

</code></pre></div><h4 id="pros-1">Pros</h4>
<ul>
<li>Simple and intutive to understand and implement</li>
<li>Captures the semantics of documents, if two docs have similar words, they will be close to each other in the word space</li>
<li>Fixed matrix representation no matter how long a sentence is</li>
</ul>
<h4 id="cons-1">cons</h4>
<ul>
<li>It ignores the word order(context), so <code>Cat bites man</code> and <code>Man bites cat</code> have the same representation</li>
<li>The size of the matrix is proportional to the size of vocabulary</li>
<li>It doesn&rsquo;t deal with out-of-vocabulary(OOV) problem</li>
<li>It doesn&rsquo;t capture the similarity between different words that have the same meaning, e.g <code>cat eats, cat ate</code>, BOW will treat them as different vectors though they convey the same semantics.</li>
<li>As mentioned earlier, the most frequent words are often function words like pronouns, determiners and conjuctions. However, they are of no help for classification.</li>
</ul>
<h3 id="bag-of-n-gram">Bag of N-gram</h3>
<p>Basically, n-gram is a sequence of N tokens. Generally, we treat each word as an independent unit, in this case, a word is 1-gram or unigram. Similarly, a two-word sequence of words is 2-gram (bigram), three words is 3-gram (trigram), and so on so forth. A simple implementation is shown below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_ngrams</span>(tokens, n<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
	ngrams <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>[tokens[i:] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n)])
	<span style="color:#66d9ef">return</span> [<span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(gram) <span style="color:#66d9ef">for</span> gram <span style="color:#f92672">in</span> ngrams]
</code></pre></div><p>So what&rsquo;s the use of n-gram?</p>
<ul>
<li>estimate the proability of the last word of an n-gram given the previous words</li>
<li>assign probabilities to entire sequences</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">corpus <span style="color:#f92672">=</span> [
  <span style="color:#e6db74">&#39;Dog bites man&#39;</span>,
  <span style="color:#e6db74">&#39;Man bites dog&#39;</span>,
  <span style="color:#e6db74">&#39;Dog eats meat&#39;</span>,
  <span style="color:#e6db74">&#39;Man eats food&#39;</span>
]

count_vect <span style="color:#f92672">=</span> CountVectorizer(ngram_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
bow_rep <span style="color:#f92672">=</span> count_vect<span style="color:#f92672">.</span>fit_transform(corpus)

count_vect<span style="color:#f92672">.</span>vocabulary_
<span style="color:#75715e"># {&#39;dog&#39;: 3, &#39;bites&#39;: 0, &#39;man&#39;: 10, &#39;dog bites&#39;: 4,</span>
<span style="color:#75715e"># &#39;bites man&#39;: 2, &#39;man bites&#39;: 11, &#39;bites dog&#39;: 1, &#39;eats&#39;: 6, &#39;meat&#39;: 13,</span>
<span style="color:#75715e"># &#39;dog eats&#39;: 5, &#39;eats meat&#39;: 8, &#39;food&#39;: 9, &#39;man eats&#39;: 12, &#39;eats food&#39;: 7}</span>
</code></pre></div><p>Pros and Cons</p>
<ul>
<li>
<p>captures word order and context in a way</p>
</li>
<li>
<p>dimentionality increases as $n$ increases</p>
</li>
<li>
<p>it still have OOV problem</p>
</li>
</ul>
<h3 id="tf-idf">TF-IDF</h3>
<p>So far, we have learned that one-hot encoding focuses more on the occurrence of words in text while BOW pays more attention to word frequency. In both cases, they consider each word in the corpus euqally (with the same weight).</p>
<p>In contrast, TF-IDF allows us to measure the importance of each word relative to other words in the doc and the corpus. This is useful for <strong>information retrieval systems</strong>, where we expect that the most relevant documents should appear first.</p>
<p>How does TF-IDF work? As the name suggests, it calculates two quantities:</p>
<ul>
<li>
<p>term frequency(TF), the normalized frequency of each token $w_i$ in a given doc $d_j$</p>
<p>$$
TF(w_i, d_j) = \frac{|w_i^{d_j}|}{|d_j|}
$$</p>
<ul>
<li>The intution is that the more frequent a word appears in a doc, the more important it is. Thus, we need to increase its importance</li>
</ul>
</li>
<li>
<p>inverse document frequency(IDF), the logarithm of the inverse normalized frequency of each token across all documents</p>
<p>​
$$
IDF(w_i) = \text {log} \frac{|D|}{|w_i^D|}
$$</p>
<ul>
<li>
<p>The intuiton is fair straightforward — if a word appears across all the docs, for instance, stop words like <code>a</code>, <code>is</code>, <code>and</code>, it&rsquo;s unlikely to capture the characteristics of the doc it belong to. That is, they are more common compared to other less frequent words in the same doc. In other words, we need to reduce its importance, that&rsquo;s why we invert the calculation.</p>
</li>
<li>
<p>we use logarithm to further punish terms that appear more frequently across all the docs</p>
</li>
</ul>
</li>
</ul>
<p>Putting it together, the TF-IDF is defined as</p>
<p>$$
TF\_IDF = TF(w_i, d_j) * IDF(w_i)
$$</p>
<h2 id="distributed-representation">Distributed Representation</h2>
<p>Continuous Bag Of Words (CBOW) uses context words to predict the center word while Skip-Gram use the current word to predict its neighbouring words. The number of context words is determined by a parameter called &ldquo;window size&rdquo;.</p>
<h3 id="cbow">CBOW</h3>
<p>Basically, CBOW is a multi-class classification, which can be descibed as follows,</p>
<ul>
<li>words are encoded in one-hot format</li>
<li>each one-hot encoded vector is fed into the first layer in order to get the embedding of that word</li>
<li>combine the above real valued vectors in some way such that it captures the overall context</li>
<li>finally, the linear layer and softmax layer are used to predict probability distribution over vocabulary. The largest prob indicates the most likely target word</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CBOW</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, voc_size, embd_size):
        super(CBOW, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(voc_size, embd_size)
        self<span style="color:#f92672">.</span>fc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embd_size, voc_size)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(x)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(out)
        out_prob <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(out)
        <span style="color:#66d9ef">return</span> out_prob
</code></pre></div><p>However, there are two serious problems as one-hot matrix becomes sparse due to increasing vocabulary increases</p>
<ul>
<li>the calculation between one-hot and Embedding layer</li>
<li>the calculation between Embedding layer and the linear layer</li>
<li>the calculation of softmax layer</li>
</ul>
<h4 id="vectorization">Vectorization</h4>
<p>The first problem is easy to solve because there is no need to store the entire one-hot matrix. After all, it&rsquo;s just used to extract the embedding of the corresponding token, which corrsponds the row of the embdding matrix $E$. In other words, we can just store the row index of the words in one dimensional array.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CBOW</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#f92672">...</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">one_hot</span>(self, context):
        <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">        context: &#39;thank very much&#39;
</span><span style="color:#e6db74">        target: you
</span><span style="color:#e6db74">        &#39;&#39;&#39;</span>
        
        indices <span style="color:#f92672">=</span> [ lookup[token] <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> context<span style="color:#f92672">.</span>split() ]
        context_vec <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(len(indices))

        <span style="color:#75715e"># instead of one-hot (sparse matrix), we keep the row index of the embedding matrix</span>
        context_vec[:len(indices)] <span style="color:#f92672">=</span> indices
        context_vec[len(indices):] <span style="color:#f92672">=</span> mask <span style="color:#75715e"># in case of the number of tokens in the context is less than the window size</span>
        
        <span style="color:#66d9ef">return</span> context_vec

</code></pre></div><p>One thing we should notice is that same word could occur many times. For example, &ldquo;Like&rdquo; occurs twice  in the sentence &ldquo;Like (Sunday) Like Rain&rdquo;. When performing backpropagation, we should accumulate the gradients for word &ldquo;Like&rdquo;. This is because that we aggerate the embeddings of all the context words for each instance, and then fed it into a linear layer to make a prediction in the forward process as shown below.</p>
<p>$$
E_{Like} * 2 + E_{Rain} = E_{aggerate} \\ Out = E_{aggerate} * W_{fc}
$$
where $E_{Like}$ is the embedding of word &ldquo;Like&rdquo;.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(x)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (batch, |D|, |E|)</span>
out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(out)
</code></pre></div><h3 id="skip-gram">Skip-gram</h3>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>The idea is to transform a multi-class problem into many binary classification problems by sampling the positive example and several negative examples (typically 10 ~ 50) according the frequency distribution of the vocabulary. Frequent words tend to be sampled and rare words is likely not to be sampled, since rare words barely occur in real life and more frequent words would lead to a better generalisation.</p>
<h3 id="subsampling">Subsampling</h3>
<p>The idea of subsampling is that the vecot representation of frequent words do not change significantly, so we do not need to include them all for each training example.  For example, &ldquo;the&rdquo; is frequently used in almost every sentence, so this is no need to train &ldquo;France&rdquo; and &quot; the&quot;.  Word $w_i$ in the training set will be discarded with a higher probability computed by the formula</p>
<p>$$
P(w_i) = 1 - \sqrt {\frac{t}{f(w_i)}}
$$
where $f(w_i)$ is the frequency of word $w_i$ and $t$ is a chosen threshold (typically around $10^{-5}$). If a word appears very frequently, then $P(w_i)$ is close to $1$, which means it will not be used as the context word and target word. This technique could also be applied to discard less frequent phrases, see <a href="https://medium.datadriveninvestor.com/skip-gram-model-broken-down-subsampling-n-grams-feab04a6f220#:~:text=In%20layman%E2%80%99s%20terms%2C%20subsampling%20is%20to%20sample%20the,to%20sample%20a%20word%20has%20to%20be%20defined.">this post</a>.</p>
<h3 id="evaluation">Evaluation</h3>
<p>King - man  + woman =  Queen</p>
<h2 id="refer">Refer</h2>
<ul>
<li><a href="https://kelvinniu.com/posts/word2vec-and-negative-sampling/">https://kelvinniu.com/posts/word2vec-and-negative-sampling/</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Skip-Gram Paper</a></li>
</ul>
        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/misc/sort/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Sorting</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/dl/text-preprocessing/">
                <span class="next-text nav-default">NLP - Text Preprocessing</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#frequency-based">Frequency-Based</a>
      <ul>
        <li><a href="#one-hot">One-hot</a></li>
        <li><a href="#bag-of-word">Bag of Word</a></li>
        <li><a href="#bag-of-n-gram">Bag of N-gram</a></li>
        <li><a href="#tf-idf">TF-IDF</a></li>
      </ul>
    </li>
    <li><a href="#distributed-representation">Distributed Representation</a>
      <ul>
        <li><a href="#cbow">CBOW</a></li>
        <li><a href="#skip-gram">Skip-gram</a></li>
        <li><a href="#negative-sampling">Negative Sampling</a></li>
        <li><a href="#subsampling">Subsampling</a></li>
        <li><a href="#evaluation">Evaluation</a></li>
      </ul>
    </li>
    <li><a href="#refer">Refer</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.4484b3f29dd568c80320701800e1b69704b179f367b8223a43c728d819f39b97.js" integrity="sha256-RISz8p3VaMgDIHAYAOG2lwSxefNnuCI6Q8co2Bnzm5c=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
