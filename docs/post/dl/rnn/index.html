<!DOCTYPE html>
<html
  lang="en"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          Deep Learning - RNN - xiaopan&#39;s blog
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="xiaopan" />
  <meta name="description" content="When I first learned RNN, I found lots of articles about it. That&amp;rsquo;s good news, but most of them focus on the design of the architecture.The network itself is not difficult to understand, the real problem is why it works. What&amp;rsquo;s the intuition behind it? Why do we need &amp;ldquo;recurrence&amp;rdquo;? Unfortunately, few articles explain it clearly. Luckily, the book Deep Learning gives the answers.
" />







<meta name="generator" content="Hugo 0.91.0" />


<link rel="canonical" href="https://ixiaopan.github.io/blog/post/dl/rnn/" />





<link rel="icon" href="/blog/favicon.ico" />











<link rel="stylesheet" href="/blog/sass/jane.min.e826e860368147e5a6685e686355e4d7789023c18c9ea2e78b35f6786ce92736.css" integrity="sha256-6CboYDaBR&#43;WmaF5oY1Xk13iQI8GMnqLnizX2eGzpJzY=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="Deep Learning - RNN" />
<meta property="og:description" content="When I first learned RNN, I found lots of articles about it. That&rsquo;s good news, but most of them focus on the design of the architecture.The network itself is not difficult to understand, the real problem is why it works. What&rsquo;s the intuition behind it? Why do we need &ldquo;recurrence&rdquo;? Unfortunately, few articles explain it clearly. Luckily, the book Deep Learning gives the answers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ixiaopan.github.io/blog/post/dl/rnn/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-09-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-09-26T00:00:00+00:00" />

<meta itemprop="name" content="Deep Learning - RNN">
<meta itemprop="description" content="When I first learned RNN, I found lots of articles about it. That&rsquo;s good news, but most of them focus on the design of the architecture.The network itself is not difficult to understand, the real problem is why it works. What&rsquo;s the intuition behind it? Why do we need &ldquo;recurrence&rdquo;? Unfortunately, few articles explain it clearly. Luckily, the book Deep Learning gives the answers."><meta itemprop="datePublished" content="2021-09-26T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-09-26T00:00:00+00:00" />
<meta itemprop="wordCount" content="2872">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning - RNN"/>
<meta name="twitter:description" content="When I first learned RNN, I found lots of articles about it. That&rsquo;s good news, but most of them focus on the design of the architecture.The network itself is not difficult to understand, the real problem is why it works. What&rsquo;s the intuition behind it? Why do we need &ldquo;recurrence&rdquo;? Unfortunately, few articles explain it clearly. Luckily, the book Deep Learning gives the answers."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/blog/" class="logo">Pan</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/blog/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/blog/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/blog/" class="logo">
    
      Pan
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/">This is Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://ixiaopan.github.io/blog/about/">About</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">Deep Learning - RNN</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      <a href="/blog/about">
        <span class="post-meta-author-name">
          xiaopan
        </span>
      </a>
    
  </div>

  <div class="post-meta-time">
    <time datetime="2021-09-26">
      2021-09-26
    </time>
  </div>

  


  <div class="post-meta__right">
    

    <div class="post-meta-category">
        <a href="https://ixiaopan.github.io/blog/categories/deep-learning/"> Deep Learning </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <p>When I first learned RNN, I found lots of articles about it. That&rsquo;s good news, but most of them focus on the design of the architecture.The network itself is not difficult to understand, the real problem is why it works. What&rsquo;s the intuition behind it? Why do we need &ldquo;recurrence&rdquo;? Unfortunately, few articles explain it clearly. Luckily, the book <a href="https://www.amazon.co.uk/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618">Deep Learning</a> gives the answers.</p>
<h2 id="rnn">RNN</h2>
<h3 id="sequence-modeling">Sequence Modeling</h3>
<p>Recurrent neural networks (RNN) is another kind of neural networks, which are good at processing sequential data, such as stock prices, text, and audio. Machine translation and speech recognition are typical applications. So, what is special about sequential data? Why not still use MLP? Why do we bother designing another type of network to suit it?</p>
<h4 id="q1-the-characteristics-of-sequential-data">Q1: the characteristics of sequential data</h4>
<p>Sequential data consists of a series of data points across time, which have a dependency on each other. In CNN, we feed a single image into the network each time, but the order of these images doesn&rsquo;t matter. Each image is an independent individual. However, when we process text, the order of words matters. For example, &ldquo;I like running&rdquo; is a correct sentence while &ldquo;Running like I&rdquo; is wrong. More importantly, there are dependencies between words. We understand the meaning of a piece of text or audio based on the understanding of the previous words.</p>
<h4 id="q2-why-not-mlp">Q2: Why not MLP?</h4>
<p>Remember that there are two main reasons why we use CNN rather than MLP</p>
<ul>
<li>MLP cannot capture spatial information
<ul>
<li>Flattening will cause the loss of spatial information of image pixels, especially there are multiple channels (depth). For example, the colour of a pixel can be estimated by considering the nearest neighbouring pixel&rsquo;s value</li>
</ul>
</li>
<li>Repeated pattern is hard to detect
<ul>
<li>The object to be detected (e.g. Cats) can appear anywhere in the image. Actually, we do not care about locations but the pattern.</li>
</ul>
</li>
</ul>
<p>Well, the reasons why MLP is not suitable for sequential data are similar.</p>
<ul>
<li>Spatial information in sequential data means the order of data and dependencies among data.
<ul>
<li>Each word is considered as an input feature when using MLP, but the representation of each word does not contain dependencies between words.</li>
</ul>
</li>
<li>A sentence can be written in different ways using the same words.
<ul>
<li>For example, &ldquo;Today, I am going to watch a movie.&rdquo; is the same as &ldquo;I am going to watch a movie today.&rdquo; When we feed each word into MLP, &ldquo;Today&rdquo; will be assigned with different weights.</li>
</ul>
</li>
<li>Sentence length is variable.</li>
</ul>
<h4 id="q3-what-kind-of-networks-is-suitable-for-sequential-data">Q3: What kind of networks is suitable for sequential data?</h4>
<p>From the above analysis, we conclude that a network that is capable of modeling sequential data should be able to</p>
<ul>
<li>handle with variable sequence length</li>
<li>retain the order of data and dependencies between data</li>
<li>detect repeated patterns</li>
</ul>
<h3 id="architecture">Architecture</h3>
<p>So, how RNN works? Figure 1 shows that RNN is somehow a kind of state machine, which only accepts two inputs:</p>
<ul>
<li>the previous system state $h_{t-1}$</li>
<li>the current input $x_t$</li>
</ul>
<p><img src="/blog/post/images/rnn-unfold.png#full" alt="" title="Figure 1: The architecture of RNN."></p>
<p>Specifically, RNN is defined as</p>
<p>$$
h_t = \sigma (W_x x_t + W_h h_{t-1} + b_h) \\ o_t = W_y h_t + b_y \\ \hat y_t = \sigma(o_t)
$$
First, unlike the Feed Forward Networks that requires fixed length of the inputs, RNN can accormadate variable sequence length by looping. Second, the input data are fed into the network sequentially to maintain the order. At each time $t$, the system will reach a new state $h_t$ by receiving the previous state $h_{t-1}$ and the current input $x_t$. And the new state will be fed into the network recursively. In other words, at each time $t$, the system has already known what happend before through $h_{t-1}$. Finally, the weights of RNN are shared ( $W_t, W_h, W_y, b_h, b_y$ ) each time step to capture the repeated pattern.</p>
<h3 id="bptt">BPTT</h3>
<p>Traning a RNN is nothing special. First, let&rsquo;s define the loss function. For a sequence whose length is $T$, the loss is computed as</p>
<p>$$
L = \sum_{t=1}^T L_t (\hat y_t, y_t) = \sum_{t=1}^T -y_t \text{log}\hat y_t = \sum_{t=1}^T -y_t \text{log} \sigma(o_t)
$$
where $\sigma$ is the softmax function. The computation graph is shown below.</p>
<p><img src="/blog/post/images/rnn-graph.png" alt="" title="Figure 2: Computation Graph of RNN"></p>
<h4 id="w_y">$W_y$</h4>
<p>First, we compute the gradient of $L$ w.r.t $W_y$. From Figure 2, we can see that $W_y$ has three parents $o_0, o_1, o_2$. We can generalize it to $T$ time steps
$$
\frac {\partial L}{\partial W_y} = \sum_{t=1}^T \frac {\partial o_t}{\partial W_y} \frac {\partial L}{\partial o_t} = \sum_{t=1}^T \frac {\partial o_t}{\partial W_y} \frac {\partial \hat y_t}{\partial o_t} \frac {\partial L}{\partial \hat y_t}
$$</p>
<p>Now the question is — what&rsquo;s the derivative of $L$ w.r.t $o_t$?</p>
<p>$$
\frac {\partial L}{\partial o_t} = -y_t \frac{1}{\sigma(o_t)} \frac{\partial \sigma(o_t)}{\partial o_t} - \sum_{i \neq t} ^N y_i \frac{1}{\sigma(o_i)} \frac{\partial \sigma(o_i)}{\partial o_t}
$$</p>
<p>Generally, $\sigma$ is the softmax function, so the derivative of $\sigma$ w.r.t $o_i$ is given as</p>
<p>$$
\frac{\partial \frac{e^{o_t}}{\sum_{k=1}^K e^{o_k}}}{\partial o_i} = \frac{ \frac{\partial e^{o_t}}{\partial o_i} \sum_{k=1}^K e^{o_k} - e^{o_i} e^{o_t} }{ [\sum_{k=1}^K e^{o_k} ]^2}
$$</p>
<p>Here, we use the derivative of $f(x) = \frac{g(x)}{h(x)}$ directly.</p>
<p>$$
f'(x) = \frac{g'(x)h(x) - h'(x)g(x)}{h^2(x)}
$$</p>
<p>For $i = t$, we have</p>
<p>$$
\frac{\partial \sigma(o_t)}{\partial o_t} = \sigma(o_t) (1 - \sigma(o_t))
$$
For $i \ne t$, we have</p>
<p>$$
\frac{\partial \sigma(o_i)}{\partial o_t} = - \sigma(o_i) \sigma(o_t)
$$
Thus,</p>
<p>$$
\frac {\partial L}{\partial o_t} = -y_t \frac{1}{\sigma(o_t)} \sigma(o_t) (1 - \sigma(o_t)) + \sum_{i \neq t} ^N y_i \frac{1}{\sigma(o_i)} \sigma(o_i) \sigma(o_t) = -y_t (1 - \sigma(o_t)) + \sum_{i \neq t} ^N y_i \sigma(o_t) \\ = -y_t + y_t \sigma(o_t) + \sum_{i \neq t} ^N y_i \sigma(o_t) = \sigma(o_t) - y_t
$$</p>
<p>Finally, we have
$$
\frac {\partial L}{\partial W_y} = \sum_{t=1}^T \frac {\partial o_t}{\partial W_y} \frac {\partial L}{\partial o_t} = \sum_{t=1}^T  (\hat y_t - y_t) \otimes h_t
$$</p>
<p>where $\otimes$ is the outer product of vectors.</p>
<h4 id="w_h">$W_h$</h4>
<p>Similarly, the gradient of $L_2$ w.r.t $W_h$ is the sum of gradients of all nodes that are the parents of $W_h$.</p>
<p>$$
\frac {\partial L_2}{\partial W_h} = \frac {\partial h_0}{\partial W_h} \frac {\partial L_2}{\partial h_0}+ \frac {\partial h_1}{\partial W_h} \frac {\partial L_2}{\partial h_1} + \frac {\partial h_2}{\partial W_h} \frac {\partial L_2}{\partial h_2} \\  \  \\ = \frac {\partial h_0}{\partial W_h} \frac {\partial h_1}{\partial h_0} \frac {\partial h_2}{\partial h_1}\frac {\partial L_2}{\partial h_2} + \frac {\partial h_1}{\partial W_h} \frac {\partial h_2}{\partial h_1} \frac {\partial L_2}{\partial h_2} + \frac {\partial h_2}{\partial W_h} \frac {\partial L_2}{\partial h_2}
\\  \ \\ = \sum_{i=0}^2  \frac {\partial h_i}{\partial W_h} (\prod_{j=i}^2 \frac {\partial h_{j+1}}{\partial h_j} ) \frac {\partial L_2}{\partial h_2} \\  \ \\ = \sum_{i=0}^2  \frac {\partial h_i}{\partial W_h} (\prod_{j=i}^2 \frac {\partial h_{j+1}}{\partial h_j} ) \frac {\partial \hat y_2}{\partial h_2} \frac {\partial L_2}{\partial \hat y_2}
$$</p>
<p>So, the gradient of $L$ w.r.t $W_h$ is</p>
<p>$$
\frac {\partial L}{\partial W_h} = \sum_{t=0}^T \sum_{i=0}^t \frac {\partial h_i}{\partial W_h} (\prod_{j=i}^t \frac {\partial h_{j+1}}{\partial h_j} ) \frac {\partial \hat y_t}{\partial h_t} \frac {\partial L_t}{\partial \hat y_t}
$$</p>
<h4 id="w_x">$W_x$</h4>
<p>Similarly, the gradient of $L$ w.r.t $W_x$ is defined as follows,</p>
<p>$$
\frac {\partial L}{\partial W_x} = \sum_{t=0}^T \sum_{i=0}^t \frac {\partial h_i}{\partial W_x} (\prod_{j=i}^t \frac {\partial h_{j+1}}{\partial h_j} ) \frac {\partial \hat y_t}{\partial h_t} \frac {\partial L_t}{\partial \hat y_t}
$$</p>
<h3 id="truncated-bptt">Truncated BPTT</h3>
<h2 id="vanishingexploding-gradient">Vanishing/Exploding Gradient</h2>
<h3 id="problem">Problem</h3>
<p>The main problem of RNN is the well-known vanishing/exploding gradient. Why? From the above equations, we can see that it could be due to the repeated multiplication of $\frac {\partial h_{j+1}}{\partial h_j}$.</p>
<p>Remember that $h_{j+1}$ is derived from</p>
<p>$$
h_{j+1} = \text{tanh} (W_h h_j + W_x x_{j+1} + b_h)
$$
Thus, the derivative is</p>
<p>$$
\frac {\partial h_{j+1}}{\partial h_j} = \text{diag} [\text{tanh}'(o_j)] W_h
$$</p>
<p>So, the gradient is determined by the weights and the derivatives of the activation functions. (PS: the result is a Jacobian matrix because we are taking the derivative of a vector function w.r.t a vector.)</p>
<p>First, the activation function is usually tanh or sigomid funtion, and they have the following properties:</p>
<ul>
<li>the derivatives are always less than 1</li>
<li>the derivatives tend to be saturated and close to zero when the input are far away from zero.</li>
</ul>
<p>Thus, with small values in the matrix and multiple multiplications, the gradient will shrink quickly.</p>
<p>Second, if $W_h$ overpowers $\text{tanh}'(o_j)$, the gradient value can be inferred by the eigenvalues of $W_h$, as the below equation shows,</p>
<p>$$
\prod_{j=i}^t \frac {\partial h_{j+1}}{\partial h_j} = Q \Lambda^{t-i} Q^{-1}
$$</p>
<ul>
<li>If the largest eigenvalue of $W_h$ is greater than 1, the gradient will grow quickly and go to infinity</li>
<li>On the contrary, if it is less than 1, the gradient will shrink exponentially</li>
</ul>
<h3 id="identify-vanishingexploding">Identify vanishing/exploding</h3>
<p>So, there are some signals that might indicate that we are suffering from vanishing or exploding</p>
<p>vanishing</p>
<ul>
<li>the parameters of the deepest layers changes greatly but the parameters of the front layers change little</li>
<li>the model learns slowly</li>
</ul>
<p>exploding</p>
<ul>
<li>the parameters changes exponentially</li>
<li>the parameters might be NaN</li>
</ul>
<h3 id="solution">Solution</h3>
<h4 id="gradient-clipping">Gradient Clipping</h4>
<p>There is a specific solution to mitigate the exploding gradient —— gradient clipping. Since the gradient is too huge, it&rsquo;s likely to move out of the parameter plane. An intuitive way is to control the size of the gradient (threshold), which is denoted by $\eta$. The intuition is to take a step in the same directions, but a smaller step.</p>
<p>$$
\text{if } ||g|| \ge \eta:
g = \eta \frac{g}{||g||}
$$</p>
<h4 id="non-saturating-activation-function">Non-saturating Activation Function</h4>
<p>Instead of using sigmoid and tanh functions that prone to be saturated, we use non-saturating functions, such as ReLu.</p>
<h4 id="weight-initialization">Weight Initialization</h4>
<h4 id="batch-normalization">Batch Normalization</h4>
<p>The purpose of normalization is to transform data into a fixed range by translation and scale. It is a common technique used in data preprocessing. Min-max and z-score are two common normalization methods. Batch normalization means normalization performed on a batch of data rather than the whole data as we train data by group in NN. Below is the algorithm of batch normalization,</p>
<p>$$
\mu = \frac{1}{|B|} \sum x_i \\ \sigma^2 = \frac{1}{|B|}\sum(x_i - \mu)^2 \\ x_i' = \frac{x_i - \mu}{\sqrt { \sigma^2 + \epsilon}} \\ y_i = r x_i' + \beta
$$</p>
<p>Figure 3 shows what batch normalization does. Before normalization, $x$ could be anywhere in the input space. After normalization, we restrict the input space to a small range (the shadow area), preventing $x$ from reaching the edges of the sigmoid function. In doing so, the gradient of the sigmoid function is unlikely to be small value.</p>
<p><img src="/blog/post/images/batch-norm-visual.png" alt="" title="Figure 3: Sigmoid function with normalized inputs"></p>
<p>Pros</p>
<ul>
<li>avoid vanishing gradient</li>
</ul>
<p>Cons</p>
<ul>
<li>depends on the batch size, small batch size may cause unstable results</li>
<li>not suitable for dynamic networks, e.g. RNN (different time steps)</li>
</ul>
<h4 id="re-design-networks">Re-design Networks</h4>
<p>Some effective new networks are residual networks or gated RNNs.</p>
<h2 id="gated-rnn">Gated RNN</h2>
<h3 id="intuition">Intuition</h3>
<p>The introduce of gate in gated RNNs can be considered in this way: the input data at each time step are not of equal importance. For example, in a sentence, the first word might be more important than the second word, so we&rsquo;d better to keep the previous system stable and not change too much at the second time step. Thus, we need some controls on the current system state and the current input data, determining how much they could affect on the system.</p>
<p>Strictly speaking, the purpose of the gates is to ensure the integrity of information, as <a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">Written Memories: Understanding, Deriving and Extending the LSTM</a> said. In RNN, $h_t$ is paricipated in the creation of new information at each time step. However, the new state will be ultimately fed into the non-linear function, such as tanh, causing the problem of information morphing. Therefore, it is difficult to decode the past information even if it is included in the current system state due to the distortion of information.</p>
<p>How to solve it? We explicitly add and subtract information from the system. We also need a read interface to avoid information overload. This is because the system contains so much information that not all of them are useful to the current time step. Moreover, we should read something first before writing, because we know nothing about the current system (we shall know something before writting). Otherwise, we run the risk of overwriting the system without having the old information (break the incremental change).</p>
<p>At the very beginning, we can read from the initial state. After that, we read the previous state ($s_{t-1}$), then decide what new information to write based on it. The new information is known as the candidate state, denoted by $\widetilde s_t$. We selectively forget some useless information (sbstraction), and update the system using the candidate state (addition). In doing so, we can ensure that the change in the system state is incremental,
$$
s_t = s_{t-1} + \Delta s_t
$$</p>
<p><img src="/blog/post/images/proto_lstm.png" alt="" title="Figure 4: The Prototype LSTM. Source: Written Memories."></p>
<p>Figure 4 depicts the internal structure of the initial LSTM. Below are the complete equations from <a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">Written Memories</a></p>
<p>$$
f_t = \sigma (W^fs_{t-1}+ U^fx_t + b^f) \\ i_t = \sigma (W^i s_{t-1} + U^ix_t + b^i) \\ o_t = \sigma (W^o s_{t-1} + U^o x_t + b^o) \\ \   \\ \widetilde s_t = \phi (W(o_t \odot s_{t-1}) + Ux_t + b) \\ s_t = f_t \odot s_{t-1} + i_t \odot \widetilde s_t
$$</p>
<h3 id="lstm">LSTM</h3>
<p>LSTM, short for long short-term memory, is designed to retain long-time dependencies. The main differences with vanilla RNNs are the cell state and the design of three gates, as shown in Figure 5.</p>
<p><img src="/blog/post/images/lstm.png" alt="" title="Figure 5: The internal structure of LSTM"></p>
<ul>
<li>There are three gates in LSTM, namely, forget gate, input gate and output gate. The value of gates is between 0 and 1, which is often achieved using the sigmoid function. 0 means nothing will flow through the gate, while 1 indicates all information will pass.
<ul>
<li>$$f_t = \sigma (W_x^fx_t + W_h^fh_{t-1} + b^f)$$</li>
<li>$$i_t = \sigma (W_x^ix_t + W_h^ih_{t-1} + b^i)$$</li>
<li>$$o_t = \sigma (W_x^ox_t + W_h^oh_{t-1} + b^o)$$</li>
</ul>
</li>
<li>The new information at the current step is the same as the vanilla RNN
<ul>
<li>$$g_t = \text{tanh} (W_x^gx_t + W_h^gh_{t-1} + b^g)$$</li>
</ul>
</li>
<li>The cell state, denoted by $C_t$, is the horizontal line across LSTM. It contains the whole information of the system and is visible to LSTM only. As mentioned above, we can control the extent to which we want to forget the previous information and how much the new information to be added. For example, if $f$ is close to 1 and $i$ is close to 0, the previous information retains, which means the inputs at the current step might not be so important (the first word might be more important than the second word)
<ul>
<li>$$C_t = f_t \odot C_{t-1} + i_t \odot g_t$$</li>
</ul>
</li>
<li>The current hidden state (used for making decision) is derived from the cell state and the current inputs.</li>
</ul>
<p>$$
h_t = o_t \odot \text{tanh} (C_t)
$$</p>
<p>Unlike the prototype LSTM, the real LSTM in practice has several differences.</p>
<ul>
<li>the state to be used for writting has already been ready, that is the hidden state $h_{t-1}$ ( $h_{t-1} = o_{t-1} \odot s_{t-1}$)</li>
<li>we can immediately obtain the candidate write $g_t$, and then update the main state $c_t$</li>
<li>finally, we obtain the next hidden state from $c_t$ using $o_t$</li>
</ul>
<p>So why does LSTM mitigate the problem of vanishing gradient? The reason why gradient vanishes is the recursive multiplication of weight and derivative of the activation functions, as explained above.  However, in LSTM, gradient is calculated by addition instead of multiplication, as shown below.</p>
<p>$$
\frac{\partial C_t}{\partial  C_{t-1}} = \frac{\partial f_t \odot C_{t-1}}{\partial C_{t-1}}  + \frac{\partial i_t \odot g_t}{\partial C_{t-1}}  \\ = \frac{\partial f_t }{\partial C_{t-1}} \odot C_{t-1} + \frac{\partial C_{t-1} }{\partial C_{t-1}} \odot f_t + \frac{\partial i_t \odot g_t}{\partial C_{t-1}} \\ = f_t + C_{t-1} \odot \frac{\partial f_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial C_{t-1}} + g_t \odot \frac{\partial i_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial C_{t-1}} + i_t \odot \frac{\partial g_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial C_{t-1}} \\ = f_t + C_{t-1} \odot \sigma' W_h^f  o_{t-1} \odot \text{tanh}'(C_{t-1}) + g_t \odot \sigma' W_h^i  o_{t-1} \odot \text{tanh}'(C_{t-1}) + i_t \odot \sigma' W_h^g o_{t-1} \odot \text{tanh}'(C_{t-1})
$$</p>
<p>Let&rsquo;s compare it with the vanilla RNN</p>
<p>$$
\frac {\partial h_{j+1}}{\partial h_j} = \text{diag} [\text{tanh}'(o_j)] W_h
$$</p>
<p>From the above equations, we can see that the gradient in LSTM is more flexible than RNN in two aspects</p>
<ul>
<li>$f_t, i_t, g_t$ are learned from the current time step</li>
<li>gradient can vary at each time step in LSTM (at step 1, it could be greater 1; at step 2, it might be less than 1), while it&rsquo;s either less than 1 or greater than 1 in RNN</li>
</ul>
<p>In this way, it is possible for LSTM to retain gradient for a long time.</p>
<h3 id="gru">GRU</h3>
<p>There are many variants of LSTM, and one of them is GRU, as Figure 6 shows.</p>
<p><img src="/blog/post/images/gru.png" alt="" title="Figure 6: The internal structure of GRU"></p>
<p>From Figure 6, we see that there are 2 gates and only one state in GRU.</p>
<ul>
<li>the reset gate $r_t$, which is also the read gate in the prototype LSTM</li>
<li>the update gate $z_t$, which is the concise version of forget ($z_t$) and update ($1-z_t$) gates in the prototype LSTM</li>
</ul>
<p>Thus, the equations in GRU can be derived as follows,</p>
<p>$$
r_t = \sigma (W^rh_{t-1}+ U^rx_t + b^r) \\ z_t = \sigma (W^z h_{t-1} + U^zx_t + b^z) \\ \   \\ \widetilde h_t = \phi (W(r_t \odot h_{t-1}) + Ux_t + b) \\ h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \widetilde h_t
$$</p>
<h2 id="birnn">BiRNN</h2>
<h2 id="references">References</h2>
<ul>
<li><a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf">Exploding and Vanishing Gradients</a></li>
<li><a href="https://mmuratarat.github.io/2019-02-07/bptt-of-rnn">Backpropagation Through Time for Recurrent Neural Network</a></li>
<li><a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture07-fancy-rnn.pdf">Fancy RNN</a></li>
<li><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">Written Memories: Understanding, Deriving and Extending the LSTM</a></li>
<li><a href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484">The Vanishing Gradient Problem</a></li>
<li><a href="https://adriandliu.github.io/2020/04/15/why-lstm/">Why LSTM Solves the Gradient Vanishing Problem of RNN</a></li>
<li><a href="https://medium.com/learn-love-ai/and-of-course-lstm-part-ii-3337ce3aafa0">And of course, LSTM — Part II</a></li>
<li><a href="https://christinakouridi.blog/2019/06/19/backpropagation-lstm/">Deriving the backpropagation equations for a LSTM</a></li>
<li><a href="https://agustinus.kristia.de/techblog/2016/08/12/lstm-backprop/">Deriving LSTM Gradient for Backpropagation</a></li>
</ul>
        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/blog/post/ml/metrics-classification/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">Metrics for classification</span>
                <span class="prev-text nav-mobile">Prev</span>
              </a>
            
              <a class="next" href="/blog/post/dl/cnn/">
                <span class="next-text nav-default">Deep Learning - CNN</span>
                <span class="prev-text nav-mobile">Next</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      


      
      

  

  
  

  
  

  

  

    

  

  


    </div>

    
    <nav class="toc" id="toc">
    <div class="toc-title">Table of Contents</div>
    <div class="toc-content custom-scrollbar">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#rnn">RNN</a>
      <ul>
        <li><a href="#sequence-modeling">Sequence Modeling</a></li>
        <li><a href="#architecture">Architecture</a></li>
        <li><a href="#bptt">BPTT</a></li>
        <li><a href="#truncated-bptt">Truncated BPTT</a></li>
      </ul>
    </li>
    <li><a href="#vanishingexploding-gradient">Vanishing/Exploding Gradient</a>
      <ul>
        <li><a href="#problem">Problem</a></li>
        <li><a href="#identify-vanishingexploding">Identify vanishing/exploding</a></li>
        <li><a href="#solution">Solution</a></li>
      </ul>
    </li>
    <li><a href="#gated-rnn">Gated RNN</a>
      <ul>
        <li><a href="#intuition">Intuition</a></li>
        <li><a href="#lstm">LSTM</a></li>
        <li><a href="#gru">GRU</a></li>
      </ul>
    </li>
    <li><a href="#birnn">BiRNN</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </div>
  </nav>


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  
  
    <a href="wxp201013@163.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://www.linkedin.com/in/ixiaopan" rel="me noopener" class="iconfont"
      title="linkedin"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="33" height="33">
  <path d="M872.405333 872.618667h-151.637333v-237.610667c0-56.661333-1.152-129.578667-79.018667-129.578667-79.061333 0-91.136 61.653333-91.136 125.397334v241.792H398.976V384h145.664v66.602667h1.962667c20.352-38.4 69.845333-78.933333 143.786666-78.933334 153.642667 0 182.058667 101.12 182.058667 232.746667v268.202667zM227.712 317.141333a87.978667 87.978667 0 0 1-88.021333-88.106666 88.064 88.064 0 1 1 88.021333 88.106666z m76.032 555.477334H151.68V384h152.064v488.618667zM948.266667 0H75.562667C33.792 0 0 33.024 0 73.770667v876.458666C0 991.018667 33.792 1024 75.562667 1024h872.576C989.866667 1024 1024 991.018667 1024 950.229333V73.770667C1024 33.024 989.866667 0 948.138667 0h0.128z"></path>
</svg>

    </a>
  
    <a href="https://github.com/ixiaopan" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://space.bilibili.com/22910840" rel="me noopener" class="iconfont"
      title="bilibili"  target="_blank"
      >
      <svg
  class="icon" style="" viewBox="0 0 1024 1024" version="1.1" width="36"
  height="36" id="svg8">
  <path
      style=""
      d="M 744.60599,0.00486267 A 41.779915,41.779915 0 0 0 710.4184,18.673394 L 548.5048,255.32642 h -11.70046 a 41.779915,41.779915 0 0 0 -10.80295,-7.84928 L 235.66,97.084498 a 41.779915,41.779915 0 0 0 -20.07193,-4.960864 41.779915,41.779915 0 0 0 -18.3748,79.145436 L 359.4859,255.32642 H 128.16909 c -49.458302,0 -89.27932,39.82105 -89.27932,89.27932 v 508.65224 c 0,49.4583 39.821018,89.27934 89.27932,89.27934 h 19.48445 C 149.12802,984.5043 179.92773,1024 224.79179,1024 c 44.86407,0 75.66379,-39.4957 77.13826,-81.46268 H 719.98116 C 721.45559,984.5043 752.25533,1024 797.1194,1024 c 44.86406,0 75.6638,-39.4957 77.13824,-81.46268 h 21.57323 c 49.45831,0 89.27936,-39.82104 89.27936,-89.27934 V 344.60574 c 0,-49.45827 -39.82105,-89.27932 -89.27936,-89.27932 H 649.74567 L 779.38103,65.866924 A 41.779915,41.779915 0 0 0 744.60599,0.00486267 Z M 644.49108,418.70871 c 6.29985,0.21538 12.44451,2.01107 17.86888,5.22196 l 171.36218,98.10771 c 18.23417,10.21935 24.63334,33.34627 14.24614,51.48533 -10.38726,18.13909 -33.57344,24.32718 -51.61587,13.77296 L 624.9903,489.18895 c -15.21356,-8.41858 -22.66871,-26.1765 -18.03211,-42.93436 4.63664,-16.75784 20.15573,-28.14465 37.53289,-27.54588 z M 350.2006,432.31846 c 16.89952,0.0317 31.69582,11.33328 36.17844,27.62747 4.48262,16.2942 -2.44981,33.57765 -16.95507,42.24898 l -140.7157,86.91312 c -17.68528,11.18244 -41.09629,5.77692 -52.08912,-12.02686 -10.99282,-17.80373 -5.33855,-41.15658 12.58167,-51.95857 L 329.9002,438.2095 c 6.0643,-3.86439 13.10951,-5.90891 20.3004,-5.89104 z M 501.605,641.53985 c 3.75002,-0.15248 7.48645,0.53903 10.93349,2.0235 0.15842,0.0637 0.31618,0.12888 0.47325,0.19582 0.59328,0.27092 1.17574,0.56489 1.74609,0.88121 0.15868,0.0854 0.31643,0.17233 0.47325,0.2611 0.55694,0.32165 1.10131,0.66458 1.63185,1.02807 0.16455,0.1123 0.32777,0.2265 0.48956,0.34269 0.50382,0.36781 0.99371,0.75428 1.46868,1.15864 0.18724,0.15504 0.37218,0.31282 0.55484,0.47323 0.43271,0.38784 0.8518,0.79061 1.25653,1.20756 0.15449,0.16114 0.30679,0.32437 0.45693,0.48959 0.40798,0.44266 0.79989,0.89988 1.17494,1.37076 0.17799,0.22544 0.35205,0.45395 0.5222,0.68538 0.25932,0.34701 0.50964,0.70071 0.75064,1.06071 0.26712,0.39516 0.52286,0.79784 0.76699,1.20757 0.16907,0.29043 0.33231,0.58424 0.48957,0.88123 0.21836,0.41297 0.42513,0.83199 0.62009,1.25653 0.14836,0.32333 0.28983,0.64976 0.42429,0.97911 0.21319,0.51552 0.40915,1.03801 0.58747,1.5666 0.0677,0.19499 0.13296,0.39085 0.19582,0.58748 0.18652,0.60823 0.34984,1.22334 0.48957,1.84399 0.0397,0.16277 0.0779,0.32601 0.11423,0.48957 0.1436,0.69112 0.25788,1.38801 0.34269,2.08877 0.005,0.0381 0.0111,0.0761 0.0163,0.11424 0.0857,0.78056 0.13474,1.56471 0.14687,2.34988 0.005,0.0543 0.0111,0.10879 0.0163,0.1632 0,0 -0.008,1.12132 0,1.45234 0,0 -0.14697,17.84761 5.89102,34.12231 3.01902,8.13734 7.33278,15.10615 12.61433,19.61501 5.28157,4.50889 11.42894,7.62081 23.64572,7.62081 12.2168,0 18.36416,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.5953,-11.47767 12.6143,-19.61501 6.03799,-16.2747 5.89103,-34.12231 5.89103,-34.12231 -0.44885,-13.87045 10.45922,-25.46302 24.3311,-25.86506 13.87189,-0.40201 25.42828,10.53953 25.78348,24.41272 0,0 1.11929,25.7226 -9.00791,53.01927 -5.06359,13.64832 -13.1986,28.46036 -27.05631,40.29073 -13.85772,11.83039 -33.5454,19.63135 -56.20142,19.63135 -22.65603,0 -42.34371,-7.80096 -56.20141,-19.63135 -4.1801,-3.56856 -7.78733,-7.42433 -10.99878,-11.42303 -3.21235,4.00037 -6.81703,7.85309 -10.99876,11.42303 -13.85773,11.83039 -33.5454,19.63135 -56.20144,19.63135 -22.65601,0 -42.3437,-7.80096 -56.2014,-19.63135 -13.85775,-11.83037 -21.99272,-26.64241 -27.05632,-40.29073 -10.12725,-27.29667 -9.00789,-53.01928 -9.00789,-53.01927 0.20714,-13.83687 11.58744,-24.88848 25.42444,-24.69013 14.1263,0.19991 25.2971,12.0278 24.69011,26.14247 0,0 -0.14697,17.84761 5.89103,34.12231 3.01902,8.13734 7.31646,15.10615 12.598,19.61501 5.28155,4.50889 11.44526,7.62081 23.66203,7.62081 12.21681,0 18.36418,-3.11192 23.64573,-7.62081 5.28154,-4.50886 9.57899,-11.47767 12.598,-19.61501 5.76352,-15.53489 5.89112,-32.05691 5.89103,-33.56746 0.006,-0.37466 0.0111,-1.05336 0.0163,-1.20759 -0.0117,-0.74583 0.0105,-1.49177 0.0652,-2.23565 0.009,-0.15784 0.0204,-0.31561 0.0327,-0.47324 0.14204,-1.56859 0.43163,-3.12027 0.86487,-4.63449 0.0213,-0.0763 0.0433,-0.15244 0.0652,-0.22848 3.0335,-10.25748 12.24157,-17.46007 22.92769,-17.93417 z"
      id="rect824"/>
</svg>

    </a>


<a href="https://ixiaopan.github.io/blog/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2021 -
    2024
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        xiaopan
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/blog/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/blog/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/blog/js/main.4484b3f29dd568c80320701800e1b69704b179f367b8223a43c728d819f39b97.js" integrity="sha256-RISz8p3VaMgDIHAYAOG2lwSxefNnuCI6Q8co2Bnzm5c=" crossorigin="anonymous"></script>






  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/blog/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  

















  </body>
</html>
