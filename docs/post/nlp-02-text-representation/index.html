<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>NLP - Text Representation</title>



  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-203640627-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-203640627-1');
  </script>
  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NLP - Text Representation"/>
<meta name="twitter:description" content="In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation."/>




<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />
<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

 




  <link rel="stylesheet" href="https://ixiaopan.github.io/blog/css/main.css" />


<meta name="generator" content="Hugo 0.81.0" />
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/me.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


    
    <div class="intro-header">
      <div class="container">
        <div class="post-heading">
          
            <h1>
              NLP - Text Representation
            </h1>
          
          
            <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Jul 10, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;9 min read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/nlp/"
        >NLP</a
      >&nbsp;
    
  
</span>

          
        </div>
      </div>
    </div>
    
  </header>

    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called <strong>text representation</strong>.</p>
<p>There are two aspects to consider: the level of representation and the meaning of numbers. We know that a sentence is composed of words and each word consists of a group of characters. This means we can represent text at sentence level, character-level, or both. As for numbers, the simplest way is to count the number of occurrences of each word. The most common methods based on this idea includes one-hot encoding, bag-of-word and TF-IDF.</p>
<h2 id="frequency-based">Frequency-Based</h2>
<h3 id="one-hot">One-hot</h3>
<p>One-hot representation indicates whether a word/character is present in a sentence/word. If true, we assign the value of 1 to that word, otherwise 0.</p>
<h4 id="character-level">character-level</h4>
<p>Figure 1 shows a simple word representation at character level. The whole vocabulary contains 26 English letters. For each word, for example, the word <code>impossible</code></p>
<ul>
<li>each row represents a character in <code>impossible</code>, so there are 10 rows</li>
<li>The corresponding value of each row is a vector whose element value is either 0 or 1, indicating whether the corresponding letter is present in the given word</li>
</ul>
<p>Thus, we will get a <code>(10, 26)</code> matrix for the word <code>impossible</code>.</p>
<p>
  <figure>
    <img src="/blog/post/images/word-vocab-matrix.png#full" alt="">
    <figcaption>Figure 1: character-vocabulary occurrence matrix with the shape of (|word|, |vocabulary|)</figcaption>
  </figure>
</p>
<p>Below are  simple codes to construct such a matrix.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Impossible Mr Bennet impossible when I am not acquainted with him&#39;</span>

line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
line, len(line)
<span style="color:#75715e"># (&#39;impossiblemrbennetimpossiblewheniamnotacquaintedwithhim&#39;, 55)</span>

<span style="color:#75715e"># each row represent a character in the sentence</span>
alphabet <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;abcdefghijklmnopqrstuvwxyz&#39;</span>
letter_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(len(line), len(alphabet)
<span style="color:#66d9ef">for</span> i, letter <span style="color:#f92672">in</span> enumerate(line):
    idx <span style="color:#f92672">=</span> alphabet<span style="color:#f92672">.</span>index(letter)
    letter_tensor[i][idx] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

</code></pre></div><h4 id="word-level">word-level</h4>
<p>From the view of sentences, the vocabulary is all the unique words in the corpus and each row represents each word. The implementation of one-hot encoding at word-level is similar to the above codes.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Impossible Mr Bennet impossible when I am not acquainted with him&#39;</span>

<span style="color:#75715e"># build vocabulary</span>
vocabulary <span style="color:#f92672">=</span> set(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
word2idx <span style="color:#f92672">=</span> {w: idx <span style="color:#66d9ef">for</span> idx, w <span style="color:#f92672">in</span> enumerate(vocabulary)}
len(vocabulary), word2idx[<span style="color:#e6db74">&#39;impossible&#39;</span>]
<span style="color:#75715e"># 10, {&#39;impossible&#39;: 0, &#39;not&#39;: 1, &#39;i&#39;: 2, &#39;with&#39;: 3, &#39;acquainted&#39;: 4, &#39;bennet&#39;: 5, &#39;mr&#39;: 6, &#39;him&#39;: 7, &#39;am&#39;: 8, &#39;when&#39;: 9}</span>

<span style="color:#75715e"># each row represent a word in the sentence</span>
word_vector <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(line<span style="color:#f92672">.</span>split()), len(vocabulary)))
<span style="color:#66d9ef">for</span> idx, w <span style="color:#f92672">in</span> enumerate(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split()):
    j <span style="color:#f92672">=</span> word2idx[w]
    word_vector[idx][j] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

word_vector<span style="color:#f92672">.</span>shape
<span style="color:#75715e"># (11, 10)</span>
</code></pre></div><p>The results are shown below, we can see that this sentence can be represented as a  <code>11 x 10</code> matrix.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># (|sentence|, |vocabulary|)</span>
Impossible [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]
Mr         [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]
Bennet     [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]

</code></pre></div><h4 id="pros">Pros</h4>
<ul>
<li>simple and intuitive to understand and implement</li>
</ul>
<h4 id="cons">Cons</h4>
<ul>
<li>The size of a ont-hot vector is proportional to the size of vocabulary, resulting in a sparse representation when we have a large corpus</li>
<li>The representation matrix doesn&rsquo;t have fixed size. The dimension varies in sentences or words with different lengths.</li>
<li>Too naive to capture the similarity between words</li>
<li>It cannot deal with out-of-vocabulary(OOV) problem</li>
</ul>
<h3 id="bag-of-word">Bag of Word</h3>
<p>The idea of Bag-of-word(BOW) is that all the words in the corpus are in a bag without considering the orders and context. The intuition is similar to the concepts introduced in LDA — a topic is characterized by a  small specific set of words. Therefore, BOW is commonly used to classify documents. If two documents are similar (have the same words), they are likely to be classified into the same group. Each document is represented as a vector of <code>|V|</code> dimensions, where the element value of this vector is the frequency of the word in the corresponding doc. Say we have the following corpus,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">corpus <span style="color:#f92672">=</span> [
  <span style="color:#e6db74">&#39;cat eats meat and dog eats meat&#39;</span>,
  <span style="color:#e6db74">&#39;cat eats fish&#39;</span>,
  <span style="color:#e6db74">&#39;dog eats bones&#39;</span>
]
</code></pre></div><p>The vocabulary of this small corpus and the doc-term matrix are</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">bagofWord</span>(corpus):
    vocab <span style="color:#f92672">=</span> set(<span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(corpus)<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
    word2idx <span style="color:#f92672">=</span> {word: idx <span style="color:#66d9ef">for</span> idx, word <span style="color:#f92672">in</span> enumerate(vocab)}

    doc_term <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> corpus:
        doc_v <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>]<span style="color:#f92672">*</span>len(vocab)
        <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> doc<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split():
            <span style="color:#66d9ef">if</span> word <span style="color:#f92672">in</span> vocab:
                doc_v[ word2idx[word]] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
        doc_term<span style="color:#f92672">.</span>append(doc_v)

    <span style="color:#66d9ef">print</span>(word2idx)
    <span style="color:#66d9ef">for</span> i, doc_v <span style="color:#f92672">in</span> enumerate(doc_term):
        <span style="color:#66d9ef">print</span>(corpus[i], list(doc_v))

<span style="color:#75715e">#</span>
<span style="color:#75715e"># {&#39;eats&#39;: 0, &#39;and&#39;: 1, &#39;dog&#39;: 2, &#39;fish&#39;: 3, &#39;cat&#39;: 4, &#39;meat&#39;: 5, &#39;bones&#39;: 6}</span>
<span style="color:#75715e"># cat eats meat and dog eats meat [2, 1, 1, 0, 1, 2, 0]</span>
<span style="color:#75715e"># cat eats fish [1, 0, 0, 1, 1, 0, 0]</span>
<span style="color:#75715e"># dog eats bones [1, 0, 1, 0, 0, 0, 1]</span>
</code></pre></div><h4 id="one-hot-1">one-hot</h4>
<p>Sometimes, we don&rsquo;t care about the number of occurrence of words. Just like one-hot encoding, we only want to know whether a word is present in the sentence or not, which would be useful for sentiment analysis. Well, that&rsquo;s easy to implement using Sklearn</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># occurrence</span>
vectorizer <span style="color:#f92672">=</span> CountVectorizer(binary<span style="color:#f92672">=</span>True)

X_train_dtm <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>fit_transform(X_train)
X_test_dtm <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>transform(X_test)

</code></pre></div><h4 id="pros-1">Pros</h4>
<ul>
<li>Simple and intutive to understand and implement</li>
<li>Captures the semantics of documents, if two docs have similar words, they will be close to each other in the word space</li>
<li>Fixed matrix representation no matter how long a sentence is</li>
</ul>
<h4 id="cons-1">cons</h4>
<ul>
<li>It ignores the word order(context), so <code>Cat bites man</code> and <code>Man bites cat</code> have the same representation</li>
<li>The size of the matrix is proportional to the size of vocabulary</li>
<li>It doesn&rsquo;t deal with out-of-vocabulary(OOV) problem</li>
<li>It doesn&rsquo;t capture the similarity between different words that have the same meaning, e.g <code>cat eats, cat ate</code>, BOW will treat them as different vectors though they convey the same semantics.</li>
<li>As mentioned earlier, the most frequent words are often function words like pronouns, determiners and conjuctions. However, they are of no help for classification.</li>
</ul>
<h3 id="bag-of-n-gram">Bag of N-gram</h3>
<p>Basically, n-gram is a sequence of N tokens. Generally, we treat each word as an independent unit, in this case, a word is 1-gram or unigram. Similarly, a two-word sequence of words is 2-gram (bigram), three words is 3-gram (trigram), and so on so forth. A simple implementation is shown below.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_ngrams</span>(tokens, n<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
	ngrams <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>[tokens[i:] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n)])
	<span style="color:#66d9ef">return</span> [<span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(gram) <span style="color:#66d9ef">for</span> gram <span style="color:#f92672">in</span> ngrams]
</code></pre></div><p>So what&rsquo;s the use of n-gram?</p>
<ul>
<li>estimate the proability of the last word of an n-gram given the previous words</li>
<li>assign probabilities to entire sequences</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">corpus <span style="color:#f92672">=</span> [
  <span style="color:#e6db74">&#39;Dog bites man&#39;</span>,
  <span style="color:#e6db74">&#39;Man bites dog&#39;</span>,
  <span style="color:#e6db74">&#39;Dog eats meat&#39;</span>,
  <span style="color:#e6db74">&#39;Man eats food&#39;</span>
]

count_vect <span style="color:#f92672">=</span> CountVectorizer(ngram_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
bow_rep <span style="color:#f92672">=</span> count_vect<span style="color:#f92672">.</span>fit_transform(corpus)

count_vect<span style="color:#f92672">.</span>vocabulary_
<span style="color:#75715e"># {&#39;dog&#39;: 3, &#39;bites&#39;: 0, &#39;man&#39;: 10, &#39;dog bites&#39;: 4,</span>
<span style="color:#75715e"># &#39;bites man&#39;: 2, &#39;man bites&#39;: 11, &#39;bites dog&#39;: 1, &#39;eats&#39;: 6, &#39;meat&#39;: 13,</span>
<span style="color:#75715e"># &#39;dog eats&#39;: 5, &#39;eats meat&#39;: 8, &#39;food&#39;: 9, &#39;man eats&#39;: 12, &#39;eats food&#39;: 7}</span>
</code></pre></div><p>Pros and Cons</p>
<ul>
<li>
<p>captures word order and context in a way</p>
</li>
<li>
<p>dimentionality increases as $n$ increases</p>
</li>
<li>
<p>it still have OOV problem</p>
</li>
</ul>
<h3 id="tf-idf">TF-IDF</h3>
<p>So far, we have learned that one-hot encoding focuses more on the occurrence of words in text while BOW pays more attention to word frequency. In both cases, they consider each word in the corpus euqally (with the same weight).</p>
<p>In contrast, TF-IDF allows us to measure the importance of each word relative to other words in the doc and the corpus. This is useful for <strong>information retrieval systems</strong>, where we expect that the most relevant documents should appear first.</p>
<p>How does TF-IDF work? As the name suggests, it calculates two quantities:</p>
<ul>
<li>
<p>term frequency(TF), the normalized frequency of each token $w_i$ in a given doc $d_j$</p>
<p>$$
TF(w_i, d_j) = \frac{|w_i^{d_j}|}{|d_j|}
$$</p>
<ul>
<li>The intution is that the more frequent a word appears in a doc, the more important it is. Thus, we need to increase its importance</li>
</ul>
</li>
<li>
<p>inverse document frequency(IDF), the logarithm of the inverse normalized frequency of each token across all documents</p>
<p>​
$$
IDF(w_i) = \text {log} \frac{|D|}{|w_i^D|}
$$</p>
<ul>
<li>
<p>The intuiton is fair straightforward — if a word appears across all the docs, for instance, stop words like <code>a</code>, <code>is</code>, <code>and</code>, it&rsquo;s unlikely to capture the characteristics of the doc it belong to. That is, they are more common compared to other less frequent words in the same doc. In other words, we need to reduce its importance, that&rsquo;s why we invert the calculation.</p>
</li>
<li>
<p>we use logarithm to further punish terms that appear more frequently across all the docs</p>
</li>
</ul>
</li>
</ul>
<p>Putting it together, the TF-IDF is defined as</p>
<p>$$
TF\_IDF = TF(w_i, d_j) * IDF(w_i)
$$</p>
<h2 id="distributed-representation">Distributed Representation</h2>
<p>Continuous Bag Of Words (CBOW) uses context words to predict the center word while Skip-Gram use the current word to predict its neighbouring words. The number of context words is determined by a parameter called &ldquo;window size&rdquo;.</p>
<h3 id="cbow">CBOW</h3>
<p>Basically, CBOW is a multi-class classification, which can be descibed as follows,</p>
<ul>
<li>words are encoded in one-hot format</li>
<li>each one-hot encoded vector is fed into the first layer in order to get the embedding of that word</li>
<li>combine the above real valued vectors in some way such that it captures the overall context</li>
<li>finally, the linear layer and softmax layer are used to predict probability distribution over vocabulary. The largest prob indicates the most likely target word</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CBOW</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, voc_size, embd_size):
        super(CBOW, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(voc_size, embd_size)
        self<span style="color:#f92672">.</span>fc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embd_size, voc_size)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(x)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(out)
        out_prob <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(out)
        <span style="color:#66d9ef">return</span> out_prob
</code></pre></div><p>However, there are two serious problems as one-hot matrix becomes sparse due to increasing vocabulary increases</p>
<ul>
<li>the calculation between one-hot and Embedding layer</li>
<li>the calculation between Embedding layer and the linear layer</li>
<li>the calculation of softmax layer</li>
</ul>
<h4 id="vectorization">Vectorization</h4>
<p>The first problem is easy to solve because there is no need to store the entire one-hot matrix. After all, it&rsquo;s just used to extract the embedding of the corresponding token, which corrsponds the row of the embdding matrix $E$. In other words, we can just store the row index of the words in one dimensional array.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CBOW</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#f92672">...</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">one_hot</span>(self, context):
        <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">        context: &#39;thank very much&#39;
</span><span style="color:#e6db74">        target: you
</span><span style="color:#e6db74">        &#39;&#39;&#39;</span>
        
        indices <span style="color:#f92672">=</span> [ lookup[token] <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> context<span style="color:#f92672">.</span>split() ]
        context_vec <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(len(indices))

        <span style="color:#75715e"># instead of one-hot (sparse matrix), we keep the row index of the embedding matrix</span>
        context_vec[:len(indices)] <span style="color:#f92672">=</span> indices
        context_vec[len(indices):] <span style="color:#f92672">=</span> mask <span style="color:#75715e"># in case of the number of tokens in the context is less than the window size</span>
        
        <span style="color:#66d9ef">return</span> context_vec

</code></pre></div><p>One thing we should notice is that same word could occur many times. For example, &ldquo;Like&rdquo; occurs twice  in the sentence &ldquo;Like (Sunday) Like Rain&rdquo;. When performing backpropagation, we should accumulate the gradients for word &ldquo;Like&rdquo;. This is because that we aggerate the embeddings of all the context words for each instance, and then fed it into a linear layer to make a prediction in the forward process as shown below.</p>
<p>$$
E_{Like} * 2 + E_{Rain} = E_{aggerate} \\ Out = E_{aggerate} * W_{fc}
$$
where $E_{Like}$ is the embedding of word &ldquo;Like&rdquo;.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(x)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (batch, |D|, |E|)</span>
out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(out)
</code></pre></div><h3 id="skip-gram">Skip-gram</h3>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>The idea is to transform a multi-class problem into many binary classification problems by sampling the positive example and several negative examples (typically 10 ~ 50) according the frequency distribution of the vocabulary. Frequent words tend to be sampled and rare words is likely not to be sampled, since rare words barely occur in real life and more frequent words would lead to a better generalisation.</p>
<h3 id="subsampling">Subsampling</h3>
<p>The idea of subsampling is that the vecot representation of frequent words do not change significantly, so we do not need to include them all for each training example.  For example, &ldquo;the&rdquo; is frequently used in almost every sentence, so this is no need to train &ldquo;France&rdquo; and &quot; the&quot;.  Word $w_i$ in the training set will be discarded with a higher probability computed by the formula</p>
<p>$$
P(w_i) = 1 - \sqrt {\frac{t}{f(w_i)}}
$$
where $f(w_i)$ is the frequency of word $w_i$ and $t$ is a chosen threshold (typically around $10^{-5}$). If a word appears very frequently, then $P(w_i)$ is close to $1$, which means it will not be used as the context word and target word. This technique could also be applied to discard less frequent phrases, see <a href="https://medium.datadriveninvestor.com/skip-gram-model-broken-down-subsampling-n-grams-feab04a6f220#:~:text=In%20layman%E2%80%99s%20terms%2C%20subsampling%20is%20to%20sample%20the,to%20sample%20a%20word%20has%20to%20be%20defined.">this post</a>.</p>
<h3 id="evaluation">Evaluation</h3>
<p>King - man  + woman =  Queen</p>
<h2 id="refer">Refer</h2>
<ul>
<li><a href="https://kelvinniu.com/posts/word2vec-and-negative-sampling/">https://kelvinniu.com/posts/word2vec-and-negative-sampling/</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Skip-Gram Paper</a></li>
</ul>



        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#frequency-based">Frequency-Based</a>
      <ul>
        <li><a href="#one-hot">One-hot</a></li>
        <li><a href="#bag-of-word">Bag of Word</a></li>
        <li><a href="#bag-of-n-gram">Bag of N-gram</a></li>
        <li><a href="#tf-idf">TF-IDF</a></li>
      </ul>
    </li>
    <li><a href="#distributed-representation">Distributed Representation</a>
      <ul>
        <li><a href="#cbow">CBOW</a></li>
        <li><a href="#skip-gram">Skip-gram</a></li>
        <li><a href="#negative-sampling">Negative Sampling</a></li>
        <li><a href="#subsampling">Subsampling</a></li>
        <li><a href="#evaluation">Evaluation</a></li>
      </ul>
    </li>
    <li><a href="#refer">Refer</a></li>
  </ul>
</nav>
      
      </aside>
    </div>
    
    
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=xiaopan.wpp@outlook.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/ixiaopan" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/ixiaopan" name="Linkedin">
        <em class="fab fa-linkedin"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
