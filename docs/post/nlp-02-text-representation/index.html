<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<title>NLP - Text representation</title>



  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://ixiaopan.github.io/blog/index.xml"
  title=""
/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NLP - Text representation"/>
<meta name="twitter:description" content="In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called text representation."/>




<link rel="stylesheet" href="https://ixiaopan.github.io/blog/fontawesome/css/all.min.css" />
<link
  id="dark-mode-theme"
  rel="stylesheet"
  href="https://ixiaopan.github.io/blog/css/dark.css"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')
  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script src="https://ixiaopan.github.io/blog/js/main.bundle.js"></script>
<script src="https://ixiaopan.github.io/blog/js/instantpage.min.js" type="module" defer></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

 




  <link rel="stylesheet" href="https://ixiaopan.github.io/blog/css/main.css" />


<meta name="generator" content="Hugo 0.81.0" />
  </head>
  <body>
    
  




  <header>
    <nav class="navbar">
  <div class="nav">
    
      <a href="https://ixiaopan.github.io/blog/" class="nav-logo">
        <img
          src="https://ixiaopan.github.io/blog/images/me.JPG"
          width="50"
          height="50"
          alt="Logo"
        />
      </a>
    

    <ul class="nav-links">
      
        
          <li>
            <a href="/blog/about/" id="About"
              ><em class="fas fa-user fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/search" id="Search"
              ><em class="fas fa-search fa-lg"></em
            ></a>
          </li>
          
      
        
          <li>
            <a href="/blog/archives" id="Archives"
              ><em class="fas fa-archive fa-lg"></em
            ></a>
          </li>
          
      
    </ul>
  </div>
</nav>


    
    <div class="intro-header">
      <div class="container">
        <div class="post-heading">
          
            <h1>
              NLP - Text representation
            </h1>
          
          
            <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Jul 10, 2021
  
  &nbsp;&nbsp;<i class="fas fa-clock"></i>&nbsp;5 min read

  
    &nbsp;&nbsp;&nbsp;<em class="fa fa-folder-open"></em>&nbsp;
    
      <a
        href="https://ixiaopan.github.io/blog/categories/nlp/"
        >NLP</a
      >&nbsp;
    
  
</span>

          
        </div>
      </div>
    </div>
    
  </header>

    
  <div class="container" role="main">
    <div class="inner">
      <article class="article" class="blog-post">
        
  <p>In the last post, we talked about text preprocessing techniques. However, even the data is clean now, they are still text. We still haven&rsquo;t answered the question: how to covert text into numbers? In NLP parlance, this is called <strong>text representation</strong>.</p>
<p>There are two aspects to consider: the level of representation and the meaning of numbers. We know that a sentence is composed of words and each word consists of a group of characters. This means we can represent text at sentence level, character-level, or both. As for numbers, the simplest way is to count the number of occurrences of each word. The most common methods based on this idea includes one-hot encoding, bag-of-word and TF-IDF.</p>
<h2 id="frequency-based">Frequency-Based</h2>
<h3 id="one-hot">One-hot</h3>
<p>One-hot representation indicates whether a word/character is present in a sentence/word. If true, we assign the value of 1 to that word, otherwise 0.</p>
<h4 id="character-level">character-level</h4>
<p>Figure 1 shows a simple word representation at character level. The whole vocabulary contains 26 English letters. For each word, for example, the word <code>impossible</code></p>
<ul>
<li>each row represents a character in <code>impossible</code>, so there are 10 rows</li>
<li>The corresponding value of each row is a vector whose element value is either 0 or 1, indicating whether the corresponding letter is present in the given word</li>
</ul>
<p>Thus, we will get a <code>(10, 26)</code> matrix for the word <code>impossible</code>.</p>
<p>
  <figure>
    <img src="/blog/post/images/word-vocab-matrix.png#full" alt="">
    <figcaption>Figure 1: character-vocabulary occurrence matrix with the shape of (|word|, |vocabulary|)</figcaption>
  </figure>
</p>
<p>Below are  simple codes to construct such a matrix.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Impossible Mr Bennet impossible when I am not acquainted with him&#39;</span>

line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
line, len(line)
<span style="color:#75715e"># (&#39;impossiblemrbennetimpossiblewheniamnotacquaintedwithhim&#39;, 55)</span>

<span style="color:#75715e"># each row represent a character in the sentence</span>
alphabet <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;abcdefghijklmnopqrstuvwxyz&#39;</span>
letter_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(len(line), len(alphabet)
<span style="color:#66d9ef">for</span> i, letter <span style="color:#f92672">in</span> enumerate(line):
    idx <span style="color:#f92672">=</span> alphabet<span style="color:#f92672">.</span>index(letter)
    letter_tensor[i][idx] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

</code></pre></div><h4 id="word-level">word-level</h4>
<p>From the view of sentences, the vocabulary is all the unique words in the corpus and each row represents each word. The implementation of one-hot encoding at word-level is similar to the above codes.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">line <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Impossible Mr Bennet impossible when I am not acquainted with him&#39;</span>

<span style="color:#75715e"># build vocabulary</span>
vocabulary <span style="color:#f92672">=</span> set(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split())
word2idx <span style="color:#f92672">=</span> {w: idx <span style="color:#66d9ef">for</span> idx, w <span style="color:#f92672">in</span> enumerate(vocabulary)}
len(vocabulary), word2idx[<span style="color:#e6db74">&#39;impossible&#39;</span>]
<span style="color:#75715e"># 10, {&#39;impossible&#39;: 0, &#39;not&#39;: 1, &#39;i&#39;: 2, &#39;with&#39;: 3, &#39;acquainted&#39;: 4, &#39;bennet&#39;: 5, &#39;mr&#39;: 6, &#39;him&#39;: 7, &#39;am&#39;: 8, &#39;when&#39;: 9}</span>

<span style="color:#75715e"># each row represent a word in the sentence</span>
word_vector <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(line<span style="color:#f92672">.</span>split()), len(vocabulary)))
<span style="color:#66d9ef">for</span> idx, w <span style="color:#f92672">in</span> enumerate(line<span style="color:#f92672">.</span>lower()<span style="color:#f92672">.</span>split()):
    j <span style="color:#f92672">=</span> word2idx[w]
    word_vector[idx][j] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

word_vector<span style="color:#f92672">.</span>shape
<span style="color:#75715e"># (11, 10)</span>
</code></pre></div><p>The results are shown below, we can see that this sentence can be represented as a  <code>11 x 10</code> matrix.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># (|sentence|, |vocabulary|)</span>
Impossible [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]
Mr         [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]
Bennet     [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]

</code></pre></div><h4 id="pros">Pros</h4>
<ul>
<li>simple and intuitive to understand and implement</li>
</ul>
<h4 id="cons">Cons</h4>
<ul>
<li>The size of a ont-hot vector is proportional to the size of vocabulary, resulting in a sparse representation when we have a large corpus</li>
<li>The representation matrix doesn&rsquo;t have fixed size. The dimension varies in sentences or words with different lengths.</li>
<li>Too naive to capture the similarity between words</li>
<li>It cannot deal with out-of-vocabulary(OOV) problem</li>
</ul>
<h3 id="bag-of-word">Bag-of-word</h3>
<p>The idea of Bag-of-word(BOW) is that all the words in the corpus are in a bag without considering the orders and context. The intuition is similar to the concepts introduced in LDA — a topic is characterized by a  small specific set of words. Therefore, BOW is commonly used to classify documents. If two documents are similar (have the same words), they are likely to be classified into the same group. Each document is represented as a vector of <code>|V|</code> dimensions, where the element value of this vector is the frequency of the word in the corresponding doc. Say we have the following corpus,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">corpus <span style="color:#f92672">=</span> [
  <span style="color:#e6db74">&#39;cat eats meat and dog eats meat&#39;</span>,
  <span style="color:#e6db74">&#39;cat eats fish&#39;</span>,
  <span style="color:#e6db74">&#39;dog eats bones&#39;</span>
]
</code></pre></div><p>The vocabulary of this small corpus and the doc-term matrix are</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">[<span style="color:#e6db74">&#39;and&#39;</span> <span style="color:#e6db74">&#39;bones&#39;</span> <span style="color:#e6db74">&#39;cat&#39;</span> <span style="color:#e6db74">&#39;dog&#39;</span> <span style="color:#e6db74">&#39;eats&#39;</span> <span style="color:#e6db74">&#39;fish&#39;</span> <span style="color:#e6db74">&#39;meat&#39;</span>]

cat eats meat <span style="color:#f92672">and</span> dog eats meat [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>]
cat eats fish [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>]
dog eats bones [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]


</code></pre></div><h4 id="one-hot-1">one-hot</h4>
<p>Sometimes, we don&rsquo;t care about the number of occurrence of words. Just like one-hot encoding, we only want to know whether a word is present in the sentence or not, which would be useful for sentiment analysis. Well, that&rsquo;s easy to implement using Sklearn</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># occurrence</span>
vectorizer <span style="color:#f92672">=</span> CountVectorizer(binary<span style="color:#f92672">=</span>True)

X_train_dtm <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>fit_transform(X_train)
X_test_dtm <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>transform(X_test)

</code></pre></div><h4 id="pros-1">Pros</h4>
<ul>
<li>Simple and intutive to understand and implement</li>
<li>Captures the semantics of documents, if two docs have similar words, they will be close to each other in the word space</li>
<li>Fixed matrix representation no matter how long a sentence is</li>
</ul>
<h4 id="cons-1">cons</h4>
<ul>
<li>It ignores the word order(context), so <code>Cat bites man</code> and <code>Man bites cat</code> have the same representation</li>
<li>The size of the matrix is proportional to the size of vocabulary</li>
<li>It doesn&rsquo;t deal with out-of-vocabulary(OOV) problem</li>
<li>It doesn&rsquo;t capture the similarity between different words that have the same meaning, e.g <code>cat eats, cat ate</code>, BOW will treat them as different vectors though they convey the same semantics.</li>
<li>As mentioned earlier, the most frequent words are often function words like pronouns, determiners and conjuctions. However, they are of no help for classification.</li>
</ul>
<h3 id="tf-idf">TF-IDF</h3>
<p>So far, we have learned that one-hot encoding focuses more on the occurrence of words in text while BOW pays more attention to word frequency. In both cases, they consider each word in the corpus euqally(with the same weight).</p>
<p>In contrast, TF-IDF or term frequency-inverse document frequency allows us to measure the importance of each word relative to other words in the doc and the corpus. This is useful for information retrieval systems, where we expect that the most relevant documents should appear first.</p>
<p>So how does TF-IDF work? Well, as the name suggests, it calculates two things</p>
<ul>
<li>
<p>term frequency(TF), the normalized frequency of each token $w_i$ in a given doc $d_j$</p>
<p>$$
TF(w_i, d_j) = \frac{|w_i^{d_j}|}{|d_j|}
$$</p>
<ul>
<li>The intution is that the more frequent a word appears in a doc, the more important it is. Thus, we need to increase its importance</li>
</ul>
</li>
<li>
<p>inverse document frequency(IDF), the logarithm of the inverse normalized frequency of each token across all documents</p>
<p>​
$$
IDF(w_i) = \text {log} \frac{|D|}{|w_i^D|}
$$</p>
<ul>
<li>
<p>The intuiton is fair straightforward — if a word appears across all the docs, for instance, stop words like <code>a</code>, <code>is</code>, <code>and</code>, it&rsquo;s unlikely to capture the characteristics of the doc it belong to, indicating that it&rsquo;s a more common word relative to other words in the same doc. In other words, we need to reduce its importance, that&rsquo;s why we invert the calculation.</p>
</li>
<li>
<p>we use logarithm to further punish terms that appear more frequently across all the docs</p>
</li>
</ul>
</li>
</ul>
<p>Putting it together, the TF-IDF is defined as</p>
<p>$$
TF\_IDF = TF(w_i, d_j) * IDF(w_i)
$$</p>
<h2 id="distributed-representation">Distributed Representation</h2>
<h3 id="cbow">CBOW</h3>
<h3 id="skip-gram">Skip-gram</h3>
<h3 id="pre-trained">pre-trained</h3>



        
      </article>

      
      <aside class="article-aside">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#frequency-based">Frequency-Based</a>
      <ul>
        <li><a href="#one-hot">One-hot</a></li>
        <li><a href="#bag-of-word">Bag-of-word</a></li>
        <li><a href="#tf-idf">TF-IDF</a></li>
      </ul>
    </li>
    <li><a href="#distributed-representation">Distributed Representation</a>
      <ul>
        <li><a href="#cbow">CBOW</a></li>
        <li><a href="#skip-gram">Skip-gram</a></li>
        <li><a href="#pre-trained">pre-trained</a></li>
      </ul>
    </li>
  </ul>
</nav>
      
      </aside>
    </div>
    
    
    
    
      

    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="mailto:?to=xiaopan.wpp@outlook.com" name="Email">
        <em class="fas fa-envelope"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://github.com/ixiaopan" name="GitHub">
        <em class="fab fa-github"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://www.linkedin.com/in/ixiaopan" name="Linkedin">
        <em class="fab fa-linkedin"></em>
      </a>
    
       &nbsp;&nbsp;&nbsp;
      <a href="https://space.bilibili.com/22910840" name="Bilibili">
        <em class="fab fa-youtube"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://ixiaopan.github.io/blog/about">ixiaopan</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
      Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;
      Theme
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
